3 2 0 2
t c O 2
] L C . s c [
1 v 7 8 3 1 0 . 0 1 3 2 : v i X r a
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Amanda Bertsch∗ and Alex Xie∗ and Graham Neubig and Matthew R. Gormley Carnegie Mellon University [abertsch, alexx]@cs.cmu.edu
Abstract
Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks with- out any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoret- ical justification for the performance of these methods, explaining some results that were pre- viously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.
than the consensus). MBR thus provides an alter- native to the more standard maximum-likelihood decoding; when a sample of sufficient size is taken, MBR almost uniformly outperforms beam search and single-output sampling across tasks, metrics, and datasets (see §6). It is also notable in its flexibil- ity; in §3 we organize and discuss several different design decisions that go into the use of MBR and how they affect the efficacy of the method.
While MBR is rarely applied by name in modern NLP, a number of methods with similar intuitions have gained popularity. In §4, we demonstrate that a number of generation techniques widely used with modern language models can be viewed as special instances of MBR: self-consistency (Wang et al., 2023) and its extensions, range vot- ing (Borgeaud and Emerson, 2020), output en- sembling (DeNero et al., 2010; Martínez Lorenzo et al., 2023), and some types of density estimation (Kobayashi, 2018). This view exposes connections between seemingly disparate methods and presents theoretical justifications for existing empirical re- sults using these methods. We also discuss how insights from the MBR literature can inform the use of these other MBR-like methods.
1
Introduction
“Sometimes innovation is only old ideas reappearing in new guises . . . [b]ut the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.”
(Jones, 1994)
Minimum Bayes Risk (MBR) decoding (Bickel and Doksum (1977); §2) is a decoding method fol- lowing a simple intuition: when choosing a best output from a set of candidates, the desirable output should be both 1) high probability and 2) relatively consistent with the rest of the outputs (i.e., outputs that are not consistent with the other outputs are high risk– they may be dramatically better or worse
With the framing of MBR, the theoretical jus- tification for the empirical performance of sev- eral methods becomes clear; the extension of self- consistency to open-ended generations becomes trivial; and several promising modifications to self- consistency and output ensembling are exposed. In particular, modern MBR-like methods often do not apply the insights from research on MBR, suggest- ing that these methods could be further improved. In §5, we show that some design choices, though seemingly intuitive to a practitioner accustomed to search-based decoding methods, should be avoided when applying MBR.
2 Formalization
∗Denotes equal contribution.
We begin with the basics of decoding and MBR.


2.1 Standard decoding
Decoding from an autoregressive model (such as a transformer decoder) is performed tokenwise. The distribution at each decoding step is conditioned on the prior tokens and the input text:
p(yi|y<i, x)
The model is locally normalized; the probabilities of next tokens sum to 1. The probability of a se- quence under this global model distribution is
p(y|x) =
T (cid:89)
p(yi|y<i, x)
i=1
Given this distribution, there are several ways of extracting an output: by sampling at each de- coding step from the distribution over next tokens (often with some modification to the distribution, e.g. temperature, nucleus, or epsilon sampling; Holtzman et al. (2019)); by always choosing the most probable next token (i.e. greedy decoding); or by performing a search over some subset of the output space, guided by the distribution (e.g. beam search, best-first search). These methods generally return a single output; if multiple output candidates are present, the one with the maximum likelihood under the model distribution is returned.
2.2 Minimum Bayes Risk decoding
The traditional formulation of MBR is as a mini- mization objective. Given a output space Y and a probability distribution over this space p(y|x), we compute the risk R(y′) of a candidate decoding y′ as the expected error (also called loss) under this distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008). The MBR de- coding is then the y′ within Y that minimizes risk:
ˆy = argmin
R(y′)
y′∈Y
= argmin y′∈Y
Ey|x[L(y, y′)]
= argmin y′∈Y
(cid:88)
y∈Y
L(y, y′)p(y|x)
We can trivially rewrite the risk as a maximiza- tion of gain (also called utility) rather than a mini- mization of error, where G(y, y′) = −L(y, y′).
Approximating risk Computing this sum over the space of all possible outputs Y is intractable
(1)
(2)
(3)
(4)
(5)
for most models.1 In these cases, we approximate the risk R(y′) by using a subset of the full space Y ⊂ Y ; that is, instead of exact computation of the expectation, we approximate it with a sum over independent samples from p(y|x). Generally, this is performed by sampling repeatedly from a model (or several models) and estimating the prob- ability of each individual output as proportional to the relative frequency that the output occurs.2 For an unbiased sampling method3 (e.g. ancestral sampling), as the number of outputs drawn goes to infinity, this recovers the model’s true distribution of probability over sequences. Thus, we approxi- mate risk using this sample:
R(y′) ≈
= −
1 |Y|
1 |Y|
(cid:88)
y∈Y (cid:88)
y∈Y
L(y, y′)
G(y, y′)
Thus, given a sample (which may include dupli- cates) Y and a function (e.g. a metric) that com- pares two sequences G : Y × Y → R, we approx- imate the true MBR decoding rule as:
ˆy = argmax
y′∈Y
1 |Y|
(cid:88)
y∈Ye
G(y, y′)
Separation of evidence and hypothesis sets In many cases, the same subset of the output space is used for both the risk estimate and the candidate outputs. However, when the sample is substantially smaller than the full output space, it is often benefi- cial to use separate sets (Eikema and Aziz, 2022; Yan et al., 2023). Following prior work (§2.2), we refer to these as the evidence set (Ye) and hypothe- sis set (Yh).
This separation is beneficial because there are distinct and potentially contradictory desiderata for the two sets. We wish for our evidence set to cover a large, representative portion of the search space to obtain a more accurate estimate of risk. However, we want our hypothesis set to only cover the narrower, high-quality region of the space, as we do not want to consider candidate hypotheses that are low-quality. Applying the separation of evidence and hypothesis sets yields the equation for MBR over two subsets of the output space:
1This is the case for many deep generative models, such as a transformer language model and other autoregressive models without conditional independence assumptions. 2This is called a Monte Carlo approximation. 3We discuss the use of biased samplers in §3.2 and §3.1.
(6)
(7)
(8)


ˆy = argmax
(cid:88)
G(y, y′)
y′∈Yh
y∈Ye
3 Taxonomy of MBR
Equation 9 demonstrates four major axes along which an MBR method may vary:
1. Choice of hypothesis set Yh
2. Choice of evidence set Ye 3. Choice of gain (or error) function G(y, y′)
4. Choice of evidence distribution p(y|x)
In this section, we examine how these four factors affect the efficacy of MBR and give recommenda- tions for each; in Section 4, we discuss how these apply to other MBR-like methods.
3.1 Sampling a hypothesis set
Several recent works show benefits from improv- ing the quality of the hypothesis space. Fernandes et al. (2022) apply a two-stage approach where they first apply an N -best (referenceless) reranker and then do MBR over only the most highly ranked hypotheses, which they also use as the evidence set. Eikema and Aziz (2022) introduce a method, Coarse-to-Fine MBR, that first uses MBR with a cheap-to-compute metric to filter a large hypothe- sis space to a smaller set, then uses MBR with a better but more expensive to compute metric over the smaller set; they separate evidence and hypoth- esis sets. (Freitag et al., 2023) further investigates sampling strategies for MBR, finding that epsilon sampling (Hewitt et al., 2022) outperforms other strategies in automated and human evaluations.
Another earlier line of work has considered growing post hoc the hypothesis set in order to obtain hypotheses with higher expected gain (González-Rubio et al., 2011; González-Rubio and Casacuberta, 2013; Hoang et al., 2021).
3.2 Sampling an evidence set
Comparatively less work has studied strategies for sampling the evidence set. Most recent work has adopted the unbiased sampling strategy of (Eikema and Aziz, 2020), i.e. drawing i.i.d. samples from the model distribution p(y|x) (equation 2). This strategy is motivated by their observation that unbi- ased sampling is reasonably reflective of the data distribution, much more so than beam search. How- ever, their approach is incompatible with models
(9)
trained via label smoothing (Szegedy et al., 2016). (Yan et al., 2023) attempt to remedy this by sam- pling the evidence set with temperature τ < 1, sharpening the model distribution.
3.3 What metric do we want to maximize?
The gain G (alternatively, error L) may be an arbi- trary function Ye × Yh → R. Early work focused on simple, token-level metrics like word error rate and BLEU (Kumar and Byrne, 2004; Ehling et al., 2007), but more recent work has explored the use of neural metrics (Amrhein and Sennrich, 2022; Freitag et al., 2022), as well as executing outputs in code generation (Shi et al., 2022; Li et al., 2022).
Generally, for both neural and non-neural met- rics, MBR with metric G as a gain function will yield the largest downstream improvements on G (Müller and Sennrich, 2021; Freitag et al., 2022; Fernandes et al., 2022). In other words, if one aims to optimize system performance on metric M , one should perform MBR with M as gain.
However, MBR also inherits the weaknesses and biases of the gain metric used. MBR has been shown to suffer from length and token fre- quency biases brought on by the metric, i.e. MBR with BLEU prefers shorter sentences (Nakov et al., 2012; Müller and Sennrich, 2021). Similarly, (Am- rhein and Sennrich, 2022) find that MBR over COMET causes higher rates of errors for named entities and numbers due to a lack of sensitivity in the metric. Moreover, MBR is susceptible to overfitting to the metric; (Freitag et al., 2023) show that the MBR setting that maximizes the metric is not the one that humans prefer.
Note that in the most trivial case, where the met- ric is G(y, y′) = 1[y = y′], MBR recovers mode- seeking methods like beam search– i.e. MBR un- der this metric, in expectation, yields the maximum likelihood decoding.
3.4 What probability distribution should we
use to estimate risk?
Most MBR decoding methods use the model’s score distribution over outputs, s, as the (unnor- malized) evidence distribution. Alternately, this distribution may be normalized by a temperature (during minimum risk training (Smith and Eisner, 2006) or decoding (Yan et al., 2023)). Some work (Suzgun et al., 2023) interprets this as a weak proxy for the human or true distribution, arguing that the true objective is to minimize error under the human


Method
Evidence Gen.
Hypothesis Gen.
Metric
p(y|x)
Lattice MBR (Tromble et al., 2008) Coarse-to-fine MBR (Eikema and Aziz, 2022) Wiher et al. (2022) MBR-DC (Yan et al., 2023) Ours (§ 3.3) Ours (§ 3.4) (Freitag et al., 2023) Crowd sampling2 (Suzgun et al., 2023) MBR-Exec (Shi et al., 2022)
N-best list ancestral sampling ancestral sampling temperature sampling1 ancestral sampling ancestral sampling
BLEU N-best list filter(sample) BEER evidence + more decodings BEER temperature sampling1 temperature sampling temperature sampling
BLEURT BERTScore BERTScore BLEURT neural score metric execution match
epsilon sampling temperature sampling temperature sampling
translation lattice single model single model single model single model length-corrected scores single model single model single model
Self-consistency (SC) (Wang et al., 2023) Complex SC (Fu et al., 2022) SC for open-ended gen (Jain et al., 2023) Range voting (Borgeaud and Emerson, 2020) Post-Ensemble (Kobayashi, 2018) AMRs Assemble! (Martínez Lorenzo et al., 2023) model set
temperature sampling filter(temperature sample) temperature sampling beam search beam search for each model in ensemble
beam search
exact answer match exact answer match n-gram overlap n-gram overlap cosine similarity perplexity
single model single model single model single model model set model set
Table 1: Recent work under our taxonomy. The line separates methods that are explicitly MBR (above) from those that we identify as MBR-like (below). 1 Different temperatures used for evidence and hypothesis. 2 While Suzgun et al. (2023) coin the new term crowd sampling, they also explicitly refer to their method as MBR.
distribution:
argmin y′∈Yh
Ey∼phuman[L(y, y′)]
the reasoning chain, or by defining a gain function G(y, y′) = 1(ans(y) = ans(y′)) over full outputs O; though notationally different, they are mathe- matically equivalent.
Note that this is not the only reasonable choice of p(y|x); other possible distributions include a dis- tribution over outputs from multiple models (§4.2) or the length-penalized distribution over a single model’s outputs pl(y|x) (§5.3).
Thus, self-consistency is a type of MBR decod- ing in which we approximate the risk with a Monte Carlo estimate (cf. Eq. 6), the answers are sampled from the model (conditioned on the prompt), and the metric is exact match of the “final answer.”
4 MBR as a frame for other methods
Self-consistency, range voting, output ensembling, and density estimation can all be viewed through the framing of MBR. This exposes unstated con- nections between the methods and provides some theoretical backing to the empirical success of these methods. We discuss each in turn.
4.1 Self-consistency as MBR
Self-consistency (Wang et al., 2023) is a method for choosing outputs from language models. In self- consistency, the model is prompted to generate an explanation and then an answer. Multiple outputs O = {y1, . . . , ym} are sampled from the model, the answers A = {a1, . . . , am} are extracted ai = ans(yi), and the most frequent answer is returned:
This framing additionally explains some re- sults from the self-consistency paper. Wang et al. (2023) compare the performance of self- consistency across sampling strategies, finding that the best of the strategies they tried are those that are closest to ancestral sampling (nucleus sampling with p = 0.95 and τ = 0.7 without top-k sam- pling). They also find that self-consistency works better with a sampled output rather than outputs from beam search (their Table 6). Through the lens of MBR, this empirical result has a clear theoret- ical justification: ancestral sampling of evidence sets generally yields the best performance for MBR because this provides an unbiased estimator of the probabilities of the sampled sequences. This also presents an opportunity for improvement: while Wang et al. (2023) do not evaluate on ancestral sampling, it is possible that this would outperform their best results.
argmax a
m (cid:88)
i=1
1(ai = a)
Self-consistency only computes exact match over the answer, not the reasoning chain. It is possible to recover MBR from this method by ei- ther taking the hypothesis/evidence sets to be the set of resulting answers Yh = Ye = A discarding
(10)
Self-consistency is a special case of MBR. Pro- posed extensions to self-consistency have recov- ered aspects of generalized MBR decoding, includ- ing filtering to smaller hypothesis/evidence sets (Fu et al., 2022) and the use of alternative gain metrics (Jain et al., 2023). As a result, the term self-consistency has widened in definition from a


specific type of MBR to a catch-all for MBR-based decoding methods on large language models.
4.2 Output Ensembling as MBR
Model ensembling techniques that operate on com- pleted outputs of models may also be cast in MBR terms. Note that this does not include methods that operate on model weights or partial outputs. Common ensembling methods such as averaging model weights (Izmailov et al., 2018) or averag- ing token-level probabilities (Sennrich et al., 2016; Manakul et al., 2023) cannot be explicitly formu- lated as MBR.
The connection to MBR is most straightforward in methods that perform MBR decoding over the outputs of multiple models (DeNero et al., 2010; Duh et al., 2011; Barzdins and Gosko, 2016; Lee et al., 2022, inter alia). Representative of this fam- ily of methods is Post-Ensemble (Kobayashi, 2018), which ensembles multiple text generation models θ1, θ2, . . . , θn by separately decoding from each model, computing pairwise sentence embedding similarity between all pairs of outputs, and yielding the output with greatest average similarity. Observe that this may be framed as MBR minimizing the expected risk over the mixture distribution
pensemble(y|x) =
 

pθ1(y|x) with probability π1 · · · pθn(y|x) with probability πn
where (cid:80)n i=1 πi = 1. While πi is usually taken to be uniform over the ensemble, this need not always be the case (Duan et al., 2010).
Other methods may be viewed as relaxations of MBR decoding. Assemble! (Martínez Lorenzo et al., 2023) ensembles Abstract Meaning Repre- sentation (AMR) graph parsers by computing the pairwise perplexities of each output under each parser. While this is not precisely MBR, it may be viewed as a variation where the evidence set is a set of models, not a set of model outputs.
ˆy = argmin
y′∈Yh
Eθ∼π(·)[L(θ, y′)]
In this case, the error L(θ, y′) is the perplexity of y′ under model θ, i.e. exp(− log pθ(y′)) = 1 pθ(y′) , and π(·) is the distribution over models.
4.3 MBR as Density Estimation
Interestingly, Post-Ensemble (Kobayashi, 2018) (§4.2) was not formulated as MBR (and in fact
never referred to by name as MBR), but rather as kernel density estimation. Kernel density estima- tion is a non-parametric method for estimating the probability density function p of an unknown distri- bution, given samples (x1, x2, · · · , xn) from that distribution (Rosenblatt, 1956; Parzen, 1962).
ˆp(x) =
1 n
n (cid:88)
i=1
K(x, xi)
Indeed, Equation 11 very closely resembles the Monte Carlo estimator of expected loss in Equa- tion 6. This connection allowed (Kobayashi, 2018) to propose approximation error bounds on MBR, drawing from the density estimation literature.4
Note that the kernel function K(x, xi) is more commonly written as K(x − xi), or K(xT xi) for directional statistics. While this may seem limiting, we can rewrite commonly used MBR metrics in this form; we show this for ROUGE-n as an example. For a sequence y, define Tn(y) to be a vector of size |V |n, where |V | is the size of the vocabulary, containing the number of times every possible n- gram appears in y. Then we can rewrite ROUGE-n as the following:
= 1 −
KR(Tn(y) − Tn(y′)) |Tn(y) − Tn(y′)|1 |Tn(y)|1 + |Tn(y′)|1
where | · |1 is the L1 norm.
The similarity between density estimation and MBR yields an alternative interpretation of MBR as a mode-seeking search. However, we are not seeking the mode of the model’s distribution over outputs, p(y|x), but rather that of a distribution over some features ϕ(y) of our output, p′(ϕ(y)|x). For instance, in the case of ROUGE-n MBR,
ˆy = argmax
(cid:88)
KR(Tn(y′) − Tn(y))
y′∈Yh
≈ argmax
y∈Ye p′(Tn(y′)|x)
y′∈Yh
We posit alternative distribution p′(Tn(y′)|x) may be better correlated with performance on specific downstream metrics than the original model distribution, potentially adding an additional justification for MBR’s effectiveness. We hope this may inspire future work investigating the theoretical underpinnings of MBR.
that
this
4We do not reproduce their bounds here; we direct inter-
ested readers to the original paper.
(11)
(12)
(13)
(14)


4.4 Range Voting as MBR
Methods that take inspiration from outside of NLP may also be MBR-like; in particular, some MBR- like algorithms in the literature are formulated from a voting theory perspective where candidate hy- potheses are assigned votes based on similarity to some set of voters (Wang et al., 2023; Jain et al., 2023; Suzgun et al., 2023; Hoang et al., 2021). We show here that range voting (Borgeaud and Emerson, 2020), which broadly encapsulates these proposed voting methods, reduces to MBR.
Range voting describes a family of voting sys- tems in which each voter assigns each candidate a score and the candidate with the greatest total or average score is elected. Observe that the set of candidates C corresponds to the hypothesis set Yh and the set of voters V corresponds to the evidence set Ye. Then, if voter v’s score for candidate c is taken to be a gain G(v, c) and each voter is as- signed uniform weight, range voting is equivalent to the MBR decision rule in Equation 8:
celected = argmax
c∈C
1 |V |
(cid:88)
v∈V
G(v, c)
(15)
Other range-voting methods can similarly be cast
as MBR variants.
5 Design Decisions Impact MBR
Performance
Although all the methods in Section 4 are MBR- like, they make very different decisions about the four design choices in our MBR taxonomy. To demonstrate the importance of the method design, we consider empirically two cases where changing design impacts the performance of the method.
5.1 Experimental Details
We run MBR experiments for abstractive sum- marization on CNN/DM (Nallapati et al., 2016) with a fine-tuned BART-Large5 released by the BART authors (Lewis et al., 2020) as our base model. In §5.3, we additionally report results for translation on WMT’16 Romanian-English (Ro- En) (Bojar et al., 2016) using mBART-50 (Liu et al., 2020).6 We draw ne ancestral samples for our evi- dence set and nt temperature samples (τ = 0.5 for CNN/DM, τ = 0.3 for WMT’16 Ro-En) for our hypothesis set. We set ne = nt = 30 in §5.2 and
5facebook/bart-large-cnn on HuggingFace
(Wolf et al., 2020)
6facebook/mbart-large-50-many-to-many-mmt
Method
R1
R2
RL
BS
Greedy BS (k = 5) BS (k = 10) DBS (k = g = 5)
43.98 43.16 42.62 43.77
20.88 20.63 20.23 20.85
30.88 30.53 30.02 30.77
88.04 87.82 87.71 87.97
MBR ROUGE-1 MBR BEER MBR BERTSCORE
46.89 46.31 46.04
22.29 22.36 22.09
32.01 32.02 32.09
88.41 88.38 88.68
Table 2: MBR results on CNN/DM for various gain functions. We additionally test the same non-MBR, (approximate) mode- seeking baselines as Wiher et al. (2022). All MBR methods outperform all non-MBR methods tested.
ne = nt = 50 in §5.3. Unless otherwise specified, we take ROUGE-1 (Lin, 2004) as our gain metric for summarization and BLEU-4 (Papineni et al., 2002)7 as our gain metric for translation.
5.2 The MBR metric matters – but perhaps not as much as the hypothesis set
We find that using MBR with the summariza- tion n-gram metric ROUGE-1 (Lin, 2004) im- proves abstractive summarization performance over beam search on CNN/DM, even when evalu- ating performance with neural metrics; using the general-purpose neural metric BERTScore (Zhang et al., 2020) as the MBR metric yields highest BERTScore but smaller gains on non-neural met- rics, a finding consistent with past work; and even BEER (Stanojevi´c and Sima’an, 2014), a transla- tion metric, works as an MBR metric for this task.
However, prior work using the same dataset and model (Wiher et al., 2022) found that BEER (Stanojevi´c and Sima’an, 2014) underperforms beam search. This divergence in results is likely due to our different choices in hypothesis set – Wi- her et al. (2022) use the evidence set plus additional outputs from other decoding methods as hypothe- ses, while we use temperature samples at τ = 0.5. While reusing the evidence set is more efficient than sampling a separate set of hypotheses, it leads to performance degregation in this case; this fur- ther emphasizes the importance of choosing the hypothesis set in MBR.
7We sacrebleu nrefs:1|case:mixed|eff:yes|tok:13a| smooth:exp|version:2.3.1
use
the
implementation
(Post,
2018)
with
from signature


5.3 Varying the risk distribution: lessons from beam search don’t translate to MBR
By nature, autoregressive text generation models suffer from length bias: sequence probability mono- tonically decreases with increasing length, caus- ing shorter, potentially less informative sequences to be favored by the model distribution (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019). For non-sampling methods such as beam search, the sequence probabilities are generally modified with a length-dependent term when comparing se- quences (Murray and Chiang, 2018; Cho et al., 2014). Hence, it stands to reason that a length- corrected distribution with these biases alleviated may provide a better estimate of the risk R(y′).
Vanilla Monte Carlo MBR (as depicted in Equa- tion 6) yields an estimate of the expected risk un- der the distribution that our evidence samples are drawn from. To modify the distribution used in our estimate, we turn to importance sampling, a method for estimating the expected value of a quan- tity under target distribution p, given samples from proposal distribution q (Kloek and van Dijk, 1978). For a brief tutorial on importance sampling and description of our estimator, see Appendix A.
We take the score of a sequence to be the log probability: We then experiment with two of the strategies described in (Murray and Chiang, 2018) for constructing the length corrected score sl(y|x): (a) Length normalization: The model distribu- tion is smoothed with temperature T β, where T is the sequence length and β is the length penalty, a hyperparameter. A larger β more heavily prioritizes longer sequences.
sl(y|x) = s(y|x)/T β
(b) Length reward (He et al., 2016): A fixed reward γ is added to the score per token gen- erated.
sl(y|x) = s(y|x) + γT
The length-corrected distribution is then pl(y|x) ∝ exp sl(y|x). We apply normalized importance sampling (Rubinstein and Kroese, 2016) to esti- mate the risk under the length corrected distribu- tion, i.e. R(y′) = Ey∼pl[L(y, y′)], given samples drawn from the model distribution p(y|x).
We compare our MBR results against beam search both with and without length normaliza- tion. We use the models’ default values for length penalty (β = 2 for BART, β = 1 for mBART).
(16)
(17)
Method
R1
R2
RL
BS
Beam search, no correction Beam search
43.88 43.95
20.96 21.00
30.77 30.84
87.79 87.81
MBR, No correction MBR, Length norm, β = 0.5 MBR, Length norm, β = 1.0 MBR, Length reward, γ = 0.5 MBR, Length reward, γ = 1.0
47.70 44.29 44.29 47.60 47.41
23.00 19.95 19.98 22.93 22.72
32.54 29.99 30.0 32.48 32.25
88.50 88.03 88.03 88.48 88.43
Table 3: MBR results for various length correction schemes on CNN/DM. We report ROUGE-1, ROUGE-2, ROUGE-L, BERTSCORE, and length ratio, respectively.
Method
BLEU
chrF BLEURT
BS
Beam search, no correction Beam search
33.21 33.06
59.81 60.05
65.50 65.60
94.95 94.96
MBR, No correction MBR, Length norm, β = 0.5 MBR, Length norm, β = 1.0 MBR, Length reward, γ = 0.5 MBR, Length reward, γ = 1.0
33.56 31.14 31.09 32.09 31.29
60.00 58.53 58.51 59.63 59.17
65.53 64.70 64.68 65.19 64.91
94.96 94.71 94.71 94.82 94.73
Table 4: MBR results for various length correction schemes on WMT’16 Romanian-English. We report BLEU, chrF, BLEURT, BERTSCORE, and length ratio, respectively. We use the chrF (Popovi´c, 2015) implementation from sacrebleu. We use the smaller BLEURT-20-D6 checkpoint for effi- ciency (Sellam et al., 2020; Pu et al., 2021).
Our results are Tables 3 and 4. In line with past work, we find that beam search generally bene- fits from incorporating a length penalty. However, we find that length-corrected MBR underperforms vanilla MBR. This may be due to a gap between the sampling and length-correction distibutions, lead- ing to a high-variance estimator of risk.
However, our results are also emblematic of a wider trend among minimum-risk techniques. Past work has found that models trained with Minimum Error Rate Training (Och, 2003; Shen et al., 2016), an error-aware training method, do not require length correction in beam search (Neubig, 2016). Similarly, we find that MBR without length cor- rection generates outputs relatively close in length to the references, more so than length-normalized beam search. This suggests that MBR may be to some extent immune from length biases, when they are not introduced by the MBR metric (Müller and Sennrich, 2021).
6 MBR applications in NLP
The use of minimum Bayes risk decoding in NLP predates these MBR-like methods; MBR has been applied by name in NLP since the 1990s.
Historical context Minimum Bayes Risk decod- ing has roots in Bayesian decision theory, a field of study that dates as far back as the Age of En-
LR
108.00 114.39
111.64 110.75 110.77 112.52 112.50
LR
99.37 101.58
100.04 102.82 102.60 105.00 105.63


lightenment (Bernoulli, 1738; Parmigiani, 2001). Central to Bayesian decision theory is the principle of risk minimization: in the face of uncertainty, an optimal decision maker should choose the option that minimizes the amount of error they can expect to suffer – or, in other terms, maximizes the amount of utility they can expect to enjoy (DeGroot, 1970; Bickel and Doksum, 1977). This is precisely the intuition encoded in MBR (i.e. Equation 3).
Adoption in NLP MBR was adopted by the speech and NLP communities in the 1990s and early 2000s, finding applications in syntactical pars- ing (Goodman, 1996; Sima’an, 2003), automatic speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000), and statistical machine translation (Kumar and Byrne, 2004; Tromble et al., 2008; Ku- mar et al., 2009). Many NLP tasks during this time relied upon graph structures as inductive biases (i.e. parse trees or translation lattices/hypergraphs). As such, early MBR works often used these graphical models as hypothesis and evidence spaces. Work on lattice MBR (Tromble et al., 2008), for instance, treated the set of all hypotheses encoded in a word lattice, of which there are exponentially many, as both evidence and hypothesis sets. This is in con- trast to most later MBR work, which operates on a relatively small list of text outputs obtained from a neural model. As a result, early work relied on rather involved dynamic programming algorithms for exact MBR decoding and were restricted to token-factorizable metrics such as BLEU and edit distance. Later work additionally demonstrated the efficacy of MBR for question answering (Duan, 2013) and for joining statistical and neural ap- proaches to translation (Stahlberg et al., 2017).
Recent usage to move past In an effort beam search, which has well-known pathologies (Stahlberg and Byrne, 2019), MBR has in re- cent years resurfaced as a decision rule for text- generation models (Eikema and Aziz, 2020). As discussed earlier in §3, several lines of work have sprung up investigating the properties of MBR in modern neural text generation setups. Notably, however, most of these works have focused on ap- plications of the method to neural machine transla- tion, with only a few very recent works studying its applications in other text generation tasks (Shi et al., 2022; Wiher et al., 2022; Suzgun et al., 2023). Outside of these areas, the method has largely been applied in shared task papers (e.g. Manakul
Mentions of methods by year
2005
mbr decoding
0.005
0.010
2010
bayes risk decoding
minimum bayes risk
2020Year
0.025Percentage of papers that mention
mbr
2000
0.000
2015
0.020
0.015
Figure 1: The use of MBR (by name) peaked in the mid-2010s. This graph shows the percentage of ACL Anthology papers that mention several MBR-related phrases by year, from 2000 to 2022.
et al. (2023); Yan et al. (2022); Barzdins and Gosko (2016)), as it provides a reliable boost in perfor- mance. The fraction of papers in the ACL Anthol- ogy that reference MBR (at least by this name) has declined from its peak around 2009 (Figure 1).
7 Conclusion
Minimum Bayes Risk decoding has declined in popularity, but the underlying concept of sam- pling a set from a distribution and choosing an output to minimize risk according to that set has remained. This concept now takes many surface forms– from self-consistency to range voting to output ensembles– and current research in these areas rarely draws connections to MBR. While re- discovery is a key part of science, so is recontex- tualizing new methods within a broader research narrative. This can often reveal new insights or cast findings in a different light. For instance, the empirical benefits of self-consistency can be justi- fied through an MBR framing; work on extensions to self-consistency has rediscovered other proper- ties of MBR; and work on ensembling has raised questions about how to weight mixtures of models that can be reasoned about within the framework of noisy estimates of global probability distributions. The adoption of newer terms for MBR-like meth- ods may be a type of terminology drift. Related phenomena have been studied in the philosophy of science literature, including pressures to coin new terms (Dyke, 1992; Merton, 1957), potential negative consequences of divergent terminology (Calvert, 1956; Samigullina et al., 2020), and de- creased citation of older methods in NLP (Singh


et al., 2023). For a more involved discussion of the literature on term coining and possible connections, see Appendix B.
Language is not static, so some degree of ter- minology drift in scientific literature is unavoid- able. However, recognizing the connections be- tween modern techniques and older work is crucial to understanding why such methods are effective. We must not forget the lessons of the past as we search for the methods of the future.
Acknowledgments
We would like to thank Jason Eisner and Patrick Fernandes for useful early discussions about this work, and Saujas Vaduguru, Daniel Fried, and Shuyan Zhou for feedback on this draft.
This work was supported in part by grants from the Singapore Defence Science and Technology the Air Force Re- Agency, 3M — M*Modal, search Laboratory (AFRL), and the National Sci- ence Foundation Graduate Research Fellowship Program under Grant No. DGE2140739. Any opinions, findings, and conclusions or recommen- dations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.
References
Chantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for In Proceedings of the 2nd Conference COMET. of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125–1141, Online only. Association for Computational Linguistics.
Guntis Barzdins and Didzis Gosko. 2016. RIGA at SemEval-2016 task 8: Impact of Smatch extensions and character-level neural translation on AMR pars- ing accuracy. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 1143–1147, San Diego, California. As- sociation for Computational Linguistics.
Daniel Bernoulli. 1738. Specimen theoriae novae de mensura sortis. Commentarii academiae scientiarum imperialis Petropolitanae, 5:175–192.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe- matical Statistic: Basic Ideas and Selected Topics. Holden-Day Inc., Oakland, CA.
Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo- gacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Pa- pers, pages 131–198, Berlin, Germany. Association for Computational Linguistics.
Marcel Bollmann and Desmond Elliott. 2020. On for- getting to cite older papers: An analysis of the ACL Anthology. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7819–7827, Online. Association for Computa- tional Linguistics.
Sebastian Borgeaud and Guy Emerson. 2020. Lever- aging sentence similarity in natural language gener- ation: Improving beam search using range voting. In Proceedings of the Fourth Workshop on Neural Generation and Translation, pages 97–109, Online. Association for Computational Linguistics.
E. S. Calvert. 1956. Technical terms in science and technology. The American Journal of Psychology, 69(3):476–479.
Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111, Doha, Qatar. Associa- tion for Computational Linguistics.
Morris H. DeGroot. 1970. Optimal Statistical Decisions.
McGraw-Hill, Inc., New York.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 975–983, Los Angeles, California. Association for Computational Linguistics.
Nan Duan. 2013. Minimum Bayes risk based answer re-ranking for question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 424–428, Sofia, Bulgaria. Association for Com- putational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture model-based minimum Bayes risk decoding using multiple machine translation systems. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313–321, Beijing, China. Coling 2010 Organizing Committee.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada, and Masaaki Nagata. 2011. Generalized


minimum Bayes risk system combination. In Pro- ceedings of 5th International Joint Conference on Natural Language Processing, pages 1356–1360, Chiang Mai, Thailand. Asian Federation of Natural Language Processing.
Carolynn Van Dyke. 1992. Old words for new worlds: Modern scientific and technological word-formation. American Speech, 67(4):383–405.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Minimum Bayes risk decoding for BLEU. In Pro- ceedings of the 45th Annual Meeting of the Associa- tion for Computational Linguistics Companion Vol- ume Proceedings of the Demo and Poster Sessions, pages 101–104, Prague, Czech Republic. Association for Computational Linguistics.
Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 4506–4520, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.
Bryan Eikema and Wilker Aziz. 2022. Sampling-based approximations to minimum Bayes risk decoding for neural machine translation. In Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing, pages 10978–10993, Abu Dhabi, United Arab Emirates. Association for Com- putational Linguistics.
Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 1396–1412, Seattle, United States. Association for Computational Linguistics.
Markus Freitag, Behrooz Ghorbani, and Patrick Fer- nandes. 2023. Epsilon sampling rocks: Investigating sampling strategies for minimum bayes risk decoding for machine translation.
Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022. High quality rather than high model probability: Minimum Bayes risk decoding with neu- ral metrics. Transactions of the Association for Com- putational Linguistics, 10:811–825.
Yao Fu, Hao-Chun Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompt- ing for multi-step reasoning. ArXiv, abs/2210.00720.
Vaibhava Goel and William J Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech & Language, 14(2):115–135.
Jesús González-Rubio and Francisco Casacuberta. 2013. Improving the minimum Bayes’ risk combination of machine translation systems. In Proceedings of the 10th International Workshop on Spoken Language Translation: Papers, Heidelberg, Germany.
Jesús González-Rubio, Alfons Juan, and Francisco Casacuberta. 2011. Minimum Bayes-risk system In Proceedings of the 49th Annual combination. Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 1268–1277, Portland, Oregon, USA. Association for Computational Linguistics.
Joshua Goodman. 1996. Efficient algorithms for pars- In Conference on Empirical
ing the DOP model. Methods in Natural Language Processing.
Wei He, Zhongjun He, Hua Wu, and Haifeng Wang. 2016. Improved neural machine translation with smt features. In Proceedings of the Thirtieth AAAI Con- ference on Artificial Intelligence, AAAI’16, page 151–157. AAAI Press.
John Hewitt, Christopher Manning, and Percy Liang. 2022. Truncation sampling as language model desmoothing. In Findings of the Association for Com- putational Linguistics: EMNLP 2022, pages 3414– 3427, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Thanh Lam Hoang, Gabriele Picco, Yufang Hou, Young- Suk Lee, Lam Nguyen, Dzung Phan, Vanessa Lopez, and Ramon Fernandez Astudillo. 2021. Ensembling graph predictions for amr parsing. In Advances in Neural Information Processing Systems, volume 34, pages 8495–8505. Curran Associates, Inc.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. International Conference on Learning Representations.
Anna Kristina Hultgren. 2013. Lexical borrowing from english into danish in the sciences: An empirical investigation of ‘domain loss’. International Journal of Applied Linguistics, 23(2):166–182.
Pavel Izmailov, Dmitrii Podoprikhin, T. Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and bet- ter generalization. In Conference on Uncertainty in Artificial Intelligence.
Siddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. 2023. Self-consistency for open-ended gener- ations.
Karen Sparck Jones. 1994. Natural Language Pro- cessing: A Historical Review, pages 3–16. Springer Netherlands, Dordrecht.
T. Kloek and H. K. van Dijk. 1978. Bayesian estimates of equation system parameters: An application of integration by monte carlo. Econometrica, 46(1):1– 19.
Hayato Kobayashi. 2018. Frustratingly easy model en- semble for abstractive summarization. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4165–4176, Brussels, Belgium. Association for Computational Linguistics.


Philipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for Computa- tional Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine transla- tion. In Proceedings of the Human Language Tech- nology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169–176, Boston, Mas- sachusetts, USA. Association for Computational Lin- guistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate train- ing and minimum Bayes-risk decoding for transla- tion hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171, Suntec, Singapore. Association for Compu- tational Linguistics.
Young-Suk Lee, Ramón Astudillo, Hoang Thanh Lam, Tahira Naseem, Radu Florian, and Salim Roukos. 2022. Maximum Bayes Smatch ensemble distilla- tion for AMR parsing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5379–5392, Seattle, United States. Association for Computational Lin- guistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computa- tional Linguistics.
Yujia Li, David Choi, Junyoung Chung, Nate Kush- man, Julian Schrittwieser, Rémi Leblond, Tom Ec- cles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas- son d’Autume, Igor Babuschkin, Xinyun Chen, Po- Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alpha- code. Science, 378(6624):1092–1097.
Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pre- training for neural machine translation. Transac- tions of the Association for Computational Linguis- tics, 8:726–742.
Potsawee Manakul, Yassir Fathullah, Adian Liusie, Vyas Raina, Vatsal Raina, and Mark Gales. 2023. CUED at ProbSum 2023: Hierarchical ensemble of summarization models. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 516–523, Toronto, Canada. Association for Computational Linguistics.
Abelardo Carlos Martínez Lorenzo, Pere Lluís Huguet Cabot, and Roberto Navigli. 2023. AMRs assemble! learning to ensemble with autoregressive In Proceedings of the models for AMR parsing. 61st Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers), pages 1595–1605, Toronto, Canada. Association for Com- putational Linguistics.
Robert K. Merton. 1957. Priorities in scientific discov- ery: A chapter in the sociology of science. American Sociological Review, 22(6):635–659.
Saif M. Mohammad. 2020. Gender gap in natural lan- guage processing research: Disparities in authorship In Proceedings of the 58th Annual and citations. Meeting of the Association for Computational Lin- guistics, pages 7860–7870, Online. Association for Computational Linguistics.
Mathias Müller and Rico Sennrich. 2021. Understand- ing the properties of minimum Bayes risk decoding in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 259–272, Online. Asso- ciation for Computational Linguistics.
Kenton Murray and David Chiang. 2018. Correcting length bias in neural machine translation. In Proceed- ings of the Third Conference on Machine Translation: Research Papers, pages 212–223, Brussels, Belgium. Association for Computational Linguistics.
Preslav Nakov, Francisco Guzman, and Stephan Vo- gel. 2012. Optimizing for sentence-level BLEU+1 In Proceedings of COL- yields short translations. ING 2012, pages 1979–1994, Mumbai, India. The COLING 2012 Organizing Committee.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence In Proceedings of the 20th RNNs and beyond. SIGNLL Conference on Computational Natural Lan- guage Learning, pages 280–290, Berlin, Germany. Association for Computational Linguistics.
Graham Neubig. 2016. Lexicons and minimum risk training for neural machine translation: NAIST- CMU at WAT2016. In Proceedings of the 3rd Work-


shop on Asian Translation (WAT2016), pages 119– 125, Osaka, Japan. The COLING 2016 Organizing Committee.
Franz Josef Och. 2003. Minimum error rate training In Proceedings in statistical machine translation. of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.
G. Parmigiani. 2001. Decision theory: Bayesian. In Neil J. Smelser and Paul B. Baltes, editors, Inter- national Encyclopedia of the Social & Behavioral Sciences, pages 3327–3334. Pergamon, Oxford.
Emanuel Parzen. 1962. On Estimation of a Probability Density Function and Mode. The Annals of Mathe- matical Statistics, 33(3):1065 – 1076.
Maja Popovi´c. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.
Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computa- tional Linguistics.
Amy Pu, Hyung Won Chung, Ankur Parikh, Sebastian Gehrmann, and Thibault Sellam. 2021. Learning compact metrics for MT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pages 751–762, Online and Punta Cana, Dominican Republic. Association for Compu- tational Linguistics.
B. L. Raad. 1989. Modern trends in scientific terminol- ogy: Morphology and metaphor. American Speech, 64(2):128–136.
Murray Rosenblatt. 1956. Remarks on Some Nonpara- metric Estimates of a Density Function. The Annals of Mathematical Statistics, 27(3):832 – 837.
Reuven Y. Rubinstein and Dirk P. Kroese. 2016. Simula- tion and the Monte Carlo Method, 3rd edition. Wiley Publishing.
Mukund Rungta, Janvijay Singh, Saif M. Mohammad, and Diyi Yang. 2022. Geographic citation gaps in NLP research. In Proceedings of the 2022 Confer- ence on Empirical Methods in Natural Language Pro- cessing, pages 1371–1383, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
L.Z. Samigullina, E.F. Samigullina, O.V. Danilova, and I.A. Latypova. 2020. Linguistic borrowing as a way to enrich oil and gas terminology. In Proceedings of the International Session on Factors of Regional Extensive Development (FRED 2019), pages 58–61. Atlantis Press.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text genera- tion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86–96, Berlin, Germany. Association for Computational Lin- guistics.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 1683–1692, Berlin, Germany. Associ- ation for Computational Linguistics.
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. 2022. Natural lan- guage to code translation with execution. In Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533–3546, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Khalil Sima’an. 2003. On maximizing metrics for syn- tactic disambiguation. In Proceedings of the Eighth International Conference on Parsing Technologies, pages 183–194, Nancy, France.
Janvijay Singh, Mukund Rungta, Diyi Yang, and Saif Mohammad. 2023. Forgotten knowledge: Examin- ing the citational amnesia in NLP. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6192–6208, Toronto, Canada. Association for Computational Linguistics.
David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Pro- ceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794, Sydney, Australia. Association for Computational Linguistics.
Felix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3356– 3362, Hong Kong, China. Association for Computa- tional Linguistics.


Felix Stahlberg, Adrià de Gispert, Eva Hasler, and Bill Byrne. 2017. Neural machine translation by min- imising the Bayes-risk with respect to syntactic trans- lation lattices. In Proceedings of the 15th Confer- ence of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 362–368, Valencia, Spain. Association for Computational Linguistics.
Miloš Stanojevi´c and Khalil Sima’an. 2014. Fitting sentence level translation evaluation with many dense features. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 202–206, Doha, Qatar. Association for Computational Linguistics.
Andreas Stolcke, Yochai Konig, and Mitchel Weintraub. 1997. Explicit word error minimization in n-best list rescoring.
Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2023. Follow the wisdom of the crowd: Effective text generation via minimum Bayes risk decoding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4265–4293, Toronto, Canada. Association for Computational Linguistics.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
Roy Tromble, Shankar Kumar, Franz Och, and Wolf- gang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Pro- ceedings of the 2008 Conference on Empirical Meth- ods in Natural Language Processing, pages 620–629, Honolulu, Hawaii. Association for Computational Linguistics.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.
Gian Wiher, Clara Meister, and Ryan Cotterell. 2022. On decoding strategies for neural text generators. Transactions of the Association for Computational Linguistics, 10:997–1012.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.
Brian Yan, Patrick Fernandes, Siddharth Dalmia, Jia- tong Shi, Yifan Peng, Dan Berrebbi, Xinyi Wang, Graham Neubig, and Shinji Watanabe. 2022. CMU’s IWSLT 2022 dialect speech translation system. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 298–307, Dublin, Ireland (in-person and online). As- sociation for Computational Linguistics.
Jianhao Yan, Jin Xu, Fandong Meng, Jie Zhou, and Yue Zhang. 2023. Dc-mbr: Distributional cooling for minimum bayesian risk decoding.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with bert.


A More details on importance sampling
for MBR
We present in this section the normalized impor- tance sampling estimator of risk used in our experi- ments in §5.3.
The core insight of importance sampling is that we can rewrite the expected value of a random variable f (x) under target distribution p as another expectation under some proposal distribution q:
Ep[f (x)] =
(cid:88)
f (x)p(x)
=
x (cid:88)
x
f (x)
p(x) q(x)
q(x)
= Eq
(cid:20)
f (x)
p(x) q(x)
(cid:21)
Importance sampling can be particularly useful when sampling from the proposal distribution is easy, but sampling from the target distribution is costly or intractable; this is indeed the case for MBR, as sampling from the length-corrected distri- bution pl(y|x) requires computation of its partition function, which has exponential complexity.
Hence, for MBR, if we draw evidence samples Ye according to model distribution p(y|x) but wish to compute the risk under some length-corrected distribution pl(y|x), we may compute
R(y′) = Ey∼pl[L(y, y′)]
= Ey∼p
=
(cid:88)
(cid:20)
pl(y|x) p(y|x) pl(y|x) p(y|x)
L(y, y′)
L(y, y′)
(cid:21)
=
y∈Ye (cid:88)
L(y, y′)w(y)
y∈Ye
where we let w(y) = pl(y|x)/p(y|x), commonly referred to as the importance weight.
Note, however, that importance sampling re- quires us to be able to exactly compute the prob- abilities p(y|x) and pl(y|x); while the former can be computed efficiently (Equation 2), the latter is intractable, again because it requires the partition function. What we can efficiently compute is the unnormalized probability ˜pl(y|x) = exp sl(y|x), where sl is the length-corrected score given by ei- ther Equation 16 or 17.
Fortunately, we can use normalized importance sampling to obtain a consistent estimator of the
risk by adjusting importance weights (Rubinstein and Kroese, 2016):
R(y′) = Ey∼pl[L(y, y′)]
=
Ey∼p[L(y, y′) ˜w(y)] Ey∼p[ ˜w(y)]
=
(cid:88)
y∈Ye
L(y, y′) ·
(cid:80)
˜w(y)
y∈Ye
˜w(y)
where ˜w(y) = ˜pl(y|x)/p(y|x). As it is the ratio of two estimates, the normalized importance sampling estimator is biased for finite sample sizes.
(18)
(19)
(20)


B Contextualizing this work within
philosophy of science
In this section, we contextualize our work in the broader framings of meta-analysis of scientific re- search.
Patterns of citation in NLP Several factors have been shown to correlate with citation rate in NLP, including author geographic location (Rungta et al., 2022), author gender (Mohammad, 2020), and pub- lication date (Bollmann and Elliott, 2020; Singh et al., 2023). Bollmann and Elliott (2020) con- duct a bibliometric anaylsis of the ACL Anthology, finding that the mean age of papers cited decreased significantly from 2010 to 2019. Singh et al. (2023) expand this analysis to the full anthology, finding that, while citations of older papers rose briefly in the mid-2010s, it has since declined, with 2021 marking a historic low for the percentage of cita- tions that went to older papers8. They term this citational amnesia and discuss several possible rea- sons for the result, including the shift to neural methods and the rise of new areas of NLP.
Our work raises another potential explanation: some citational amnesia is due to terminology drift over time, as old methods begin to be referred to by newer names.
Term coining in science Work in science and technology studies has examined the broader phe- nomenon of term coining in science. Dyke (1992) argues that neologisms emerge more frequently in fields that prize novelty and see science as funda- mentally about leaps of discovery, and fields that are perceived as synthesizing findings from mul- tiple fields are most likely to recycle terms from other disciplines. She cites computer science as an example of a field where most new terms of art emerge from recycling common words, often those that draw a metaphor to some basic physical or human concept; this is reflected in the adop- tion of the humanizing “self-consistency” and the political-science-inspired “range voting” in decod- ing. Raad (1989) suggests that evocative, metaphor- laden names are more likely to emerge as a scien- tific field grows more public-facing and in times where many new terms are being coined; both of these descriptors apply to modern NLP. While sev- eral works in linguistics and STS have considered
8They define an “older paper” as one that is more than 10
years older than the paper that is citing it.
the coining of new terms for new phenomena, rela- tively little work has focused on the divergence of terminology for previously observed phenomena. The consequences of divergent or distinct termi- nology have also been studied, with differences in terminology across fields blamed for slow adap- tation of research to practical applications (e.g. in studying visual distortions during plane take- off (Calvert, 1956)). Borrowing terminology from another language (often Latin or Greek) or from an- other field has been described as a method to build common ground between researchers (Samigullina et al., 2020) and as a possibly concerning pressure against developing language-specific scientific ter- minology in lower-resourced languages (Hultgren, 2013). However, most work on lexical divides in science has focused on divides across language or field rather than divides across time in the same field.