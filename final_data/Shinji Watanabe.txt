Shinji Watanabe6722222597
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', '2023', ['iScience'], '', ['Sho Miyamoto', 'Y. Kuroda', 'T. Kanno', 'A. Ueno', 'N. Shiwa-Sudo', 'N. Iwata-Yoshikawa', 'Yusuke Sakai', 'N. Nagata', 'T. Arashiro', 'A. Ainai', 'Saya Moriyama', 'N. Kishida', 'Shinji Watanabe', 'K. Nojima', 'Y. Seki', 'T. Mizukami', 'H. Hasegawa', 'H. Ebihara', 'S. Fukushi', 'Yoshimasa Takahashi', 'Maeda Ken', 'Tadaki Suzuki']]
['Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit  Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \\%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \\%$ of XLS-R’s performance with only $3 \\%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.', ['William Chen', 'Jiatong Shi', 'Brian Yan', 'Dan Berrebbi', 'Wangyou Zhang', 'Yifan Peng', 'Xuankai Chang', 'Soumi Maiti', 'Shinji Watanabe']]
['Tensor decomposition for minimization of E2E SLU model toward on-device processing', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.', ['Yosuke Kashiwagi', 'Siddhant Arora', 'Hayato Futami', 'Jessica Huynh', 'Shih-Lun Wu', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']]
['Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation', '2023', ['arXiv.org', 'ArXiv'], 'Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.', ['E. Tsunoo', 'Hayato Futami', 'Yosuke Kashiwagi', 'Siddhant Arora', 'Shinji Watanabe']]
['ML-SUPERB: Multilingual Speech Universal PERformance Benchmark', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.', ['Jiatong Shi', 'Dan Berrebbi', 'William Chen', 'Ho-Lam Chung', 'En-Pei Hu', 'Wei Huang', 'Xuankai Chang', 'Shang-Wen Li', 'Abdel-rahman Mohamed', 'Hung-yi Lee', 'Shinji Watanabe']]
['Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Prashant Sridhar', 'Shinji Watanabe']]
['Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper', ['Puyuan Peng', 'Brian Yan', 'Shinji Watanabe', 'David F. Harwath']]
['A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.', ['Yifan Peng', 'Kwangyoun Kim', 'Felix Wu', 'Brian Yan', 'Siddhant Arora', 'William Chen', 'Jiyang Tang', 'Suwon Shon', 'Prashant Sridhar', 'Shinji Watanabe']]
['Efficient Sequence Transduction by Jointly Predicting Tokens and Durations', '2023', ['International Conference on Machine Learning', 'ICML', 'Int Conf Mach Learn'], 'This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.', ['Hainan Xu', 'Fei Jia', 'Somshubra Majumdar', 'Hengguan Huang', 'Shinji Watanabe', 'Boris Ginsburg']]
['Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.', ['Takatomo Kano', 'A. Ogawa', 'Marc Delcroix', 'Roshan Sharma', 'Kohei Matsuura', 'Shinji Watanabe']]
['UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures', '2023', ['arXiv.org', 'ArXiv'], 'In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\textbf{u}$nsupervised $\\textbf{n}$eural $\\textbf{s}$peech $\\textbf{s}$eparation by leveraging $\\textbf{o}$ver-determined training mixtu$\\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.', ['Zhong-Qiu Wang', 'Shinji Watanabe']]
['Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit  Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.', ['Masao Someki', 'N. Eng', 'Yosuke Higuchi', 'Shinji Watanabe']]
['Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study', '2023', ['arXiv.org', 'ArXiv'], '.', ['Massa Baali', 'Tomoki Hayashi', 'Hamdy Mubarak', 'Soumi Maiti', 'Shinji Watanabe', 'W. El-Hajj', 'Ahmed Ali']]
['BASS: Block-wise Adaptation for Speech Summarization', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.', ['Roshan Sharma', 'Kenneth Zheng', 'Siddhant Arora', 'Shinji Watanabe', 'Rita Singh', 'B. Raj']]
['I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.', ['Yifan Peng', 'Jaesong Lee', 'Shinji Watanabe']]
['A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech', '2023', ['AAAI Conference on Artificial Intelligence', 'National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.', ['Li-Wei Chen', 'Shinji Watanabe', 'Alexander I. Rudnicky']]
['A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.', ['Jiyang Tang', 'William Chen', 'Xuankai Chang', 'Shinji Watanabe', 'B. MacWhinney']]
['Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses', '2023', ['Viruses'], 'The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.', ['E. Takashita', 'S. Murakami', 'Y. Matsuzaki', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'Misa Katayama', 'K. Mizuta', 'H. Nishimura', 'Shinji Watanabe', 'T. Horimoto', 'H. Hasegawa']]
['Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.', ['Muqiao Yang', 'Joseph Konan', 'David Bick', 'YUNYANG ZENG', 'Shuo Han', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']]
['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Spéc Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']]
['Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', '2023', ['IEEE Workshop on Applications of Signal Processing to Audio and Acoustics', 'IEEE Workshop Appl Signal Process Audio Acoust', 'Workshop on Applications of Signal Processing to Audio and Acoustics', 'Workshop Appl Signal Process Audio Acoust', 'WASPAA'], 'Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).', ['Yoshiki Masuyama', 'Xuankai Chang', 'Wangyou Zhang', 'Samuele Cornell', 'Zhongqiu Wang', 'Nobutaka Ono', 'Y. Qian', 'Shinji Watanabe']]
['Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech', '2023', ['arXiv.org', 'ArXiv'], 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.', ['Chien-yu Huang', 'Ke-Han Lu', 'Shi Wang', 'Chi-Yuan Hsiao', 'Chun-Yi Kuan', 'Haibin Wu', 'Siddhant Arora', 'Kai-Wei Chang', 'Jiatong Shi', 'Yifan Peng', 'Roshan Sharma', 'Shinji Watanabe', 'Bhiksha Ramakrishnan', 'Shady Shehata', 'Hung-yi Lee']]
['Speaker-Independent Acoustic-to-Articulatory Speech Inversion', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.', ['Peter Wu', 'Li-Wei Chen', 'Cheol Jun Cho', 'Shinji Watanabe', 'L. Goldstein', 'A. Black', 'G. Anumanchipalli']]
['Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens', '2023', ['arXiv.org', 'ArXiv'], 'In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.', ['Minsu Kim', 'J. Choi', 'Soumi Maiti', 'Jeong Hun Yeo', 'Shinji Watanabe', 'Y. Ro']]
['Enhancing Speech-To-Speech Translation with Multiple TTS Targets', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.', ['Jiatong Shi', 'Yun Tang', 'Ann Lee', 'H. Inaguma', 'Changhan Wang', 'J. Pino', 'Shinji Watanabe']]
['AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head', '2023', ['arXiv.org', 'ArXiv'], 'Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.', ['Rongjie Huang', 'Mingze Li', 'Dongchao Yang', 'Jiatong Shi', 'Xuankai Chang', 'Zhenhui Ye', 'Yuning Wu', 'Zhiqing Hong', 'Jia-Bin Huang', 'Jinglin Liu', 'Yixiang Ren', 'Zhou Zhao', 'Shinji Watanabe']]
['Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation', '2023', ['arXiv.org', 'ArXiv'], 'Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.', ['Shih-Lun Wu', 'Xuankai Chang', 'G. Wichern', 'Jee-weon Jung', 'Franccois G. Germain', 'Jonathan Le Roux', 'Shinji Watanabe']]
['Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks', '2023', ['arXiv.org', 'ArXiv'], 'We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.', ['Soumi Maiti', 'Yifan Peng', 'Shukjae Choi', 'Jee-weon Jung', 'Xuankai Chang', 'Shinji Watanabe']]
['Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining', '2023', ['International Joint Conference on Artificial Intelligence', 'Int Jt Conf Artif Intell', 'IJCAI'], 'While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.', ['Takaaki Saeki', 'Soumi Maiti', 'Xinjian Li', 'Shinji Watanabe', 'Shinnosuke Takamichi', 'H. Saruwatari']]
['FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.', ['Zhongqiu Wang', 'Samuele Cornell', 'Shukjae Choi', 'Younglo Lee', 'Byeonghak Kim', 'Shinji Watanabe']]
['Improving Massively Multilingual ASR with Auxiliary CTC Objectives', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.', ['William Chen', 'Brian Yan', 'Jiatong Shi', 'Yifan Peng', 'Soumi Maiti', 'Shinji Watanabe']]
['Toward Universal Speech Enhancement For Diverse Input Conditions', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit  Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.', ['Wangyou Zhang', 'Kohei Saijo', 'Zhong-Qiu Wang', 'Shinji Watanabe', 'Yanmin Qian']]
['The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.', ['Hayato Futami', 'Jessica Huynh', 'Siddhant Arora', 'Shih-Lun Wu', 'Yosuke Kashiwagi', 'Yifan Peng', 'Brian Yan', 'E. Tsunoo', 'Shinji Watanabe']]
['A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.', ['Siddhant Arora', 'Hayato Futami', 'Shih-Lun Wu', 'Jessica Huynh', 'Yifan Peng', 'Yosuke Kashiwagi', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']]
['A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023', '2023', ['', ''], 'A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.', ['E. Takashita', 'Seiichiro Fujisaki', 'H. Morita', 'Shiho Nagata', 'H. Miura', 'Yuki Matsuura', 'Saya Yamamoto', 'Shoko Chiba', 'Yumiko Inoue', 'Iori Minami', 'Sayaka Yoshikawa', 'Seiko Yamazaki', 'N. Kishida', 'Kazuya Nakamura', 'Masayuki Shirakura', 'Shinji Watanabe', 'Hideki Hasegawa']]
['Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', '2023', ['arXiv.org', 'ArXiv'], 'Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.', ['Xuankai Chang', 'Brian Yan', 'Kwanghee Choi', 'Jee-weon Jung', 'Yichen Lu', 'Soumi Maiti', 'Roshan Sharma', 'Jiatong Shi', 'Jinchuan Tian', 'Shinji Watanabe', 'Yuya Fujita', 'Takashi Maekaku', 'Pengcheng Guo', 'Yao-Fei Cheng', 'Pavel Denisov', 'Kohei Saijo', 'Hsiu-Hsuan Wang']]
['Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1', ['Siddhant Arora', 'Hayato Futami', 'E. Tsunoo', 'Brian Yan', 'Shinji Watanabe']]
['Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization', '2023', ['arXiv.org', 'ArXiv'], 'Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.', ['A. Hussein', 'Brian Yan', 'Antonios Anastasopoulos', 'Shinji Watanabe', 'S. Khudanpur']]
['Challenges of Corporate Alliance CLOMA toward Plastic Litter', '2023', ['Oleoscience'], '', ['Shinji Watanabe']]
['The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction', '2023', ['arXiv.org', 'ArXiv'], 'Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.', ['Shilong Wu', 'Chenxi Wang', 'Hang Chen', 'Yusheng Dai', 'Chenyue Zhang', 'Ruoyu Wang', 'Hongbo Lan', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Zhong-Qiu Wang', 'Jia Pan', 'Jianqing Gao']]
['Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data', '2023', ['Automatic Speech Recognition & Understanding', 'IEEE Automatic Speech Recognition and Understanding Workshop', 'Autom Speech Recognit  Underst', 'ASRU', 'IEEE Autom Speech Recognit Underst Workshop'], 'Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet', ['Yifan Peng', 'Jinchuan Tian', 'Brian Yan', 'Dan Berrebbi', 'Xuankai Chang', 'Xinjian Li', 'Jiatong Shi', 'Siddhant Arora', 'William Chen', 'Roshan Sharma', 'Wangyou Zhang', 'Yui Sudo', 'Muhammad Shakeel', 'Jee-weon Jung', 'Soumi Maiti', 'Shinji Watanabe']]
['The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios', '2023', ['', ''], 'The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).', ['Samuele Cornell', 'Matthew Wiesner', 'Shinji Watanabe', 'Desh Raj', 'Xuankai Chang', 'Paola García', 'Yoshiki Masuyama', 'Zhong-Qiu Wang', 'S. Squartini', 'S. Khudanpur']]
['Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing', '2023', ['arXiv.org', 'ArXiv'], 'Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.', ['Brian Yan', 'Xuankai Chang', 'Antonios Anastasopoulos', 'Yuya Fujita', 'Shinji Watanabe']]
['ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit', '2023', ['Annual Meeting of the Association for Computational Linguistics', 'Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.', ['Brian Yan', 'Jiatong Shi', 'Yun Tang', 'H. Inaguma', 'Yifan Peng', 'Siddharth Dalmia', "Peter Pol'ak", 'Patrick Fernandes', 'Dan Berrebbi', 'Tomoki Hayashi', 'Xiaohui Zhang', 'Zhaoheng Ni', 'Moto Hira', 'Soumi Maiti', 'J. Pino', 'Shinji Watanabe']]
['Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', '2023', ['arXiv.org', 'ArXiv'], 'This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.', ['Samuele Cornell', 'Zhongqiu Wang', 'Yoshiki Masuyama', 'Shinji Watanabe', 'Manuel Pariente', 'Nobutaka Ono']]
['Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.', ['Takashi Maekaku', 'Yuya Fujita', 'Xuankai Chang', 'Shinji Watanabe']]
['TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.', ['YUNYANG ZENG', 'Joseph Konan', 'Shuo Han', 'David Bick', 'Muqiao Yang', 'Anurag Kumar', 'Shinji Watanabe', 'B. Raj']]
['End-to-End Speech Recognition: A Survey', '2023', ['IEEE/ACM Transactions on Audio Speech and Language Processing', 'IEEE/ACM Trans Audio Speech Lang Process'], 'In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.', ['Rohit Prabhavalkar', 'Takaaki Hori', 'Tara N. Sainath', 'R. Schluter', 'Shinji Watanabe']]
['The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve "who spoken when" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing "who spoken what when" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.', ['Zhe Wang', 'Shilong Wu', 'Hang Chen', 'Maokui He', 'Jun Du', 'Chin-Hui Lee', 'Jingdong Chen', 'Shinji Watanabe', 'Sabato Marco Siniscalchi', 'O. Scharenborg', 'Diyuan Liu', 'Baocai Yin', 'Jia Pan', 'Jianqing Gao', 'Cong Liu']]
['DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.', ['Yifan Peng', 'Yui Sudo', 'Muhammad Shakeel', 'Shinji Watanabe']]
['An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study', '2023', ['Frontiers in Immunology', 'Front Immunol'], 'Introduction External Quality Assessment (EQA) schemes are designed to provide a snapshot of laboratory proficiency, identifying issues and providing feedback to improve laboratory performance and inter-laboratory agreement in testing. Currently there are no international EQA schemes for seasonal influenza serology testing. Here we present a feasibility study for conducting an EQA scheme for influenza serology methods. Methods We invited participant laboratories from industry, contract research organizations (CROs), academia and public health institutions who regularly conduct hemagglutination inhibition (HAI) and microneutralization (MN) assays and have an interest in serology standardization. In total 16 laboratories returned data including 19 data sets for HAI assays and 9 data sets for MN assays. Results Within run analysis demonstrated good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays. Between run analysis showed laboratory and strain specific issues, particularly with B strains for HAI, whilst MN testing was consistently good across labs and strains. Inter-laboratory variability was higher for MN assays than HAI, however both assays showed a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization. Discussion This study has received positive feedback from participants, highlighting the benefit such an EQA scheme would have on improving laboratory performance, reducing inter laboratory variation and raising awareness of both harmonized protocol use and the benefit of biological standards for seasonal influenza serology testing.', ['J. Waldock', 'C. Weiss', 'Wei Wang', 'M. Levine', 'Stacie N. Jefferson', 'S. Ho', 'K. Hoschler', 'B. Londt', 'E. Masat', 'Louise A. Carolan', 'Stephany Sánchez-Ovando', 'A. Fox', 'Shinji Watanabe', 'Miki Akimoto', 'Aya Sato', 'N. Kishida', 'A. Buys', 'Lorens Maake', 'Cardia Fourie', 'Catherine Caillet', 'Sandrine Raynaud', 'R. Webby', 'J. Debeauchamp', 'R. Cox', 'Sarah Lartey', 'C. Trombetta', 'S. Marchi', 'E. Montomoli', 'I. Sanz-Muñoz', 'J. Eiros', 'Javier Sánchez-Martínez', 'D. Duijsings', 'O. Engelhardt']]
['Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge', '2023', ['IEEE International Conference on Acoustics, Speech, and Signal Processing', 'Int Conf Acoust Speech Signal Process', 'IEEE Int Conf Acoust Speech Signal Process', 'ICASSP', 'International Conference on Acoustics, Speech, and Signal Processing'], 'In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.', ['Samuele Cornell', 'Zhongqiu Wang', 'Yoshiki Masuyama', 'Shinji Watanabe', 'Manuel Pariente', 'Nobutaka Ono', 'S. Squartini']]
['AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models', '2023', ['arXiv.org', 'ArXiv'], 'Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.', ['Yuan Tseng', 'Layne Berry', 'Yi-Ting Chen', 'I-Hsiang Chiu', 'Hsuan-Hao Lin', 'Max Liu', 'Puyuan Peng', 'Yi-Jen Shih', 'Hung-Yu Wang', 'Haibin Wu', 'Po-Yao Huang', 'Chun-Mao Lai', 'Shang-Wen Li', 'David F. Harwath', 'Yu Tsao', 'Shinji Watanabe', 'Abdel-rahman Mohamed', 'Chi-Luen Feng', 'Hung-yi Lee']]
['Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', '2023', ['arXiv.org', 'ArXiv'], 'Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.', ['Yu-Kuan Fu', 'Liang-Hsuan Tseng', 'Jiatong Shi', 'Chen-An Li', 'Tsung-Yuan Hsu', 'Shinji Watanabe', 'Hung-yi Lee']]
['Deep Speech Synthesis from MRI-Based Articulatory Representations', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.', ['Peter Wu', 'Tingle Li', 'Yijingxiu Lu', 'Yubin Zhang', 'Jiachen Lian', 'A. Black', 'L. Goldstein', 'Shinji Watanabe', 'G. Anumanchipalli']]
['FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', '2023', ['International Workshop on Spoken Language Translation', 'IWSLT', 'Int Workshop Spok Lang Transl'], 'This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.', ['Sweta Agrawal', 'Antonios Anastasopoulos', 'L. Bentivogli', 'Ondrej Bojar', 'Claudia Borg', 'Marine Carpuat', 'Roldano Cattoni', 'Mauro Cettolo', 'Mingda Chen', 'William Chen', 'K. Choukri', 'Alexandra Chronopoulou', 'Anna Currey', 'T. Declerck', 'Qianqian Dong', 'Kevin Duh', 'Y. Estève', 'Marcello Federico', 'Souhir Gahbiche', 'B. Haddow', 'B. Hsu', 'Phu Mon Htut', 'H. Inaguma', 'Dávid Javorský', 'J. Judge', 'Yasumasa Kano', 'Tom Ko', 'Rishu Kumar', 'Peng Li', 'Xutai Ma', 'Prashant Mathur', 'E. Matusov', 'Paul McNamee', 'John P. McCrae', 'Kenton Murray', 'Maria Nadejde', 'Satoshi Nakamura', 'Matteo Negri', 'H. Nguyen', 'J. Niehues', 'Xing Niu', 'Atul Kr. Ojha', 'John E. Ortega', 'Proyag Pal', 'J. Pino', 'Lonneke van der Plas', 'Peter Polák', 'Elijah Matthew Rippeth', 'Elizabeth Salesky', 'Jiatong Shi', 'Matthias Sperber', 'Sebastian Stüker', 'Katsuhito Sudoh', 'Yun Tang', 'Brian Thompson', 'Ke M. Tran', 'M. Turchi', 'A. Waibel', 'Mingxuan Wang', 'Shinji Watanabe', 'Rodolfo Zevallos']]
['Speech collage: code-switched audio generation by collaging monolingual corpora', '2023', ['arXiv.org', 'ArXiv'], "Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.", ['A. Hussein', 'Dorsa Zeinali', 'Ondrej Klejch', 'Matthew Wiesner', 'Brian Yan', 'Shammur A. Chowdhury', 'Ahmed Ali', 'Shinji Watanabe', 'S. Khudanpur']]
['Exploration on HuBERT with Multiple Resolutions', '2023', ['Interspeech', 'Conf Int Speech Commun Assoc', 'INTERSPEECH', 'Conference of the International Speech Communication Association'], 'Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.', ['Jiatong Shi', 'Yun Tang', 'H. Inaguma', 'Hongyu Gong', 'J. Pino', 'Shinji Watanabe']]
