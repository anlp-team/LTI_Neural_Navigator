C. Rosé5411882441
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions', '2023', ['Journal of Educational Media', 'J Educ Media', 'Learning, Media and Technology', 'Learn Media Technol'], 'ABSTRACT It’s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students’ data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students’ processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.', ['Shiyan Jiang', 'Hengtao Tang', 'Can Tatar', 'C. Rosé', 'J. Chao']]
['Linguistic representations for fewer-shot relation extraction across domains', '2023', ['Annual Meeting of the Association for Computational Linguistics', 'Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.', ['Sireesh Gururaja', 'Ritam Dutt', 'Ting-gen Liao', 'C. Rosé']]
['Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning', '2023', ['Annual Meeting of the Association for Computational Linguistics', 'Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.', ['Armineh Nourbakhsh', 'Sameena Shah', 'C. Rosé']]
['Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models', '2023', ['Workshop on Innovative Use of NLP for Building Educational Applications', 'Workshop Innov Use NLP Build Educ Appl', 'UNLPBEA', 'BEA'], 'By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models’ decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.', ['James Fiacco', 'David Adamson', 'C. Rosé']]
['Exploring Artificial Intelligence in English Language Arts with StoryQ', '2023', ['AAAI Conference on Artificial Intelligence', 'National Conference on Artificial Intelligence', 'National Conf Artif Intell', 'AAAI Conf Artif Intell', 'AAAI'], 'Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.', ['J. Chao', 'Rebecca Ellis', 'Shiyan Jiang', 'C. Rosé', 'W. Finzer', 'Can Tatar', 'James Fiacco', 'Kenia Wiedemann']]
