3 2 0 2
y a M 6
] L C . s c [
2 v 0 2 6 1 0 . 5 0 3 2 : v i X r a
A STUDY ON THE INTEGRATION OF PIPELINE AND E2E SLU SYSTEMS FOR SPOKEN SEMANTIC PARSING TOWARD STOP QUALITY CHALLENGE
Siddhant Arora1, Hayato Futami2, Shih-Lun Wu1,Jessica Huynh1, Yifan Peng1,Yosuke Kashiwagi2, Emiru Tsunoo2, Brian Yan1, Shinji Watanabe1
1Carnegie Mellon University, 2Sony Group Corporation, Japan
ABSTRACT
Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken se- mantic parsing system for the quality track (Track 1) in Spo- ken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We ex- periment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.
Index Terms— STOP Challenge, spoken language un-
derstanding, end-to-end systems
1. INTRODUCTION
Spoken Language Understanding Grand Challenge or Spo- ken Task Oriented Parsing (STOP) Challenge, which is part of ICASSP Signal Processing Grand Challenge 2023, aims to build systems that can convert a spoken utterance to a se- mantic parse sequence to facilitate the execution of tasks by the voice assistant. This work discusses our team PittOsaki’s approach for Track 1 to improve the quality of generated se- mantic parse using open source model and ASR datasets.
is a semantic parse represented as a linearized tree structure. The attention-based encoder-decoder architecture is adopted in our end-to-end (E2E) SLU approaches. Our ASR model is also based on encoder-decoder architecture. For both ASR and SLU training, we employ SSL representations like WavLM as a frontend. A weighted sum of multiple hidden states is utilized and the parameters are frozen during train- ing. We also experiment with utilizing the recently released Whisper model in our SLU framework. The entire Whisper model is ﬁne-tuned instead of it being used as a frontend since it achieved superior performance in our initial experi- mentations. Our NLU model is incorporated by ﬁne-tuning pre-trained LMs. Similar to prior work [1], we improve the semantic modeling of our E2E SLU models by adopting a 2-pass SLU approach [2], where the second pass combines both acoustic and semantic information generated by pre- trained LM from ASR hypotheses. Inspired by the principles of task compositionality, we also train compositional E2E SLU model [3] that ﬁrst convert the spoken utterance to a sequence of token representations, which can then be used in the traditional NLU framework. To combine hypotheses from multiple models, we directly apply the recognizer output vot- ing error reduction (ROVER) method and extract exact match (EM) accuracy from the combined semantic parse sequence.
3. EXPERIMENT SETUP
In this work, we experiment with various pipeline, and end-to-end (E2E) SLU approaches. Pretrained self-supervised speech (SSL) representations like WavLM and Hubert are employed in our SLU framework. We also incorporate pre- trained LMs like BART large. Finally, a system combination of various models shows a signiﬁcant performance gain over the baseline systems.
2. METHODOLOGY
We formulate the SLU task of semantic parsing as a uniﬁed sequence-to-sequence problem. The input is a sequence of speech features extracted from the raw audio, and the output
We adopt the evaluation metrics in the STOP benchmark i.e. EM accuracy. The encoder of our E2E SLU model is a 12- layer Conformer, while the decoder is a 6-layer Transformer. The number of heads and dimension of a self-attention layer is set to 8 and 512. The linear units are 2048 for the en- coder and the decoder. Speed perturbation and SpecAugment are performed for data augmentation. Our ASR model has the same architecture as our E2E SLU model. We investigate using external ASR datasets like Librispeech and Common- voice for pretraining. For 2 pass SLU models, our delibera- tion encoder consists of 4-layer conformer, and second-pass decoder has the same architecture as the ASR decoder. Our Compositional model consists of the same architecture as the ASR model in it’s ASR component and 6-layer transformer


Pre-trained Model
Test EM (↑)
STOP benchmark [5]
E2E E2E
Wav2Vec2 Hubert
68.7 69.23
Pipeline
Wav2Vec2+BART-L
72.36
Our E2E approach
w/ SSL
Hubert WavLM
71.6 73.3
w/ 2 pass SLU
WavLM+BART-L
75.8
w/ Compositional E2E SLU
WavLM
75.9
Our Pipeline approach
WavLM+BART-L
78.2
Table 1: Exact Match (EM) accuracy for semantic parsing on STOP dataset.
Pre-trained Model
Test EM (↑)
Our E2E approach
w/ SSL
Whisper Whisper large
78.8 79.1
w/ 2 pass SLU
w/ Whisper transcripts
77.0
w/ Compositional E2E SLU
w/ Whisper transcripts
77.4
Our Pipeline approach
Best ASR+BART-L Best ASR+T5
80.5 80.1
Our System Combination
4 best models
80.8
Table 2: Exact Match (EM) accuracy for semantic parsing using Whisper on STOP dataset.
encoder, and 6-layer transformer decoder in it’s NLU com- ponent. Dropout and label smoothing are applied. For pre- trained LMs, we use BART large and T5 large, which are trained with HuggingFace Seq2Seq Trainer. We add special tokens in the vocabulary for slot and intent tags.
We also experiment with Whisper medium for ASR and both Whisper medium and large models for SLU. For ASR, we train in 3 settings: (a) using lowercase transcripts (b) tran- scripts with original casing (as in “utterance” ﬁeld) referred to as Whisper w/ casing (c) original casing transcripts and more frequent saving of checkpoints (i.e. after 1k iterations instead of epoch) referred to as Whisper w/ freq. checkpoint. Similar to pretrained LMs, we add special tokens in vocabulary for slot and intent tags while training E2E SLU models. More details about our models and the conﬁg ﬁles will be publicly available as part of the ESPnet-SLU [4] toolkit.
4. RESULTS
Our results on the semantic parsing task without using the Whisper model are shown in Table 1. 2-pass and composi- tional E2E SLU models perform better than traditional E2E models. We further observe that the pipeline model is sig-
Pre-trained Model
Pretrained Dataset
Test WER (↓)
STOP benchmark [5]
Wav2Vec2 Hubert
STOP STOP
4.45 4.26
Our ASR models
Hubert WavLM WavLM w/ LM WavLM w/ LM Whisper Whisper w/ casing Whisper w/ freq. checkpoint
STOP STOP STOP Librispeech+Commonvoice+STOP STOP STOP STOP
3.8 3.3 3.1 2.7 2.4 2.3 2.3
Our System Combination
4 best models
2.2
Table 3: Word Error Rate (WER) on STOP dataset.
niﬁcantly better than all the E2E SLU models. Table 2 shows that ﬁnetuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data. Table 3 similarly shows that Whisper achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signiﬁcant performance gains. We were not able to investigate incorporating the Whisper model in our Composi- tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work.
Finally, we use the ROVER combination on the hypothe- sis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to signif- icantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Ta- ble 2) to achieve EM 80.8.
5. ACKNOWLEDGEMENT
This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily reﬂect those of that funding agency.
6. REFERENCES
[1] D. Le, A. Shrivastava, P. D. Tomasello, et al., “Deliberation model for on-device spoken language understanding,” in Proc. Interspeech, 2022, pp. 3468–3472.
[2]
S. Arora, S. Dalmia, X. Chang, et al., “Two-pass low latency end-to- end spoken language understanding,” in Arxiv preprint arXiv:2207.06670, 2022.
[3]
S. Arora, S. Dalmia, B. Yan, et al., “Token-level sequence labeling for spoken language understanding using compositional end-to-end mod- els,” in Proc. EMNLP, 2022.


[4]
[5]
S. Arora, S. Dalmia, P. Denisov, et al., “Espnet-slu: Advancing spo- ken language understanding through espnet,” in Proc. ICASSP, 2022, pp. 7167–7171.
P. Tomasello, A. Shrivastava, D. Lazar, et al., “STOP: A dataset for Spoken Task Oriented Semantic Parsing,” in CoRR.