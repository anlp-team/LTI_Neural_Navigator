"Youmightthinkaboutslightlyrevisingthetitle":IdentifyingHedgesinPeer-tutoringInteractionsYannRaphalen1,ChloéClavel2,JustineCassell1,31InriaParis2LTCI,InstitutPolytechniquedeParis,Telecom-Paris3CarnegieMellonUniversityyann.raphalen.pro@gmail.com,justine@cs.cmu.edu,chloe.clavel@telecom-paris.frAbstractHedgesplayanimportantroleinthemanage-mentofconversationalinteraction.Inpeer-tutoring,theyarenotablyusedbytutorsindyads(pairsofinterlocutors)experiencinglowrapporttotonedowntheimpactofinstructionsandnegativefeedback.Pursuingtheobjectiveofbuildingatutoringagentthatmanagesrap-portwithstudentsinordertoimprovelearning,weusedamultimodalpeer-tutoringdatasettoconstructacomputationalframeworkforiden-tifyinghedges.Wecomparedapproachesre-lyingonpre-trainedresourceswithothersthatintegrateinsightsfromthesocialsciencelitera-ture.Ourbestperformanceinvolvedahybridapproachthatoutperformstheexistingbase-linewhilebeingeasiertointerpret.Weemployamodelexplainabilitytooltoexplorethefea-turesthatcharacterizehedgesinpeer-tutoringconversations,andweidentifysomenovelfea-tures,andthebenefitsofsuchahybridmodelapproach.1IntroductionRapport,mostsimplydefinedasthe“...relativeharmonyandsmoothnessofrelationsbetweenpeo-ple...”(Spencer-Oatey,2005),hasbeenshowntoplayaroleinthesuccessofactivitiesasvariedaspsychotherapy(Leach,2005)andsurveyinterview-ing(LuneandBerg,2017).Inpeer-tutoring,rap-port,asmeasuredbytheannotationofthinslicesofvideo,hasbeenshowntobebeneficialforlearningoutcomes(Zhaoetal.,2014;SinhaandCassell,2015).Thelevelofrapportrisesandfallswithconversationalstrategiesdeployedbytutorsandtuteesatappropriatetimes,andasafunctionofthecontentofpriorturns.Thesestrategiesincludeself-disclosure,referringtosharedexperience,and,onthepartoftutors,givinginstructionsinanindirectmanner.Someworkhasattemptedtoautomaticallydetectthesestrategiesintheserviceofintelligenttutors(Zhaoetal.,2016a),butonlyafewstrate-gieshavebeenattempted.Otherworkhascon-centratedona"socialreasoningmodule"(Romeroetal.,2017)todecidewhichstrategiesshouldbegeneratedinagivencontext,butindirectnesswasnotamongthestrategiestargeted.Inthispaper,wefocusontheautomaticclassificationofonespe-cificstrategythatisparticularlyimportantforthetutoringdomain,andthereforeimportantforintel-ligenttutors:hedging,asub-partofindirectnessthat"softens"whatwesay.Thisworkispartofalargerresearchprogramwiththelong-termgoalofautomaticallygeneratingindirectnessbehaviorsforatutoringagent.
Figure1:Amockconversationdisplayingeachtypeofhedgedformulation.AccordingtoBrownandLevinson(1987),hedgesarepartofthelinguistictoolsthatinterlocu-torsusetoproducepoliteness,bylimitingthefacethreattotheinterlocutor(basicallybylimitingtheextenttowhichtheinterlocutormightexperienceembarrassmentbecauseofsomekindofpoorper-formance).Anexampleis"that’skindofawronganswer".Hedgesarealsofoundwhenspeakerswishtoavoidlosingfacethemselves,forexam-plewhensaying("IthinkImighthavetoadd6.").Madaioetal.(2017)foundthatinapeer-tutoringtask,whenrapportbetweeninterlocutorsislow,tu-teesattemptedmoreproblemsandcorrectlysolvedmoreproblemswhentheirtutorshedgedinstruc-
2160 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 2160 - 2174 May 22-27, 2022 c(cid:13)2022 Association for Computational Linguistics


tions,whichlikewisepointstowardsa"mitigationoffacethreat"function.Hedgescanalsobeasso-ciatedwithanonverbalcomponent,forexampleavertedeyegazeduringcriticism(BurgoonandKoper,1984).Hedgesarenot,however,alwaysap-propriate,asin"Ikindofthinkit’srainingtoday."whentheinterlocutorscanbothseerain(althoughitmightbetakenashumorous).Thesefactsabouthedgesmotivateawaytoautomaticallydetectthemand,ultimately(althoughnotinthecurrentwork)alsogeneratethem.Inbothcaseswefirsthavetobeabletocharacterizethemusinginterpretablelinguisticfeatures,whichiswhatweaddressinthecurrentpaper.Thus,intheworkdescribedhere,basedonlinguisticdescriptionsofhedges(BrownandLevinson,1987;Fraser,2010),webuiltarule-basedclassifier.Weshowthatthisclassifierincombinationwithadditionalmultimodalinter-pretablecontext-dependentfeaturessignificantlyimprovestheperformanceofamachinelearningmodelforhedges,comparedtoalessinterpretabledeeplearningbaselinefromGoeletal.(2019)us-ingwordembeddings.Wealsoreliedonamachinelearningmodelexplanationtool(LundbergandLee,2017)toinvestigatethelinguisticfeaturesrelatedtohedgesinthecontextofpeer-tutoring,primarilytoseeifwecoulddiscoversurprisingfeaturesthattheclassificationmodelwouldassociatetohedgesinthiscontext,andwedescribethosebelow.Thecodeofthemodelsdescribedinthepaperisalsoprovided.12RelatedworkHedges:AccordingtoFraser(2010),hedgingisarhetoricalstrategythatattenuatesthestrengthofastatement.Onewaytoproduceahedgeisbyalteringthefullsemanticvalueofaparticu-larexpressionthroughPropositionalhedges(alsocalledApproximatorsinPrinceetal.(1982)),asin"Youarekindofwrong,"thatreduceprototypical-ity(i.eaccuracyofthecorrespondencebetweenthepropositionandtherealitythatthespeakerseekstodescribe).Propositionalhedgesarerelatedtofuzzylanguage(Lakoff,1975),andthereforetotheproductionofvagueness(Williamson,2002)anduncertainty(Vincze,2014).AsecondkindareRelationalHedges(alsocalledShieldsinPrinceetal.(1982)),suchas“Ithinkthatyouarewrong.”or“Thedoctorwantsyoutostopsmoking.”,conveyingthatthepropositionis
1https://github.com/AnonymousHedges/HedgeDetectionconsideredbythespeakerassubjective.Inafurthersub-division,AttributionShields,asin"Thedoc-torwantsyou...",theinvolvementofthespeakerinthetruthvalueofthepropositionisnotmadeexplicit,whichallowsspeakersnottotakeastance.Asdescribedabove,Madaioetal.(2017)foundthattutorswhoshowedlowerrapportwiththeirtuteesusedmorehedgedinstructions(theyalsoemployedmorepositivefeedback),howeverthiswasonlythecasefortutorswithagreaterbeliefintheirabilitytotutor.Tuteesinthiscontextsolvedmoreproblemscorrectlywhentheirtutorshedgedinstructions.Noeffectofhedgingwasfoundfordyads(pairsofinterlocutors)withgreatersocialcloseness.However,theauthorsdidnotlookatthespecificlinguisticformstheseteenagersused.Rowland(2007)alsodescribestherolethathedg-ingplaysinthisagegroup,showingthatstudentsusebothrelational("IthinkthatJohnissmart.")andpropositional("Johniskindofsmart.")hedgesformuchthesameshieldingfunctionofdemon-stratinguncertainty,tosavethemfromtheriskofembarrassmentiftheyarewrong.TheauthorobservedthatteensusedfewAdaptors(kindof,somewhat)andpreferredtouseRounders(around,closeto).However,thisstudywasperformedwithanadultandtwochildren,possiblybiasingthere-sultsduetotheparticipationoftheadultinvestiga-tor.Hedgeshavebeenincludedinvirtualtutoringagentsbeforenow.(Howardetal.,2015)integratedhedgesinatutoragentforundergraduatesinCS,asawaytoencouragethestudenttotaketheinitiative.Hedgeshavealsobeenusedasawayofintegrat-ingBrownandLevinson’spolitenessframework(Wangetal.,2008;Schneideretal.,2015)invir-tualtutoringagents.Resultswerenotbrokenoutbystrategy,butpolitenessingeneralwasshowntopositivelyinfluencemotivationandlearning,incertainconditions.Computationalmethodsforhedgedetection:Anumberofstudieshavetargetedthedetectionofhedgesanduncertaintyintext(MedlockandBriscoe,2007;GanterandStrube,2009;Tangetal.,2010;Velldal,2011;Szarvasetal.,2012),partic-ularlyfollowingtheCoNLL2010datasetrelease(Farkasetal.,2010).However,thisworkisnotasrelatedtohedgesinconversation,asitfocusesonaformalandacademiclanguageregister(Hy-land,1998;Varttala,1999).AsnotedbyProkofievaandHirschberg(2014),thefunctionsofhedgesaredomain-andgenre-dependent,thereforethisbias
2161


towardsformalityimpliesthattheexistingworkmaynotadaptwelltothedetectionofhedgesinconversationbetweenteenagers.Aconsequenceisthattheexistingworkdoesnotconsidertermslike"Ithink,"sinceopinionsrarelyappearinanaca-demicwritingdataset.Instructionsarealsoalmostabsent("Ithinkyouhavetoaddtentobothsides."),astronglimitationforthestudyofconversationalhedgessinceitisinrequests(includingtutoringin-structions)thatindirectformulationsmostlyoccuraccordingtoBlum-Kulka(1987).ProkofievaandHirschberg(2014)alsonotethatitisdifficulttodetecthedgesbecausethewordpatternsassociatedwiththemhaveothersemanticandpragmaticfunc-tions:considering"Ithinkthatyouhavetoaddxtobothsides."vs"Ithinkthatyouareanidiot.",itisnotclearthattheseconduseof"Ithinkthat"isanhedgemarker.Theyadvocateusingmachinelearningapproachestodealwiththeambiguityofthesemarkers.Workingonaconversationaldataset,Ulinskietal.(2018)builtacomputationalsystemtoassessspeakercommitment(i.e.atwhichpointthespeakerseemsconvincedbythetruthvalueofastatement),inparticularbyrelyingonarule-baseddetectionsystemforhedges.Comparedtothatwork,ourrule-basedclassificationmodelisdirectlydetectinghedgeclasses,andweemploythepredictionsoftherule-basedmodelasafeatureforstrongermachinelearningmodels,designedtolessentheimpactoftheimbalancebetweenclasses.Wealsoconsiderapologieswhentheyserveamit-igationfunction(wethencallthemApologizers),aswasdonebytheauthorsofourcorpus,andwealsousethetermsubjectivizersasdefinedbelow,tobeabletocomparedirectlywiththepreviousworkcarriedoutonthiscorpus.Asfarasweknow,onlyGoeletal.(2019)haveworkedwithapeer-tutoringdataset(thesameonethatwealsouse),andtheyachievedtheirbestclassificationresultbyemployinganAttention-CNNmodel,inspiredbyAdelandSchütze(2017).3ProblemstatementWeconsiderasetDofconversationsD=(c1,c2,...,c|D|),whereeachconversationiscom-posedofasequenceofindependentsyntacticclausesci=(u1,u2,...,uM),whereMisthenumberofclausesintheconversation.Notethattwoconsecutiveclausescanbeproducedbythesamespeaker.Eachclauseisassociatedwithauniquelabelcorrespondingtothediffer-enthedgeclassesdescribedinTable1:yi∈C={PropositionalHedges,Apologizers,Subjec-tivizers,Nothedged}.Finally,anutteranceuicanberepresentedasavectoroffeaturesX=(x1,x2,...,xN),whereNrepresentsthenumberoffeaturesweusedtodescribeaclause.Ourfirstgoalistodesignamodelthatcorrectlypredictsthelabelyiassociatedtoui.Itcanbeunderstoodasthefollowingresearchquestion:RQ1:"Whichmodelsandfeaturescanbeusedtoautomaticallycharacterizehedgesinapeer-tutoringinteraction?"Oursecondgoalistoidentify,foreachhedgeclass,thesetoffeaturesFclass={fk},k∈[1,N]sortedbyfeatureimportanceintheclassificationofclass.Itcorrespondstothefollowingresearchquestion:RQ2:"Whatarethemostimportantlinguisticfeaturesthatcharacterizeourhedgeclassesinapeer-tutoringsetting?"4Methodology4.1CorpusDatacollection:Thedialoguecorpususedherewascollectedaspartofalargerstudyontheeffectsofrapport-buildingonreciprocalpeertutoring.24Americanteenagers(meanage=13.5,min=12,max=15),halfmaleandhalffemale,cametoalabwherehalfoftheparticipantswerepairedwithasame-age,same-genderfriend,andtheotherhalfpairedwithastranger.Theparticipantswereassignedtoatotalof12dyadsinwhichthepar-ticipantsalternatedtutoringoneanotherinlinearalgebraequationsolvingfor5weeklyhour-longsessions,foratotalcorpusofnearly60hoursofface-to-faceinteractions.Eachsessionwasstruc-turedsuchthatthestudentsengagedinbriefsocialchitchatinthebeginning,thenoneofthestudentswasrandomlyassignedtotutortheotherfor20minutes.Theythenengagedinanothersocialpe-riod,andconcludedwithasecondtutoringperiodwheretheotherstudentwasassignedtheroleoftutor.Audioandvideodatawererecorded,tran-scribed,andsegmentedforclause-leveldialogueannotation,providingnearly24000clauses.Non-speechsegments(notablyfillersandlaughter)weremaintained.Becauseoftemporalmisalignmentforpartsofthecorpus,manyparaverbalphenomena,suchasprosody,wereunfortunatelynotavailabletous.SinceouraccesstothedatasetiscoveredbyaNon-DisclosureAgreement,itcannotbereleased
2162


2https://github.com/AnonymousHedges/HedgeDetectionclasses.4.2FeaturesLabelfromrule-basedclassifier(LabelRB):Weusetheclasslabelpredictedbytherule-basedclas-sifierdescribedinSection4.3asafeature.Ourhypothesisisthatthemachinelearningmodelcanusethisinformationtocounterbalancetheclassimbalance.Totakeintoaccountthefactthatsomerulesaremoreefficientthanothers,weweightedtheclasslabelresultingfromtherule-basedmodelbytheprecisionoftherulethatgeneratedit.Unigramandbigram:Wecountthenumberofoccurrencesofunigramsandbigramsofthecorpusineachclause.Weusedthelemmaofthewordsforunigramsandbigramsusingthenltklemmatizer(Loper,2002)andselectedunigramsandbigramsthatoccurredinthetrainingdatasetatleastfiftytimes.Thegoalwastoinvestigate,withabottom-upapproach,towhatextenttheuseofcertainwordscharacterizeshedgeclassesintutoring.InSection5weexaminetheoverlapbetweenthesewordsandthoseaprioriidentifiedbytherules.Part-of-speech(POS):Hedgeclassesseemtobeassociatedwithdifferentsyntacticpatterns:forex-ample,subjectivizersmostoftencontainapersonalpronounfollowedbyaverb,asin"Iguess","Ibelieve","Ithink".WethereforeconsideredthenumberofoccurrencesofPOS-Tagn-grams(n=1,2,3)asfeatures.WeusedthespaCyPOS-taggerandconsideredPOSunigrams,bigramsandtri-gramsthatoccuratleast10timesinthetrainingdataset.LIWC:LinguisticInquiryandWordCount(LIWC)(Pennebakeretal.,2015)isstandardsoft-wareforextractingthecountofwordsbelongingtospecificpsycho-socialcategories(e.g.,emotions,religion).Ithasbeensuccessfullyusedinthede-tectionofconversationalstrategies(Zhaoetal.,2016a).Wethereforecountthenumberofoccur-rencesofallthe73categoriesfromLIWC.Tutoringmoves(TM):Intelligenttutoringsys-temsrelyonspecifictutoringmovestosuccess-fullyconveycontent(asdohumantutors).Wethereforelookedatthelinkbetweenthetutoringmoves,asannotatedinMadaioetal.(2017),andhedges.Fortutors,thesemovesare(1)instruc-tionaldirectivesandsuggestions,(2)feedback,and(3)affirmations,mostlyexplicitreflectionsontheirpartners’comprehension,whilefortutees,theyare(1)questions,(2)feedbacks,and(3)affirmations,
publicly.Howevertheoriginalexperimenters’In-stitutionalReviewBoard(IRB)approvalallowsustoview,annotate,andusethedatatotrainmodels.Thisalsoallowsustoprovidealinktoapixe-latedvideoexampleintheGitHubrepositoryoftheproject2.Dataannotation:Thedatasetwaspreviouslyan-notatedbyMadaioetal.(2017),followingananno-tationmanualthatusedhedgeclassesderivedfromRowland(2007)(seeTable1).Onlythetaskperi-odsoftheinteractionswereannotated.Comparingtheannotationswiththeclassesmentionedintherelatedworksection,SubjectivizerscorrespondtoRelationalhedges(Fraser,2010),PropositionalhedgesandExtenderscorrespondtoApproxima-tors(Princeetal.,1982)withtheadditionofsomediscoursemarkerssuchasjust.ApologizersarementionedaslinguistictoolsrelatedtonegativepolitenessinBrownandLevinson(1987).Krippen-dorff’salphaobtainedforthiscorpusannotatedbyfourcoderswasover0.7forallclasses(denotinganacceptableinter-coderreliabilityaccordingtoKrippendorff(2004)).Thedatasetiswidelyim-balanced,withmorethan90%oftheutterancesbelongingtotheNothedgedclass.Inreviewingthecorpusandtheannotationman-ual,however,wenoticedtwoissues.First,theannotationoftheExtendersclasswasinconsis-tent,leadingtotheExtendersandPropositionalhedgesclassescarryingsimilarsemanticfunctions.WethereforemergedthetwoclassesandgroupedutteranceslabeledasExtendersandthoselabeledasPropositionalhedgesundertheheadingofPropositionalhedges.Second,theannotationofclausescontainingthetokens"just"and"would"(twotermsoccurringfrequentlyinthedatasetthatarekeycomponentsofPropositionalHedgesandSubjectivizersbutthatarenotinfacthedgesinallcases)wasalsoinconsistent,leadingtovirtuallyallclauseswiththosetwotokensbeingconsideredhedges.Wethereforere-consideredalltheclausesassociatedwithanyofthehedgeclasses,aswellasalltheclausesinthe"Nothedged"classthatcontained"just"or"would".There-annotationwascarriedoutbytwoannotatorswhoachievedaKrippendorff’salphainter-raterreliabilityof.9orbetterforApologizers,Subjectivizers,andPropo-sitionalhedgesbeforeindependentlyre-annotatingtherelevantclauses.Anexampleofare-annotationwasremoving"Iwouldkillyou!"fromthehedge
2163


Prop.hedgesApologizersSubjectivizersNothedgedTotal
Table1:Definitionoftheclasses
FeaturesnameAutomaticextractionVectorsize
SubjectivizersWordsthatreduceintensityorcertainty“SothenIwoulddividebytwo.”ApologizersApologiesusedtosoftendirectspeechacts“Ohsorrysixb.”PropositionalhedgesQualifyingwordstoreduceintensityorcertaintyofutterances“It’sactuallyeight.”ExtendersWordsusedtoindicateuncertaintybyreferringtovaguecategories“It’llbethenumberxorwhatevervariableyouhave.”
12101286262119223156
Table2:Distributionoftheclasses
ClassDefinitionExample
Rule-basedlabelYes4UnigramYes~250BigramYes~250POSYes~1200LIWCYes73NonverbalNo24TutoringmovesNo6Total~1800
Table3:Listofautomaticallyextractedandmanuallyannotatedfeatureswiththeirsize.mostlytentativeanswers.Nonverbalandparaverbalbehaviors:AsinGoeletal.(2019),weincludedthenonverbalandpar-averbalbehaviorsthatarerelatedtohedges.Specif-ically,weconsiderlaughterandsmiles,thathavebeenshowntobeeffectivemethodsofmitiga-tion(Warner-Garcia,2014),cut-offsindicatingself-repairs,fillerslike"Um",gazeshifts(annotatedas’GazeatPartner’,’GazeattheMathWorksheet’,and’Gazeelsewhere’),andheadnods.Eachfea-turewaspresenttwiceinthefeaturevector,onetimeforeachinterlocutor.Inter-raterreliabilityfornonverbalbehaviorwas0.89(asmeasuredbyKrippendorff’salpha)foreyegaze,0.75forsmilecount,0.64forsmiledurationand0.99forheadnod.Laughterisalsoreportedinthetranscriptatthewordlevel.Weseparatethetutor’sbehaviorsfromthoseofthetutee.ThecollectionprocessforthesebehaviorsisdetailedfurtherinZhaoetal.(2016b).Theclause-levelfeaturevectorwasnormalizedbythelengthoftheclause(exceptfortherule-basedlabel).Thislengthwasalsoaddedasafeature.Table3presentsanoverviewofthefinalfeaturevector.4.3ClassificationmodelsTheclassificationmodelsusedarepresentedhereaccordingtotheirlevelofintegrationofexternallinguisticknowledge.Rule-basedmodel:OnthebasisoftheannotationmanualusedtoconstructthedatasetfromMadaioetal.(2017),andwithdescriptionsofhedgesfromRowland(2007),Fraser(2010)andBrownandLevinson(1987),weconstructedarule-basedclas-sifierthatmatchesregularexpressionsindicativeofhedges.TherulesaredetailedinTable7intheAppendix.LGBM:Sincehedgesareoftencharacterizedbyexplicitlexicalmarkers,wetestedtheassumptionthatamachinelearningmodelwithaknowledge-drivenrepresentationforclausescouldcompetewithaBERTmodelinperformance,whilebeingmuchmoreinterpretable.WereliedonLightGBM,anensembleofdecisiontreestrainedwithgradi-entboosting(Keetal.,2017).Thismodelwasselectedbecauseofitsperformancewithsmalltrainingdatasetsandbecauseitcanignoreunin-formativefeatures,butalsoforitstrainingspeedcomparedtoalternativeimplementationsofgradi-entboostingmethods.Multi-layerperceptron(MLP):Asasimplebase-line,webuiltamulti-layerperceptronusingthreesetsoffeatures:apre-trainedcontextualrepre-sentationoftheclause(SentBERT;ReimersandGurevych(2019));theconcatenationofthiscon-textualrepresentationoftheclauseandarule-basedlabel(notrelyingonthepreviousclauses);andfinallytheconcatenationofallthefeaturesmen-tionedinsection4.2,withoutthecontextualizedrepresentation.LSTMoverasequenceofclauses:Sinceweareworkingwithconversationaldata,wealsowantedtotestwhethertakingintoac-countthepreviousclauseshelpstodetectthetypeofhedgeclassinthenextclause.Formally,wewanttoinferyiusingyi=maxy∈ClassesP(y|X(ui),X(ui−1),...,X(ui−K)),whereKisthenumberofpreviousclausesthatthemodelwilltakeintoaccount.The
2164


MLPmodelpresentedaboveinfersyiusingyi=maxy∈ClassesP(y|X(ui)),thereforeadifferenceofperformancebetweenthetwomodelswouldbeasignthatusinginformationfromthepreviousclausescouldhelptodetectthehedgedformulationinthecurrentclause.WetestedaLSTMmodelwiththesamerepresentationsforclausesasfortheMLPmodel.CNNwithattention:Goeletal.(2019)estab-lishedtheirbestperformanceonhedgedetec-tionusingaCNNmodelwithadditiveattentionoverword(andnotclause)embeddings.Con-trarytotheMLPandLSTMmodelsmentionedabove,thismodeltriestoinferyiusingyi=maxy∈ClassesP(y|g(w0),g(w1),...,g(wL)),withLrepresentingthemaximumclauselengthweal-low,andgrepresentingafunctionthatturnsthewordwj,j∈[0,L]intoavectorrepresentation(formoredetails,pleaseseeAdelandSchütze(2017)).BERT:Tobenefitfromdeepsemanticandcon-textualrepresentationsoftheutterances,wealsofine-tunedBERT(Devlinetal.,2019)onourclas-sificationtask.BERTisapre-trainedTransformersencoder(Vaswanietal.,2017)thathassignificantlyimprovedthestateoftheartonanumberofNLPtasks,includingsentimentanalysis.Itproducesacontextualrepresentationofeachwordinasen-tence,makingitcapableofdisambiguatingthemeaningofwordslike"think"or"just"thatarerepresentativeofcertainclassesofhedges.BERT,however,isnotablyhardtointerpret.4.4AnalysistoolsLookingatwhichfeaturesimprovetheperfor-manceofourclassificationmodelstellsuswhetherthesefeaturesareinformativeornot,butdoesnotexplainhowthesefeaturesareusedbythemod-elstomakeagivenprediction.Wethereforepro-ducedacomplementaryanalysisusinganinter-pretabilitytool.Asdemonstratedby(LundbergandLee,2017),LightGBMinternalfeatureimpor-tancescoresareinconsistentwithboththemodelbehaviorandhumanintuition,soweinsteadusedamodel-agnostictool.SHAP(LundbergandLee,2017)assignstoeachfeatureanimportancevalue(calledShapleyvalues)foraparticularpredictiondependingontheextentofitscontribution(ade-tailedintroductiontoShapleyvaluesandSHAPcanbefoundinMolnar(2020)).SHAPisamodel-agnosticframework,thereforethevaluesassoci-atedwithasetoffeaturescanbecomparedacrossmodels.ItshouldbenotedthatSHAPproducesexplanationsonacase-by-casebasis,thereforeitcanbothprovidelocalandglobalexplanations.FortheGradientBoostingmodel,weuseanadaptedversionofSHAP(Lundbergetal.,2018),calledTreeSHAP.5Experimentsandresults5.1ExperimentalsettingTodetectthebestsetoffeatures,weusedLight-GBMandproceededincrementally,byaddingthegroupoffeatureswethoughttobemostlikelyasso-ciatedwithhedges.Wedidnotconsidertheriskofrelyingonasub-optimalsetoffeaturesthroughthisprocedurebecauseofthestrongabilityofLight-GBMtoignoreuninformativefeatures.Weusethisincrementalapproachasawaytotestourintuitionabouttheperformativityofgroupsoffeatures(i.e.doesaddingafeatureimprovetheperformanceofthemodel)withregardtothetaskofclassifica-tion.Tocompareourmodels,wetrainedthemonthe4-classtask,andlookedattheaverageoftheweightedF1-scoresforthethreehedgeclasses(i.e.howwellthemodelsinferminorityclasses)thatwereporthereas"3-classes",andattheaverageoftheweightedF1-scoresforthe4classes,thatwereportas"4-classes".DetailsofthehyperparametersandexperimentalsettingsareprovidedinAppendixA.5.2ModelcomparisonandfeatureanalysisOverallresults:Table4presentstheresultsob-tainedbythe6modelspresentedinSection4.3forthemulti-classproblem.Bestperformance(F1-scoreof79.0)isobtainedwithLightGBMlever-agingalmostallthefeatures.Intheappendix(seeTable8andTable9)weindicatetheconfidenceintervalstorepresentthesignificanceofthediffer-encesbetweenthemodels.First,andperhapssurprisingly,wenoticethattheuseof"Knowledge-Driven"featuresbasedonrulesbuiltfromlinguisticknowledgeofhedgesintheLightGBMmodeloutperformstheuseofpre-trainedembeddingswithinafine-tunedBERTmodel(79.0vs.70.6),andintheneuralbaselinefrom(Goeletal.,2019)(79.0vs64.5).ThelowscoresobtainedbytheLGBM,LSTMandMLPmodelswithpre-trainedsentenceem-beddingsversusKnowledge-Drivenfeaturesmightsignalthatthewordpatternscharacterizinghedgesarenotsalientintheserepresentations(i.e.the
2165


68.5(1.6)35.8(3.1)64.8(1.1)Attention-CNN(3-classes)
65.1(5.7)39.8(8.0)65.2(5.1)BERT(3-classes)
∅70.6(2.3)∅LGBM(3-classes)
79.0(1.3)35.0(2.2)70.1(1.4)
Rule-based(3-classes)
94.7∅∅MLP(4-classes)
3Notethatthereisstrongredundancybetweensomefea-turesofLIWCandthespaCyPOStaggerthatbothproducea"Pronoun"category,usingalexiconinthefirstcase,andaneuralinferenceinthesecond.
∅94.9(0.4)∅LGBM(4-classes)
Rule-based(4-classes)
Table4:AveragedweightedF1-scores(andstandarddeviation)forthethreeminorityclassesandforthe4classes,forallmodels."KD"standsfor"Knowledge-Driven",meaningthatthefeaturesarederivedfromlexicon,n-grammodelsandannotations.distancebetween"Ithinkyoushouldadd5."and"Youshouldadd5."isshort.).KDFeaturesseemtoprovideabetterseparabilityoftheclasses.ThecombinationofKDfeaturesandPre-trainedem-beddingsdoesnotsignificantlyimprovetheperfor-manceofthemodelscomparedtotheKDFeaturesonly,whichsuggeststhattheinformationfromthePre-trainedembeddingsisredundantwiththeonefromtheKDFeatures.Thisresultmaybeduetothehighdimensionalityoftheinputvector(868withPCAontheKDFeatures;2500otherwise).Asecondfindingisthattheuseofgradientboost-ingmodelsontopofrule-basedclassifiersbettermodelsthehedgeclasses.Theothermachinelearn-ingmodelsdidnotprovetobeaseffective,exceptforBERT.FeatureanalysisusingLightGBM:Usingthebestperformingmodel,Table5showstheroleofeachfeaturesetinthepredictiontask.ThesignificanceofthedifferencesisshowninTable10andTable11.Comparedtotherule-basedmodel,theintroductionofn-gramssignificantlyimprovedtheperformanceofourclassifier,suggestingthatsomelexicalandsyntacticinformationdescribingthehedgeclasseswasnotpresentintherule-basedmodel.LookingatTable5,wedonotobservesignificantdifferencesbetweentheLGBMmodelusingonlythelabelrulebased+(1-gramsand2-grams)andthemodelsin-corporatingmorefeatures.Tooursurprise,neitherthetutoringmovesnorthenonverbalfeaturessig-nificantlyimprovedtheperformanceofthemodel.The2featureswereincludedtoindexthespecificpeertutoringcontextofthesehedges,sothisindi-catesthatinfutureworkwemightwishtoapplythecurrentmodeltoanothercontextofusetoseeifthismodelofhedgesismoregenerallyapplicablethanweoriginallythought.Bycombiningthisresultwiththeincreasedperformanceofthemodelus-ingKnowledge-Driven(i.e.explicit)featurescom-paredtopre-trainedembeddings,itwouldseemthathedgesareaboveallalexicalphenomenon(i.e.producedbyspecificlexicalelements).5.3In-depthanalysisoftheinformativefeaturesWetrainedtheSHAPexplanationmodelsonLight-GBMwithallfeatures.Themostinformativefea-tures(inabsolutevalue)foreachclassareshowninTable6,andtheplotsbyclassarepresentedintheAppendix.Themostimportantfeaturesseemtobetherule-basedlabels,whichappearinatleastthefourthpositionforthreeclasses(seeTable6),andinthefirstpositionforPropositionalHedgesandNothedgedclasses.Surprisingly,theRule-Basedlabeldoesnotappearinthetop20featuresforApologizers.However,giventhattheclassrarelyappearsinthedata,therulesseldomactivate,sothefeaturemaysimplybeinformativeforaverysmallnumberofclauses.Unigrams(Oh,Sorry,just,Would,andI)arealsopresentinthe5top-rankedfeatures.Thisconfirmsthefindingsmen-tionedinrelatedworkforthecharacterizationofthedifferenthedgeclasses(justwithPropositionalHedges,sorrywithApologizer,IwithSubjectiviz-ers).ThepresenceofOhalsohashighimportanceforthecharacterizationofApologizer(n=2),asillustratedinexamplessuchas"Ohsorry,that’snine.".Wenotethattheoccurrencesof"Ohsorry"asastand-aloneclausewereexcludedbyourrule-basedmodelbecausetheydonotcorrespondtoanapologizer(theycannotmitigatethecontentofapropositionifthereisnopropositionassociated).Thisexampleillustratestheinterestofamachinelearningmodelapproachtodisambiguatethefunc-tionofconventionalnon-propositionalphraseslike"Ohsorry".Inaddition,SHAPhighlightstheimportanceofnovelfeatureswhosefunctionwasnotidentifiedinthehedgesliterature:(i)whatLIWCclassifiesasinformalwordsbutthataremostlyinterjectionslikeahandoharestronglyassociatedwithApol-ogizer,asaredisfluencies(n=12);(ii)theuseofPOStagsseemstobeveryrelevantforcharac-terizingthedifferentclasses(2-gramofPOStagfeatures3occurinthetop-rankedfeaturesofallthe
∅64.5(3.0)∅LSTM(3-classes)
KDFeat.(KDF)Pre-TrainedEmb.(PTE)KDF+PTE
∅94.4(0.2)∅LSTM(4-classes)
96.7(0.2)91.0(0.2)95.4(0.2)
94.8(0.3)89.7(0.4)93.9(0.4)Attention-CNN(4-classes)
93.9(1.4)89.1(1.4)94.1(1.2)BERT(4-classes)
67.6∅∅MLP(3-classes)
Models
2166


95.0(0.2)96.5(0.3)96.5(0.2)96.7(0.2)96.6(0.4)96.7(0.3)
3-classes
LabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal
68.8(0.8)78.2(1.6)78.1(1.3)79.0(1.3)78.5(2.4)78.7(1.8)
Table5:AveragedweightedF1-scoresforthethreeclassesofhedgesandthefourclasses,withanadditiveintegrationofKDFfeaturesintheLightGBMmodel.Thestandarddeviationiscomputedacrossfivefolds.
4-classes
Models
RankApologizerSubjectivizersProp.HedgesNothedged
1Functionwords(LIWC)"I"ClasslabelClasslabel2"Oh"(LIWC)"Yeah""Would""Would"3"Sorry"Noun(POS)"Just""Yeah"4Affect(LIWC)ClasslabelFunctionword(LIWC)Noun(POS)5ClauselengthCognitiveprocess(LIWC)Netspeak(LIWC)Cognitiveprocess(LIWC)Table6:Mostimportantclause-levelfeaturesforLightGBMaccordingtotheSHAPanalysis.classes(seeFiguresintheAppendix).Itmeansthattherearesomerecurringsyntacticpatternsineachclass;(iii)Regardingtheutterancesize,aclauseshorterthanthemeanisweaklyassociatedwithdirectness(n=17)whilealongerclausesuggeststhatitcontainsaSubjectivizer(n=6).Apologizersarecharacterizedbyameanclauselength(n=5),withfewvariationsfromit;(iv)Tutoringmovesarenotstrongpredictorsofanyclasses:"Affirma-tionfromtutor"istheonlyfeatureappearingasapredictorofPropositionalhedges(n=20).ThisisconsistentwiththefeatureanalysisinTable5,suggestingthattutoringmovesdonotsignificantlyimprovetheperformanceoftheclassifier;(v)Non-verbalbehaviorsdonotappearasimportantfea-turesfortheclassification.Thisiscoherentwithresultsfrom(Goeletal.,2019).Notethatprosodymightplayaroleindetectinginstructionsthattrailoff,but,asdescribed,paraverbalfeatureswerenotavailable;(vi)Wouldplaysanimportantroleintheproductionofhedges,asitisstronglyassociatedtoPropositionalhedges(n=2).Itisinterestingtonotethat,whendesigningtherule-basedclassifier,wesawitdecreaseinperformancewhenwestartedtoincludewouldinourregularexpressionpatterns,probablybecausetheformishardtodisambiguateforadeterministicsystem.WhileexploringtheShapleyvaluesassociatedtoeachclause,weobservedthatfeaturesliketutoringmovesareextremelyinformativeforaverysmallnumberofclauses(thereforenotsignificantlyinflu-encingtheoverallperformanceoftheprediction),andmoreorlessnotinformativefortherest.Infer-ringtheglobalimportanceofafeatureasameanacrosstheshapleyvaluesinthedatasetmaynotbetheonlywaytoexplorethebehaviorofgradi-entboostingmethods.ItmightbemoreusefultoclusterclausesbasedontheimportancethatSHAPgivestothatfeatureinitsclassification,asthiscouldhelpdiscoversub-classesofhedgesthataredifferentiatedfromtherestbytheirinteractionwithaspecificfeature(inthewaythatsomeApologiz-ersarecharacterizedbyan"oh").Wealsonotethattheexplanationmodelissensitivetospuri-ouscorrelationsinthedataset,causedbythesmallrepresentationofsomeclass:forexample,"nine"(n=7)and"four"(n=20)arepositivepredictorsofApologizers.6ConclusionandfutureworkThroughourclassificationperformanceexperi-ments,weshowedthatitispossibletousema-chinelearningmethodstodiminishtheambigu-ityofhedges,andthatthehybridapproachofus-ingrule-basedlabelfeaturesderivedfromsocialscience(includinglinguistics)literaturewithinamachinelearningmodelhelpedsignificantlytoin-creasethemodel’sperformance.Nonverbalbehav-iorsandtutoringmovesdidnotprovideinformationatthesentencelevel;boththeperformanceofthemodelandthefeaturecontributionanalysissug-gestedthattheirimpactonthemodeloutputwasnotstrong.ThisisconsistentwithresultsfromGoeletal.(2019).However,infutureworkwewouldliketoinvestigatethepotentialofmultimodalpat-ternswhenweareabletobettermodelsequentiality(e.g.,negativefeedbackfollowedbyasmile).Re-gardingtheSHAPanalysis,mostofthefeaturesthatareconsideredasimportantarecoherentwiththedefinitionoftheclasses(Iforsubjectivizers,sorryforapologizers,justforpropositionalhedges).However,wediscoveredthatfeatureslikeutterance
2167


sizecanalsoserveasindicatorsofcertainclassesofhedges.AlimitationofSHAPisthatitmakesafeatureindependenceassumption,whichpromptstheexplanatorymodeltounderestimatetheimpor-tanceofredundantfeatures(likepronounsinourwork).Inthefuturewewillexploreexplanatorymodelscapableoftakingintoaccountthecorre-lationbetweenfeaturesinthedatasetlikeSAGE(Covertetal.,2020),butsuitedforveryimbal-anceddatasets.Inthedomainofpeer-tutoring,wewouldliketobeabletofurthertestthelinkbe-tweenhedgesandrapport,andthelinkbetweenhedgesandlearninggainsinthesubjectbeingtu-tored.Asnotedabove,thiskindofstudyrequiresafine-grainedcontrolofthelanguageproducedbyoneoftheinterlocutors,whichisdifficulttoachieveinahuman-humanexperience.Wenotethatthehedgeclassifiercanbeusednotjusttoclassify,butalsotoworktowardsimprovingthegenerationofhedgesfortutoragents.Infutureworkwewillexploreusingtheclassifiertore-rankgenerationoutputs,takingadvantageoftherecur-ringsyntacticpatterns(see(ii)inSection5.3)toimprovethegenerationprocessofhedges,andre-generatingclausesthatdon’tcontainoneofthesesyntacticpatterns.AcknowledgmentsManythankstomembersoftheArticuLaboatIN-RIAParisfortheirpreciousassistance.ThisworkwassupportedinpartbythetheFrenchgovern-mentundermanagementofAgenceNationaledelaRechercheaspartofthe“Investissementsd’avenir”program,referenceANR-19-P3IA-0001(PRAIRIE3IAInstitute).ReferencesHeikeAdelandHinrichSchütze.2017.Exploringdif-ferentdimensionsofattentionforuncertaintydetec-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputa-tionalLinguistics:Volume1,LongPapers,pages22–34,Valencia,Spain.AssociationforComputa-tionalLinguistics.ShoshanaBlum-Kulka.1987.Indirectnessandpolite-nessinrequests:Sameordifferent?Journalofpragmatics,11(2):131–146.PenelopeBrownandStephenCLevinson.1987.Polite-ness:Someuniversalsinlanguageusage,volume4.Cambridgeuniversitypress.JudeeKBurgoonandRandallJKoper.1984.Nonverbalandrelationalcommunicationassociatedwithreti-cence.HumanCommunicationResearch,10(4):601–626.IanCovert,ScottMLundberg,andSu-InLee.2020.Understandingglobalfeaturecontributionswithad-ditiveimportancemeasures.AdvancesinNeuralInformationProcessingSystems,33:17212–17223.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforCom-putationalLinguistics:HumanLanguageTechnolo-gies,Volume1(LongandShortPapers),pages4171–4186.RichárdFarkas,VeronikaVincze,GyörgyMóra,JánosCsirik,andGyörgySzarvas.2010.Theconll-2010sharedtask:learningtodetecthedgesandtheirscopeinnaturallanguagetext.InProceedingsofthefour-teenthconferenceoncomputationalnaturallanguagelearning–Sharedtask,pages1–12.BruceFraser.2010.Pragmaticcompetence:Thecaseofhedging.Newapproachestohedging,1534.ViolaGanterandMichaelStrube.2009.Findinghedgesbychasingweasels:Hedgedetectionusingwikipediatagsandshallowlinguisticfeatures.InProceedingsoftheACL-IJCNLP2009ConferenceShortPapers,pages173–176.PranavGoel,YoichiMatsuyama,MichaelMadaio,andJustineCassell.2019.“ithinkitmighthelpifwemultiply,andnotadd”:Detectingindirectnessincon-versation.In9thInternationalWorkshoponSpokenDialogueSystemTechnology,pages27–40.Springer.CynthiaHoward,PamelaW.Jordan,BarbaraMariaDiEugenio,andSandraKatz.2015.Shiftingtheload:apeerdialogueagentthatencouragesitshumancollab-oratortocontributemoretoproblemsolving.Interna-tionalJournalofArtificialIntelligenceinEducation,27:101–129.KenHyland.1998.Hedginginscientificresearcharti-cles,volume54.JohnBenjaminsPublishing.GuolinKe,QiMeng,ThomasFinley,TaifengWang,WeiChen,WeidongMa,QiweiYe,andTie-YanLiu.2017.Lightgbm:Ahighlyefficientgradientboost-ingdecisiontree.Advancesinneuralinformationprocessingsystems,30:3146–3154.KlausKrippendorff.2004.Reliabilityincontentanaly-sis:Somecommonmisconceptionsandrecommen-dations.Humancommunicationresearch,30(3):411–433.GeorgeLakoff.1975.Hedges:Astudyinmeaningcriteriaandthelogicoffuzzyconcepts.InContem-poraryresearchinphilosophicallogicandlinguisticsemantics,pages221–271.Springer.
2168


MatthewLeach.2005.Rapport:Akeytotreatmentsuc-cess.Complementarytherapiesinclinicalpractice,11:262–5.IlyaLoshchilovandFrankHutter.2018.Decoupledweightdecayregularization.InInternationalConfer-enceonLearningRepresentations.ScottMLundberg,GabrielGErion,andSu-InLee.2018.Consistentindividualizedfeatureat-tributionfortreeensembles.arXivpreprintarXiv:1802.03888.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.InProceed-ingsofthe31stinternationalconferenceonneuralinformationprocessingsystems,pages4768–4777.HowardLuneandBruceLBerg.2017.Qualitativeresearchmethodsforthesocialsciences.Pearson.MichaelMadaio,JustineCassell,andAmyOgan.2017.Theimpactofpeertutors’useofindirectfeedbackandinstructions.Philadelphia,PA:InternationalSo-cietyoftheLearningSciences.BenMedlockandTedBriscoe.2007.Weaklysuper-visedlearningforhedgeclassificationinscientificliterature.InProceedingsofthe45thannualmeetingoftheassociationofcomputationallinguistics,pages992–999.ChristophMolnar.2020.Interpretablemachinelearn-ing.Lulu.com.AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesinneuralinformationprocessingsystems,32:8026–8037.JamesWPennebaker,RyanLBoyd,KaylaJordan,andKateBlackburn.2015.Thedevelopmentandpsycho-metricpropertiesofliwc2015.Technicalreport.JeffreyPennington,RichardSocher,andChristopherDManning.2014.Glove:Globalvectorsforwordrep-resentation.InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532–1543.EllenFPrince,JoelFrader,CharlesBosk,etal.1982.Onhedginginphysician-physiciandiscourse.Lin-guisticsandtheProfessions,8(1):83–97.AnnaProkofievaandJuliaHirschberg.2014.Hedgingandspeakercommitment.In5thIntl.WorkshoponEmotion,SocialSignals,Sentiment&LinkedOpenData,Reykjavik,Iceland.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982–3992.OscarJRomero,RanZhao,andJustineCassell.2017.Cognitive-inspiredconversational-strategyreasonerforsocially-awareagents.InIJCAI,pages3807–3813.TimRowland.2007.‘wellmaybenotexactly,butit’saroundfiftybasically?’:Vaguelanguageinmath-ematicsclassrooms.InVaguelanguageexplored,pages79–96.Springer.SaschaSchneider,SteveNebel,SimonPradel,andGün-terDanielRey.2015.Mindyourpsandqs!howpoliteinstructionsaffectlearningwithmultimedia.ComputersinHumanBehavior,51:546–555.TanmaySinhaandJustineCassell.2015.Weclick,wealign,welearn:Impactofinfluenceandconvergenceprocessesonstudentlearningandrapportbuilding.InProceedingsofthe1stWorkshoponModelingINTERPERsonalSynchrONyAndInfLuence,INTER-PERSONAL’15,page13–20,NewYork,NY,USA.AssociationforComputingMachinery.HelenSpencer-Oatey.2005.(im)politeness,faceandperceptionsofrapport:Unpackagingtheirbasesandinterrelationships.1(1):95–119.GyörgySzarvas,VeronikaVincze,RichárdFarkas,GyörgyMóra,andIrynaGurevych.2012.Cross-genreandcross-domaindetectionofsemanticuncer-tainty.ComputationalLinguistics,38(2):335–367.BuzhouTang,XiaolongWang,XuanWang,BoYuan,andShixiFan.2010.Acascademethodfordetectinghedgesandtheirscopeinnaturallanguagetext.InProceedingsoftheFourteenthConferenceonCom-putationalNaturalLanguageLearning–SharedTask,pages13–17.MorganUlinski,SethBenjamin,andJuliaHirschberg.2018.Usinghedgedetectiontoimprovecommittedbelieftagging.InProceedingsoftheWorkshoponComputationalSemanticsbeyondEventsandRoles,pages1–5.TeppoVarttala.1999.Remarksonthecommunicativefunctionsofhedginginpopularscientificandspecial-istresearcharticlesonmedicine.Englishforspecificpurposes,18(2):177–200.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017.Attentionisallyouneed.InAdvancesinneuralinformationpro-cessingsystems,pages5998–6008.ErikVelldal.2011.Predictingspeculation:asimpledis-ambiguationapproachtohedgedetectioninbiomed-icalliterature.JournalofBiomedicalSemantics,2(5):1–14.VeronikaVincze.2014.Uncertaintydetectioninnaturallanguagetexts.PhD,UniversityofSzeged,page141.
2169


NingWang,WLewisJohnson,RichardEMayer,PaolaRizzo,ErinShaw,andHeatherCollins.2008.Thepolitenesseffect:Pedagogicalagentsandlearningoutcomes.Internationaljournalofhuman-computerstudies,66(2):98–112.ShawnWarner-Garcia.2014.Laughingwhennothing’sfunny:Thepragmaticuseofcopinglaughterinthenegotiationofconversationaldisagreement.Prag-matics,24(1):157–180.TimothyWilliamson.2002.Vagueness.Routledge.RanZhao,AlexandrosPapangelis,andJustineCassell.2014.Towardsadyadiccomputationalmodelofrap-portmanagementforhuman-virtualagentinteraction.InInternationalConferenceonIntelligentVirtualAgents,pages514–527.Springer.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016a.Automaticrecognitionofconversa-tionalstrategiesintheserviceofasocially-awaredialogsystem.InProceedingsofthe17thAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages381–392.RanZhao,TanmaySinha,AlanWBlack,andJustineCassell.2016b.Socially-awarevirtualagents:Au-tomaticallyassessingdyadicrapportfromtemporalpatternsofbehavior.InInternationalconferenceonintelligentvirtualagents,pages218–233.Springer.AAdditionalinformationontheexperimentalsettingsWeusedPyTorch(Paszkeetal.,2019)toimple-menttheneuralmodels.Foreachsetoffeatures,hyperparameterswereselectedusingOptuna(Ak-iba,2019),aparametersearchframework.Were-implementedtheAttention-CNNwithGlove(Pen-ningtonetal.,2014)300-Dwordsembeddingsasthevectorrepresentation.Foreachmodels,theresultsarecross-validatedusing5folds(wechose5insteadof10toavoidhavingfoldswithtoofewsamplesperclass).Wecorrectedthelossfunctionforclassimbalancetoforcethemodeltoadaptmoretothelessfrequentclasses.Thestrengthofthiscorrectiondependedonthemodel,andwasselectedbecauseitprovidedasatisfyingcompro-misebetweenfavoringrecallandprecisionintheclassificationresultsofthatmodel.ForLightGBM,a"squarerootofthesquarerootoftheinverseclassproportion"correctionwasselected.Neu-ralmodelsweretrainedusingAdamWasanop-timizer(LoshchilovandHutter,2018),andusedareducedfeaturevector,obtainedwiththeap-plicationofPCA(dinit=1800;d=100;99.8%oftheinformationisconserved).Nosignifi-cantperformancedifferenceswereobservedbe-tweentheoriginalvectorandthereducedvectorfortrainingthemodels.TocomputetheSHAPvaluesmentionedinthepaper,wekeptonesplittoperformthe5-splitofthedataset,andleave1splittovalidateandearlystopthemodel,inor-dertoavoidoverfitting.Acompleteconfigura-tionofhyperparametersusedforeachmodelisre-portedintheGitHubrepositorywiththecodeofthepaper:https://github.com/YannRaphalen/Hedges-Detection.TheBERTmodelwasfine-tunedonaNvidiaQuadroRTX8000GPU.BTables
2170


?(whether|if|is|that|it|this)?.*Subj.
RBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)
RBMLP(KDF)MLP(PTE)MLP(K+P)CNN(PTE)LSTM(KDF)LSTM(PTE)LSTM(K+P)BERT(PTE)LGB(KDF)LGB(PTE)LGB(K+P)
NoNoYesNoNoNoYesYesYesYesYesBERT(PTE)
exceptionally|forthemostpart|inamannerofspeaking|
NoYesNoNoNoYesNoNoYesYesNoMLP(KDF)
somethingbetween|essentially|only).*Prop.
Table8:Significancetableforthe3-classespartofTable4."Yes"meansthatthedifferenceisstatisticallysignificant.
YesYesYesYesYesNoYesYesYesNoYesMLP(KDF+PTE)
technically|typically|virtually|approximately|
YesYesYesYesYes+1-gramand2-gram
LabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal
LabelRB
much|somewhat|exactly|almost|littlebit|quite|
Class
relatively|roughly|sotosay|strictlyspeaking|
NoNoYesNoNoYesNoYesYesYesYesLSTM(KDF)
YesNoNoNoNo+TM
Table10:Significancetableforthe3-classespartofTable5."Yes"meansthatthedifferenceisstatisticallysignificant.
NoNoYesYesYesYesYesYesNoYesYes
.*(it)(looks|seems|appears)[,]?.*",".*(or|and)(that|something|stuff|soforth)Table7:Regexprulesusedfortheclassifier.
Rule-based
Rule-based
.*(you(might|may)(believe|think)).*Subj.
YesYesYesYesYesYesYesYesYesYesYesLSTM(KDF+PTE)
YesYesNoYesYesYesYesYesYesYesYesLSTM(KDF+PTE)
NoNoYesNoNoYesNoNoYesYesYesLSTM(KDF)
YesYesNoYesYesYesYesYesYesYesYesLGBM(KDF+PTE)
.*(i|i’m|you|it’s)(am|are)(apparently|surely)[,]?.*Prop.
NoNoYesNoNoYesNoYesYesYesYesLSTM(PTE)
NoNoYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)
NoNoYesYesNoYesYesYesYesYesNoLGBM(KDF)
NoNoYesYesYesYesYesYesYesYesNoLGBM(KDF)
YesNoNoNoNo+Nonverbal
YesNoNoNoNo+LIWC
YesNoYesYesYesYesYesYesNoYesYes
YesYesYesYesYesYesYesYesYesYesYesMLP(KDF+PTE)
regular|regularly|actually|almost|asitwere|basically|
NoYesYesNoYesYesYesNoYesYesNoMLP(PTE)
Rule(regexp)
.*(i|i’m|we)?(was|am|wasn’t)??(not)?(sure|certain).*Subj.
.*(accordingto|presumably).*Subj.
(?!.*(be|been|was)likeexcuseme)((excuseme|sorry)[w,’]+|[w,’]+(excuseme|sorry))Prop.
.*(i|you|we)haveto(check|look|verify).*Subj.
.*(i’m|i|we’re)(am|are)??(apologize|sorry).*Apol.
onthetallside|parexcellence|particularly|
(?!what).*(i|we)?(don’t|didn’t|did)??(not)?
inarealsense|inasense|inaway|largely|literally|
YesYesYesNoNoNoYesYesYesYesYesBERT(PTE)
Models
Models
Models
YesYesYesNoNoYesNoNoYesYesYesLSTM(PTE)
YesNoNoNoNo
YesYesYesNoNoYesNoYesYesYesYesAttention-CNN(PTE)
Subj.
Table9:Significancetableforthe4-classespartofTable4."Yes"meansthatthedifferenceisstatisticallysignificant.
NoYesNoNoNoYesNoNoYesYesNoMLP(PTE)
YesYesYesYesYesYesYesYesYesYesYesLGBM(KDF+PTE)
.*(ifeellikeyou).*Subj.
looselyspeaking|kinda|moreorless|mostly|often|
.*(unlessi).*Apol.
probably|canbeviewas|crypto-|especially|essentially|
.*(ifi’mnotwrong|ifi’mright|ifthat’strue).*Subj.
YesNoNoNoNo+POS
(guess|guessed|thought|think|believe|believed|suppose|supposed)
.*(just|alittle|maybe|actually|sortof|kindof|pretty
YesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)
YesYesYesYesYesYesYesYesYesYesYesLGBM(PTE)
NoYesYesNoYesYesNoNoYesYesYesMLP(KDF)
prettymuch|principally|pseudo-|quintessentially|
2171


Figure2:Absoluteaveragedfeaturecontribution,asindicatedbySHAP.Thelongerthebarisforonecolor,themorethefeatureisassociatedwiththeclassrepresentedbythatcolor.
Figure3:Averagedcontributionoffeaturestothedetectionofthe"Notindirect"class,asindicatedbySHAP.Eachdotcorrespondstoaclassifiedclause.Areddotindicatesthatthefeatureispresentintheclause,whileabluedotindicatesthatthefeatureisabsent.Thefartherontherightthedotis,themorethefeaturecontributedtoitsclassificationasahedge.
2172


Figure4:Averagedcontributionoffeaturestothedetectionof"Apologizers",asindicatedbySHAP.
Figure5:Averagedcontributionoffeaturestothedetectionof"Propositionalhedges",asindicatedbySHAP.
2173


YesYesYesYesYes+1-gramand2-gram
LabelRB+1-gramand2-gram+POS+LIWC+TM+Nonverbal
LabelRB
YesNoNoNoNo+TM
YesNoNoNoNo+Nonverbal
YesNoNoNoNo+LIWC
Figure6:Averagedcontributionoffeaturestothedetectionof"Subjectivizers",asindicatedbySHAP.
Models
YesNoNoNoNo
Table11:Significancetableforthe4-classespartofTable5."Yes"meansthatthedifferenceisstatisticallysignificant.
YesNoNoNoNo+POS
2174