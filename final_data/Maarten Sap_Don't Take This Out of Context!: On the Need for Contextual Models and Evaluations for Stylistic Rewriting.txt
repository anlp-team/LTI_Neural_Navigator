Or make it less toxic / change to positive sentiment
Rewrite it as formal
“Don’tTakeThisOutofContext!”OntheNeedforContextualModelsandEvaluationsforStylisticRewritingAkhilaYerukola♡XuhuiZhou♡ElizabethClark♢MaartenSap♡♣♡LanguageTechnologiesInstitute,CarnegieMellonUniversity♢GoogleDeepMind♣AllenInstituteforAI#ayerukol@andrew.cmu.eduAbstractMostexistingstylistictextrewritingmethodsandevaluationmetricsoperateonasentencelevel,butignoringthebroadercontextofthetextcanleadtopreferringgeneric,ambigu-ous,andincoherentrewrites.Inthispaper,weinvestigateintegratingtheprecedingtextualcontextintoboththerewritingandevaluationstagesofstylistictextrewriting,andintroduceanewcompositecontextualevaluationmetricCtxSimFitthatcombinessimilaritytotheorig-inalsentencewithcontextualcohesiveness.Wecomparativelyevaluatenon-contextualandcon-textualrewritesinformality,toxicity,andsenti-menttransfertasks.Ourexperimentsshowthathumanssignificantlyprefercontextualrewritesasmorefittingandnaturalovernon-contextualones,yetexistingsentence-levelautomaticmet-rics(e.g.,ROUGE,SBERT)correlatepoorlywithhumanpreferences(ρ=0–0.3).Incontrast,humanpreferencesaremuchbetterreflectedbybothournovelCtxSimFit(ρ=0.7–0.9)aswellasproposedcontext-infusedversionsofcom-monmetrics(ρ=0.4–0.7).Overall,ourfindingshighlighttheimportanceofintegratingcontextintothegenerationandespeciallytheevalua-tionstagesofstylistictextrewriting.1IntroductionExistingmethodsforstylistictextrewriting,i.e.,adaptingthetexttoaparticularstylewhilepreserv-ingitsoriginallyintendedmeaning,oftenfailtoac-countforastatement’scontext(e.g.,Huetal.,2017;Shenetal.,2017;Fuetal.,2018;Lietal.,2018;Lampleetal.,2019;Madaanetal.,2020;Hallinanetal.,2023).Asaresult,thesesystemsmaychangethespeakers’originalcommunicativeintentsandgeneratecontextuallyirrelevantandgenericout-puts.Forexample,inFigure1,anon-contextualmodelrewritinganinformalresponsetoaformalonesimplyreplaceswordswithmoreformalsyn-onyms,whereasacontextualrewritingmodelcanusethebroaderconversationalcontexttoproduceamorespecificandnaturalformalrewrite.
Figure1:Exampleofusingtheprecedingdialogutter-ancetohelpwithstylisticrewriting:here,wetransformaninformalresponseintoformallanguage.Incorporat-ing“workload”and“overwhelming”enhancesthecon-textualcohesivenessoftherewrittentext,whilesolelyusing“inundated”resultsinamoregenericrewrite.Similarly,precedingtextualcontexthaslargelybeenoverlookedinautomaticevaluationsforstylis-ticrewriting,withmostworkfocusingonsentence-levelmetrics(e.g.,Lietal.,2018;Reifetal.,2022).ThislackofcontextatthemodelingandevaluationstageshindersthecreationofeffectiveAI-assistedrewritingtoolsforusers(e.g.,forassistivewritingtools;MacArthur,2009;Clarketal.,2018).Inthispaper,wepresentacomprehensiveanaly-sisoftheneedforcontextinstylisticrewritinganditsevaluation,onthreedifferentrewritingtasks(formalitytransfer,sentimentchange,andtextdetoxification)andtwotypesoftextualcon-texts(precedingturnsinaconversation,precedingsentencesinadocument).Tostudytheseeffects,wedesignacontextualhumanevaluationframe-work(§5)tocomparativelyevaluatenon-contextualandcontextualrewritingmethodsbuiltonfew-shotpromptedlargelanguagemodels(§4).
Contextual rewriteIndeed, I concur. The workload is quite overwhelming.
Informal response: “I know, right? I'm drowning in them.”
Non-contextual RewriteIndeed, I concur. I am inundated with them.
Preceding Dialog: “I can't believe how much work we have to do.”
11419 Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11419–11444 December 6-10, 2023 ©2023 Association for Computational Linguistics


Weshowthathumanevaluatorsprefercontex-tualrewritesintermsofnaturalness,stylestrength,andintendedmeaningpreservation,acrossallthreetasks.However,non-contextualautomaticmet-ricsforlexicalorsemanticmeaningpreservationcorrelatepoorlywiththesepreferences(ρ=0–0.3;§6),despitebeingcommonlyusedinpreviousstyletransferworktomeasuremeaningpreservation(Miretal.,2019;Madaanetal.,2020).Toaddresstheneedforcontextinautomaticeval-uations,weintroduceCtxSimFit,anewcompositemetricthatcombinesoriginalsentencesimilarityandcontextualcohesivenesstoevaluatethequalityofrewrites,takingintoaccounttheprecedingcon-text(§7).Additionally,weproposecontext-infusedversionsofcommonlyusedautomaticmetricsformeaningpreservation.Ourresultsshowthathu-manpreferencesaresignificantlycorrelatedwiththesecontextualmetrics—especiallyCtxSimFit(ρ=0.7–0.9),muchmorethannon-contextualones.Ourcontributionsaresummarizedasfollows:(1)Weinvestigatetheneedforcontextintextrewriting,showingthatincorporatingit,whetheratthedocu-mentorconversationallevel,leadstocontextuallycoherentandrelevantrewritespreferredbyhumanannotatorsacrossstyletransfertasks.(2)Wecon-ductacomprehensiveanalysisontheneedforcon-textinautomaticevaluation,revealingthatexistingmetricsdon’talignwithhumanpreferences.(3)Weproposeacustommetric,CtxSimFit,alongwithcontext-infusedversionsofcommonauto-maticmetrics,tobridgethegapbetweencontextualunderstandingandautomatedmetrics.Overall,ourcontributionsprovideamorenuancedunderstand-ingoftheimportanceofcontext,whichiscriti-calfordevelopmentofmoreeffectiveandreliablestylistictextrewritingtechniques.2Background&RelatedWorkInthissection,wediscusstheincreasinginterestinincorporatingcontextintoNLPtasksandmotivatethesignificanceofcontextduringtherephrasingandevaluationphasesofstylistictextrewriting.StylisticTextRewritingDespitebeingintro-ducedovertenyearsago(Xuetal.,2012),cur-rentmethodsforstylisticrewriting(e.g.Shenetal.,2017;Xuetal.,2018b;Fuetal.,2018;Lampleetal.,2019;Jinetal.,2022;ChawlaandYang,2020;Yerukolaetal.,2021;Daleetal.,2021;Lo-gachevaetal.,2022,etc.)stillrelysolelyonparal-lelsource-to-targetsentencepairs,primarilyduetoalackofdatasetsthatincludecontextualinforma-tion.Whilenewmodelshaveemergedthatdonotrequireparalleldatafortraining(Huetal.,2017;Lietal.,2018;Maetal.,2020;Hallinanetal.,2023),theyalsooperatewithoutcontex-tualinformation.Buildingonsomepreliminaryresearchthatexploredcontextinsmallcustom-trainedseq2seqrewritingmodels(Chengetal.,2020;Atwelletal.,2022)andlargelanguagemod-elsforexemplar-basedconversation-levelrewriting(Royetal.,2023),weextendtheinvestigationtolargelanguagemodelswithdefinedstyleattributeslikeformality,toxicity,andsentiment.Importantly,wealsoexploretheneedforcontextinevaluationsinadditiontomodeling,andproposeanewsuiteofcontextualizedmetricsforautomaticevaluation.EvaluationofStylisticTextRewritingEvaluat-ingwhethersentencerewritingpreservesmeaningwhileachievingthedesiredtargetstylehasprovedchallenging.Existingmetricsandapproachescandisentanglemeaningandstyle(Mukherjeeetal.,2022;Yuetal.,2021).However,determiningwhatconstitutes“meaningpreservation”remainsincon-sistent.Someworks(Lietal.,2018;Sudhakaretal.,2019;Miretal.,2019;Reifetal.,2022;Madaanetal.,2020)usemetricssuchasBLEU,ROUGE,andMETEOR,whichmeasuren-gramoverlapsandlex-icalsimilarityasindicatorsofmeaningpreserva-tionrespectively,whileotherstudies(Wangetal.,2019;ReidandZhong,2021;Royetal.,2023)adoptmetricslikeSBERTandBERTScoremeasur-ingsemanticsimilarityofembeddingsasproxiesformeaningpreservation.Further,themajorityofwork(Huetal.,2022;Madaanetal.,2020;Lietal.,2018)doesnotprovideannotatorswithanypreced-ingcontextduringhumanevaluations.Thus,morestandardizedandcontext-awareevaluationmetricsareneededfortextrewritingapproaches.3TaskandDatasetsTomeasuretheimportanceofcontextinrewriting,wescopeourinvestigationsaroundthreespecificat-tributecontrols:formality,sentiment,andtoxicity,chosenbecausetheynecessitatevaryingdegreesofmeaningpreservationandstyleintensity.WepresentstatisticsforeachofthedatasetsusedinourrewritingtasksinTable1.3.1Tasks&DatasetsChangingFormalityFormalitytransfer(RaoandTetreault,2018)aimstotransformsentences
11420


Or
Open sourced
Table1:Statisticsofthecollecteddatasets,presentedbytaskandcontexttype,consideringbothprecedingsentencesinadocumentandturnsinaconversation.frominformalorcasuallanguageintoformallan-guage,andviceversa.Thisrequiresmakingstylis-ticadjustmentswhileensuringthattheoriginalcontentandintentionremainintact.WeuseaconversationaldatasetfromReddit1andcuratedadocument-baseddatasetfromCNNDailyMail(formal;Nallapatietal.,2016)andtheBlogAu-thorshipCorpus(informal;Schleretal.,2006).RewritingSentimentForsentimenttransfer(Huetal.,2017),ourfocusliesinconvertingsentenceswithpositivesentimenttonegativesentiment,andviceversa,aswellastransformingneutralsen-tencestoconveypositiveornegativesentiment.Here,boththecontentandintentionarealtered;however,themainsubjectentitiesremainconsis-tent,althoughwithachangeinsentiment.WeobtainaconversationaldatasetfromtheDailyDia-log(Lietal.,2017)datasetandadocument-baseddatasetfromYelpreviews(Zhangetal.,2015).De-toxifyingTextHere,ourobjectiveistorewritetextinamannerthatreducestoxicity,asintroducedbyNogueiradosSantosetal.(2018).Rewritingmaymodifytheoriginalcontent,buttheinitialintentshouldbepreservedandconveyedusinglessoffensivelanguage.Inthistask,weexaminethreeconversationaldatasets:theCivilCommentsinContext(CCC)dataset(Xenosetal.,2021),theMulti-LabelDialogueMalevolenceDe-tection(MDMD)dataset(Zhangetal.,2022),andtheProsocialDialogdataset(Kimetal.,2022).3.2DataPreparationForconversationaldatasets(asdepictedintheex-ampleinFigure1),wefocusontwo-turns,repre-sentingparentcontextandresponseforrewriting.
1Weusereddit-corpus-smallfromhttp://convokit.cornell.edu/documentation/subreddit.html
TaskContextTypeDatasets#Instances
Preceding Document/Dialogue
Noncontextual rewrite
2https://huggingface.co/s-nlp/roberta-base-formality-ranker3https://perspectiveapi.com/
Original Sentence/Utterance
Original Sentence/Utterance
Original Sentence/Utterance
Contextual rewrite
GPT-NeoX
ToxicityConversationCCC1000ConversationMDMD900ConversationProsocialDialog1000
FormalityConversationReddit1000DocumentCNNDailyMail+BlogAuthorship1000
&
SentimentConversationDailyDialog1000DocumentYelpReviews1500
Random Document/Dialogue
Random contextual rewrite
Contextual Eval
Non-contextual Eval
Figure2:Overviewofourapproach:Weexaminethreekindsofrewrites-contextual,non-contextual,andran-domcontextualrewrites.GPT-3.5andGPT-NeoXareutilizedforrewritingviain-contextlearning.Evaluationmethodsconsistofnon-contextualevaluation,whichdoesnotconsidercontext,andcontextualevaluation,whichincorporatescontextintotheassessmentprocess.Fordocument-baseddatasets,weselectthreecon-textsentencesandoneforrewriting.Welabelthecontextandresponseusingpre-trainedstyleclassifiers:RoBERTa-Baseformal-ityclassifier,2XLM-RoBERTa-Basesentimentclassifier(Barbierietal.,2022)andtoxicityscoresfromPerspectiveAPI,3HateBert(Tommaso-CaselliandJelenaMitrovic,2021)andHateRoberta(Hartvigsenetal.,2022).Weselectastratifiedsam-plethatincludesawiderangeofstylestrengths.SeeAppendixAfordatasetsandclassifiersdetails.4ModelingContextinRewritingInthissection,weintroduceourmethodologyforcontextualstylisticrewritingutilizinglargelan-guagemodels(LLMs)andin-contextlearning.Weconductacomparisonofthreetypesofrewrites:thosegeneratedwithcontext,thosegeneratedwith-outcontext,andthosegeneratedwitharandomcontext(asacounterfactualbaseline).Figure2providesavisualrepresentationofourapproach.4.1ContextualRewritingToaddressthechallengeofinsufficientparalleldata,asdiscussedin§2,weproposeacontextual
GPT 3.5
11421


(f)(cid:220)mythroatfeelsweirdanddryiquit(cid:215)@USERoof.see,therearedownsidestosuckingdick.GPT-3.5lesstoxic(cid:215)@USERIhopeyoufeelbettersoon.(cid:215)@USER,therearenegativeaspectstoperformingoralsex.
Table2:ExamplesfromourdatasetsandcorrespondingrewritesgeneratedbyGPT-3.5andGPT-NeoX,forallthreetasks:formalitychange,de-toxification,andsentimenttransfer.@=documentcontext(truncated),(cid:15)=sentencetorewrite,(cid:15)=contextualrewrite,(cid:15)=non-contextualrewrite;(cid:220)=previousturninconversation,(cid:215)=responsetorewrite,(cid:215)=contextualrewriteofresponse,(cid:215)=non-contextualrewriteofresponse,"Style"headerreferstothestyleconversion.rewritingmodelthatutilizesthein-contextlearningcapabilitiesofLLMs,inspiredbyapproachespre-sentedinReifetal.(2022)andRoyetal.(2023).Weconductfew-shotpromptingexperimentswithtwoLLMs:GPT-3.54(Ouyangetal.,2022)andGPTNeoX5(Blacketal.,2022).Eachexampleincludestheprecedingcontext,theoriginalinputwithaspecifiedstyle,andtherewriteinanotherstyle,factoringinthecontext.ForGPT-3.5,weuse2few-shotexamplestoobtainrewritesinthedesiredformat,whileforGPT-NeoX,weuse10examples.SeeAppendixBformoredetails.4.2Non-contextualRewritingWeareinterestedincomparingcontextualrewriteswithnon-contextualrewritesthatdonotdependonpriorcontext.Togeneratenon-contextualrewrites,
4Weusetext-davinci-0035Weusethe20BparametermodelweemployLLMstorewriteanoriginalsentencefromonestyletoanother.Similartocontextualrewriting,wemanuallyconstructfew-shotexam-plesthatsolelyconsistoftheoriginalsentencetoberewritten,aninstructionalpromptspecifyingthedesiredstyle,andanexamplerewrite,withoutanyprecedingcontext.4.3RewritingwithaRandomContextTodemonstratetheimportanceofincorporatingcontextualinformationintherewritingprocess,weemployabaselinemethodthatgeneratesrewritesusingarandomcontext.Thisapproachservestwokeypurposes:first,itassessesthecontextualsensi-tivityofautomaticmetrics;andsecond,itensuresthatourcontextualrewritingmethodeffectivelyac-countsforthegivencontext.Inourexperiments,werandomlypickacontextfromourdatasetin-steadofusingthetrueprecedingcontext.
ContextOriginalSentenceModelStyleContextualRewriteNon-contextualRewrite
(e)(cid:220)It’sremarkablehowtheBiblehasrecordedhistoricalevents,isn’tit?(cid:215)YouappeartobesayingtheBibleisareliablesourceforhistory.Itisn’t.Andifyoudisagree,gotoRSNwhereyouwillbeshotdownliterallywithfire.GPT-NeoXlesstoxic(cid:215)YouseemtothinkthattheBibleisanaccuratesourceforhistory.Whileitmaybeasacredcollectionoftexts,itisnotare-liablesourceforhistoricalfacts.Forfurtherdiscussion,IsuggestvisitingtheRSNforums.(cid:215)IdisagreewiththeideathattheBibleisareliablesourceforhistory.Ifyouhaveotheropin-ions,pleasegotoRSNtobedis-heartened
Formality(a)(cid:220)BestBondmovieever!ShittiestBondthemeever!!!(cid:215)Haven’tseenSpectre,haveyou?Forthelatter,nottheformer.GPT-3.5formal"informal(cid:215)Haven’tyouseenSpectre?Itmaynothavehadthebestthemesong,butit’sdefinitelyoneofthebestBondmovies!(cid:215)Haven’tyouseenSpectre?Notyou,butsomeoneelse.
(c)(cid:220)WhatexactlyhasTaylorSwiftdonethatmakesanyonewantto"smackher?"Thisisn’tfunnyanddoesn’tevenmakesense.(cid:215)For$835,000,000,000I’dsmackherthough.ForthatmuchmoneyI’dsmackmygrandmother.GPT-3.5informal"formal(cid:215)Afinancialincentiveof$835billionmightleadmetoconsiderstrikingTaylorSwift.Withsuchanenormousamountatstake,Iwouldevenbeinclinedtostrikemyowngrandmother.(cid:215)Foreighthundredandthirty-fivebilliondollars,Iwouldbein-clinedtocommittheactofphys-icalassaultwithregardstomygrandmother.
Detoxification(d)(cid:220)Soifazombieapocalypseoccurred,whatwouldyoudo(cid:215)@USERfuckitI’mkillingmyselfGPT-3.5lesstoxic(cid:215)I’drathernotthinkaboutazombieapocalypse.(cid:215)Iamfeelingoverwhelmedandhopeless,soI’lldie
Sentiment(g)(cid:220)ShallIseeyouonFridaymorning?(cid:215)That’snotgoodformeatall.It’llhavetobeanothertime.GPT-3.5negative"positive(cid:215)Fridaymorningdoesn’tworkforme,butlet’splanforanothertime.(cid:215)That’sgreatforme.Let’splanforanothertime.
(i)(cid:220)Letmesee.Weshouldgotobuysomefruitfirst.(cid:215)Itisheavyandnotconvenienttotakethemtothetop.GPT-3.5negative"positive(cid:215)Buyingfruitisagreatidea!Itwillbeanice,refreshingsnacktohaveonourwayup.(cid:215)Itislightandveryconvenienttotakethemtothetop.
(b)@Iamreadingabook,777MathematicalConversationStartersbyJohndePillis.Itsaninterestingbookwithlotsofquotes,poems(cid:15)Herezsomeofthem,andfurtherIwillkeepupdatingthispagewithwhatIlike.GPT-NeoXinformal"formal(cid:15)Herearesomeofthem,andinalaterpostIwillkeepupdat-ingthispagewithmyfavouritequotesfromthebook.(cid:15)HereissomeoftheoneandfurtherIwillupdatethispagewithwhatelseIfindinteresting.
(h)@NotonlydidTimfixmycarquicklyandinexpensively,heactuallycametopickituphimselfsothatIwouldn’thavetospendmoneyonatowtruck!(cid:15)He’salsoanextremelyfriendly,funnyguywhoputsyouateaseimmediately.GPT-NeoXpositive"negative(cid:15)However,Tim’snotveryfriendlyorhumorouswhichmightmakeyoufeeluneasy(cid:15)Heisalsoaveryrude,una-musedguywhomakesyoufeelanxious.
11422


6WeoptednottouseYelpreviewsinoursamplingduetodifficultiesencounteredduringpilotexperiments.Annotatorsfoundittoughtoselectrewritesthatretainedmeaningwhileeffectivelytransferringsentiment,suchasfrompositivetoneg-ative.Generally,evencontextsclassifiedas“neutral”seemedpositivewhenpartofanoverallpositivereview,complicatingtheannotators’abilitytoagreeontherewrites’effectiveness.7equalnumberfrombothRedditandCNN/DailyMail+BlogAuthorshipCorpus8equalnumberofexamplesfromCCC,MDMDandProso-cialDialogwhichwerescoredashighlytoxicbyallthreetoxicityclassifiers-hateroberta,hatebertandPerspectiveAPIworkersonAmazonMechanicalTurk(MTurk)andqualifiedthemusingapre-qualificationtestforeachtask(SeeAppCforqualificationdetails).AgreementWeemploythreeannotatorstorankeachpairofrewrites.Averagingacrossthreetasks,ourannotatoragreementwasKrippendorff’sα=0.43andFleiss’sκ=0.31.Fordimension-specificannotatoragreements,pleaserefertoTa-bles7—9inAppC.1.Weobtainthefinalhumanjudgmentpreferencesusingmajorityvotingofthethreeannotators.5.2HumanEvaluationResultsOurresultsshowthatannotatorsprefercon-textualrewritesovernon-contextualrewritesacrossallthreetasksandcontexttypes(Figure3).Thiseffectisespeciallypronouncedforformal-ityandtoxicity(see(a)–(f)inTable2).ContextualrewritesaremorenaturalandfittingThesuccessrateforcontextualrewritesintoxicityandformalitycaseswasapproximately50%,whilethatfornon-contextualrewriteswascloseto20%and30%,respectively(p<0.1).9Regardingsenti-ment,thesuccessrateforcontextualrewriteswasaround35%asopposedtonon-contextualrewriteswithasuccessrateofabout30%(p>0.1).ContextualrewritesbetterpreservetheintendedmeaningContextualrewritesbetterpreservetheauthor’sintention,tone,andimpliedmeaningmoreeffectively(p<0.1).Inthedetoxificationtaskexample(d)showninTable2,theuser’sintendedmeaningisnotaboutactuallykillingoneselfbutratheraboutavoidingthezombieapocalypse.Thecontextualrewritecapturesthismeaningmoreef-fectivelycomparedtotheliteralrephrasingpro-videdbynon-contextualrewriting.Contextualrewritesstrugglewithpreservingevent-levelsimilarityExamples(a),(f),and(i)inTable2demonstratethatcontextualrewritesof-tenincludeextraentity/eventdetails,whilenon-contextualrewritesalignmorecloselywiththeoriginalsentenceatann-gramlevel.10Despitethis,annotatorsstillprefercontextualrewritesfortheirnaturalnessandfit,indicatingthatextraevent
9p<0.1,CI=90%usingabinomialtestandsplittingthe‘tie’optionevenlybetweencontextualandnon-contextualpreferences.10Event-levelsimilarityistheonlydimensionwhichshowsnosignificantdifferencesbetweencontextualandnon-contextualrewritesforallthreetasks.
5ContextualHumanEvaluationSinceinrealisticrewritingscenarios,contextwillalwaysbeavailableandcrucialtouserswhowishtorewritetheirdialogueutterancesorstorysentences(Atwelletal.,2022),westartbyconductingacon-textualhumanevaluationtogaugeuserpreferencesbetweennon-contextualandcontextualrewrites.Thiscontextualhumanevaluationisadeparturefrommostpreviousworkwhichhaspredominantlynotusedcontext(§2).5.1ExperimentalSetupWeconductahead-to-headhumanevaluationofnon-contextualandcontextualrewritesinthepres-enceofprecedingtextualcontext,followingthesetupinKiritchenkoandMohammad(2017).Par-ticipantsaregivenprecedingcontext,pairsofrewrittensentences(non-contextualandcontex-tual),andthedesiredstyleattribute.Theyarethenaskedtoranktherewriteswithrespectto:•Naturalness:whichrewritedotheannotatorsprefer/whichoneappearsmostnatural•StyleStrength:whichrewritebestachievestherequiredstyle,independentofmeaningchanges•Event-levelSimilarity:whichrewritemostef-fectivelyretainstheessentialevents,entities,andrelationspresentintheoriginalsentence,with-outconsideringtheprecedingcontext•IntendedMeaning:whichrewritemosteffec-tivelypreservesandconveystheoriginalsen-tence’soverallmessageorintendedmeaning•OverallFit:whichrewriteisoverallmostsuit-ableorrelevantinrelationtothegivencontextWesample100examplesforsentimentfromDailyDialog,6100examplesforformality,7and90examplesfortoxicity8,focusingonthosewiththehigheststylestrengthineachcategory(e.g.,50mostformaland50mostinformal).Weconductsignificancetestingforallthreetasks.Werecruited
11423


Non-Contextual(b)Toxicity
IntendedMeaning
IntendedMeaning
IntendedMeaning
EventSimilarity
EventSimilarity
EventSimilarity
Non-Contextual(a)Formality
Naturalness
Naturalness
Naturalness
StyleStrength
StyleStrength
StyleStrength
Fit0.450.380.400.360.390.160.170.220.310.250.390.450.390.330.36
Non-Contextual(c)SentimentFigure3:Head-to-headhumanevaluationwithcontextforallthreetasks-formalitychange,detoxification,andsentimenttransfer.Contextualrewritesaregenerallyfavoredovernon-contextualrewritesacrossalltasks,particularlyintermsofstylestrength,preservationofintendedmeaning,naturalness,andoverallcoherencewiththeprecedingcontext.Thenumbersonthebarsrepresenttheproportionofpreferencesforeachrespectivecategory.detailsareacceptableaslongastheyfitappropri-atelywithinthecontext.SentimentStyleTransferMightbeIll-definedThetrendsinthesentimentstyletransfertaskarelesspronouncedthaninothertasks(p>0.1foralldimensions)andshowloweragreementcomparedtotoxicityandformality(seeTable9inAppC.1).Example(g)inTable2highlightsthechallengesinsentimenttransferduetotheinherentneedformeaningchangeswhilepreservingtheoriginalin-tent(especiallyforreviewswhichwerewrittenspecificallytocommunicatesentiment;Yuetal.,2021).Thiscomplicationleadstoinconsistencies,resultinginannotatorshavingdifficultyreachingaconsensusonmeaningpreservation,asevidencedbyloweragreementrates(Table9).6Non-contextualAutomaticEvaluationOverall,ourcontextualhumanevaluationsrevealageneralpreferenceforcontextualrewritesovernon-contextualones.Giventhatpriorworkpri-marilyevaluatedutterance-levelrewritesinbothhumanandautomaticevaluations,itraisestheques-tionofhowwellnon-contextualautomaticmetricsmirrorhumanpreferences.Inthissection,wein-vestigatecommonlyusedmetricsinpreviouswork(Miretal.,2019;Huetal.,2022)formeaningpreservation,stylestrength,andfluency.6.1MetricsConsideredWedistinguishtwotypesof“meaningpreservation”metrics,namely,lexicalandsemanticsimilaritybetweenarewriteXandtheoriginalinputI.StyleStrengthFollowingpreviousstudies(Lietal.,2018;Madaanetal.,2020),weassessstylestrengthofrewrittentextbyexaminingtheproba-bilitiesofthetargetstylesunderourstyleclassifier.LexicalSimilarityWeuseword-overlapmet-ricslikeROUGE(Lin,2004),METEOR(BanerjeeandLavie,2005)andworderrorrate(WER;ZechnerandWaibel,2000),forlexicalsimilarity(Lexical).SemanticSimilarityTomeasuresemanticsimi-larity(Semantic),weuseBERTScore(Zhangetal.,2019)andSBERT(ReimersandGurevych,2019),asemployedinpreviouswork.WealsoconsiderSmatch(CaiandKnight,2013),whichcomparesthesimilaritybetweentwoAbstractMeaningRep-resentation(AMR)graphs,providingadistinctive,structuredviewonsemanticrelatednessnotconsid-eredinpriorrewritingstudies.FluencyToassessfluency,weemployalanguagemodel,specificallyGPT-2(Radfordetal.,2019),anduseperplexity(pplx)asthemetric,inlinewithpreviousresearch(Holtzmanetal.,2018;Xuetal.,2018a;Maetal.,2020).6.2Non-ContextualEvaluationResultsInouranalysis,weevaluatetheperformanceofbothGPT-3andNeoXmodelsinproducingnon-contextualrewrites,contextualrewrites,andrewritesgeneratedwitharandomprecedingcon-text.Wepresentaggregateresultsoftheperfor-manceinTable3acrossalltasks,datasets,andmetrics.Fordetailedresultsonindividualtasksanddatasets,wereferthereadertoAppendixD.Non-contextualrewritesaremoresimilarinmeaningtotheoriginalinputsentencecom-paredtocontextualrewritesUtterancelevellexicalandsemanticmeaningpreservationmet-ricsscorenon-contextualrewriteshigher,acrossallthreetasksandthetwotypesofcontext(seeTa-bles16–22inAppendixD).Additionally,wefindthatourpatternsareconsistentforbothGPT-3.5andNeoX,thoughwenoteamarkeddecreaseinperformancefromGPT-NeoX.Thissuggeststhatmodelsthatedittheoriginalsentencemore(i.e.,preservelexicalandseman-ticsimilarityless)arebetteratachievingthede-siredstyle.Forfluencymeasuredbyperplexity,we
Tie
Tie
Tie
Contextual
Contextual
Contextual
Fit0.480.360.510.500.520.260.300.230.170.200.270.340.260.330.28
Fit0.460.410.440.560.510.260.130.260.140.140.290.460.300.300.34
11424


11Lowerperplexitygenerallyindicateshighersentencequal-ityandgrammaticality,butmaynotdirectlycorrelatewithmeaningpreservation,style,orcontentrelevance.originalinputsentenceIbeforecomparingittotherewriteX:sim(C+I,X).Theintuitionbehindthisalterationisthattheprecedingtextualcontextcouldcapturemoreofthetopicalorsemanticin-formationnecessarytofullyderivethespeaker’sintendedmeaning.ContextualLexicalandSemanticSimilarityForlexicalsimilarity,werefertothesemetricsasROUGECtx,METEORCtxandWERCtx.Forseman-ticsimilarity,werefertothemasBERTScoreCtx,SBERTCtxandSmatchCtx.ContextualCoherenceandCohesivenessInlin-guistics,coherenceandcohesivenessaretermstyp-icallyusedtodenotetheconnectednessembeddedorimpliedinspokenorwrittendiscourse.(a)Coherence:Coherenceisgenerallydefinedastheoverallpicturepresentedbyallthesentencesinapieceofwriting,similartothewaypuzzlepiecesformtheimageonthebox(Williams,1990;Zienkowskietal.,2011).Thisdefinitionisoftenoperationalizedbymodelingthefitofasentencegivenitsprecedingcontext,asdemonstratedbypriorwork(Seeetal.,2019;Pangetal.,2020).Specifically,thisinvolvesmeasuringperplexityoftherewriteconditionedonthecontextusingGPT-2(Radfordetal.,2019).(b)Cohesiveness:Cohesivenessreferstothesemanticrelationshipsbetweensentences,link-ingcurrentelementswithprecedingorfollowingonesthroughlexicalandstructuralmeans,muchlikehowtwojigsawpuzzlepiecesfittogether(Williams,1990;Zienkowskietal.,2011).Fol-lowingpriorworkthatusedthisdefinition(ShiandDemberg,2019;Abhisheketal.,2021;Nguyen,2021),wemeasurecohesivenessusingtheprob-abilitiesfromtheNextSentencePrediction(NSP)headofBERT(Devlinetal.,2018),whichmea-suresiftherewritefollowsandfitswithitsthe
modelrewrite typeLexicalSemanticFluencyStyleROUMETWERBERT-SSBERTSmatchPPL GPT-3.5contextual0.180.381.660.900.590.4540.250.74non-contextual0.280.480.880.920.700.5747.690.70random-context0.160.342.040.890.500.42GPT-NeoXcontextual0.250.371.430.900.570.4455.310.52non-contextual0.410.560.780.930.740.6064.650.44random-context0.240.371.550.900.540.43Table3:Non-contextualAutomaticEvaluationResults:Non-contextualrewritesachievehigherscoresinlexicalandsemanticsimilaritymetricswhereascontextualrewritesdemonstrateenhancedstylestrengthandfluency.Theseresultsareobtainedbyaveragingacrossalltasksanddatasets.Thisheatmapdisplaysthebest-performingrewriteforeachspecificmetric–darkerorangeindicateshigherpreference.Formoredetailsonindividualtasksanddatasetsexhibitingsimilartrends,seeAppD.findthatbothapproachesgeneratedecentlyfluentrewritesregardlessofcontext,asexpected.11Non-contextualmetricsdonotcorrelatewithhumanjudgmentsWeseeinFigure3andTa-ble3,thatthenon-contextualautomaticmetricspaintanincompletepicturecomparedtohumanevaluations.WecomputeSpearmanrankρcorrela-tionandKendall’sτforthedatasetsamplesusedduringthecontextualhumanevaluation§5.1.Non-contextualautomaticmetricsexhibitveryweak,non-significantcorrelationwithhumanjudgmentsofoverallfit(averagedacrossalltasks):ρ=0.09,τ=0.09forlexicalmetrics(p>0.05)andρ=0.23,τ=0.22forsemanticmetrics(p>0.05).SeeAppendixD.1formetric-specificcorrelationscoresforoverallfitandnaturalnessdimensions.7ContextualAutomaticEvaluationAsshownintheprevioussection,non-contextualautomaticmetrics,especiallyformeaningpreserva-tion,arenotsufficienttoevaluatetheperformanceofrewritingmodels.Toaddressthis,incorporatingcontextintotheevaluationprocessisnecessaryforbetterrepresentingrealisticdownstreamusecases.Drawinginspirationfromreference-freemetricsindialogevaluation(Yehetal.,2021;Zhaoetal.,2017),whichconsidersboththedialogcontextandgeneratedresponsestoassessresponseswithinthedialoguehistory,weproposeincludingcontextintoexistingautomaticevaluationmetricsandfurtherintroduceCtxSimFit,anewcontextualmetric.7.1InfusingAutomaticMetricswithContextSincecontextiscrucialtoderiveintendedmeaning(Searle,1975),wealterexistingmeaningsimilar-itymeasuresbyprependingthecontextCtothe
11425


modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.150.240.890.880.590.3528.700.950.93non-contextual0.160.220.880.880.490.3242.940.890.91random-context0.100.181.060.870.390.2945.730.800.85GPT-NeoXcontextual0.190.220.870.880.530.3131.100.930.92non-contextual0.210.230.860.880.480.3049.810.900.91random-context0.130.170.980.860.390.2652.930.830.86Table4:ContextualAutomaticEvaluationResults:Onaverage,acrossalltasksanddatasets,contextualrewritesachievehigherscoresthannon-contextualrewriteswhenevaluatedusingcontext-infusedautomaticmetricsandourCtxSimFitmetric.Thisheatmapshowsthebest-performingrewriteforaparticularmetric–darkergreenindicateshigherpreference.Formoredetailsonindividualtasksanddatasetsdisplayingsimilartrends,seeAppE.precedingcontext.7.2NovelCompositeMetric:CtxSimFitWeintroduceCtxSimFit,asimplemetricthatcom-binescontextualcohesivenessandsemanticsim-ilaritytoassesstheoverallqualityofarewrite.CtxSimFitcomputestheweightedaverageofboththeBERTScorebetweentheoriginalandrewrittensentences,andtheprobabilitiesfromtheBERT’sNSPheadbetweentheprecedingcontextandtherewrite,thusdetermininghowwelltherewritefitstheprecedingcontextandmaintainssemanticsimi-larity.CtxSimFit=α∗BERTSCORE(S,X)+(1−α)∗NSP(C,X)whereαisahyperparameterthatprovidesuserswithcontrolovertheirpreferenceforbalancingmeaningpreservationandcontextualfit.Unlessspecifiedotherwise,wesetα=0.5.Contextualrewritesarescoredhigheronstylestrengthcomparedtonon-contextualrewrites7.3ContextualEvaluationResultsSimilarto§6.2,weaggregatetheresultsofbothGPT-3.5andNeoXacrossalltasks,datasetsandmetrics(seeTable4).Fordetailedresultsonin-dividualtasksanddatasets,wereferthereadertoTables23–29inAppendixE.Contextualrewritesarepreferredbynearlyallofourcontextualautomaticmetricscomparedtonon-contextualrewritesTheseresultsmir-rorhumanpreferencesonnaturalness,fitandin-tendedmeaningpreservation.Asarealitycheck,contextualrewriteswithrandomcontextsperformtheworstacrossallmetrics,indicatingthatcontex-tualmodelsareindeedtakingcontextintoaccount.Furtherasexpected,contextualrewritesalsohavebettercoherencecomparedtonon-contextualones.ContextualmetricscorrelatesignificantlywithhumanjudgmentsWefindthatcontextualau-tomaticmetricscorrelatesignificantlywithhu-manjudgmentsof‘overallfit’(averagedacrossalltasks):ρ=0.6,τ=0.58forlexicalmetrics(p<0.05)andρ=0.56,τ=0.57forsemanticmetrics(p<0.05).SeeAppendixE.1formetric-wisecorrelationscoresforbothoverallfitandnat-uralnesshumanjudgmentdimensions.CtxSimFitcorrelatesthebestwithhumanjudgementsComparedtocontextualversionsofexistingmetrics,CtxSimFitcorrelatesverystronglywithhumanjudgementsof‘overallfit’(averagedacrossalltasks):ρ=0.85,τ=0.82(p<0.01).Weseesimilartrendsfor‘naturalness’:ρ=0.85,τ=0.81(p<0.01).Thissuggeststhatcombiningmeaningpreservationandcontextualco-hesivenessintoacompositemeasurebettermirrorshumanpreferencesthanindividualmetricsalone.7.4SensitivityanalysisforαinCtxSimFitInourexperiments,wesetα=0.5toequallyweightcontextualcohesivenessandsemanticsim-ilarity.WefurtherexaminetheimpactofαinCtxSimFit,asdetailedbyTable5.OurCtxSimFitsignificantlycorrelateswithhu-manjudgmentsof‘overallfit’forαvalueswithintherangeof0.2–0.6,withcorrelationandsignifi-cancediminishingoutsidethisrange.Thehighestalignmentwithhumanjudgmentsisachievedatα=0.5.Thelongerrangeof0.2–0.5forα<0.5highlightstheeffectandimportanceofcontextualcohesivenessinstylistictextrewriting.Whileabalancedapproach(α=0.5)offersthestrongestalignmentwithhumanjudgmentsforformality,sentimentandde-toxificationtasks,thedegreeofemphasisoncontextualcohesivenessandsemanticsimilarityshouldbeadjustedbasedonspecifictasksandusers’priorities.
11426


0.9Formality0.25nsToxicity0.28*Sentiment0.20ns
Table5:SensitivityoftheαinCtxSimFitacrossalltasks.ρindicatescorrelationofCtxSimFitwithhumanjudgmentsof‘overallfit’.nsindicatesnotsignificant(p>0.05),*isp<0.05,**isp<0.01,***p<0.0018Summary&DiscussionofFindingsExistingworkonstylistictextrewritinghasof-tenneglectedthesurroundingcontextofthesen-tence.Inourstudy,wefocusonincorporatingtheprecedingtextualcontextindocumentsandcon-versationsintoboththemodelingandevaluationstagesofrewriting.Wedevelopacontextualhu-manevaluationframeworkandcompareitsresultstonon-contextualautomaticmetrics,contextual-izedversionsofthesemetrics,aswellastoournewcompositemetricCtxSimFit.ContextiscrucialforrewritingCorroboratingfindingsbyChengetal.(2020)andRoyetal.(2023),contextualrewritesaresignificantlypre-ferredbyhumanannotatorsintermsofnaturalness,intendedmeaningpreservation,andstylestrength.Additionally,wedemonstratethathavingtherightcontextiscrucialforcontextualrewriting,asev-idencedbythepoorperformanceofcontextualrewritesgeneratedusingarandomcontext.Qualitativeexamination(Table2)showsthatcontextualrewritesarebetteratdisambiguatingentitiesandbettervocabularyusage(examples(a),(c)),retainingrelevantdetailsfromcontextforabetterflow(examples(b),(i))andpreservingtheintendedmeanings(examples(d),(g)).Existingmeaningpreservationmetricsdonotalignwithhumanpreferencesforformality,sen-timentandtoxicitytransfertasksNext,wedemonstratethatcommonnon-contextualauto-maticmetricsforlexicalandsemanticsimilarity,i.e.,oftenusedasproxiesformeaningpreservationinpriorwork(Lietal.,2018;Sudhakaretal.,2019;Miretal.,2019;Reifetal.,2022;Madaanetal.,2020;Wangetal.,2019;ReidandZhong,2021;Royetal.,2023),donotalignwithhumanpref-erencesconcerningnaturalness,fit,andintendedmeaning.Sincetheoverarchingmeaningofasentencelargelydependsonitscontext(Searle,1975;Clark,1997,1996),non-contextualproxiesformeaningpreservationwillalwaysbeintensionwithanystylisticchangetothesentence,makingthetrade-offhardtonavigate(Miretal.,2019;Huetal.,2022).Therefore,weadvocatefordiscontin-uingnon-contextualmeaningpreservationmetricsinstylisticrewritingtasksandformoreresearchintobettermodelingofcommunicativeintentsorgoals(Adolphsetal.,2022;Zhouetal.,2022).Contextualautomaticmetrics,especiallyCtxSimFit,bettermirrorhumanjudgmentsInourwork,weattempttobridgethegapbetweennon-contextualmetricsandcontextualhumanevaluationsbyintegratingcontextintoautomatedmetrics(§7).Ourproposedcompositemetric,CtxSimFit,balancesmeaningpreserva-tionwithcontextualcohesiveness,providingamorecomprehensivemeasurethatbetteralignswithhumanjudgments.Whilecommonly-usedautomaticmetricsenrichedwithcontextalignwithhumanpreferences,ourproposedCtxSimFitdemonstratesastrongercorrelation.Initialworkinevaluatingopen-domaindialoguegenerationwithcontext(Wellecketal.,2019;Pangetal.,2020)hasbeendone,butweencouragefur-therdevelopmentofbettercontextualizedmetricsforstylisticrewritingevaluation.Improvementscouldincludemodelingthemes,tones,sentencestructures(Zhangetal.,2014;Khatrietal.,2018;ChenandYang,2020;Toubiaetal.,2021;Shenetal.,2023),andsocialdynamics,andemotionalstatesinconversations(Sapetal.,2017;Rashkinetal.,2018,2019;Mostafazadehetal.,2020).
0.8Formality0.24*Toxicity0.34*Sentiment0.17ns
0.7Formality0.32*Toxicity0.38**Sentiment0.20ns
0.3Formality0.75***Toxicity0.75***Sentiment0.67***
0.4Formality0.71***Toxicity0.67***Sentiment0.60***
0.6Formality0.57***Toxicity0.53***Sentiment0.42***
0.5Formality0.88***Toxicity0.82***Sentiment0.73***
HyperparameterαinCtxSimFitTaskCorrelationρwith‘overallfit’Significance
0.1Formality-0.03nsToxicity-.05nsSentiment-0.04ns
0.2Formality0.66**Toxicity0.65**Sentiment0.54**
11427


9Limitations&EthicalConsiderationsDespitetakingthefirststeptowardsincorporatingcontextintostylisticrewritinganditsevaluationframeworks,thereareseverallimitationsandethi-calconcerns,whichwelistbelow.LimitedContextScopeInthisstudy,ourpri-maryfocusisonincorporatingtextualcontext,particularlyfromprecedingsentencesorpreviousturnsinaconversation.Futureworkshouldex-plorehowtoincorporateotherformsofcontextintorewritingmodelsandevaluations,suchasdis-coursestructure(Wellecketal.,2019),externalknowledge(Ghazvininejadetal.,2018),orrichersocialandpowerdynamics(Antoniaketal.,2023),emotionalstates(Zhouetal.,2023),andcommu-nicativeintent(Zhouetal.,2022),allofwhichcansignificantlycontributetounderstandingthetext.AmountofContextInourexperiments,weoptedtoinvestigatethecontextofthreepreced-ingsentencesinadocumentandoneprecedingconversationalturn,consideringonlyaspecificlength.However,theamountofcontextatthemod-elingandevaluationstagescouldalsochangetheresults.Wehypothesizethatmorecontextcouldimproverewritingmethods,butitcouldpotentiallyalsonegativelyimpactcontextualmeaningpreser-vationmetrics.Futureworkshouldexploretheseeffectsofvaryinglengthsofcontext.BroadDefinitionofMeaningPreservationWhilewehavetriedtodefinemeaningpreserva-tionasthepreservationofaneventorentity-leveldetailsandintendedoverallmeaning,thisdefini-tionremainsbroadandsubjective(Searle,1975;Adolphsetal.,2022;Zhouetal.,2022).Inthiswork,wedonotdelveintomoreintricatedimen-sionsofmeaningpreservation,suchasspatialandtemporalaccuracy,ortheretentionofculturalcon-text,includingreferences,nuances,anddialects.ApplicabilitytoSmallerModelsOurworkre-liesonfew-shotpromptingofLLMstoincorporatetextualcontext,giventheirdemonstratedstrongrewritingcapabilitiesbothwithandwithouttextualcontextusage(Brownetal.,2020).Otherexist-inggenerativemodels,suchasthoseusedforchit-chatandgoal-orientedconversationalagents,aswellaspretrainedlanguagemodels,havestruggledwitheffectivelyutilizingprecedingtextualcontext(Sankaretal.,2019;O’ConnorandAndreas,2021;Parthasarathietal.,2021;Suetal.,2023).More-over,custom-maderewritingmodelsfrompriorresearchoftenlackthemodelingofcontext(Maetal.,2020;Daleetal.,2021).Webelievetheourresultsstillapplyforsmallermodels,givensomepreliminaryresearch(Chengetal.,2020;Atwelletal.,2022)onanincreasedhumanpreferenceforcontextualrewritesfromcustom-trainedseq2seqmodels.Weencouragefutureworktothoroughlyinvestigatestrategiesforeffectivemodelingandevaluationofcontextinsmallermodels.HarmsofExposingWorkerstoToxicContentInourwork,weexposedhumanannotatorstotoxiccontentduringtheevaluationofthede-toxificationtask.Exposuretosuchoffensivecontentcanbeharmfultotheannotators(Liuetal.,2016).Weaimtoworktowardsdevelopingevaluationstrategiesthatcanminimizetheexposureofannotatorstotoxiccontent.PotentiallyInconsistentHumanEvaluationsInourwork,wealsoassumehumanjudgmentsasthegoldstandard.Concurrentworkhasshownthathumanevaluationmightnotalwaysbecon-sistent(Clarketal.,2021;Karpinskaetal.,2021);howeverhumanjudgmentscontinuetobethegoldstandardforevaluatingopen-endedtextgeneration.AcknowledgementsWewouldliketothankourworkersonMTurkfortheirresponses.Wearealsogratefultotheanonymousreviewersfortheirhelpfulcomments.SpecialthankstoSaadiaGabriel,JocelynShen,AshutoshBaheti,andthemembersoftheCMULTICOMEDYgroupfortheirfeedback,andOpe-nAIforprovidingaccesstotheGPT-3.5API.ThisresearchwassupportedinpartbytheMetaFun-damentalAIResearchLaboratories(FAIR)“Dyn-abenchDataCollectionandBenchmarkingPlat-form”award“ContExTox:Context-AwareandEx-plainableToxicityDetection.”ReferencesTusharAbhishek,DakshRawat,ManishGupta,andVasudevaVarma.2021.Transformermodelsfortextcoherenceassessment.arXivpreprintarXiv:2109.02176.LeonardAdolphs,KurtShuster,JackUrbanek,ArthurSzlam,andJasonWeston.2022.Reasonfirst,thenrespond:Modulargenerationforknowledge-infused
11428


dialogue.InFindingsoftheAssociationforCom-putationalLinguistics:EMNLP2022,pages7112–7132.MariaAntoniak,AnjalieField,JiMinMun,MelanieWalsh,LaurenF.Klein,andMaartenSap.2023.Riv-eter:Measuringpowerandsocialdynamicsbetweenentities.InACLdemonstrations.KatherineAtwell,SabitHassan,andMaliheAlikhani.2022.Appdia:Adiscourse-awaretransformer-basedstyletransfermodelforoffensivesocialmediacon-versations.InProceedingsofthe29thInternationalConferenceonComputationalLinguistics,pages6063–6074.SatanjeevBanerjeeandAlonLavie.2005.Meteor:Anautomaticmetricformtevaluationwithimprovedcor-relationwithhumanjudgments.InProceedingsoftheaclworkshoponintrinsicandextrinsicevaluationmeasuresformachinetranslationand/orsummariza-tion,pages65–72.FrancescoBarbieri,LuisEspinosaAnke,andJoseCamacho-Collados.2022.Xlm-t:Multilinguallan-guagemodelsintwitterforsentimentanalysisandbeyond.InProceedingsoftheThirteenthLanguageResourcesandEvaluationConference,pages258–266.SidneyBlack,StellaBiderman,EricHallahan,QuentinAnthony,LeoGao,LaurenceGolding,HoraceHe,ConnorLeahy,KyleMcDonell,JasonPhang,etal.2022.Gpt-neox-20b:Anopen-sourceautoregres-sivelanguagemodel.InProceedingsofBigScienceEpisode\#5–WorkshoponChallenges&Perspec-tivesinCreatingLargeLanguageModels,pages95–136.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,Ma-teuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei.2020.Languagemodelsarefew-shotlearners.InAd-vancesinNeuralInformationProcessingSystems,volume33,pages1877–1901.CurranAssociates,Inc.ShuCaiandKevinKnight.2013.Smatch:anevaluationmetricforsemanticfeaturestructures.InProceed-ingsofthe51stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages748–752.KunalChawlaandDiyiYang.2020.Semi-supervisedformalitystyletransferusinglanguagemodeldis-criminatorandmutualinformationmaximization.InFindingsoftheAssociationforComputationalLin-guistics:EMNLP2020,pages2340–2354,Online.AssociationforComputationalLinguistics.JiaaoChenandDiyiYang.2020.Multi-viewsequence-to-sequencemodelswithconversationalstructureforabstractivedialoguesummarization.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages4106–4118.YuCheng,ZheGan,YizheZhang,OussamaElachqar,DianqiLi,andJingjingLiu.2020.Contextualtextstyletransfer.InFindingsoftheAssociationforCom-putationalLinguistics:EMNLP2020,pages2915–2924.ElizabethClark,TalAugust,SofiaSerrano,NikitaHaduong,SuchinGururangan,andNoahASmith.2021.Allthat’s‘human’isnotgold:Evaluatinghu-manevaluationofgeneratedtext.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages7282–7296.ElizabethClark,AnneSpencerRoss,ChenhaoTan,YangfengJi,andNoahASmith.2018.Creativewritingwithamachineintheloop:Casestudiesonslogansandstories.In23rdInternationalConferenceonIntelligentUserInterfaces,pages329–340.HerbertHClark.1996.Usinglanguage.Cambridgeuniversitypress.HerbertHClark.1997.Dogmasofunderstanding.Dis-courseProcesses,23(3):567–598.DavidDale,AntonVoronov,DarynaDementieva,Var-varaLogacheva,OlgaKozlova,NikitaSemenov,andAlexanderPanchenko.2021.Textdetoxificationus-inglargepre-trainedneuralmodels.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNat-uralLanguageProcessing,pages7979–7996,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.arXivpreprintarXiv:1810.04805.ZhenxinFu,XiaoyeTan,NanyunPeng,DongyanZhao,andRuiYan.2018.Styletransferintext:Explo-rationandevaluation.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32.MarjanGhazvininejad,ChrisBrockett,Ming-WeiChang,BillDolan,JianfengGao,Wen-tauYih,andMichelGalley.2018.Aknowledge-groundedneu-ralconversationmodel.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32.SkylerHallinan,AlisaLiu,YejinChoi,andMaartenSap.2023.Detoxifyingtextwithmarco:Controllablerevisionwithexpertsandanti-experts.InACL.ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar.2022.ToxiGen:Alarge-scalemachine-generateddataset
11429


foradversarialandimplicithatespeechdetection.InProceedingsofthe60thAnnualMeetingoftheAssociationofComputationalLinguistics.AriHoltzman,JanBuys,MaxwellForbes,AntoineBosselut,DavidGolub,andYejinChoi.2018.Learn-ingtowritewithcooperativediscriminators.InPro-ceedingsofthe56thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),pages1638–1649.ZhiqiangHu,RoyKa-WeiLee,CharuCAggarwal,andAstonZhang.2022.Textstyletransfer:Areviewandexperimentalevaluation.ACMSIGKDDExplo-rationsNewsletter,24(1):14–45.ZhitingHu,ZichaoYang,XiaodanLiang,RuslanSalakhutdinov,andEricPXing.2017.Towardcon-trolledgenerationoftext.InInternationalconferenceonmachinelearning,pages1587–1596.PMLR.DiJin,ZhijingJin,ZhitingHu,OlgaVechtomova,andRadaMihalcea.2022.Deeplearningfortextstyletransfer:Asurvey.ComputationalLinguistics,48(1):155–205.MarzenaKarpinska,NaderAkoury,andMohitIyyer.2021.Theperilsofusingmechanicalturktoevaluateopen-endedtextgeneration.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1265–1285.ChandraKhatri,RahulGoel,BehnamHedayatnia,AngelikiMetanillou,AnushreeVenkatesh,RaeferGabriel,andArindamMandal.2018.Contextualtopicmodelingfordialogsystems.In2018ieeespokenlanguagetechnologyworkshop(slt),pages892–899.IEEE.HyunwooKim,YoungjaeYu,LiweiJiang,XimingLu,DanielKhashabi,GunheeKim,YejinChoi,andMaartenSap.2022.Prosocialdialog:Aprosocialbackboneforconversationalagents.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages4005–4029.SvetlanaKiritchenkoandSaifMohammad.2017.Best-worstscalingmorereliablethanratingscales:Acasestudyonsentimentintensityannotation.InProceed-ingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages465–470.GuillaumeLample,SandeepSubramanian,EricSmith,LudovicDenoyer,Marc’AurelioRanzato,andY-LanBoureau.2019.Multiple-attributetextrewriting.InInternationalConferenceonLearningRepresenta-tions.JuncenLi,RobinJia,HeHe,andPercyLiang.2018.Delete,retrieve,generate:asimpleapproachtosenti-mentandstyletransfer.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages1865–1874.YanranLi,HuiSu,XiaoyuShen,WenjieLi,ZiqiangCao,andShuziNiu.2017.Dailydialog:Amanuallylabelledmulti-turndialoguedataset.InProceedingsoftheEighthInternationalJointConferenceonNat-uralLanguageProcessing(Volume1:LongPapers),pages986–995.Chin-YewLin.2004.Rouge:Apackageforautomaticevaluationofsummaries.InTextsummarizationbranchesout,pages74–81.Chia-WeiLiu,RyanLowe,IulianVladSerban,MikeNoseworthy,LaurentCharlin,andJoellePineau.2016.Hownottoevaluateyourdialoguesystem:Anempiricalstudyofunsupervisedevaluationmet-ricsfordialogueresponsegeneration.InProceedingsofthe2016ConferenceonEmpiricalMethodsinNat-uralLanguageProcessing,pages2122–2132.VarvaraLogacheva,DarynaDementieva,SergeyUstyantsev,DaniilMoskovskiy,DavidDale,IrinaKrotova,NikitaSemenov,andAlexanderPanchenko.2022.Paradetox:Detoxificationwithparalleldata.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages6804–6818.XinyaoMa,MaartenSap,HannahRashkin,andYejinChoi.2020.Powertransformer:Unsupervisedcon-trollablerevisionforbiasedlanguagecorrection.InEMNLP.CharlesAMacArthur.2009.Reflectionsonresearchonwritingandtechnologyforstrugglingwriters.Learn-ingDisabilitiesResearch&Practice,24(2):93–103.AmanMadaan,AmrithSetlur,TanmayParekh,Barn-abásPoczós,GrahamNeubig,YimingYang,RuslanSalakhutdinov,AlanWBlack,andShrimaiPrabhu-moye.2020.Politenesstransfer:Atagandgenerateapproach.InProceedingsofthe58thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages1869–1881.RemiMir,BjarkeFelbo,NickObradovich,andIyadRahwan.2019.Evaluatingstyletransferfortext.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),pages495–504.NasrinMostafazadeh,AdityaKalyanpur,LoriMoon,DavidBuchanan,LaurenBerkowitz,OrBiran,andJenniferChu-Carroll.2020.Glucose:Generalizedandcontextualizedstoryexplanations.InProceed-ingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages4569–4586.SourabrataMukherjee,ZdenˇekKasner,andOndˇrejDušek.2022.Balancingthestyle-contenttrade-offinsentimenttransferusingpolarity-awaredenoising.InText,Speech,andDialogue,pages172–186,Cham.SpringerInternationalPublishing.
11430


RameshNallapati,BowenZhou,CicerodosSantos,Ça˘glarGulçehre,andBingXiang.2016.Abstractivetextsummarizationusingsequence-to-sequencernnsandbeyond.InProceedingsofthe20thSIGNLLCon-ferenceonComputationalNaturalLanguageLearn-ing,pages280–290.AnNguyen.2021.Languagemodelevaluationinopen-endedtextgeneration.arXivpreprintarXiv:2108.03578.CiceroNogueiradosSantos,IgorMelnyk,andInkitPadhi.2018.Fightingoffensivelanguageonsocialmediawithunsupervisedtextstyletransfer.InPro-ceedingsofthe56thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume2:ShortPapers),pages189–194,Melbourne,Australia.As-sociationforComputationalLinguistics.LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022.Traininglanguagemodelstofollowinstruc-tionswithhumanfeedback.AdvancesinNeuralInformationProcessingSystems,35:27730–27744.JoeO’ConnorandJacobAndreas.2021.Whatcontextfeaturescantransformerlanguagemodelsuse?InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages851–864.BoPang,ErikNijkamp,WenjuanHan,LinqiZhou,Yix-ianLiu,andKeweiTu.2020.Towardsholisticandautomaticevaluationofopen-domaindialoguegener-ation.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages3619–3629.PrasannaParthasarathi,JoellePineau,andSarathChan-dar.2021.Doencoderrepresentationsofgenerativedialoguemodelshavesufficientsummaryofthein-formationaboutthetask?InProceedingsofthe22ndAnnualMeetingoftheSpecialInterestGrouponDiscourseandDialogue,pages477–488.AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog,1(8):9.SudhaRaoandJoelTetreault.2018.Dearsirormadam,mayIintroducetheGYAFCdataset:Corpus,bench-marksandmetricsforformalitystyletransfer.InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers),pages129–140,NewOr-leans,Louisiana.AssociationforComputationalLin-guistics.HannahRashkin,AntoineBosselut,MaartenSap,KevinKnight,andYejinChoi.2018.Modelingnaivepsy-chologyofcharactersinsimplecommonsensestories.InACL.HannahRashkin,EricMichaelSmith,MargaretLi,andY-LanBoureau.2019.Towardsempatheticopen-domainconversationmodels:Anewbenchmarkanddataset.InProceedingsofthe57thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages5370–5381.MachelReidandVictorZhong.2021.Lewis:Leven-shteineditingforunsupervisedtextstyletransfer.InFindingsoftheAssociationforComputationalLin-guistics:ACL-IJCNLP2021,pages3932–3944.EmilyReif,DaphneIppolito,AnnYuan,AndyCoenen,ChrisCallison-Burch,andJasonWei.2022.Arecipeforarbitrarytextstyletransferwithlargelanguagemodels.InProceedingsofthe60thAnnualMeet-ingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages837–848.NilsReimersandIrynaGurevych.2019.Sentence-bert:Sentenceembeddingsusingsiamesebert-networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3982–3992.ShamikRoy,RaphaelShu,NikolaosPappas,ElmanMansimov,YiZhang,SaabMansour,andDanRoth.2023.Conversationstyletransferusingfew-shotlearning.arXivpreprintarXiv:2302.08362.ChinnadhuraiSankar,SandeepSubramanian,Christo-pherPal,SarathChandar,andYoshuaBengio.2019.Doneuraldialogsystemsusetheconversationhis-toryeffectively?anempiricalstudy.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages32–37.MaartenSap,MarcellaCindyPrasetio,AriHoltzman,HannahRashkin,andYejinChoi.2017.Connotationframesofpowerandagencyinmodernfilms.InEMNLP.JonathanSchler,MosheKoppel,ShlomoArgamon,andJamesWPennebaker.2006.Effectsofageandgen-deronblogging.InAAAIspringsymposium:Compu-tationalapproachestoanalyzingweblogs,volume6,pages199–205.JohnRSearle.1975.Ataxonomyofillocutionaryacts.AbigailSee,AneeshPappu,RohunSaxena,AkhilaYerukola,andChristopherDManning.2019.Domassivelypretrainedlanguagemodelsmakebetterstorytellers?InProceedingsofthe23rdConfer-enceonComputationalNaturalLanguageLearning(CoNLL),pages843–861.JocelynShen,MaartenSap,PedroColon-Hernandez,HaeWonPark,andCynthiaBreazeal.2023.Model-ingempathicsimilarityinpersonalnarratives.TianxiaoShen,TaoLei,ReginaBarzilay,andTommiJaakkola.2017.Styletransferfromnon-paralleltextbycross-alignment.Advancesinneuralinformationprocessingsystems,30.
11431


WeiShiandVeraDemberg.2019.Nextsentencepre-dictionhelpsimplicitdiscourserelationclassificationwithinandacrossdomains.InProceedingsofthe2019conferenceonempiricalmethodsinnaturallanguageprocessingandthe9thinternationaljointconferenceonnaturallanguageprocessing(EMNLP-IJCNLP),pages5790–5796.HsuanSu,ShachiHKumar,SahisnuMazumder,WendaChen,RameshManuvinakurike,EdaOkur,SauravSahay,LamaNachman,Shang-TseChen,andHung-yiLee.2023.Positionmatters!empiricalstudyofordereffectinknowledge-groundeddialogue.arXivpreprintarXiv:2302.05888.AkhileshSudhakar,BhargavUpadhyay,andArjunMa-heswaran.2019.“transforming”delete,retrieve,gen-erateapproachforcontrolledtextstyletransfer.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3269–3279.ValerioBasileTommasoCaselliandMichaelGranitzerJelenaMitrovic.2021.Hatebert:Retrainingbertforabusivelanguagedetectioninenglish.WOAH2021,page17.OlivierToubia,JonahBerger,andJehoshuaEliashberg.2021.Howquantifyingtheshapeofstoriespredictstheirsuccess.ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesofAmerica,118(26).YunliWang,YuWu,LiliMou,ZhoujunLi,andWenhanChao.2019.Harnessingpre-trainedneuralnetworkswithrulesforformalitystyletransfer.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNat-uralLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages3573–3578.SeanWelleck,JasonWeston,ArthurSzlam,andKyunghyunCho.2019.Dialoguenaturallanguageinference.InProceedingsofthe57thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages3731–3741.JosephWilliams.1990.Towardclarityandgrace.Chicago:TheUniversityofChicago.AlexandrosXenos,JohnPavlopoulos,andIonAndrout-sopoulos.2021.Contextsensitivityestimationintoxicitydetection.InProceedingsofthe5thWork-shoponOnlineAbuseandHarms(WOAH2021),pages140–145.JingjingXu,XuanchengRen,JunyangLin,andXuSun.2018a.Diversity-promotinggan:Across-entropybasedgenerativeadversarialnetworkfordiversifiedtextgeneration.InProceedingsofthe2018con-ferenceonempiricalmethodsinnaturallanguageprocessing,pages3940–3949.JingjingXu,XuSun,QiZeng,XiaodongZhang,Xu-anchengRen,HoufengWang,andWenjieLi.2018b.Unpairedsentiment-to-sentimenttranslation:Acy-cledreinforcementlearningapproach.InProceed-ingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages979–988,Melbourne,Australia.AssociationforComputationalLinguistics.WeiXu,AlanRitter,BillDolan,RalphGrishman,andColinCherry.2012.Paraphrasingforstyle.InPro-ceedingsofCOLING2012,pages2899–2914,Mum-bai,India.TheCOLING2012OrganizingCommit-tee.Yi-TingYeh,MaxineEskenazi,andShikibMehri.2021.Acomprehensiveassessmentofdialogevaluationmetrics.InTheFirstWorkshoponEvaluationsandAssessmentsofNeuralConversationSystems,pages15–33.AkhilaYerukola,MasonBretan,andHongxiaJin.2021.Dataaugmentationforvoice-assistantNLUusingBERT-basedinterchangeablerephrase.InProceed-ingsofthe16thConferenceoftheEuropeanChap-teroftheAssociationforComputationalLinguistics:MainVolume.PingYu,YangZhao,ChunyuanLi,andChangyouChen.2021.Rethinkingsentimentstyletransfer.InFind-ingsoftheAssociationforComputationalLinguistics:EMNLP2021,pages1569–1582.KlausZechnerandAlexWaibel.2000.Minimizingworderrorrateintextualsummariesofspokenlan-guage.In1stMeetingoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics.KaiZhang,WeiWu,HaochengWu,ZhoujunLi,andMingZhou.2014.Questionretrievalwithhighqual-ityanswersincommunityquestionanswering.InProceedingsofthe23rdACMinternationalconfer-enceonconferenceoninformationandknowledgemanagement,pages371–380.TianyiZhang,VarshaKishore,FelixWu,KilianQWeinberger,andYoavArtzi.2019.Bertscore:Eval-uatingtextgenerationwithbert.arXivpreprintarXiv:1904.09675.XiangZhang,JunboZhao,andYannLeCun.2015.Character-levelconvolutionalnetworksfortextclassi-fication.Advancesinneuralinformationprocessingsystems,28.YangjunZhang,PengjieRen,WentaoDeng,ZhuminChen,andMaartenRijke.2022.Improvingmulti-labelmalevolencedetectionindialoguesthroughmulti-facetedlabelcorrelationenhancement.InPro-ceedingsofthe60thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),pages3543–3555.TianchengZhao,RanZhao,andMaxineEskenazi.2017.Learningdiscourse-leveldiversityforneuraldialogmodelsusingconditionalvariationalautoencoders.InProceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages654–664.
11432


PeiZhou,HyundongCho,PegahJandaghi,Dong-HoLee,BillYuchenLin,JayPujara,andXiangRen.2022.Reflect,notreflex:Inference-basedcommongroundimprovesdialogueresponsequality.InPro-ceedingsofthe2022ConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing,pages10450–10468.XuhuiZhou,HaoZhu,AkhilaYerukola,ThomasDavid-son,JenaD.Hwang,SwabhaSwayamdipta,andMaartenSap.2023.Cobraframes:Contextualrea-soningabouteffectsandharmsofoffensivestate-ments.InFindingsofACL.JanZienkowski,JefVerschueren,andJoÖstman.2011.Discursivepragmatics:Aplatformforthepragmaticstudyofdiscourse.InDisursivePragmatics,pages1–13.JohnBenjaminsPublishingCompany.
11433


ATasksandDatasetsFormalityDataWeobtainaconversationaldatasetfromReddit12bysamplingconversa-tionalthreadsfromsubredditssuchasr/news,r/askscience,andr/Economics(formalconversa-tions),aswellasr/movies,r/fantasyfootball,andr/relationships(informalconversations).Wefo-cusontwo-turnRedditthreads:aparent/precedingcontextandtheresponsetoberewritten.Next,wesampledocumentsfromCNNDailyMail(formaldocuments;Nallapatietal.,2016)andtheBlogAu-thorshipCorpus(informaldocuments;Schleretal.,2006).Weselectfoursentencesfromeachdatasample:threesentencesastheprecedingparentcontext,andthefollowingsentenceastheonetoberewritten.Foreachdatasample,welabelthecontextandresponseusingapre-trainedformalityclassifier.13SentimentDataWeobtainaconversationaldatasetfromtheDailyDialog(Lietal.,2017)dataset,focusingontwo-turnconversations:apar-ent/precedingcontextandtheresponsetoberewrit-ten.Next,wesampleentriesfromtheYelpreviews(Zhangetal.,2015)dataset.Analogoustothedoc-umentdatasetusedinformality,wechoosefoursentencesfromeachdatasample:threeasthepre-cedingparentcontextandthesubsequentsentenceastheonetoberewritten.Foreachdatasample,weannotatethecontextandresponseusingasenti-mentclassifier.14Wepartitionthedatatotransformsentencesfrompositivetonegativesentimentandviceversa,aswellastoconvertneutralsentencestopositiveornegativesentiment.ToxicityDataWeexaminethreeconversationaldatasets:theCivilCommentsinContext(CCC)dataset(Xenosetal.,2021),theMulti-LabelDi-alogueMalevolenceDetection(MDMD)dataset(Zhangetal.,2022),andtheProsocialDialogdataset(Kimetal.,2022).Foreachdataset,weselecttwoturnsfromeachconversationalthread,representingtheprecedingparentcontextandthesubsequentresponseasthesentencetoberewrit-ten.WeusetoxicityscoresfromPerspectiveAPI,15
Fleiss’κ0.19260.29060.30030.19070.5167Table7:Inter-rateragreementscoresforhumanevalua-tionresultsofde-toxificationtask
16https://huggingface.co/tomh/toxigen_hatebert17https://huggingface.co/tomh/toxigen_roberta
Krippendorff’sα0.27570.37780.43460.24070.6855
12Weusereddit-corpus-smallfromhttp://convokit.cornell.edu/documentation/subreddit.html13https://huggingface.co/s-nlp/roberta-base-formality-ranker14https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment15https://perspectiveapi.com/HateBert16andHateRoberta17tomeasurethetoxi-cityofthecontextandresponses.BModelingContextinRewritingWeperformfew-shotpromptingexperimentswithGPT-3.5andGPT-NeoX.ForGPT-3.5,weuse2few-shotexamples,whileforGPT-NeoX,weuse10few-shotexamples.Eachfew-shotexamplewasmanuallyconstructedwiththeprecedingcontext,anoriginalsentencetoberewritten,aninstructionspecifyingtherequiredstyle,andasamplerewrite.Figures6and7displaythefew-shotpromptexam-plesthatweutilizedforgeneratingrewritesintheformalitychangetask.B.1In-contextlearningsampleRewritesTable6showssomeadditionalexamplerewritesfromGPT-3.5andGPT-NeoXforalltasks.CContextualHumanEvaluationWorkerselectionWeinvolveannotatorsfromUSAandCanadaonAmazonMechanicalTurk(MTurk),whovoluntarilyopt-inforeachtask.Werecruitannotatorsforeachstyletransfertaskviaacorrespondingqualificationtask.Inthequalifi-cationtask,annotatorsmustanswertwoquestionsperpairofrewrites:whichrewritehasthestrongeststylestrength(e.g.,mostformal),andwhichrewriteisthemostnaturalgiventheprecedingcontext.An-notatorsassessthreepairsofhandcraftedrewritesineachqualificationtask.Thosewhoaccuratelyansweratleastfiveofthesixquestions(threeforstyleandatleasttwofornaturalness)areapprovedforthemaintask.Onceapproved,wepaythem$0.27USDperhead-to-headcomparison.C.1HumanEvaluationResultsWepresenttheagreementresultsofthehumaneval-uationstudiesofdetoxification(Table7),formalitychange(Table8)andsentimenttransfer(Table9).Additionally,refertoFigures4and5forscreen-shotsofthehumanevaluationinstructionsprovidedtoannotatorsandtheactualtask,respectively.inter-rateragreementStyleStrengthEventMeaningIntendedMeaningNaturalnessFit
11434


Fit0.180.280.170.27
Naturalness0.110.260.100.24Table11:Formalitytask:SpearmanrankandKendallCorrelationofnon-contextualevaluationmetricswithhumanjudgmentLexical(ρ)Semantic(ρ)Lexical(τ)Semantic(τ)
Fleiss’κ0.5520.25040.26670.2530.3627Table8:Inter-rateragreementscoresforhumanevalua-tionresultsofformalitytransfertaskinter-rateragreementStyleStrengthEventMeaningIntendedMeaningNaturalnessFit
Fleiss’κ0.1210.19640.31480.35810.2434Table9:Inter-rateragreementscoresforhumanevalua-tionresultsofsentimentchangetaskDNon-contextualAutomaticEvaluationWepresentthenon-contextualautomatedevalu-ationresultsforeachtask-specificdataset.Fig-ures16and17illustratetheformalitychangere-sultsfordocument-levelandconversation-leveldatasets,respectively.Figures18and19displaythesentimenttransferresultsfordocument-levelandconversation-leveldatasets,respectively.Fig-ures27,21,and22depictthede-toxificationresultsforconversationaldatasets.Notably,allofthesefig-uresexhibitsimilartrendstotheaggregateresultsacrossalltasksanddatasetspresentedinFigure3.D.1CorrelationwithHumanJudgmentsEffectiveevaluationmetricsshouldyieldjudg-mentsthatcorrelatehighlywithhumanjudgments,assumingthathumanevaluatorsrepresentagold-standard.Forthehumanjudgmentsalongthedi-mensionsofnaturalnessandfit,wemaphumanpreferencesasfollows:‘contextual’to1,‘tie’to0,and‘non-contextual’to−1.Fortheautomaticmetrics,weassignascoreof1ifametricscoresthecontextualrewritehigherthanthenon-contextualrewrite,and−1ifthemetricscoresarelowerforcontextualrewrites.Foragivenautomaticmetricandhumanjudg-mentdimension,wecalculatetheSpearmanrankρcorrelationandKendall’sτforthedatasetsam-plesusedduringthecontextualhumanevaluation§5.1.Thecorrelationscores,rangingfrom−1to1,areobtainedbycomparingthemappedautomaticscoreswiththemappedhumanjudgmentscores.Highervaluesindicateastrongercorrelationbe-tweenthescoresobtainedusingthecomparisonmetricandjudgmentsmadebyhumanevaluators.RefertoTables10–12forthecorrelationscoresofnon-contextualevaluationmetricswithhumanjudgmentsforeachtask.Lexical(ρ)Semantic(ρ)Lexical(τ)Semantic(τ)
Fit0.630.560.850.610.540.82
Naturalness0.680.690.940.650.660.90Table14:Formalitytask:SpearmanrankandKendallCorrelationofcontextualevaluationmetricswithhumanjudgmentLexical(ρ)Semantic(ρ)CtxSimFit(ρ)Lexical(τ)Semantic(τ)CtxSimFit(τ)
Naturalness-0.030.18-0.030.17Table10:Detoxificationtask:SpearmanrankandKendallCorrelationofnon-contextualevaluationmet-ricswithhumanjudgmentLexical(ρ)Semantic(ρ)Lexical(τ)Semantic(τ)
Naturalness0.590.580.880.560.550.84Table13:Detoxificationtask:SpearmanrankandKendallCorrelationofcontextualevaluationmetricswithhumanjudgmentLexical(ρ)Semantic(ρ)CtxSimFit(ρ)Lexical(τ)Semantic(τ)CtxSimFit(τ)
Naturalness0.440.510.730.420.480.69Table15:Sentimenttask:SpearmanrankandKendallCorrelationofcontextualevaluationmetricswithhumanjudgmentEContextualAutomaticEvaluationWepresentthecontextualautomatedevaluationresultsforeachtask-specificdataset.Figures23and24illustratetheformalitychangeresultsfordocument-levelandconversation-leveldatasets,re-spectively.Figures25and26displaythesen-timenttransferresultsfordocument-leveland
Fit0.110.260.100.25
Krippendorff’sα0.68250.33110.4280.35510.4322
Fit-0.020.14-0.020.14
Fit0.450.450.780.420.520.74
Naturalness-0.050.13-0.050.12Table12:Sentimenttask:SpearmanrankandKendallCorrelationofnon-contextualevaluationmetricswithhumanjudgmentLexical(ρ)Semantic(ρ)CtxSimFit(ρ)Lexical(τ)Semantic(τ)CtxSimFit(τ)
inter-rateragreementStyleStrengthEventMeaningIntendedMeaningNaturalnessFit
Fit0.740.680.930.710.650.89
Krippendorff’sα0.18680.26360.42920.37290.4581
11435


conversation-leveldatasets,respectively.Figures27,28,and29depictthede-toxificationresultsforconversationaldatasets.AllofthesefiguresexhibitsimilartrendstotheaggregateresultsacrossalltasksanddatasetspresentedinFigure3andtheyalignwiththefindingsfromourcontextualhumanevaluationstudy.E.1CorrelationwithHumanJudgmentsSimilarto§D.1,wemeasuretheSpearmanrankρcorrelationandKendall’sτcorrelationforthesamplesusedduringhumanevaluationin§5.1.Re-fertoTables13–15forthecorrelationscoresofnon-contextualevaluationmetricswithhumanjudgmentsforeachtask.
11436


Figure4:Screenshotoftheinstructionsforhumanevaluationannotation
11437


Figure5:Screenshotofthehumanevaluationannotationtask
11438


(f)SentimentNegative->Positive(cid:220)IhadtomoveitsothatIcangetmybikethrough.(cid:215)Howdareyoubesoinconsiderate?That’swrong.GPT-3.5(cid:215)Itwasveryconsider-ateofyoutomoveit,sothatyoucangetyourbikethrough.That’sgreat.(cid:215)Howkindofyoutobesoconsiderate!That’sright.GPT-NeoX(cid:215)Iunderstandyouneededtomoveitsoyoucouldgetyourbikethrough-thankyouforbeingthoughtful!(cid:215)Howcanyoubesocon-siderate!That’sright.
TaskConversionContextOriginalSentenceModelContextualRewriteNon-contextualRewrite
(b)FormalitySemi-formal->Informal(cid:220)Icanneveragaintakeasiteseriousthatgivesthemessand...justanawfulgame.(cid:215)Ilikedthatgame.It’salmostasifopinionsaresubjective.GPT-3.5(cid:215)IenjoyedNiNoKuni2,butit’sclearthatotherpeopledidn’t-andthat’sokay.Everyone’sgottheirownopinion!(cid:215)Ienjoyedthatgame.Itseemsopinionsmayin-deedvaryamongindividu-als.GPT-NeoX(cid:215)Ilikedthatgame,de-spitecriticalopinionstothecontrary.Thefactthatpeopleenjoyeditdoesn’tmakeitanyworsethananythingelse.(cid:215)Ilikedthatgame.It’salmostasifopinionsarerelative.
(c)DetoxificationToxic->LessToxic(cid:220)DidRolandojustcallmejusttotellmeshutup?(cid:215)@USERfuckyeahIdid!!ShutupGPT-3.5(cid:215)@USERYes,Iwasask-ingyoutobequiet.(cid:215)Iamveryproudofmyaccomplishment!yeahGPT-NeoX(cid:215)useryes,pleasekeepquiet(cid:215)yeah,Idid!Shutup.
Table6:ExamplesfromourdatasetsandcorrespondingrewritesgeneratedbyGPT-3.5andGPT-NeoX,showcasingallthreetasks:formalitychange,de-toxification,andsentimenttransfer.@=documentcontext,(cid:15)=sentencetorewrite,(cid:15)=contextualrewrite,(cid:15)=non-contextualrewrite;(cid:220)=previousturninconversation,(cid:215)=responsetorewrite,(cid:215)=contextualrewriteofresponse,(cid:215)=non-contextualrewriteofresponse
(e)SentimentPositive->Negative@Ijustwanttoechowhateveryoneelsehasalreadysaidaboutthismechanic...NotonlydidTimfixmycarquicklyandinexpensively,heactuallycametopickituphimselfsothatIwouldn’thavetospendmoneyonatowtruck!(cid:15)He’salsoanextremelyfriendly,funnyguywhoputsyouateaseimmediately.GPT-3.5(cid:15)Despitebeinganefficientmechanic,Timlackedthefriendlyandhumorousdemeanorexpectedofaserviceprovider.(cid:15)He’sanunfriendlyandhumorlesspersonwhomakesyoufeeluneasyrightaway.GPT-NeoX(cid:15)However,Tim’snotveryfriendlyorhumor-ouswhichmightmakeyoufeeluneasy(cid:15)Heisalsoaveryrude,unamusedguywhomakesyoufeelanxious.
(d)DetoxificationToxic->LessToxic(cid:220)"iamhere2playgamesiamplayin1witu10:50to2:42babyi’meverywhare"(cid:215)It’ssoamusingwhentheilliteratetrytotaunt.GPT-3.5(cid:215)I’mnotsureIunder-standwhatyou’retryingtosay.Couldyoupleaserephrase?(cid:215)It’sinterestingtoseehowsomeoneisilliteratetriestotease.GPT-NeoX(cid:215)It’samusingwhensomeoneattemptstoinsultortauntmeandtheyareclearlyuneducatedaboutlanguage.(cid:215)It’sfunnywhentheun-educatedtrytoinsult.
(a)FormalityInformal->Formal@Iamreadingabook,777MathematicalConversationStartersbyJohndePillis.Itsaninterestingbookwithlotsofquotes,poems(cid:15)Herezsomeofthem,andfurtherIwillkeepupdatingthispagewithwhatIlike.GPT-3.5(cid:15)Iplantocontinuetoup-datethispagewithfurtherselectionsthatIfindpartic-ularlyintriguingfromthebook.(cid:15)Herearesomeofthem,andIwillcontinuetoup-datethispagewithmypreferences.GPT-NeoX(cid:15)Herearesomeofthem,andinalaterpostIwillkeepupdatingthispagewithmyfavouritequotesfromthebook.(cid:15)HereissomeoftheoneandfurtherIwillupdatethispagewithwhatelseIfindinteresting.
11439


modelrewrite typeLexicalSemanticFluencyStyleROUMETWERBERT-SSBERTSmatchPPL GPT-3.5contextual0.300.541.030.910.630.5336.310.69non-contextual0.450.670.650.930.750.6842.820.64random-context0.300.521.070.910.590.53GPT-NeoXcontextual0.160.301.660.870.430.2242.390.35non-contextual0.330.480.810.900.590.5064.090.25random-context0.180.331.630.880.430.30Table19:Non-contextualAutomaticEvaluationResultsonSentiment:ConversationalcontextfromDailyDialogdataset
modelrewrite typeLexicalSemanticFluencyStyleROUMETWERBERT-SSBERTSmatchPPL GPT-3.5contextual0.180.361.570.900.590.4042.210.74non-contextual0.400.610.640.940.800.6358.380.64random-context0.140.301.740.890.490.36GPT-NeoXcontextual0.270.431.430.910.560.4157.640.49non-contextual0.450.600.670.940.740.6273.020.49random-context0.290.441.370.910.560.42Table18:Non-contextualAutomaticEvaluationResultsonSentiment:Document-levelcontextcomprisedofYelpReviews
modelrewrite typeLexicalSemanticFluencyStyleROUMETWERBERT-SSBERTSmatchPPL GPT-3.5contextual0.160.382.670.900.670.4533.780.68non-contextual0.220.411.230.910.720.5340.060.67random-context0.150.323.720.890.580.43GPT-NeoXcontextual0.240.411.970.900.650.4452.450.45non-contextual0.360.550.980.920.780.5457.120.37random-context0.270.402.700.900.600.44Table17:Non-contextualAutomaticEvaluationResultsonFormality:ConversationalcontextcomprisedofRedditthreads
modelrewrite typeLexicalSemanticFluencyStyleStyleStyleROUMETWERBERT-SSBERTSmatchPPL HateRobertaHateBertPerspectiveGPT-3.5contextual0.200.360.940.900.640.4537.920.010.410.06non-contextual0.240.410.800.910.720.5140.980.010.470.07random-context0.170.320.970.890.570.43GPT-NeoXcontextual0.320.400.780.900.600.4663.040.070.610.13non-contextual0.440.520.610.920.710.5767.470.100.690.15random-context0.320.400.770.900.580.47Table20:Non-contextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromCCCdataset
modelrewrite typeLexicalSemanticFluencyStyleROUMETWERBERT-SSBERTSmatchPPL GPT-3.5contextual0.190.402.140.920.620.5138.370.59non-contextual0.280.490.910.940.730.6743.400.58random-context0.180.353.170.910.520.47GPT-NeoXcontextual0.260.421.880.910.600.4744.590.42non-contextual0.450.630.720.950.800.6744.800.35random-context0.210.361.900.910.600.41Table16:Non-contextualAutomaticEvaluationResultsonFormality:Document-levelcontextfromCNN/DailyMail+BlogAuthorshipCorpus
11440


modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.140.270.910.890.660.3728.820.880.89non-contextual0.140.240.950.880.560.3641.020.820.87random-context0.110.201.580.870.470.3246.790.730.81GPT-NeoXcontextual0.210.290.880.890.640.3634.740.900.90non-contextual0.230.290.880.880.580.3652.450.860.89random-context0.170.221.290.870.460.3153.980.800.85Table24:ContextualAutomaticEvaluationResultsonFormality:ConversationalcontextcomprisedofRedditthreads
modelrewrite typeLexicalSemanticFluencyStyleStyleStyleROUMETWERBERT-SSBERTSmatchPPL HateRobertaHateBertPerspectiveGPT-3.5contextual0.110.321.180.870.510.4375.480.040.310.11non-contextual0.120.340.990.880.560.4778.230.050.340.12random-context0.080.281.240.860.420.40GPT-NeoXcontextual0.180.281.290.870.450.3980.100.390.620.35non-contextual0.320.490.910.900.670.54106.660.520.740.46random-context0.150.251.410.860.390.36Table21:Non-contextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromMDMDdataset
modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.110.170.920.880.530.2425.530.980.94non-contextual0.160.170.870.880.480.2441.180.930.94random-context0.070.120.930.860.380.1944.130.820.86GPT-NeoXcontextual0.130.160.900.870.470.2333.050.960.93non-contextual0.170.180.870.880.440.2248.590.930.93random-context0.120.150.910.870.410.2053.440.910.91Table25:ContextualAutomaticEvaluationResultsonSentiment:Document-levelcontextcomprisedofYelpReviews
modelrewrite typeLexicalSemanticFluencyStyleStyleStyleROUMETWERBERT-SSBERTSmatchPPL HateRobertaHateBertPerspectiveGPT-3.5contextual0.050.211.690.880.380.2922.800.030.250.06non-contextual0.110.290.970.910.520.4133.000.140.400.09random-context0.050.191.610.880.250.29GPT-NeoXcontextual0.250.401.120.910.530.4432.860.370.630.26non-contextual0.430.590.660.940.720.6337.900.640.790.38random-context0.250.401.060.910.480.44Table22:Non-contextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromProsocialDialogdataset
modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.160.230.890.900.610.3822.050.940.93non-contextual0.150.200.870.890.500.3332.790.870.91random-context0.100.161.060.870.380.2934.240.690.80GPT-NeoXcontextual0.250.280.830.900.620.3720.510.970.94non-contextual0.230.240.840.890.520.3332.150.940.94random-context0.160.190.920.870.420.2939.180.820.87Table23:ContextualAutomaticEvaluationResultsonFormality:Document-levelcontextfromCNN/DailyMail+BlogAuthorshipCorpus
11441


modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.100.220.910.860.500.3449.500.960.92non-contextual0.080.190.920.860.400.3170.910.860.87random-context0.060.160.960.840.290.2971.090.820.84GPT-NeoXcontextual0.200.250.850.870.520.3540.790.930.90non-contextual0.190.260.870.870.470.3690.640.890.90random-context0.090.150.970.850.300.2692.210.760.81Table28:ContextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromMDMDdataset
modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.090.180.900.890.540.2914.890.980.93non-contextual0.060.120.930.880.360.2129.750.890.90random-context0.030.110.950.860.210.2232.580.840.86GPT-NeoXcontextual0.190.230.850.890.520.3218.980.940.93non-contextual0.200.220.850.890.440.3034.420.880.91random-context0.120.170.900.880.330.2636.320.840.88Table29:ContextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromProsocialDialogdataset
modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.160.240.860.880.610.3628.450.950.93non-contextual0.170.250.850.880.570.3537.400.910.91random-context0.120.200.900.870.470.3138.960.890.89GPT-NeoXcontextual0.240.250.820.880.540.3437.410.960.93non-contextual0.290.290.770.890.540.3251.560.920.92random-context0.210.230.840.870.440.3252.240.890.90Table27:ContextualAutomaticEvaluationResultsonToxicity:ConversationalcontextfromCCCdataset
modelrewrite typeLexicalSemanticCoherenceCohesivenessCustomROUᶜᵗˣMETᶜᵗˣWERᶜᵗˣBERT-SᶜᵗˣSBERTᶜᵗˣSmatchᶜᵗˣPPLᶜᵗˣNSPCtxSimFitGPT-3.5contextual0.250.360.840.890.620.4333.880.970.94non-contextual0.280.350.810.890.540.4150.600.920.93random-context0.200.300.890.880.450.3854.100.870.89GPT-NeoXcontextual0.170.260.970.860.460.3432.450.880.88non-contextual0.200.240.870.870.420.2160.310.860.88random-context0.120.211.040.850.340.2141.690.790.83Table26:ContextualAutomaticEvaluationResultsonSentiment:ConversationalcontextfromDailyDialogdataset
11442


Figure6:Formality:10-shotpromptingexamplesforGPT-NeoX
11443


Figure7:Formality:2-shotpromptingexamplesforGPT-3.5
11444