TowardsExtractingandUnderstandingtheImplicitRubricsofTransformerBasedAutomatedEssayScoringModelsJamesFiaccoLanguageTechnologiesInstituteCarnegieMellonUniversityjfiacco@cs.cmu.eduDavidAdamsonTurnitindadamson@turnitin.comCarolynP.RoséLanguageTechnologiesInstituteCarnegieMellonUniversitycprose@cs.cmu.eduAbstractByaligningthefunctionalcomponentsderivedfromtheactivationsoftransformermodelstrainedforAESwithexternalknowledgesuchashuman-understandablefeaturegroups,theproposedmethodimprovestheinterpretabil-ityofaLongformerAutomatedEssayScoring(AES)systemandprovidestoolsforperform-ingsuchanalysesonfurtherneuralAESsys-tems.TheanalysisfocusesonmodelstrainedtoscoreessaysbasedonORGANIZATION,MAINIDEA,SUPPORT,andLANGUAGE.Thefind-ingsprovideinsightsintothemodels’decision-makingprocesses,biases,andlimitations,con-tributingtothedevelopmentofmoretranspar-entandreliableAESsystems.1IntroductionSinceitsinceptionover50yearsago(Page,1966),AutomatedEssayScoring(AES)hasbeenavalu-ableapproachforevaluatinglargequantitiesofstudentessays.Recentdevelopmentsinthefieldhavesoughttoharnessadvancednaturallanguageprocessingtechniquestoscoreessaysonparwithhumanraters,achievingsignificantprogresstowardthatgoal(RameshandSanampudi,2022;HuaweiandAryadoust,2023;MizumotoandEguchi,2023).Theinabilitytounderstandthelearnedrepresen-tationsindeeplearningbasedAESmodelsintro-ducesriskandvalidityconcernstotheirwidespreaduseineducationalsettings(Dingetal.,2020;Ku-maretal.,2020,2023).Inresponsetothiscon-cern,weproposeafunctionalcomponent-basedap-proachtoscrutinizetheactivationsoftransformermodelstrainedforAES.Theprimarygoalofthisstudyistoprovideamethodandtoolthatcanprovideacoherentandinterpretableunderstandingofthefunctionsper-formedbytheseneuralmodels,comparingtheiroverlapsanddifferences,andaligningthelearnedfunctionswithhuman-understandablegroupsoffeatures1.Muchinthesamewaythathumaneval-uatorsuserubricstoguidetheirscoringofessays,neuralmodelslearnasetoffeaturesandconnec-tionsthat,whencombinedandappliedtoanessay,repeatablydeterminethescorethattheywillas-sign.Throughthecomparisonandcontrastofthesecomponentsacrossmodels,weinvestigatehowthemodelsprioritizedifferentaspectsofwritingandmakestridetowardsunveilingthattheirlearnedrubricsare,alongsideanyunderlyingbiasesorlim-itationsthattheyentail.Ultimately,thisin-depthanalysiswillenhanceourunderstandingoftheneu-ralmodels’decision-makingprocesses,therebycontributingtothedevelopmentofmoretranspar-entandreliableautomatedessayscoringsystems.Ourproposedmethodologyinvolvesextendingtheemergingdomainofneuralnetworkinterpreta-tionbyusingabstractfunctionalcomponents,en-ablingarobustcomparisonbetweenprobedfunc-tionalcomponentsofanetworkandindependentfeaturegroups.Thisapproachspecificallybuildsuponrecentworkonneuralprobesandderivedmethods,aligninganeuralnetwork’sactivationswithexternalknowledgesuchastaskmetadataandimplicitfeatures(e.g.,parts-of-speech,capitaliza-tion,etc.)(Conneauetal.,2018;Belinkov,2022).WefocusourinterpretationinthedomainofAESwhereeachmodelinourinvestigationistrainedtoscoreessaysbasedondistinctevaluationtraits,namelyORGANIZATION,MAINIDEA,SUPPORT,andLANGUAGE.Toprobethesemodels,thefeaturesaredrawn
1Codeandtoolavailableathttps://github.com/jfiacco/aes_neural_functional_groups
232 Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 232–241
July 13, 2023 c (cid:13)
2023 Association for Computational Linguistics


Figure1:Diagramvisualizingthestructureofthemethodology.Nodesofeachcolorrepresentcorre-latedvalues.task.Thiscanenablemodelanalyststoquicklyidentifyunexplainedcomponentsandbegintopro-posealternativepalletsoffeatures.Furthermore,thefunctionalcomponentscanrepresentinterme-diatestepswithintheneuralnetworkwhichwouldbeunobservablewiththesealternativemethods.FromtheeducationaltechnologiesandAuto-matedEssayScoringangle,ourworkprimarilyap-pliestothebodyofdeeplearning-basedAESmod-elssuchasrecurrentneuralnetworkmodels(Jinetal.,2018;Nadeemetal.,2019),convolutionalneuralnetworkmodels(TaghipourandNg,2016),andtransformermodels(SethiandSingh,2022).Whileourmethodcouldbeappliedtoanytypeofneuralmodel,wefocusontransformersastheyrep-resentthestate-of-the-art.Byintegratingtheinter-pretabilityofneuralmodelswiththeunderstandingofthefunctionalcomponentstheylearn,wehopetobridgethegapbetweenhuman-understandablefeaturesandneuralnetwork-basedessayscoring.Theinsightsgainedfromourmethodologycanguidethedevelopmentofmoreeffectiveandef-ficientAESsystems,tailoredtothespecificneedsofeducatorsandstudents.Furthermore,thelessonslearnedfromthisresearchmayextendbeyondtheAESdomain,providingvaluableinsightsforthebroaderfieldofnaturallanguageprocessingandmachinelearninginterpretability.3MethodsInthissectionwepresentourinterpretationap-proach(Figure1),definingthekeyconceptsoffunctionalcomponents,functionalgroup,feature,andfeaturegroup.Becausetheapproachnotablyabstractsawayfromcommontermsintheneuralnetworkliterature,throughoutthissectionwedrawananalogytohowonecandefineanddescribethecommonfeaturesbetweenmammalsbycomparing
fromseveralsourcesthatcorrespondtoconceptsofbothhighandlowvalidityforessayscoring:statis-ticalfeaturesofanessay(e.g.numberofsentences,numberofparagraphs,etc.)(Woodsetal.,2017),treefeaturesgeneratedfromRhetoricalStructureTheory(RST)(MannandThompson,1987)parsesoftheessays(Jiangetal.,2019;Fiaccoetal.,2022),essaypromptandgenre(West-Smithetal.,2018),andacombinationofalgorithmicallyderived(Der-czynskietal.,2015)andourownhumandefinedstyle-basedwordlists.Thesefeaturesprovidealensthatwhileunabletocaptureallofthecapabili-tiesofthemodels,provideinsightintosomeofthekeydifferencesbetweenthem.Inthefollowingsections,weprovideadetaileddescriptionofthemethodologyusedforthisanal-ysis,discusstheassumptionsunderpinningthemethod,andpresentpotentialexplanationsforcor-relatedfunction/featurepairsthroughaseriesofexperimentsthatvalidateourmethod’sabilitytoreflecttheinternalrubricofeachoftheneuralmod-els.2RelatedWorkFromtheinterpretabilityangle,themostcloselyrelatedworktothisisthatofneuralmodelprobes(Shietal.,2016;Adietal.,2016;Conneauetal.,2018;Zhuetal.,2018;Kuncoroetal.,2018;Khandelwaletal.,2018)whichhavefrequentlybeingusedtotestwhetheramodelhaslearnedasetofproperties(RyskinaandKnight,2021;Be-linkov,2022).Theprimarygapweareworkingtofillinfromthisbodyofliteratureisthatcurrentap-proaches,withfewexceptions(Fiaccoetal.,2019;Caoetal.,2021),focusonunderstandingtherolesofindividualneuronsinthegreaterneuralnetwork.Wecontendthatstudyingtheinterpretabilityofaneuralnetworkattheindividualneuronlevelcantooeasilyobscurethebroaderpicture.Ourinter-estliesinfurtherprogressincorporatingamoreabstractperspectiveonwhatislearnedbyneuralnetworks,complementingtheworkthathasbeendoneattheneuronlevel.Comparedtoalternativeparadigmsforinter-pretabilityinmachinelearningmodels,suchasLIME(Ribeiroetal.,2016)orSHAP(LundbergandLee,2017),whichevaluatethecontributionofagivenfeaturetothepredictionofamodel,thefunctionalcomponentbasedmethodsallowforamoregranularidentificationofimportantpartsofamodel,independentfromknownfeaturesfora
233


theircommonanduniquecharacteristics.3.1FunctionalComponentsandGroupsFunctionalcomponentsrefertothelearnedfunc-tionsofaneuralnetwork,muchlikeaparticularcomponentofadogmaybea“dogleg”.InaneuralAESsystem,thesewouldbeagroupofneuronsthathavecorrelatedactivationswhenvaryingtheinputessays.Theapproachtoextractingfunctionalcomponents(“neuralpathways”asdescribedbyFi-accoetal.(2019))fromaneuralnetworkconsistsoffindingthesetsofcoordinatedneuronactiva-tions,summarizedbythefollowingsteps:1.Savetheactivationsofneuronsforeachdatainstanceinthevalidationdatasetintoanacti-vationmatrix,AofsizeM×N,whereMisthenumberofdatainstancesinthevalidationsetandNisthenumberofneuronsbeingusedfortheanalysis.2.Performadimensionalityreduction,suchasPrincipalComponentAnalysis(PCA)(Hotelling,1933),onAtogetcomponentactivationmatrix,TmodelofsizeM×P,wherePisthenumberofprincipalcomponentsforagivenmodel.Functionalgroupsarecollectionsofsimilarfunc-tionalcomponents.Continuingtheanalogy,theywouldbecomparedtothemoregeneralconceptofa“leg”.Wecomputefunctionalgroupsbyconcate-natingthedimensionalityreducedmatrixes,Tmodel,ofthetwomodelsthataretobecomparedandperforminganadditionaldimensionalityreductionoverthatmatrixtogetamatrixofgroupactiva-tions,T.Thefunctionalcomponentsthatarehighlyloadedontoeachfunctionalgroupsareconsideredmembersofthatgroup.AnimportantdeparturefromFiaccoetal.(2019),stemmingfromthelimi-tationthatdoesPCAdoesnotguaranteeindepen-dencebetweencomponents,isthatweuseIndepen-dentComponentAnalysis(ICA)(Comon,1994)instead.ICAisadimensionalityreductiontech-niquethatmaximizestheindependencebetweencomponents,resultinginmorevalidityinthetech-nique’sresultingalignments.Todetermineifafunctionalgroupisinfluentialintheperformanceofthemodel(designatingitanimportantfunctionalgroup),wecancomputethePearson’scorrelationcoefficientbetweeneachcolumnofthegroupactivationmatrixandthepre-dictionsofthemodel,theerrorsofthemodel,andthedifferencesbetweenthecomparedmodels.3.2IndependentFeatureGroupsFeaturesarehumanunderstandableattributesthatcanbeextractedfromananalysisdataset.Intheanalogytheywouldrepresentpotentialdescriptorsofacomponentsofamammal,e.g.“hairy”.InanAEScontext,thesefeaturesmaymanifestas“nocapitalizationafteraperiod”.Ideally,itwouldbepossibletocreateadirectmappingfromeachofthefunctionalcomponentstoeachofthefeaturesforwhichthefunctionalcomponentisrelated.How-ever,thisisnon-trivialduringapost-hocanalysisbecause,withoutinterventions,therearelimita-tionsonwhatinformationisobtainable.Specifi-cally,becausefeaturesarenotnecessarilyindepen-dentfromeachother,theircorrelationscannotbeseparatedfromeachother,yieldingimpreciseinter-pretations.Itisthusrequiredforonlyindependentfeaturestobeusedastheunitofanalysiswhenitcomestoalignmentwithfunctionalcomponents.Unfortunately,inpractice,thisisaprohibitivere-strictionandmostfeaturesthatwouldbeinterestingaregoingtohavecorrelations.Fortunately,muchinthesamewaythatwecanuseICAtoextractindependentfunctionalcom-ponentsfromaneuralnetwork’sactivations,wecanuseittoconstructindependentfeaturegroupsthatcanbereasonablybealignedwiththefunc-tionalgroupsoftheneuralnetworks.Intheanalogy,theseindependentfeaturegroupscantherefore,bethoughtofascollectionsofdescriptivetermsthatcanidentifyacharacteristicofthemammal,suchas“anappendagethatcomesinpairsandcanbewalkedon”whichwouldalignwiththe“leg”func-tionalgroup.InAES,anexamplefeaturegroupmaybe“usespunctuationimproperly”.ItwouldbeexpectedthatthisfeaturegroupwouldalignwellwithafunctionalgroupinaneuralAESsystemthatcorrespondswithanegativeessayscore.Further-more,featuregroupsforAEScanbethoughtofasbeingroughlyanalogoustoconditionsthatwouldbeonanessayscoringrubric(aswellaspotentiallyotherfeaturesthatmaybeintuitiveorobvioustohumanscorersbutcontributetoaccuratescoring).Thespecificprocessusedtodefinethesegroupsistoperformadimensionalityreductiononeachsetoffeaturetypesthatmayhavesignificantcor-relationsandcollectingthemintoafeaturematrix.Wedothisprocessforeachfeaturetyperatherthan
234


overallfeaturesatoncebecausespuriouscorre-lationsbetweensomeunrelatedfeaturesmaycon-volutethefeaturegroups,makingthemfarmoredifficulttointerpret.3.3AlignmentUsingICAasthedimensionalityreduction,theindependentfunctionalgroupsoftheneuralmodelcanreasonablyalignwiththeindependentfeaturegroupsusingthefollowingformalprocedure:givenaneuralnetwork,N,withactivationmatrix,A(asabove),aindependentcomponentanalysisisperformedyieldingasetoffunctionalcomponents,F.Foreachfi,fk∈F,fi⊥⊥fk|X,Y,whereXisthesetofinputstotheneuralnetworkandYisthesetofpredictionsfromtheneuralnetwork.WithasufficientnumberofcomponentssuchthatFcontainsallindependentfunctionalcomponentsinA,ifthereexistsacommonlatentvariableinbothNandthesetofindependentfeaturegroups,G,withcomponentsgi∈G,thentherewillbesomefi∝∼gj.4ExperimentsInthissection,wedelveintothespecificmethod-ologyusedtoanalyzetheactivationsofthefourtransformermodelsforAES,aswellasthestepstakentopreparethedataandfeaturesforthisanal-ysis.4.1DatasetsAlthoughscoringrubricsarespecifictothegenreandgradelevelofawritingtask,therearecom-monalitiesbetweeneachrubricthatallowtheirtraitstobereasonablycombinedformodeling.Allourrubrics,forexample,includeLANGUAGE(andstyle)andORGANIZATIONtraits,thoughtheirex-pectationsvarybygenreandgradelevel.ThegenericMAINIDEAtraitcorrespondsto“Claim”and“ClarityandFocus”traits,andSUPPORTcor-respondsto"SupportandDevelopment"aswellas"AnalysisandEvidence."Rubricsandpromptsweredevelopedforvalidity,andessayswererig-orouslyhand-scoredbyindependentratersinthesamemannerasdescribedinWest-Smithetal.(2018).Foreachgenerictrait,thetrainingsetwassam-pleddownfromover50,000availableessays,re-spondingto95writingprompts.Essaysfrom77promptswereselectedforthetrainingset,andanother18wereheldoutforevaluation.Withineachsplit,essaysweresampledtominimizeim-balancebetweenessayscore,genre,gradelevel,Intheun-sampleddata,longeressaystendtobestronglycorrelatedwithessayscore,riskingover-fittingtothissurfacefeature.Similarly,amongthesubsetofdatawhereschooldistrictdatawasavailable,districtswithpredominantlyBlackenroll-mentwereunder-representedamongessayswithascoreof"4"acrossalltraits.Tocounteractthesepotentialbiases,theavailabledatawasbinnedbylengthanddistrictdemographicinformationforeachscore,genre,andgradelevel,andessayswereunder-sampledfromthelargestbins.Inadditiontothesebalancedessays,about800“offtopic”essaysrepresentingnonsenselanguageornon-academicwritingwereincludedinthedataset,withascoreofzero.4.2ModelsLongformersareatransformer-basedneuralnet-workarchitecturethathavegainedprominenceinvariousNLPtasks(Beltagyetal.,2020).InthecontextofAES,eachgenerictrait’smodelisaLongformerwithasingle-outputregressionhead,fine-tunedonthetrait’sbalanceddataset:Fortheremainderofthispaper,themodelfine-tunedonagiventraitwillbereferredtoas“theTRAITmodel”(e.g.theORGANIZATIONmodel)forsimplicity.Althoughordinalscoresfrom0to4wereusedforsamplingandevaluation,thetrainingdatala-belswerecontinuous,averagedfromraterscores.Essayswereprefixedwithtextrepresentingtheirgenre(e.g.,"HistoricalAnalysis")andprompt’sgraderange(e.g.,"grades10-12")beforetokeniza-tion,butnoothercontextforthewritingtask(e.g.,theprompt’stitle,instructions,orsourcematerial)wasincluded.InadditiontoLongformer’sslidingattentionwindowof512tokens,thefirstandlast32tokensreceivedglobalattention.Scoreswereroundedbacktointegersbetween0and4,beforeevaluation.Ontheholdoutprompts,overallQuadraticWeightedKappa(QWK)rangedfrom0.784forMAINIDEAto0.839forLAN-GUAGE,whilecorrelationwithwordcountre-mainedacceptablylow:0.441forLANGUAGEupto0.550forSUPPORT.TheactivationsoftheLongformermodelweresavedforeachinstanceintheanalysissetatthe“classify”tokentocreateamatrixofactivationsforthefunctionalcomponentextraction.
235


ModelAModelB#EssaysExtractedFeatures#IndependentFeatureGroups#AlignedIFG
ORGANIZATIONMAINIDEA40714811424ORGANIZATIONLANGUAGE2751188639ORGANIZATIONSUPPORT144906337LANGUAGEMAINIDEA3411299526LANGUAGESUPPORT72673823SUPPORTMAINIDEA2601279427
Table1:Comparinganalysisdatasetsizeandnumbersofextractedfeaturesforeachofthemodelcomparisons,identifiedbytheModelAandModelBcolumns.4.3FeaturesThefeaturesemployedinthisanalysisencompassstatisticalpropertiesoftheessays,treefeaturesgeneratedfromRhetoricalStructureTheory(RST)parsetreesoftheessays,essaypromptandgenre,acombinationofalgorithmicallyderivedandhuman-definedstyle-basedwordlists,andcertainschool-leveldemographicfeatures.Adescriptionofeachfeaturetypeisprovidedbelow:StatisticalFeatures:Whilestatisticalfeaturessuchasessaywordcountareoftengoodindicatorsofessayscore,theyarenotintrinsicallyvaluabletothedifferenttraitsthatourmodelsarescoring.Wethuswanttoseeloweralignmentwiththesefeaturestoindicatethatthemodelisnotoverlyrelyingonrudimentaryshortcutsscoringanes-say.Wealsoincludeaveragewordlength,essayparagraphcount,essaysentencecount,averagesentencelength,andthestandarddeviationofthesentencelengthforcompleteness.RSTTreeFeatures:Thesefeatureswereinte-gratedtocapturetherhetoricalstructureofthetext,suchasthehierarchyofprincipalandsub-ordinateclauses,thelogicalandtemporalrelationsbetweenpropositions,andthecoherenceoftheargument.Theseconceptshaveahighvalidityforscoringessays(Jiangetal.,2019),especiallyforORGANIZATION,sohighalignmentbetweenfunctionalgroupswouldbeexpected.TogenerateRSTtreesforeachessay,weutilizeapretrainedRSTparserspecificallyfine-tunedforstudentwrit-ing(Fiaccoetal.,2022).Weincludethepres-enceofanRSTrelationasafeatureaswellasre-lationtriplets(RELparent,RELchild1,RELchild2)astree-equivalentn-gram-likefeatures.EssayPromptandGenre:Categoricalrepresenta-tionsoftheessaypromptandgenrewereemployedasfeaturestoexamineifcomponentsoftheAESmodelwerepreferentiallyactivatedbasedonthecontentortopicoftheessay,alowvalidityfeature.AlgorithmicallyGeneratedWordListFeatures:WecalculatethefrequencyofusageofwordswithinalgorithmicallyderivedsetsofwordsintheessaysasagroupoffeaturestoprobetheAESmodel’sconsiderationforstylisticlanguage.Togeneratethesewordlists,weobtainBrownclus-ters(Brownetal.,1992)fromessays.WegenerateseparateBrownclustersforeachpromptinourdatasetandsubsequentlyderivefinalwordlistsbasedontheoverlapsofthoseclusters.Thisap-proachemphasizescommonstylisticfeaturesasopposedtocontent-basedclusters.HumanGeneratedWordListFeatures:Inaddi-tiontothealgorithmicallydefinedwordlists,wedeviseourownwordliststhatmayreflecthowtheAESmodelscoresessays.Wecreatedwordlistsforthefollowingcategories:simplewords,informallanguage,formallanguage,literaryterms,transi-tionwords,andwordsuniquetoAfricanAmericanVernacularEnglish(AAVE).DemographicFeatures:WeusedthepercenttoparticipantsintheNationalSchoolLunchProgram(NSLP)ataschoolasaweakproxyfortheeco-nomicstatusofastudent.Alsoasweakproxiesforeconomicstatusofessayauthors,weincludetheschoollevelfeaturesofnumberofstudentsandstu-dentteacherratio.Furthermore,weuseaschoolleveldistributionofethnicitystatisticsasaweakproxyfortheethnicinformationofanessay’sau-thor.Thesefeatureswereemployedtoinvestigatethemodel’sperceptionofanyrelationshipbetweenthewriter’sbackgroundandthequality,content,andstyleoftheessay,inordertogaininsightoftheequityoftheAESmodel.4.4AnalysisSettingsTochoosethenumberofcomponentsforICA,aPCAwasperformedtodeterminehowmanycom-ponentsexplained95%ofthevarianceoftheacti-vation(or99%ofthevarianceforthefeatures)tobeusedasthenumberofcomponentsoftheICA.
236


ModelAModelB#Comp.A#Comp.B#FG#AlignedFG#AOnly#BOnly#Mixed
Table2:Comparingnumberoffunctionalgroupsextractedforeachmodelcomparisonandpresentingthenum-beroffunctionalgroupsthatwerebothdeemedimportant(Section3.1)andsufficientlyalignedwithatleastonefeaturegroup.Alsospecifiedisthenumberoffunctionalgroupsthatareuniquetoaparticularmodelandthenumberthataresharedbetweenthemodelsofgivenacomparisonpair.Todeterminethatafunctionalgroupwasimportant,itneededtohaveanabsolutevalueofPearson’srvalueofgreaterthan0.2.Thisthresholdwasalsousedtodetermineifafunctionalgroupshouldbeconsideredalignedwithafeaturegroup.5ResultsInthissection,wepresentaggregatestatisticsforeachmodelcomparisonwhenitcomestocomput-ingfeaturesandindependentfeaturegroups(Ta-ble1),extractingfunctionalgroupsandaligningimportantfunctionalgroups(Table2),andlastly,weprovideexamplestakenfromthemodelcompar-isonbetweentheLANGUAGEmodelandtheMAINIDEAmodel.Duetolengthconstraints,wepresentdetailedexamplesofthiscomparisononly.SimilarfiguresandcorrelationstatisticscancanbefoundonGithub2.5.1IndependentFeatureGroupsSinceeachtrainedmodelheldoutadifferentsetofpromptsfromitstrainingset,commonpromptsbetweenanalysissetsneededtobeidentified,andthusthenumberoffeaturesextractedandthere-sultingindependentfeaturegroupsvarybetweenmodelcomparisons.Computingtheindependentfeaturegroupsforeachmodelcomparison(Table1)yieldedbetween70%and77%oftheoriginalex-tractedfeaturesforallcomparisons,exceptLAN-GUAGEVSUPPORT,whichonlyyielded57%asmanyindependentfeaturegroupscomparedtoorig-inalfeatures.Despitehighvariabilityinthenumberofindependentfeaturegroupsidentifiedduringtheprocess,amuchmorenarrowrangeofindepen-dentfeaturegroupswasalignedduringtheanalysis.
FunctionalGroupExtractionImportantFunctionalGroupAlignment
2https://github.com/jfiacco/aes_neural_functional_groups/tree/main/supplementary_results
Figure2:Alignmentdiagramforfunctionalgroups(left)thatarespecifictotheMAINIDEAmodelwiththeiralignmenttofeaturegroups(right).Onlyfunc-tionalgroupsandfeaturegroupsareshowniftheyhaveapositivecorrelationgreaterthan0.25(blueedges)oranegativecorrelationlessthan−0.25(rededges).ThenumberscorrespondtotheIDsofthefunctionalgrouporfeaturegroupthatthenoderepre-sents(seeTable3).Thetypesoffeaturegroupsthatwerealignedvar-iedconsiderablybetweendifferentcomparisons.5.2FunctionalComponentGroupsTheinitialextractionoffunctionalcomponentsforeachmodelelicitednumbersoffunctionalcompo-nentsbetween28and119.Table1and2showthatforagivenmodel,fewerfunctionalcomponentswillbeextractedgivenafewerinstancesintheanalysisdataset.Despitethisnoise,aclearpatternemergeswheretheORGANIZATIONmodelhasthemostfunctionalcomponents,followedbytheLAN-GUAGEmodel.TheMAINIDEAmodelhasfewerfunctionalcomponents,withtheSUPPORTmodelhavingthefewest.Whenperformingthedimensionalityreductiontocomputethefunctionalgroups,thereisacon-sistentreductiontoapproximately61-71%ofthecombinedtotalfunctionalcomponents.5.3ImportantFunctionalGroupsDespitethevarianceinthenumberoffeaturegroupsandfunctionalgroupsextractedpercom-parison,thereisaremarkablyconsistentnumberof
ORGANIZATIONMAINIDEA119551252212010ORGANIZATIONLANGUAGE96661102911018ORGANIZATIONSUPPORT663668229112LANGUAGEMAINIDEA785593238312LANGUAGESUPPORT34283813229SUPPORTMAINIDEA454964252221
237


FunctionalGroup92Predictions:LANGUAGEr=−0.13(p<0.05)
FunctionalGroup46Diff:LANGUAGEVSMAINIDEAr=−0.39(p<0.001)
Table3:Selectedexamplesofcorrelatedfunctionalgroup/featuregroups.Pearson’sRvaluesforrelevantimportancemetric(modeldifference,modelpredic-tions)andfeaturegroupalignmentarepresentedwithp-values.statisticalfeaturesoftheessay.Furthermore,bycomputingthecorrelationsbetweentheindividualfeatureswithinthattype,itwasdeterminedthatnumberofparagraphsislikelythemostsalientcontributor.ThesecondsetoftrendsispresentedinTable4,wherethepercentofthetotalalignedfeaturegroupspermodelwascomputed.ThisrevealedthattheOR-GANIZATIONmodelhadconsiderablymorealignedRST-basedfeaturesthantheothermodels,whiletheMAINIDEAmodelhadtheleastproportion.TheLANGUAGEmodelhadthemostalignedwordlistfeatures,whichisthecombinationofthealgo-rithmicallyandhuman-createdwordlistfeatures.Forthelastpercentage,wecombinethepromptanddemographicfeaturesandfindthattheSUP-
ModelErrors:MAINIDEA(+),ModelPairDifference(+),ModelErrors:LANGUAGE(-)
WordCluster:PRIORITIES(+),WordClus-ter:POPULATIONCOMPARISION(+),WordClus-ter:EFFICIENCY(+),WordCluster:TEENVALUES(-),WordCluster:STORYTELLING(-),WordCluster:SCHOOL(-),WordCluster:PARENTALDECISIONS(-),WordClus-ter:INFORMAL(-),WordCluster:HISTORICALCONFLICT(-)
IndependentFeatureGroup69r=0.22(p<0.001)
EssayStats:STDDEVSENTENCELENGTH(+),Es-sayStats:NUMSENTENCES(+),EssayStats:MEAN-WORDLENGTH(+),EssayStats:NUMWORDS(-),Es-sayStats:NUMPARAGRAPHS(-),EssayStats:MEANSEN-TENCELENGTH(-)
RST:NN|CONTRAST(+),RST:SN|EVALUATION(NS|ELABORATION,LEAF)(+),RST:SN|BACKGROUND(LEAF,NS|ELABORATION)(+),RST:NS|EVIDENCE(LEAF,NN|CONJUNCTION)(+),RST:NN|JOINT(NN|CONJUNCTION,NN|JOINT)(+),RST:NN|CONTRAST(LEAF,LEAF)(+),RST:NN|CONJUNCTION(NS|ELABORATION,NN|CONJUNCTION)(+),RST:SN|EVALUATION(NN|CONJUNCTION,LEAF)(-),RST:NN|CONJUNCTION(LEAF,LEAF)(-)
Figure3:Alignmentdiagramforfunctionalgroups(left)thatarecommontoboththeLANGUAGEandMAINIDEAmodelswiththeiralignmenttofeaturegroups(right).Onlyfunctionalgroupsandfeaturegroupsareshowniftheyhaveapositivecorrelationgreaterthan0.25(blueedges)oranegativecorrelationlessthan−0.25(rededges).ThenumberscorrespondtotheIDsofthefunctionalgrouporfeaturegroupthatthenoderepresents(seeTable3).importantfunctionalgroupsthathaveatleastonesufficientalignmenttoafeaturegroup(Table2).WiththeexceptionoftheLANGUAGEVSUPPORTcomparison,allothercomparisonshadbetween21and29alignedfunctionalgroups.Asavisualaidfortheimportantfunctionalgroups,seetheleftsidesofFigures2and3.EachFigureisderivedfromthefunctionalgroupsandfeaturegroupsoftheLANGUAGEVMAINIDEAcomparison.Thenumbersoneachnodearetheidentifiersofagivenfunctionalgroup,asubsetofwhicharerepresentedinTable3.5.4AlignmentofFunctionalGroupsTheentiretyoffindingsfromthealignmentsforallofthecomparisonswouldbetoonumeroustopresentinaconferencepaperformat.However,wewillpresentthemajortrendswefoundinouranalysis.Thefirstmaintrendisthatallmodelshadfunctionalgroupsthatwecorrelatedwiththe
IndependentFeatureGroup12r=−0.20(p<0.001)
IndependentFeatureGroup21r=0.75(p<0.001)
IndependentFeatureGroup1r=−0.43(p<0.001)
FunctionalGroup56Predictions:MAINIDEAr=−0.13(p<0.05)
238


Table4:%ofalignedfeaturegroupsforagivenmodelbyfeaturetype.PORTmodeltendedtoalignwithfewerofthesetypesoffeatures.ThereasonforcombiningthedemographicandpromptfeaturesisdiscussedinSection6.5.5QualitativeAnalysisWhilethemethodthatwepresentedcanquicklyadvanceone’sunderstandingofamodelfromtheblack-boxneuralnetworktoalignedfeaturegroupsdirectly,understandingwhatfunctionafeaturegrouprepresentscanbemoredifficult.Itisthusnecessarytoresolvewhatafeaturegrouprepre-sentstoformastrongstatementonwhatthemodelisdoing.Forinstance,wefounditconcerningthatsomanyofthemodelswereconnectedwithfea-turegroupsthatcontaineddemographicfeatures(coloredredinFigures2and3).However,aquali-tativelookatthedatasetsforwhichpromptswereincluded,wefoundthatthedistributionofpromptsoverthedifferentschools,whencontrollingfores-saylength,weresuchthatcertainschools(withtheirdemographicfeatures)weretheonlysourceofcertainprompts.It,therefore,becomeslikelythatmanyofthesefeaturegroupsaremoretopic-basedratherthanthepotentiallymoreproblem-aticdemographic-based.Thisinterpretationwasreinforcedbymanyofthefeaturegroupswithdemographicinformationalsoincludingprompts(e.g.“IndependentFeatureGroup29”fromTa-ble3)andbyexaminingessaysthatpresentthosefeaturegroups.6DiscussionTheresultspresentedintheprecedingsectiondemonstratetheefficacyoftheproposedmethodinextractingsalientfeaturegroupsandfunctionalgroupsfromtheneuralmodels,particularlywhenappliedtothedatasetunderconsideration.Thetruepotentialofthismethod,however,liesinitscapac-itytobebroadlyappliedtoanyneuralAESsystem,therebyfacilitatingadeeperunderstandingofthemodelsandtheunderlyingprocessestheyemploy.Inthefollowingdiscussion,wewilldelvefurtherintotheresults,emphasizingtheprominenttrendsobservedinthealignmentoffunctionalgroupsandtheircorrelationwithessayfeatures,aswellastheimplicationsofthesefindingsforenhancingtheinterpretabilityandtransparencyofneuralAESsystems.6.1FunctionalComponentandFeatureGroupsTheproposedmethodsuccessfullyextractedmean-ingfulfunctionalgroupsfromtheanalyzedneu-ralmodels.Notably,theLANGUAGEVSUPPORTcomparisonemergedasanoutlierinseveralofouranalyses.Thisdiscrepancyislikelyattributabletotheconsiderablyfeweressayssharedbybothmodels’analysissets,whichmayresultinanois-ieranalysisandexposealimitationofthemethod.Asthesizeoftheanalysisincreases,onewouldexpecttheextractionoffeaturegroupsandfunc-tiongroupstoapproachtheiridealindependencecharacteristics.Despitethislimitation,themethodmanagedtocondensetheanalysisspacefromthou-sandsofactivationstofewerthan125whilestillaccountingforover90%ofthemodel’svariance.Interestingly,theORGANIZATIONmodelexhib-itedthehighestnumberoffunctionalgroups.ThisobservationsuggeststhatcapturingtheORGANIZA-TIONtraitisamoreintricateprocess,necessitatingthelearningofadditionalfeatures.ThisnotionisfurthercorroboratedbythecomparisonsbetweenORGANIZATIONandothermodels;modelswhichdisplayedveryfew,ifany,functionalgroupsexclu-sivelypresentinthenon-organizationmodels.6.2AlignmentofImportantFunctionalGroupsInlinewithourexpectations,theORGANIZATIONmodeldemonstratedthegreatestalignmentwiththeRSTtreefeatures,whiletheLANGUAGEmodeldisplayedthemostsignificantalignmentwiththewordlistfeatures.ItwaspostulatedthatORGA-NIZATIONwouldnecessitatethemodeltopossessknowledgeofhowideaswithinessaysarestruc-turedinrelationtoeachother,atypeofknowledgeencodedbyrhetoricalstructuretheory.AlthoughtheRSTparsetreesrecoveredfromtheparserareconsiderablynoisy(RSTparsingofstudentessaydatahasbeenshowntobemarkedlymorechalleng-ingthanstandarddatasets(Fiaccoetal.,2022)),thesignalremainedsignificant.Furthermore,wean-ticipatedthattheLANGUAGEmodelwouldhavea
%Word%Demo.&Model%RSTListPrompt
ORGANIZATION411321LANGUAGE302619SUPPORT361913MAINIDEA232123
239


greaterrelianceonwordchoice,aconceptmirroredbythewordlist-basedfeaturegroups.Contrarytoourexpectations,theMAINIDEAmodelexhibitedthehighestnumberofprompt-basedfeaturegroups.Ourmostplausibleexpla-nationforthisobservationisthatcertainpromptsmighthaveclearerexpectationsforthesisstate-mentsthanothers,anotiongenerallysupportedbyaqualitativeexaminationoftheessaysfrompromptsthatscorehigheronMAINIDEA.7ConclusionTheneuralnetworkinterpretationtechniquepre-sentedinthispaperdemonstratessignificantpromiseinlearningtheimplicitrubricsofneuralautomatedessayscoringmodels.Byeffectivelymappingtheintricaterelationshipsbetweenfeaturegroupsandthefunctionalgroupsoftheunderlyingscoringmechanism,thetechniqueprovidesasteptowardsanunderstandingofthefactorscontribut-ingtoatransformer’sevaluationofessayquality.Thisenhancedunderstandingenablesresearchersandeducatorstonotonlyidentifypotentialbiasesinscoringmodels,butalsotorefinetheirmodelstoensureamorereliableandfairassessmentofstudentperformance.ThecodeforthismethodwillbereleasedandincorporatedintoananalysistoolforapplicationtoneuralmodelsnotlimitedtotheonesexaminedinthisworkwiththegoaltopavethewayforthedevelopmentofmoretransparencyinneuralAESmodels.Theseadvancementscancontributetotheoverarchinggoalofpromotingethicalandresponsi-bleAIineducationbyfacilitatingtheexaminationandcomprehensionofcomplexneuralmodels.AcknowledgementsThisworkwassupportedinpartbyNSFgrantDRL1949110.ReferencesYossiAdi,EinatKermany,YonatanBelinkov,OferLavi,andYoavGoldberg.2016.Fine-grainedanalysisofsentenceembeddingsusingauxiliarypredictiontasks.arXivpreprintarXiv:1608.04207.YonatanBelinkov.2022.Probingclassifiers:Promises,shortcomings,andadvances.ComputationalLinguis-tics,48(1):207–219.IzBeltagy,MatthewEPeters,andArmanCohan.2020.Longformer:Thelong-documenttransformer.arXivpreprintarXiv:2004.05150.PeterFBrown,VincentJDellaPietra,PeterVDesouza,JenniferCLai,andRobertLMercer.1992.Class-basedn-grammodelsofnaturallanguage.Computa-tionallinguistics,18(4):467–480.StevenCao,VictorSanh,andAlexanderMRush.2021.Low-complexityprobingviafindingsubnetworks.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages960–966.PierreComon.1994.Independentcomponentanalysis,anewconcept?Signalprocessing,36(3):287–314.AlexisConneau,GermánKruszewski,GuillaumeLam-ple,LoïcBarrault,andMarcoBaroni.2018.Whatyoucancramintoasingle$&!#*vector:Probingsentenceembeddingsforlinguisticproperties.InProceedingsofthe56thAnnualMeetingoftheAs-sociationforComputationalLinguistics(Volume1:LongPapers),pages2126–2136.LeonDerczynski,SeanChester,andKennethSBøgh.2015.Tuneyourbrownclustering,please.InIn-ternationalConferenceRecentAdvancesinNaturalLanguageProcessing,RANLP,volume2015,pages110–117.AssociationforComputationalLinguistics.YuningDing,BrianRiordan,AndreaHorbach,AoifeCahill,andTorstenZesch.2020.Don’ttake“nswvt-nvakgxpm”forananswer–thesurprisingvulnerabil-ityofautomaticcontentscoringsystemstoadversar-ialinput.InProceedingsofthe28thinternationalconferenceoncomputationallinguistics,pages882–892.JamesFiacco,SamridhiChoudhary,andCarolynRose.2019.Deepneuralmodelinspectionandcomparisonviafunctionalneuronpathways.InProceedingsofthe57thConferenceoftheAssociationforComputa-tionalLinguistics,pages5754–5764.JamesFiacco,ShiyanJiang,DavidAdamson,andCar-olynRose.2022.Towardautomaticdiscoursepars-ingofstudentwritingmotivatedbyneuralinterpreta-tion.InProceedingsofthe17thWorkshoponInno-vativeUseofNLPforBuildingEducationalApplica-tions(BEA2022),pages204–215.HaroldHotelling.1933.Analysisofacomplexofsta-tisticalvariablesintoprincipalcomponents.Journalofeducationalpsychology,24(6):417.ShiHuaweiandVahidAryadoust.2023.Asystem-aticreviewofautomatedwritingevaluationsystems.EducationandInformationTechnologies,28(1):771–795.ShiyanJiang,KexinYang,ChandrakumariSuvarna,PoojaCasula,MingtongZhang,andCarolynRose.2019.Applyingrhetoricalstructuretheorytostudentessaysforprovidingautomatedwritingfeedback.InProceedingsoftheWorkshoponDiscourseRelationParsingandTreebanking2019,pages163–168.
240


CancanJin,BenHe,KaiHui,andLeSun.2018.Tdnn:atwo-stagedeepneuralnetworkforprompt-independentautomatedessayscoring.InProceed-ingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages1088–1097.UrvashiKhandelwal,HeHe,PengQi,andDanJu-rafsky.2018.Sharpnearby,fuzzyfaraway:Howneurallanguagemodelsusecontext.arXivpreprintarXiv:1805.04623.YamanKumar,MeharBhatia,AnubhaKabra,JessyJunyiLi,DiJin,andRajivRatnShah.2020.Callingoutbluff:attackingtherobustnessofauto-maticscoringsystemswithsimpleadversarialtesting.arXivpreprintarXiv:2007.06796.YamanKumar,SwapnilParekh,SomeshSingh,JunyiJessyLi,RajivRatnShah,andChangyouChen.2023.Automaticessayscoringsystemsarebothoverstableandoversensitive:Explainingwhyandproposingdefenses.Dialogue&Discourse,14(1):1–33.AdhigunaKuncoro,ChrisDyer,JohnHale,DaniYo-gatama,StephenClark,andPhilBlunsom.2018.Lstmscanlearnsyntax-sensitivedependencieswell,butmodelingstructuremakesthembetter.InPro-ceedingsofthe56thAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),volume1,pages1426–1436.ScottMLundbergandSu-InLee.2017.Aunifiedap-proachtointerpretingmodelpredictions.Advancesinneuralinformationprocessingsystems,30.WilliamCMannandSandraAThompson.1987.Rhetoricalstructuretheory:Atheoryoftextorganiza-tion.UniversityofSouthernCalifornia,InformationSciencesInstituteLosAngeles.AtsushiMizumotoandMasakiEguchi.2023.Exploringthepotentialofusinganailanguagemodelforauto-matedessayscoring.ResearchMethodsinAppliedLinguistics,2(2):100050.FarahNadeem,HuyNguyen,YangLiu,andMariOsten-dorf.2019.Automatedessayscoringwithdiscourse-awareneuralmodels.InProceedingsofthefour-teenthworkshoponinnovativeuseofNLPforbuild-ingeducationalapplications,pages484–493.EllisBPage.1966.Theimminenceof...gradingessaysbycomputer.ThePhiDeltaKappan,47(5):238–243.DadiRameshandSureshKumarSanampudi.2022.Anautomatedessayscoringsystems:asystem-aticliteraturereview.ArtificialIntelligenceReview,55(3):2495–2527.MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016."whyshoulditrustyou?"explainingthepredictionsofanyclassifier.InProceedingsofthe22ndACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining,pages1135–1144.MariaRyskinaandKevinKnight.2021.Learningmath-ematicalpropertiesofintegers.InProceedingsoftheFourthBlackboxNLPWorkshoponAnalyzingandIn-terpretingNeuralNetworksforNLP,pages389–395.AngadSethiandKavinderSingh.2022.Naturallan-guageprocessingbasedautomatedessayscoringwithparameter-efficienttransformerapproach.In20226thInternationalConferenceonComputingMethod-ologiesandCommunication(ICCMC),pages749–756.IEEE.XingShi,KevinKnight,andDenizYuret.2016.Whyneuraltranslationsaretherightlength.InProceed-ingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2278–2282.KavehTaghipourandHweeTouNg.2016.Aneuralapproachtoautomatedessayscoring.InProceed-ingsofthe2016conferenceonempiricalmethodsinnaturallanguageprocessing,pages1882–1891.PattiWest-Smith,StephanieButler,andElijahMayfield.2018.Trustworthyautomatedessayscoringwithoutexplicitconstructvalidity.InAAAISpringSymposia.BronwynWoods,DavidAdamson,ShayneMiel,andElijahMayfield.2017.Formativeessayfeedbackusingpredictivescoringmodels.InProceedingsofthe23rdACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining,pages2071–2080.XunjieZhu,TingfengLi,andGerardMelo.2018.Ex-ploringsemanticpropertiesofsentenceembeddings.InProceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),volume2,pages632–637.
241