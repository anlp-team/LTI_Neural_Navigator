‚àóWorkpartiallydoneduringaresearchinternshipatGoogleResearch.37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).arXiv:2306.17842v3 [cs.CV] 28 Oct 2023
SPAE:SemanticPyramidAutoEncoderforMultimodalGenerationwithFrozenLLMs
LijunYu‚Ä°‚Ä†‚àóYongCheng‚Ä†ZhiruoWang‚Ä°VivekKumar‚Ä†WolfgangMacherey‚Ä†YanpingHuang‚Ä†DavidA.Ross‚Ä†IrfanEssa‚Ä†YonatanBisk‚Ä°Ming-HsuanYang‚Ä†KevinMurphy‚Ä†AlexanderG.Hauptmann‚Ä°LuJiang‚Ä†‚Ä°‚Ä†Google,‚Ä°CarnegieMellonUniversityAbstractInthiswork,weintroduceSemanticPyramidAutoEncoder(SPAE)forenablingfrozenLLMstoperformbothunderstandingandgenerationtasksinvolvingnon-linguisticmodalitiessuchasimagesorvideos.SPAEconvertsbetweenrawpixelsandinterpretablelexicaltokens(orwords)extractedfromtheLLM‚Äôsvocabulary.Theresultingtokenscaptureboththesemanticmeaningandthefine-graineddetailsneededforvisualreconstruction,effectivelytranslatingthevisualcontentintoalanguagecomprehensibletotheLLM,andempoweringittoperformawidearrayofmultimodaltasks.Ourapproachisvalidatedthroughin-contextlearningexperimentswithfrozenPaLM2andGPT3.5onadiversesetofimageunderstandingandgenerationtasks.OurmethodmarksthefirstsuccessfulattempttoenableafrozenLLMtogenerateimagecontentwhilesurpassingstate-of-the-artperformanceinimageunderstandingtasks,underthesamesetting,byover25%.1IntroductionLargelanguagemodels(LLMs)empoweredbyTransformers[38]haveachievedremarkableprogressinaddressingabroadspectrumofNaturalLanguageProcessing(NLP)tasks[4,8,28,2].Withthecontinuousincreasesinmodelsizeandtrainingdata,LLMsaregraduallybecomingmoreversatileandagnostictospecifictasks,unlockingnewcapabilitiesinsolvingcomplexAItasks[42],likequestionanswering,codegeneration,reasoning,mathematicsproblem-solving,andunderstandinghumor,amongvariousotherapplications[2,28].LLMscapturerichconceptualknowledgeabouttheworldintheirlexicalembeddings.Thisraisesaquestion:ifprovidedwiththeappropriatevisualrepresentationsasinput,arefrozenLLMscapableofsolvingtasksinvisualmodalities?Veryrecently,therehavebeennotableadvancementsinextendingthecapabilitiesoffrozenLLMstotackleimageunderstandingandretrievaltasks[21,27].However,generatingadifferentmodalityusingafrozenLLMthathasnotbeenexplicitlytrainedonthatmodalityhasproventobechallengingandhashadlittlesuccess.TofacilitateLLMsforsuchcross-modaltasks,weproposetolearnavectorquantizertomapanimage,orsomeothernon-linguistic(‚Äúforeign‚Äù)modality,tothetokenspaceofafrozenLLM.ThiseffectivelytranslatestheimageintoalanguagethattheLLMcancomprehend,enablingustoleveragethegenerativeabilitiesoftheLLMtoperformimageunderstandingandgenerationtaskswithouthavingtotrainonimage-textpairs.Specifically,ournewapproachisthat,givenanimageprompt,convertittoatokenspacewithourlearnedencoder,usetheLLMtogeneratesuitablelexicaltokens,andconvertbacktopixelspacewithourlearneddecoder.


Top 5 layers (341 tokens)
branch, nest, eagle, ‚Ä¶, bird, veld, climb, ‚Ä¶
Decoder
Encoder
‚ùÑ4 layers5 layers6 layersReconstructed image
CLIP
Seman&c loss
Top 4 layers (85 tokens)branch, nest, eagle, ‚Ä¶
Top 6 layers (597 tokens)branch, nest, eagle, ‚Ä¶, bird, veld, climb, ‚Ä¶,wings, ËßÇ, ŸàÿØŸä, ‚Ä¶
Figure1.FrameworkoftheproposedSPAEmodel.Animageisencodedintoapyramidoflexicaltokenscapturingsemanticconceptsandappearancedetailsnecessaryforreconstruction.WeintroduceanovelSemanticPyramidAutoEncoder(SPAE)thatproducesalexicalwordsequencethat(1)carriesrichsemantics,and(2)retainsfinedetailsforsignalreconstruction.IncontrasttothemajorityofVQ-VAEapproaches[37],ourencodermapstoaninterpretablediscretelatentspace,i.e.,words.AsdepictedinFig.1,SPAEtokenshaveamulti-scalerepresentationarrangedinapyramidstructure.Theupperlayersofthepyramidcomprisesemantic-centralconcepts,whilethelowerlayersprioritizeappearancerepresentationsthatcapturesthefinedetailsforimagereconstruction.Thisdesignenablesustodynamicallyadjustthetokenlengthtoaccommodatevarioustasks,suchasusingfewertokensforunderstandingtasksandmoretokensforgenerationtasks.Weverifytheplausibilityofourapproachinanextremesettingofin-contextlearning[4],withoutanyparameterupdatestotheLLM.OurSPAEmodelistrainedstandalone,withoutbackpropagatingthroughanylanguagemodel.Weevaluateourapproachonimageunderstandingtasksincludingimageclassification,imagecaptioning,andvisualquestionanswering.WeshowcaseapromisingdirectiontoimagegenerationwithLLMsbyutilizingin-contextdenoisingtechniques.OurmethodisLLM-agnosticandhasbeentestedwithPaLM2[2]andGPT-3.5[28],suggestingcompatibilitywitharbitraryLLMs.Themaincontributionsofthisworkaresummarizedasfollows:‚Ä¢Thisisthefirstsuccessfulmethod,tothebestofourknowledge,thatusesafrozenlanguagemodel,trainedsolelyonlanguagetokens,todirectlygenerateimagecontentthroughin-contextlearning.‚Ä¢WeintroduceanewSPAEtokenizerproducinginterpretablerepresentationsofsemanticconceptsandfine-graineddetailsintheformofmultilinguallinguistictokenswithadjustablelengths.‚Ä¢Weevaluateourmethodonvisualunderstandingandgenerationtasks,andnotably,ourapproachoutperformsthebest-publishedfew-shotimageclassificationaccuracy[27]byanabsolute25%underthesamein-contextsetting.2RelatedWorkMultimodalgenerationwithLLMs.AdvanceshavebeenmadetoexpandthecapabilitiesofLLMsbeyondlanguage.Forexample,VisualChatGPT[43]usesChatGPTtogeneratepromptsandexecutesmultimodaltasksthroughanothermodel,e.g.,generatingimagefromtextpromptsbyStableDiffusion[32].FROMAGe[21]feedsCLIP[30]embeddingstoOPT[49]forimageunderstandingandretrieval.However,itrequiresbackpropagationthroughtheLLManddoesnotsupportimagesynthesis.ThisworkenablesastandalonefrozenLLMtounderstandandgenerateothermodalitieswhichareunseenintraining.Tokenizationviavectorquantization.VQ-VAE[37]compressesdataintoadiscretelatentspacedefinedbyacodebookviavectorquantization.VQGAN[14]enhancesthereconstructionqualitywithadversarialandperceptualobjectives.Thesediscretelatentquantities,oftenreferredtoastokens,arewidelyusedtolearngenerativetransformermodelsforimage[32,7],video[45,15,39],image-video[46],andaudio[3,9].OurSPAEmodelisbuiltupontheVQGANframeworkandapplicabletodifferentmodalities.2
Layer 4Layer 3Layer 2Layer 1Layer 5Layer 6
‚ùÑLLM codebook


2(cid:25)+1,w‚Ä≤j‚àí(cid:24)w‚Ä≤
2(cid:25)+1)|(i,j)‚àà([1,hl]√ó[1,wl])‚à©Z2},(1)whereh‚Ä≤=hD
hl,andw‚Ä≤=wD
wlarethedownsampleratios.Foreachembeddingzatposition(x,y),weobtainitsdiscretetokenssequentiallyfromlayer1toD.Atlayerl,if(x,y)‚ààP(l),thequantizerassignsdiscretetokenkl=argmink‚ààT‚à•zl‚àíe(k)‚à•22,3
Tokenizationintolexicalrepresentations.ThecodebooksintypicalVQGANsarelearnedjointlywiththeencoderanddecoderstacks,whicharenotdirectlyinterpretablevianaturallanguages.LQAE[27]replacesthelearnedcodebookwithfrozenwordembeddingsfromBERT[12]toconnectwithanEnglishvocabulary.However,theLQAEtokensseldomcontainsemanticconceptsinanimage,andthereconstructionqualityisworsethanthatwithalearnedcodebook.OurSPAEquantizesaninputsampleintosemanticallyrelatedtokensinamultilingualvocabularywhilepreservingthehighreconstructionqualityofaVQGANforgenerativetasks.Inaddition,SPAEtokensareorganizedinamulti-layercoarse-to-finepyramidforflexibleusageindifferenttasks.Few-shotlearningwithLLMs.In-contextlearning[4,8,2]facilitatesLLMsforfew-shotlearningviathetextinterfacewithoutparameterupdates.ThisapproachiscommonlyemployedtoassesstheperformanceofLLMsonnumerousNLPbenchmarks,e.g.,classificationandquestionanswering[41],mathematicalreasoning[24],andcodegeneration[44],whichyieldscompetitiveresultstotheirfine-tunedcounterparts.However,existingfew-shotvision-languageunderstandingandgenerationframeworks[1,21]stillrequireLLMparameterupdates.Incontrast,ourworkinheritsthein-contextlearningabilityfromfrozenLLMs.3MethodOurgoalistomodelanimage,orsomeothernon-linguisticmodality(e.g.,videooraudio),asalanguagesequencethatLLMscancomprehend.SemanticPyramidAutoEncoder(SPAE)generatesalexicalwordsequencewithdynamicallyadjustablelengththatcarriesrichsemanticsandretainsfinedetailsforsignalreconstruction.ToworkwithafrozenLLMviain-contextlearning,weintroduceaprogressivein-contextdenoisingmethodtofacilitateimagegeneration.WeusetheimagemodalityinthissectiontointroduceourSPAEmodelin2D,andlatershowcasetheresultsofa3Dvariantwiththevideomodalityinourexperiments.3.1SemanticPyramidAutoEncoderOurSPAEmodelextendstheVQ-VAE[37]framework,whichcomprisesanencoder,aquantizer,andadecoder.TheCNNencodermapsanimageI‚ààRH√óW√ó3intocontinuousembeddingsZ‚ààRh√ów√óc.Eachelementz‚ààZisthenpassedthroughthequantizer,whichassignsittotheclosestentryinacodebook,resultinginthequantizedembedding.LetÀÜZrepresentthequantizedembeddingsfortheentireimage.TheCNNdecoderreceivesÀÜZasinputandgeneratesthereconstructedimageÀÜI.BelowwehighlightthedesigndifferencesinSPAE.AsillustratedinFig.1,SPAEgenerateslexicaltokensarrangedinapyramidstructure,whichcontainssemanticconceptsintheupperlayersandappearancewithprogressivelyrefineddetailsinthelowerlayers.Weintroduceasemanticlosstoencouragetheusageofconceptuallyrelevanttokens.Frozenlanguagecodebook.Togeneratelexicaltokens,weutilizeapretrainedLLMcodebookC={(k,e(k))|k‚ààT}andfreezeitduringtraining,whereTisasubsetoftheLLMvocabulary.Here,e(¬∑)producesthetextembeddingforasub-wordkwhichmaybeobtainedfromanylayeroftheLLM.Sincethecodebookisalignedwiththelanguagevocabulary,weusetheterms‚Äútoken‚Äùand‚Äúword‚Äùinterchangeably.Tokenpyramid.TheSPAEquantizerproducesDlayersoftokenswherethetokensatlayerlaredenotedaskl‚ààThl√ówl.PriorworksuseResidualQuantization(RQ)togeneratemulti-layertokens[22,47].Inthesemethods,tokensfromalllayershaveuniformshapesanddonotcarryspecificsemanticmeanings.Incontrast,weproposeapyramidtokenstructurebyenforcingtheconstrainthl‚â§hl+1‚àßwl‚â§wl+1.Thepyramidstructureispurposefullydesignedtoconcentratesemanticswithinthewithintheupperlayersofthepyramid.Thisdesignallowsforrepresentingsemanticconceptswithnotablyfewertokens,e.g.,asfewasfivetokensforunderstandingtasks.Thehightokenefficiencystemsfromthepyramidstructure,asaconventionallayerwithoutpyramidstructuresneedsaminimumofhwtokens(e.g.,256)torepresenttheimage.Tokenefficiencyiscrucialforin-contextlearningasitenablestheaccommodationofmoreexampleswithinthecontext.AdilationsubsamplerP(l)isused,whichselectsthepositionsforquantizationatlayerlasP(l)={(h‚Ä≤i‚àí(cid:24)h‚Ä≤


|p|P|p|i=1fT(pi(k)),(5)wherepisalistofprompttemplates,suchas"aphotoof...".Duringtraining,weextracttheimagefeaturefI(I)andcomputethedot-productsimilarityass‚Ä≤(I,k)=fI(I)¬∑f‚Ä≤T(k).Thesimilarityscoreisthennormalizedtoaccountforthevaryingscalesacrossdifferentimages.s(I,k)=s‚Ä≤(I,k)‚àíminjs‚Ä≤(I,j)
Pk‚ààTexp(‚àí‚à•zl‚àíe(k)‚à•22),(7)wherewerandomlysamplesemanticallysimilartargetcodescforeachlayerembeddinginthefirstD‚Ä≤layers.Appearanceloss.Usinganimprovedobjectivefrom[45],theappearancelossiscalculatedas:Lappearance(Œ∏e,Œ∏d;I)=‚à•I‚àíÀÜI‚à•22+Œ≤PDl=1‚à•Z‚àísg(ÀÜZ‚â§l)‚à•22+ŒªLGAN+Œ∑LPerceptual+œïLLeCAM,(8)whereLGAN,LPerceptual,andLLeCAMaretheVQGAN[15],perceptual[19],andLeCAM[34]losses.Inaddition,sg(x)isthestop-gradientoperation.TheappearancelossisappliedtoboththeencoderŒ∏eanddecoderparametersŒ∏d,excludingthefrozencodebookembedding.Tostabilizethetrainingandbalancebetweenappearanceandsemantics,weaddadynamicweightforthesemanticguidancelossasw=sg(cid:16)Lappearance(I)
wherezlisthecurrentlayerembedding,calculatedfromzl=z+Pl‚àí1i=11(x,y)‚ààP(i)(z‚àíe(ki)).(2)ThequantizedembeddingreconstructedwiththefirstllayersisgivenbytheaverageoftheexistingtokenembeddingsasÀÜz‚â§l=Pli=11(x,y)‚ààP(i)e(ki)
ÀÜl+1e(kl+1),ÀÜl=Pli=11(x,y)‚ààP(i).RQ[22,47]isapplicablebutyieldsworseresultsinthiscontext,asrevealedbyourablationstudies.Thiscanbeattributedto(1)varyingscalesofembeddingsinresiduallayers,potentiallydividingthecodebookintomultipleparts,and(2)misalignmentinthesummationofwordembeddings,whichundermineslearningsemanticallymeaningfultokensinlaterlayers.Semanticloss.WeencouragethesemanticsimilaritybetweentheimageIandeachlexicaltokenkdenotedbys(I,k).Duringtraining,webuildper-layercandidatetokenpoolsasCl(I)={k‚ààT|s(I,k)‚â•œÅl},(4)whereœÅlisathreshold.WesetœÅl‚â•œÅl+1toallowdeeperlayerstohavealargerpoolofcandidatetokenswhilesacrificingsomesemantics.Todefinethesimilarityscore,thispaperemploysapretrainedCLIPmodel[29].Inmoredetails,letfIandfTbeapairofimageandtextCLIPembeddingfunctions.Weprecomputethetextfeatureforeachtokenk‚ààTasf‚Ä≤T(k)=1
maxjs‚Ä≤(I,j)‚àíminjs‚Ä≤(I,j).(6)WedefinethesemanticlossfortheencoderparametersŒ∏easLsemantic(Œ∏e;I)=El‚àà[1,D‚Ä≤]EzlEc‚ààCl(I)‚àílogexp(‚àí‚à•(zl‚àíe(c)‚à•22)
Pli=11(x,y)‚ààP(i).(3)UsingtheinputofÀÜZ‚â§lfromtokensuptolayerl,thedecodercanprogressivelyreconstructtheimagewithdynamictokenlengths,resultingingraduallyimprovedqualitywithrefinedappearancedetails.WetermthisapproachasStreamingAverageQuantization(SAQ)duetoitsresemblancetocomputingtheaverageonstreamingdata,whereÀÜz‚â§l+1=ÀÜz‚â§l+1
Lsemantic(I)(cid:17).ThetotaltraininglossexcludingtheGANdiscriminatorisLSPAE(Œ∏e,Œ∏q)=EIhLappearance(Œ∏e,Œ∏q;I)+Œ±wLsemantic(Œ∏e;I)i.(9)4


20%InputOutputQueryOriginalFigure2.Anexampleoftheconditionalimagedenoisingtaskforhighresolutionsynthesis.Thecontextcomprisesimagesrandomlycorruptedinthetokenspace.3.2ProgressiveIn-ContextDecodingWhileourmethodismoreeffectivewhenbackpropagatingthroughLLMsbyprompt[23]oradaptertuning[17,18],thisworkfocusesonverifyingtheplausibilityinanextremesettingofin-contextlearning[4].WedemonstratethatLLMsarecapableofperformingnewtasksinforeignmodalitieswithoutanyparameterupdates.Specifically,asetofKexamples{(ui,vi)}Ki=1arefedtotheLLMtolearnanewtaskandansweraqueryÀÜuwithÀÜv‚àºPLLM(¬∑|ÀÜu;{(ui,vi)}Ki=1).(10)SamplingÀÜvbyasingle-passautoregressivedecodingissuboptimalduetothedistributionalshiftintherepresentationandthepresenceofexceptionallylongsequences,e.g.,animageisquantizedintoover500tokens.Tothisend,weuseaprogressivedecodingmethod.WegeneralizeEq.(10)intoamulti-passprocess,wheretheLLMlearnstogenerateonesegmentofthetargetsequenceatatime.Thesegmentgeneratedfromthet-thpassisÀÜvt‚àºPLLM(¬∑|[ÀÜu,ÀÜv<t‚Ä≤];{([ui,vi<t‚Ä≤],vit)}Ki=1),(11)where[¬∑,¬∑]indicatesconcatenation.t‚Ä≤controlsthelengthofprevioussegmentstoconditionon,withtwocommoncases:(1)aprogressiveautoregressive(PAR)processwitht‚Ä≤=t,whereeachdecodedsegmentconditionsonallpreviouslydecodedones;(2)aprogressivenon-autoregressive(PNAR)processwitht‚Ä≤=0tosampleeachsegmentindependently,whichgreatlyreducesthesequencelengthrequirementfortheLLM.Inpractice,weusePARtogeneratethefirstfewtokenlayersgiventask-specificconditions,followedbyPNARtogeneratetheremainingtokenlayersconditionedonthepreviouslayersinanunconditionallatentrefinementprocess.Thelearningcapacityofanin-contextsetupisfarfromsufficientforamodalitythathasnotbeenseenduringtraining.Sofar,therehavebeennosuccessfulattemptsintheliteraturedemonstratingthatafrozenLLMcangenerateimagecontent.Forlow-resolutionimages,LLMscanproduceimagesdirectlyusingin-contextlearning,aswillbedemonstratedwith32√ó32MNISTimages[11].Forhigherresolutions,thecontextlengthrestrictsthenumberofexamples.Forinstance,acontextwindowof8ktokenscanonlyholdlessthanadozen128√ó128images.Therefore,weoperateinadenoisingsubspacetosynthesisbeyond32√ó32resolution.Fig.2illustratesoneexample,withdetaileddefinitionsintheAppendix.4ExperimentalResults4.1ExperimentalSettingsToverifythecompatibilitywithdifferentLLMs,wetraintwovariantsofSPAE,namelySPAEPaLMandSPAEGPT.TheSPAEPaLMcodebookistakenfromtheinputembeddinglayerofaPaLM2-Scheckpointwitha65kvocabularyofthemostfrequentsentencepieces.ThePaLM2-LAPI[2]isusedforin-contextlearningwithSPAEPaLM.SPAEGPTusesabyte-pairencodingvocabularywith99kUTF-8tokens(https://github.com/openai/tiktoken),whereweobtainthecontextualtokenembeddingsfromOpenAItext-embedding-ada-002(https://platform.openai.com/docs/models/embeddings).Forafaircomparisonwithpriorworks[27],weuseSPAEGPTwiththeGPT3.5text-davinci-003API(https://platform.openai.com/docs/models/gpt-3-5).WeconfigureSPAEtoencodea128√ó128imageintoatokenpyramidof6layerswhereeachlayerhas2k√ó2ktokensandk=[0,1,2,3,4,4].Additionally,wetrainavideo-basedSPAEmodelontheKinetics-600dataset[5],andfurtherdetailscanbefoundintheAppendix.Weapplysemanticguidancelosstothefirstfivelayers,withthresholdsof0.98,0.95,0.9,0.85,and0.8.TheCLIPwithaViT-L/14[13]visionbackboneisused.Weuse80prompttemplatesfromthezero-shotImageNet5
Context: corruption 50%


5-WayClassificationFrozen[35]-0.914.534.733.833.833.332.826.26LQAE[27]1:256GPT3.51.015.735.936.531.936.445.929.04SPAEGPT(ours)2:5GPT3.54.363.063.460.661.962.162.153.91
5-way 5-shot
Table1.Few-shotclassificationaccuracyonthemini-ImageNetbenchmarks.SPAEGPTandSPAEPaLMaretrainedusingdifferentvocabulariesandembeddingsources,withdifferentprompttemplatesforin-contextlearning.TheyshowthebroadcompatibilityofSPAEbutarenotforacomparisonbetweentheLLMs.ThebestperformancewithGPTisinitalicswhiletheoverallbestisinbold.
SPAEPaLM(ours)2:5PaLM223.664.268.069.963.462.060.258.76SPAEPaLM(ours)3:21PaLM220.265.173.774.366.467.066.361.86
# Layers: # Tokens02550751001: 12: 53: 214: 855: 341
2-way 1-shot
2-WayClassificationFrozen[35]-1.733.76666636563.751.3LQAE[27]1:256GPT3.51.535.268.269.868.568.765.953.97SPAEGPT(ours)2:5GPT3.55.377.284.486.079.477.277.169.51
classificationsetuptoprecomputetheCLIPtextembeddingsforthevocabulary.Inaddition,weusetheAdam[20]optimizerwithlossweightsŒ±=1,Œ≤=0.33,Œª=0.1,Œ∑=0.1,œï=10‚àí4andalearningrateof10‚àí4followingalinearwarmup/cooldownandrootsquaredecayschedule.Followingthepriorwork[27],SPAEistrainedontheImageNetILSVRC2012[10]dataset.Wetrainwithabatchsizeof256for450ksteps.FurtherdetailscanbefoundintheAppendix.4.2MainEvaluationFew-shotimageclassification.Weevaluatethein-contextimageunderstandingcapabilitywithafrozenLLMonthemini-ImageNet[40]few-shotclassificationbenchmark.Asetoftokenizedimagesandclasslabelsarefedtothelanguagemodelascontextforclassificationofanewimage.Following[35,27],weevaluate14settingscontrolledbyfourfactorsregardingthecontentofeachtestcase:(1)taskinduction:whetherincludingapreambletospecifytheoutputspace;(2)numberofways:thenumberofcategories;(3)numberofinnershots:thenumberofuniqueexamplesforeachcategory;(4)numberofrepeats:thenumberoftimesthateachuniqueexampleisrepeated.
5-way 1-shot
SPAEPaLM(ours)2:5PaLM232.284.088.588.485.183.682.477.74SPAEPaLM(ours)3:21PaLM227.984.892.592.684.885.285.479.03
5-way 3-shot
2-way 5-shot
TaskInduction‚úì‚úì‚úì‚úì‚úì‚úìAvgMethod#LayersInnerShots1135111:#TokensRepeats0000135
2-way 3-shot
AverageFigure3.Few-shotclassificationaccuracyonmini-ImageNetusingdifferentSPAEPaLMlayers.WecompareSPAEwiththestate-of-the-artmethodsFrozen[35]andLQAE[27].AsshowninTab.1,SPAEGPTconsistentlyoutperformsLQAE,bothusingthesameGPT3.5modelandin-contextformat,whileusingonly2%ofitstokens.Fig.3showstheperformancetrendwhenusingdifferentnumberofSPAEPaLMlay-ersacrosssixsettingswithtaskinductionand0repeats.SPAEPaLMwith3layersachievesthebestperformancewhichbalancesbetweensuf-ficientsemanticsandanimagesequencelengththatisoptimalforLLMin-contextlearning.Overall,SPAEPaLMyields+25%and+32%averageaccuracyimprovementoverthestate-of-the-artonthe2-wayand5-waybenchmarksinTab.1.Reconstructionquality.WecomparetheimageandvideoreconstructionqualityusingthetokensproducedbySPAEandtheVQGANbaselineusedinstate-of-the-artimage[7,25,6]andvideogeneration[45].WeuseFID[16],InceptionScore(IS)[33],andLPIPS[48]tocomparewiththeimageVQGANfromMaskGIT[7]ontheImageNetvalidationset,andFVD[36]tocomparethe3D-VQGANfromMAGVIT[45]ontheKinetics-600validationset.TheresultsarepresentedinTab.2.WhileSPAEmayhavemorelossyreconstructioncomparedtoVQGANwhenusingasimilarnumberoftokens,thisiscompensatedbygoingintodeeperlayers.AtthebottomofTab.2,weshowcasethescalabilityofourmodelbytrainingontheImageNet-21kdatasetwith13MimagesandlistthecomparablevariantfromLDM[32]asareference.Tokenpyramidvisualization.WevisualizethetokensproducedbySPAEinFig.4,whereweshowtherawpyramidorhistogramoftokenswithtopfrequenciesforthefirstfourlayers,withreconstructedimagesfromlayer5and6.Wehavethefollowingfindings.6


Figure4.Examplesofpyramidimagetokenizationandreconstructionbya6-layerSPAE.Weshowtherawpyramidorhistogramofmostfrequenttokensforthefirstfourlayers,andreconstructedimagesfromlayer5and6.Inthepyramid,weusedarkercellstoshowtokenswithhigherCLIPsimilaritytotheoriginalimage.Fornon-Englishsub-wordtokens,weshowautomatictranslationforreferenceinitalicfontsbelowtheoriginaltoken.CircledtokensarementionedinSection4.2.SeefullpyramidvisualizationsintheAppendix.Table2.ComparisonofreconstructionqualitybetweenSPAEandVQGANbaselinesusedinstate-of-the-artimage[7,25,6]andvideo[45]generationmodels.
Visualquestionanswering.Tab.3providesquantitativeresultsonthevisualquestionanswering(VQA)task.WecomparewiththebaselineFrozen[35]methodontheReal-Fast-VQA[35]benchmarkforfew-shotlearning.Asshown,SPAEconsistentlyoutperformsFrozen.UnlikeFrozen,SPAEtrainingdoesnotrequirebackpropagationthroughtheLLM.4.3QualitativeStudiesThissectionexploresthecapabilityofafrozenPaLM2,trainedsolelyonlanguagetokens,inperformingmultimodaltasksusingin-contextlearning.Weadoptatwo-stagedecodingprocessforimagegeneration.Instageone,weuseARdecodingtoproducethefirst5SPAElayerswithtask-specificconditions.Stagetwoisatask-agnosticNARdecodingprocessforlayer6conditionedonthefirst5layers.ImagetotextandVQA.Weexaminetwotasksinvolvingvisual-textreasoning(1)imagecaption-ingonCOCO[26]captions;and(2)visualquestionanswering(VQA)onCOCO-QA[31].Forboth7
SPAE(ours)5:3419.49109.460.1752.286:5974.41133.030.126.35
InnerShots135
128√ó128VQGAN1:2565.48119.690.136.79
Original
ResolutionMethodImageVideo#Layers(ImageNetILSVRC2012[10])(Kinetics-600[5]):#TokensFID‚ÜìIS‚ÜëLPIPS‚ÜìFVD‚Üì
OriginalLayer 1-4Layer 6Layer 5Layer 6Layer 5Layers 1-4OriginalLayers 1-4Layer 6Layer 5
First,theSPAEtokensareorganizedinapyramidstructure,witheverylayercomprisingsemanticallyrelatedtokenstotheimage.Thefewtokensinthetoplayersseemtocapturetheprimarythemeoftheimage.Forinstance,inFig.4,thetokenpresso(highlightedinorange)representstheespressomachineandothertokenslikeblenderrefertorelatedregions.Layer3andLayer4revealadditionaldetailsaboutlocalizedobjects.Forexample,thetokenThermoreferstothethermometerinthetop-leftregion,whilestoveappearsinthebottom-rightarea.Inadditiontonouns,relatedverbsalsoshowup,includingpouring,refill,spill,andbrew.Second,itisworthnotingthattheCLIPmodelhasanEnglish-onlyvocabulary.However,thankstothemultilingualvocabulariesandembeddingsfromtheLLM,SPAE‚Äôssemanticguidanceisabletomaptosimilarconceptsinotherlanguages,suchaskoffieinDutchandkaffeinDanishascorrespondingtermstotheconceptofcoffee.Third,similartoRQtokens[22],SPAEtokenscanreconstructtheimagewithprogressivelyrefineddetailswhenmorelayers,andthustokens,areutilized.Fig.4showsLayer5beginstoproduceareasonablereconstructionwhileLayer6furtherenhancesthelevelofdetailandsmoothness.Table3.Few-shotVQAperformanceonReal-Fast-VQA.
Frozen[35]7.810.110.5SPAEPaLM(ours)14.315.915.1
VQGAN(LDM[32],OpenImages)1:2565.15144.55--SPAE(ours,ImageNet-21k)6:5973.08173.790.19-
256√ó256VQGAN1:2564.04163.950.21-SPAE(ours)6:5973.60168.500.19-


an image of {}Genera)onan image of 1+7
Baseline: A man and a woman are sitting on a bench in a park.SPAE L1: A man is holding a baby in his arms.SPAE L2: A group of people are standing in a line.SPAE L3: A group of people in costumes at a Halloween party.SPAE L4: A group of people are dressed up in costumes for Halloween.SPAE L5: a group of people dressed in costumes at a partySPAE L6: a table with a bowl of fruit and a vase of flowers
Baseline: A man is standing on a rock in the middle of a river.SPAE L1: A man is standing on a rock in the middle of a river.SPAE L2: A man is wearing a coat and a hat.SPAE L3: A man is holding a small dog.SPAE L4: A teddy bear is sitting on a bed.SPAE L5: A teddy bear is sitting on a bed.SPAE L6: A teddy bear is sitting on a bed.
Q: what is the young boy riding in the empty parking lotA: Baseline: bikeSPAE: skateboardQ: how many different wines are lined up in glasses on an outdoor tableA: SPAE: 5Q: what bear walking through tall grass A: Baseline: siberianSPAE: grizzlyQ: how many computer screens are displayed with one imageA: SPAE: 3Figure5.Qualitativesamplesofimage-to-textgeneration:imagecaptioningandVQA.WecomparebetweendifferentlayersofSPAE(L1-L6)andabaselinemodelwithoutsemanticguidanceorpyramidSAQ.
Baseline: A man in a suit standing in front of a white wall.SPAE L1: A man in a red jacket and black pants standing on a snowy mountain.SPAE L2: A man in a red jacket skiing down a snowy mountain.SPAE L3: A man skiing down a snowy mountain.SPAE L4: A person skiing down a snowy mountain.SPAE L5: A person skiing down a mountain.SPAE L6: A person skiing down a mountain.Baseline: A group of people are standing in a field.SPAE L1: A group of people are standing in a room.SPAE L2: A kitchen with a stove, sink, and refrigerator.SPAE L3: A kitchen with a stove, sink, and refrigerator.SPAE L4: A kitchen with a stove, sink, and refrigerator.SPAE L5: A kitchen with a stove, sink, and cabinets.SPAE L6: A kitchen with a sink, stove, and refrigerator.
an image of the number of continents in the world
an image of the square root of 4
Context
an image of the last digit of 5*7
SPAE: A pizza with pepperoni and cheese on a white plate.SPAE: A man in a suit and tie standing next to a woman in a wedding dress.
Query
‚ùÑ LLMFigure6.Examplesoftext-to-imagegenerationonMNISTusingSPAEwithafrozenPaLM2model.WeuseSPAEtotokenize50handwrittenimagesasthecontextandaskPaLM2,anLLMtrainedsolelyontexttokens,toanswercomplexqueriesthatrequiregeneratingdigitimagesthroughSPAEastheoutput.tasks,weprovide10uniquetrainingexamplesasprompts.InthecaseofVQA,10differentanswersarepresentedtoforma10-way1-shotsetup.WecompareSPAEtoabaselinemodeltrainedwiththesamefrozenlanguagecodebookbutwithouttheproposedsemanticguidanceorpyramidSAQ.AsshowninFig.5,whenfedwithbaselinetokens,theLLMrandomlyhallucinatesacaptionorguessesananswersimplybasedonthequestion.SimilarhallucinationcanhappenifweonlyusethefirsttwolayersofSPAEorfivewordstorepresentanimage,asitprovidesinsufficientcontextforcaptioning.Reasonablecaptionsstarttoappearwith4layersor85words,whilecomplexscenesmaystillneedthefull6layersof597words.LLMgeneratingMNISTimages.Fig.6showsafewimagegenerationexamplesonMNIST[11].ThefrozenLLMlearnsabouthandwrittendigitimagesthrough50contextsamplestokenizedbySPAEtrainedonMNIST.Eachsampleconsistsofapreamble"animageofk"andthelexicaltokensrepresentinganimageofdigitk.ThenwecanasktheLLMtoanswerquestionswithdigitimages.Specifically,withaqueryof"animageof1+7",wecanuseprogressiveARdecodingwiththeLLMtoproduceatokensequencethatcanbedecodedintoanimageof8bySPAE.Wetestwithcomplexquestionsrequiringmathematicalreasoningorcommonsenseknowledge,andtheLLMisabletorespondcorrectly.Inaddition,thegenerateddigitimagesappeardifferentfromallcontextsamples.Thisdemonstratesthecross-modalreasoningcapabilityenabledbySPAEandafrozenLLM,withimagesgeneratedoverthetext-onlyinterface.Conditionalimagedenoising.Tothebestofourknowledge,therehavebeennosuccessfulattemptsthatdemonstrategenericimagegenerationcapabilityusingafrozenLLM.Tothisend,wedefineasimplerdenoisingsetuptoexplorethecapabilityofLLMs.Fig.7demonstratestheconditionalimagedenoisingtasks,e.g.,imageoutpainting,deblur,inpainting,locationtranslation,rotation,etc.Notethat,inordertogenerateimagesforeachtask,weutilize10pairsofnoisyexampleswithcorruptionratesrangingfrom50%to20%,asdiscussedinSection3.2.Thefullcontext,whichisomittedinFig.7,canbefoundintheAppendix.8
SPAE: A train is stopped at a station.


Figure7.Examplesofconditionalimagedenoising.Wecomparedifferentdecodingstridesforbothstages.Yellowandblueboxesindicatetheselectedresults.TheLLMisprovidedwithtenpairsofnoisyexampleslikeFig.2,whicharedeferredtotheAppendix.
VQGAN
Input
Stage 2: PNARTask-agnos1c=>Layer 6
Figure9.Comparisononconditionalimagedenoisingwithdifferenttokeniz-ers.AllmodelsusethesamedecodingsetupwiththesametenpairsofpromptimagesavailableintheAppendix.ThetoprowsofFig.7comparethegenerationfromdiffer-entdecodingstrideswiththesamesetofcontextexamples.Single-stepdecodingwithinfinitystridefailstoproduceareasonableimage,whichvalidatestheproposedprogressivegenerationapproach.InFig.9,wequalitativelycompareSPAEwithbaselinemeth-odsVQGANandLQAEusingthesamein-contextdenoisingprocedure.Asshown,VQGANfailstoproducereasonableimages,inpartbecausemanywordsintheLLMoutputareoutofitsvocabulary.LQAEonlyproducesvagueobjectcon-toursbutcannotrecoveranyvisualdetails.Onthecontrary,SPAEcangeneratereasonableimages.Conditionalvideodenoisingandothertasks.Duetospaceconstraints,weshowtheexamplesintheAppendix.4.4AblationStudiesTheresultsinTab.4andFig.8verifytheeffectivenessoftheproposeddesignsinSPAE,asevaluatedbyreconstructionquality(FID,IS,LPIPS)andsemanticrelevance(CLIPscore,few-shotclassificationaccuracy).Wehavethefollowingfindings.First,simplyusingafrozencodebooknegativelyaffectsthereconstructionresults,butwithsemanticguidanceitperformscomparablywiththeoriginalVQGANwhileproducingmeaningfullexicalwords.Second,RQhurtsreconstructionqualitywithafrozencodebook.ThisisdifferentfromRQ‚Äôsstandardsetup[22]wherethecodebookislearned.Third,SAQimprovesbothqualityandsemanticsimilarity,wherethepyramidenablesrepresentationwithmuchfewertokens.Thisallowsforaccommodatingmoreexampleswithinthefixedandconstrainedin-contextlength.Finally,per-layersemanticthresholdsbenefitunderstandingandthedynamicsemanticlossweighthelpsreconstruction.Theperceptuallossleveragesatrainednetworkwithaccesstoclassificationlabels,butremovingitresultsinasurprisingimprovementinclassificationaccuracywhilegreatlyhurtingthereconstruction.9
SPAELQAE
OriginalBaseline VQ+ frozen codebook+ frozen codebook+ seman7c guidance+ frozen codebook+ semantic guidance+ 2-layer RQ+ frozen codebook+ semantic guidance+ 2-layer SAQ+ frozen codebook+ semantic guidance+ 6-layer pyramid SAQFigure8.AblationexampleswithreconstructedimageandsemantictokensformodelslistedinTab.4.Fornon-pyramidtokens,weshowa4√ó4cropfromthefirstlayercorrespondingtotheregionindicatedbytheblackbox.Forpyramidtokens,weusethethirdlayerwhichconsistsof4√ó4tokens.
Stride‚àû2561286432168421Stage 1: PAROutpain1ng=>Layer 1-5


5ConclusionOurworkunveilstheuntappedpotentialoffrozenLargeLanguageModels(LLMs)intacklingmultimodalunderstandingandgenerationtasksinvolvingimagesandvideos,withoutrequiringexplicittrainingonthesemodalities.Thisisachievedbyanewmethod,SPAE,whichconvertsbetweenvisualcontentandlexicaltokensofvariablelength,imbuedwithrichsemanticmeaning.OurfindingsshowthegreatpotentialofharnessingthevastknowledgeandreasoningcapabilitiesofLLMsinthefieldofcomputervision,transcendingthelimitationsoflanguage-onlytasks.Limitations.Moretokensarerequiredtoachievethesamelevelofreconstructionwhenusingthefrozenlanguagecodebook,comparedtotheexistingVQGANmodelswithlearnedcodebooks.Thecapabilityofin-contextlearningissignificantlyconstrainedbytheacceptablesequencelength.Althoughourresultssuggesttheplausibilityofimagegeneration,theresolution,quality,anddiversityisfarfromtherecenttext-to-imagemodelstrainedonlargeimageandtextdata.Broaderimpact.OurpapershowcasestheuntappedpotentialoffrozenLLMsinmultimodalunderstandingandgenerationtasksinvolvingimagesandvideos,withoutrequiringexplicittrainingonthesemodalities.Asaninitialresearchproof-of-concept,wefocusonin-contextlearning,whichhaslimitationsinlearningcontextandconstrainedcapabilities.Consequently,thereisstillasubstantialgaptotherecentspecializedmodelsfortext-to-image(e.g.,StableDiffusion)orimage-to-textthathavebeenspecificallytrainedusingbillionsoftext-imagepairs.Thepotentialimpactofourresearchliesinitsinfluenceonfuturestudies,specificallyintheareaofintegratingvisionmodalitiesintotheLLMs.Forinstance,ourworkcanbeextendedtoexplorefinetuningoradaptertuningofLLMsonlarge-scaletext-imagedatasets.Futureresearchinthesedirectionsmayimplicateethicalissuesaroundfairnessandtransparency.Wehavefoundthatthegeneratedtokensoccasionallyincludeslangtermsorwordsthatcreateinappropriateconnotationsrelatedtothesubjectdepictedintheimageorvideo.Suchconcernsmustbethoroughlyconsideredandeffectivelyaddressedpriortodeployingthismethodinreal-worldapplications.Acknowledgmentsanddisclosureoffunding.Theauthorswouldliketothankanonymousreviewersandareachairstheirinsightfulcomments,andtoSiamakShakeri,SergeyIoffe,JayYagnik,andBoqingGongfortheirvaluablefeedbackandconstructivediscussions.ThisprojectisfundedinpartbyCarnegieMellonUniversity‚ÄôsMobility21NationalUniversityTransportationCenter,whichissponsoredbytheUSDepartmentofTransportation.10
+2-layerSAQ1:25612.3093.330.210.161356.62:5125.08125.270.140.1595-
Table4.Ablationstudiesoncodebook,trainingobjective,quantizationmethod,andtokenstructure.Classifica-tionaccuracyisevaluatedunderthemini-ImageNet5-way1-shotsetup.
BaselineVQ1:2565.48119.690.13n/a19.6+frozencodebook1:2567.44101.390.170.146419.5+semanticloss1:2565.17124.410.130.151846.2
noper-layerthresholds6:5974.33122.250.110.165059.4(layer3)nodynamicsemanticweight6:5979.0085.140.190.184765.1(layer3)noperceptualloss6:59740.4733.410.200.199469.5(layer3)
+6-layerpyramidSAQ(SPAE)1:1---0.187952.02:5---0.186864.23:21---0.181565.14:85---0.171158.55:3419.49109.460.170.160446.36:5974.41133.030.120.1577-
+2-layerRQ[22]1:25611.9489.010.220.159556.22:5126.05113.930.150.1547-
Method#LayersFID‚ÜìIS‚ÜëLPIPS‚ÜìCLIP‚ÜëClassification:#TokensAccuracy‚Üë


References[1]Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal.Flamingo:avisuallanguagemodelforfew-shotlearning.InNeurIPS,2022.3[2]RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal.PaLM2technicalreport.arXiv:2305.10403,2023.1,2,3,5[3]Zal√°nBorsos,Rapha√´lMarinier,DamienVincent,EugeneKharitonov,OlivierPietquin,MattSharifi,DominikRoblek,OlivierTeboul,DavidGrangier,MarcoTagliasacchi,etal.AudioLM:alanguagemodelingapproachtoaudiogeneration.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,2023.2[4]TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.InNeurIPS,2020.1,2,3,5[5]JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,andAndrewZisserman.AshortnoteaboutKinetics-600.arXiv:1808.01340,2018.5,7[6]HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal.Muse:Text-to-imagegenerationviamaskedgenerativetransformers.InICML,2023.6,7[7]HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.MaskGIT:Maskedgenerativeimagetransformer.InCVPR,2022.2,6,7[8]AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.PaLM:Scalinglanguagemodelingwithpathways.arXiv:2204.02311,2022.1,3[9]AlexandreD√©fossez,JadeCopet,GabrielSynnaeve,andYossiAdi.Highfidelityneuralaudiocompression.arXiv:2210.13438,2022.2[10]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scalehierarchicalimagedatabase.InCVPR,2009.6,7[11]LiDeng.Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb].IEEESignalProcessingMagazine,29(6):141‚Äì142,2012.5,8[12]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.BERT:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.InNAACL,2019.3[13]AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth16x16words:Transformersforimagerecognitionatscale.InICLR,2020.5[14]PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.InCVPR,2021.2[15]SongweiGe,ThomasHayes,HarryYang,XiYin,GuanPang,DavidJacobs,Jia-BinHuang,andDeviParikh.Longvideogenerationwithtime-agnosticVQGANandtime-sensitivetransformer.InECCV,2022.2,4[16]MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium.InNeurIPS,2017.6[17]NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly.Parameter-efficienttransferlearningfornlp.InICLR,2019.5[18]EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.Lora:Low-rankadaptationoflargelanguagemodels.InICLR,2021.5[19]JustinJohnson,AlexandreAlahi,andLiFei-Fei.Perceptuallossesforreal-timestyletransferandsuper-resolution.InECCV,2016.4[20]DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXiv:1412.6980,2014.6[21]JingYuKoh,RuslanSalakhutdinov,andDanielFried.Groundinglanguagemodelstoimagesformulti-modalgeneration.InICML,2023.1,2,3[22]DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan.Autoregressiveimagegenerationusingresidualquantization.InCVPR,2022.3,4,7,9,1011


[23]BrianLester,RamiAl-Rfou,andNoahConstant.Thepowerofscaleforparameter-efficientprompttuning.InEMNLP,2021.5[24]AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,VinayRamasesh,AmbroseSlone,CemAnil,ImanolSchlag,TheoGutman-Solo,etal.Solvingquantitativereasoningproblemswithlanguagemodels.InNeurIPS,2022.3[25]JoseLezama,TimSalimans,LuJiang,HuiwenChang,JonathanHo,andIrfanEssa.Discretepredictor-correctordiffusionmodelsforimagesynthesis.InICLR,2023.6,7[26]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDoll√°r,andCLawrenceZitnick.Microsoftcoco:Commonobjectsincontext.InECCV,2014.7[27]HaoLiu,WilsonYan,andPieterAbbeel.Languagequantizedautoencoders:Towardsunsupervisedtext-imagealignment.arXiv:2302.00902,2023.1,2,3,5,6[28]OpenAI.GPT-4technicalreport.arXiv:2303.08774,2023.1,2[29]AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.InICML,2021.4[30]AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.Zero-shottext-to-imagegeneration.InICML,2021.2[31]MengyeRen,RyanKiros,andRichardZemel.Exploringmodelsanddataforimagequestionanswering.InNeurIPS,2015.7[32]RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer.High-resolutionimagesynthesiswithlatentdiffusionmodels.InCVPR,2022.2,6,7[33]TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improvedtechniquesfortraininggans.InNeurIPS,2016.6[34]Hung-YuTseng,LuJiang,CeLiu,Ming-HsuanYang,andWeilongYang.Regularizinggenerativeadversarialnetworksunderlimiteddata.InCVPR,2021.4[35]MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill.Multimodalfew-shotlearningwithfrozenlanguagemodels.InNeurIPS,2021.6,7[36]ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,andSylvainGelly.Towardsaccurategenerativemodelsofvideo:Anewmetric&challenges.arXiv:1812.01717,2018.6[37]AaronVanDenOord,OriolVinyals,etal.Neuraldiscreterepresentationlearning.InNeurIPS,2017.2,3[38]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InNeurIPS,2017.1[39]RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,Moham-madTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan.Phenaki:Variablelengthvideogenerationfromopendomaintextualdescription.arXiv:2210.02399,2022.2[40]OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal.Matchingnetworksforoneshotlearning.InNeurIPS,2016.6[41]AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.SuperGLUE:Astickierbenchmarkforgeneral-purposelanguageunderstandingsystems.InNeurIPS,2019.3[42]JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,etal.Emergentabilitiesoflargelanguagemodels.TMLR,2022.1[43]ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang,ZechengTang,andNanDuan.VisualChatGPT:Talking,drawingandeditingwithvisualfoundationmodels.arXiv:2303.04671,2023.2[44]PengchengYin,Wen-DingLi,KefanXiao,AbhishekRao,YemingWen,KensenShi,JoshuaHowland,PaigeBailey,MicheleCatasta,HenrykMichalewski,etal.Naturallanguagetocodegenerationininteractivedatasciencenotebooks.arXiv:2212.09248,2022.3[45]LijunYu,YongCheng,KihyukSohn,Jos√©Lezama,HanZhang,HuiwenChang,AlexanderGHauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal.MAGVIT:Maskedgenerativevideotransformer.InCVPR,2023.2,4,6,7[46]LijunYu,Jos√©Lezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,YongCheng,AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal.Languagemodelbeatsdiffusion‚Äìtokenizeriskeytovisualgeneration.arXiv:2310.05737,2023.212


[47]NeilZeghidour,AlejandroLuebs,AhmedOmran,JanSkoglund,andMarcoTagliasacchi.Soundstream:Anend-to-endneuralaudiocodec.IEEE/ACMTrans.onAudio,Speech,andLanguageProcessing,30:495‚Äì507,2021.3,4[48]RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InCVPR,2018.6[49]SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.OPT:Openpre-trainedtransformerlanguagemodels.arXiv:2205.01068,2022.213


Figure11.ComparisonbetweenRQandSAQ.Weshowa2-layerquantizationprocessina2-dimensionalspaceasanexample.Atlayerl,weuseblueforthecurrentremainderembeddingszl,greenforcurrentpost-quantizationembeddingse(kl),andorangeforthere-constructedembeddingsuptolayerlasÀÜz‚â§l.Streamingaveragequantization.Fig.11comparesourproposedStreamingAverageQuantization(SAQ)withResidualQuantization(RQ)[7,11].Atlayer2,theSAQremainderembeddingz2=2z‚àíe(k1)isatamoresim-ilarscaletoz,comparedtotheRQremainderz‚àíe(k1).Wefindthatthescaleconsistencypro-motesbetterutilizationofthefrozenlanguagecodebookdespitealargenumberoflayersbeingused.Duetothepyramidstructure,quantizationinthefirstfewlayersmaybeskippedforthosepositionsnotselectedbythedilationsubsam-pler.Consideringthescaleconsistencyacrossquantizationlayers,theuseofSAQismoreap-propriateinthiscase.In-contextdenoising.Taketheimage-to-imagetaskinFig.2asanexample.Theprovidedcontextareimagesrandomlycorruptedinthetokenspacebyœµ(¬∑;r),wherethecorruptionratiorfollowsacosineschedule[2].(ui,vi)‚àº(cid:16)œµ(cid:0)T(mask(I));ri(cid:1),œµ(cid:0)T(I);ri(cid:1)(cid:17),I‚ààM(12)whereT(¬∑)representstheSPAEtokenizerandMisasmallsetofrawimages.mask(¬∑)zerosoutpixelsoftherealimagetocreatetheconditionimage,suchasmaskingoutthebottomhalffor37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).arXiv:2306.17842v3 [cs.CV] 28 Oct 2023
Figure10.Dilationsubsamplervisualization.WepresentadditionaldetailsabouttheSPAEmodelinthissection.Tokenpyramid.Fig.10showsanexampleofthedilationsubsamplerdefinedbyEq.(1).Weselectevenlydistributedpositionsineachlayertoformthetokenpyramidwithmonotonicallyincreasinglayersizes.
ùëô: ‚Ñé!√óùë§!1: 1√ó12: 2√ó23: 4√ó44: 8√ó8
ùëßÃÇ=ùíÜ
ùíõ‚àíùíÜ
ùëò"
ùëò"
ùëò"
ùíÜ
ùíÜ
ùíÜ
AppendixOverviewThissupplementarydocumentprovidesadditionaldetailstosupportourmainmanuscript,organizedasfollows:‚Ä¢AppendixApresentsmoredetailsonthemethod,includingSPAEarchitecturedesigns.‚Ä¢AppendixBprovidesadditionalimplementationdetails,includingavideoSPAEvariant.‚Ä¢AppendixCincludesmorequantitativeevaluationresults.‚Ä¢AppendixDshowsmorequalitativeexamplesofmodelgenerations.AMethodDetails
ùëò"SAQ[skipped]ùíõùíõLayer 1Layer 2
2ùíõ‚àíùíÜ
ùíõùíÜ
ùíõùíÜ
ùëò!+ùíÜ
ùëò!+ùíÜ(ùëò")ùíÜ
SPAE:SemanticPyramidAutoEncoderforMultimodalGenerationwithFrozenLLMsSupplementaryMaterials
RQSAQùëßÃÇ=12
ùëò!
ùëò!
ùëò!
ùëò!


out-painting.ThequeryÀÜuisalwayssampledfromMwithoutnoiseœµ.Toensurethegenerationisnotsimplycopyingthecontext,weenforceaminimalcorruptionrateof20%suchthatnoidenticalimagefromthecontextmatchestherealtargetimage.BImplementationDetailsB.1SPAETrainingImageSPAE.AnimageSPAEencodesa128√ó128imageinto16√ó16embeddings.FollowingtheVQGAN[5]architecture,weuse128basefilterswithchannelmultipliers[1,2,2,4]and2residualblocksateachscale,whichresultsin59Mparametersintotal.ImageSPAE-8.InadditiontotheprimarySPAEmodelwithsixpyramidlayersstudiedinthemainpaper,wealsotrainanSPAE-8modelwitheightlayerstoconductamorein-depthanalysisofthecoarse-to-finereconstructionprocess.Thetwoextralayerseachcontain16√ó16tokens.Thesemanticlossisstillappliedonthefirst5layersasintheprimarymodel.MNISTSPAE.WetrainanotherSPAEontheMNIST[4]datasetwiththesamearchitecturesetup.Wepadthehandwrittendigitimagesfrom28√ó28to32√ó32pixels,whicharethenencodedinto4√ó4embeddings.Eachimageisrepresentedby37tokensorganizedinfourlayers,withsizesof1√ó1,2√ó2,4√ó4,and4√ó4.WereplacetheCLIPimageembeddingwiththeCLIPtextembeddingofthelabelforthesemanticloss.Themodelistrainedfor10kstepswithabatchsizeof256.Forin-contextgeneration,ARdecodingwithastrideof4isusedtoproduceall37tokens.VideoSPAE.WeinitializeavideoSPAEbyVQGANinflation[10]fromapretrainedimageSPAE,whichencodes16framesat128√ó128resolutioninto4√ó16√ó16embeddings.AvideoSPAEconsistsof176Mparameters.Thepyramidlayerscontain1√ó1√ó1,1√ó2√ó2,1√ó4√ó4,2√ó8√ó8,4√ó16√ó16,and4√ó16√ó16tokens.ThevideoembeddingisobtainedastheaverageCLIPembeddingforallframes.ThemodelistrainedontheKinetics-600[1]datasetwhichcontains384kvideos.Wetrainwithabatchsizeof512for130ksteps,whichtakes5.8kTPUv4-hours.B.2LLMPromptingTogenerateprompts,weutilizeSPAEtoquantizeanimage,oranothernon-linguisticmodality,intoapyramidoflexicaltokens.Subsequently,weflattenthetokensbyconcatenatingthemlayer-by-layer,followingarasterscan,andresultingina1-Dstring.Thisstring,representingtheimage,isreferredtoastheSPAEstringinthefollowingprompts.Weusetask-specificprompttemplatestofacilitateanswergenerationwithLLMs.TheLLMoutputisalwaysparsedbyremovingleadingandtrailingwhitespaceornewlinecharacters.ImageclassificationwithGPT3.5.WeusethesameprompttemplateasLQAE[8]tointeractwithGPT3.5.Fora2-way1-shotclassificationbetweenclasslionandvase,thepromptisForeachofthefollowinginputoutputpairs,outputisoneof[‚Äòlion‚Äô,‚Äòvase‚Äô]###Input:<SPAEstringfromalionimage>Output:lion###Input:<SPAEstringfromavaseimage>Output:vase###Input:<SPAEstringfromthequeryimage>Output:Weusegreedydecodingtogetamaximumof7tokensfromGPT3.5.ImageclassificationwithPaLM2.WeusetheoriginalminiImageNet[9]formatwithPaLM2.Thepromptlookslike15


Answerwith"lion"or"vase".<SPAEstringfromalionimage>Thisisalion<SPAEstringfromavaseimage>Thisisavase<SPAEstringfromthequeryimage>Whatisthis?#Onlyusedin5-way3/5-shotsetupsThisisaWeusegreedydecodingtogetamaximumof4tokensfromPaLM2.Imagecaptioning.Weusegreedydecodingtogetamaximumof20tokensbeforethefirstnewlinecharacterwiththefollowingprompt:Generateacaptionsentencebasedonwordsdescribinganimage.Q:<SPAEstringfromimage1>A:<Captionforimage1>Q:<SPAEstringfromimage2>A:<Captionforimage2>Q:<SPAEstringfromthequeryimage>A:Visualquestionanswering.Weusegreedydecodingtogetamaximumof4tokensbeforethefirstnewlinecharacterwiththeprompttemplateasAnswerwithasingleword.C:<SPAEstringfromimage1>Q:<Questionforimage1>A:<Answerforimage1>C:<SPAEstringfromimage2>Q:<Questionforimage2>A:<Answerforimage2>C:<SPAEstringfromthequeryimage>Q:<Questionforthequeryimage>A:Image/videogenerationwithPARdecoding.Forimageorvideogenerationtasks,theconditioncanbeatextstringoranSPAEstringofaconditionimage.SupposeweusePARdecodingwithastrideof4tokens.Atthe4thstep,thepromptlookslikeLearnanewlanguageandpredictthe4tokensfollowingtheexamples.C:<conditionforimage1>Q:<SPAEstring(token1-12)forimage1>A:<SPAEstring(token13-16)forimage1>C:<conditionforimage2>Q:<SPAEstring(token1-12)forimage2>A:<SPAEstring(token13-16)forimage2>16


C:<conditionforthequery>Q:<SPAEstring(token1-12)forthegeneratedimagefromprevioussteps>A:WeusePaLM2togenerate8predictedsequencesforthenext4tokens,startingwithatemperatureT0=0.Weusethesentencepiece[6]tokenizertotokenizetheoutputstring.Ifallpredictionsareshorterthan4tokens,weretrytheLLMpredictionwithahighertemperature.Atthei-thretry,thetemperatureisgivenbyTi=œàiXj=12j(13)whereœà=0.01isused.Image/videogenerationwithPNARdecoding.WeusePNARdecodingtogenerateSPAElayer6conditionedonlayer1-5.Withastrideof16,thepromptatthe3rdsteplookslikePredicttheoutputsfollowingtheexamples.Q:<SPAEstringfromlayer1-5forimage1>A:<SPAEstringfromlayer6(token33-48)forimage1>Q:<SPAEstringfromlayer1-5forimage2>A:<SPAEstringfromlayer6(token33-48)forimage2>Q:<SPAEstringfromlayer1-5forthegeneratedimagefromARdecoding>A:WeusePaLM2togenerate8predictedsequencesforthenext16tokens.Ifthesentencepieceparsingfails,weretrywiththesametemperaturescheduleasinPARdecoding.B.3CorruptionFunctionsPixel-spacetransformation.Weusepixel-spacetransformationintheconditionalimageinterpola-tiontaskswiththefollowingsetups:‚Ä¢Brightness:[¬±0.8,¬±0.6,¬±0.4,¬±0.2].‚Ä¢Contrast:[¬±0.8,¬±0.6,¬±0.4,¬±0.2].‚Ä¢Saturation:[¬±0.4,¬±0.3,¬±0.2,¬±0.1].‚Ä¢Color(RGB):[(0.6,1.4,1),(0.7,1.3,1),(0.8,1.2,1),(0.9,1.1,1),(1.1,0.9,1),(1.2,0.8,1),(1.3,0.7,1),(1.4,0.6,1)]Overflowpixelsareclippedto[0,255].Token-spacepermutationnoise.Randompermutationisusedinthein-contextdenoisingsetupforconditionalimagedenoisingtasks.Specifically,wereplaceafractionoftokenseachwitharandomtokensampledfromtheentire65kvocabularytosatisfyagivencorruptionrate.Thecorruptionratesforthe10examplesare[0.5,0.47,0.44,0.41,0.38,0.35,0.32,0.29,0.26,0.23].Thepermutationnoisepresentsacontextdistributionwithexpectationattherealimage,butdoesnotcontainthegroundtruthtokenstopreventinformationleakage.CAdditionalQuantitativeResultsFew-shotimageclassificationwithdifferentSPAElayers.Tab.5presentthefew-shotmini-ImageNetclassificationperformancewitheachSPAEPaLMlayer.ThesedetailedquantitativenumbersaccompanythefindingsfromFig.3.Asshown,Layer3achievesthebestoverallperformanceaswellasinmostofthesetups,whichbalancesbetweenthelevelofdetailsandtheburdenoftheLLM.17


Model#LayersFID‚ÜìIS‚ÜëLPIPS‚ÜìCLIP‚ÜëRelative:#TokensCLIP‚Üë
SPAEdisjointPaLM2:5PaLM224.879.884.583.780.878.578.472.93SPAEdisjointPaLM3:21PaLM221.481.489.287.982.681.780.674.98
Few-shotimageclassificationwithSPAEdisjoint.FollowingthepreviousworkofLQAE[8],wetrainourSPAEontheImageNettrainingsplit[3]andpresentthecomparativeresultsinthemainpaper.ThereisapossibilityofoverlapbetweenthetrainingsplitofImageNetandthemini-ImageNetdatasetusedinthefew-shotclassificationtask[9].Sincefewstudieshaveinvestigatedthisbefore,wepresenttheresultsoftrainingSPAEontheImageNettrainingsplitafterexcludingthe20classesusedinthefew-shotmini-ImageNetclassificationtask.ThiscreatesaevenmorechallengingsettingasthevisualclasseshaveneverbeenseenduringthetrainingofthetokenizerortheLLMs.AsdemonstratedinTab.5,wepresenttheresultsoftrainingourtokenizeronthedisjointeddata,referredtoasSPAEdisjoint.Asexpected,weobserveaslightdecreaseinperformance,sincebothSPAEandLLMsneedtogeneralizetothetestclassesthatareoutsidethetrainingdatadistribution.Despitethefactthatthebaselineistrainedonunlabeledimagessampledfromthemini-ImageNettestclasses,SPAEdisjointPaLMstilldemonstratesasignificantimprovementoverthestate-of-the-artbaselineonthe2-waybenchmarks.TokenqualitywithmoreSPAElayers.Tab.6showstheper-layerreconstructionqualityandsemanticrelevanceoftokensfromtheSPAE-8modelincomparisontothedefaultmodel.Withmoretokenlayers,themodelgainslargercapacityforbothsemanticandappearance,wheretheappearancegetspushedintodeeperlayers.Atlayer1to6,SPAE-8yieldsconsistentlyhigherCLIPscoresthanSPAE.Atthelastthreelayers,SPAE-8alsohasbetterreconstructionqualitythanthelasttwolayers18
Table5.Few-shotclassificationaccuracyonthemini-ImageNetbenchmarks.-meansvalueunavailableduetoaninfeasiblesequencelength.
5-WayClassificationSPAEPaLM1:1PaLM226.852.050.949.951.948.447.946.83SPAEPaLM2:5PaLM223.664.268.069.963.462.060.258.76SPAEPaLM3:21PaLM220.265.173.774.366.467.066.361.86SPAEPaLM4:85PaLM216.158.567.269.164.066.467.458.39SPAEPaLM5:341PaLM212.146.355.967.243.346.3--SPAEPaLM6:597PaLM212.135.7------
SPAE-81:1---0.20510.80182:5---0.20460.79943:21---0.20120.78344:85---0.18960.72895:34143.4249.780.320.17090.64126:5978.93116.120.180.16670.62137:8534.78135.010.130.16470.61198:11093.89140.550.110.16340.6058
Table6.ReconstructionqualityandsemanticrelevanceofSPAE-8tokens.
TaskInduction‚úì‚úì‚úì‚úì‚úì‚úìAvgMethod#LayersInnerShots1135111:#TokensRepeats0000135
2-WayClassificationSPAEPaLM1:1PaLM234.877.281.280.374.073.271.570.31SPAEPaLM2:5PaLM232.284.088.588.485.183.682.477.74SPAEPaLM3:21PaLM227.984.892.592.684.885.285.479.03SPAEPaLM4:85PaLM222.881.191.490.482.684.384.776.76SPAEPaLM5:341PaLM221.277.488.079.184.874.076.171.51SPAEPaLM6:597PaLM221.873.870.862.464.862.158.659.19


Figure12.TrainingcurvesofSPAEincomparisontoVQGAN.Metricsarepresentedregardingreconstructionquality(FID,IS,LPIPS)andsemanticrelevance(CLIP).ofSPAE.Theseresultssuggestthepotentialofbetterreconstructionqualityandsemanticrelevancefromusingmoretokenlayers.Trainingefficiency.AllmodelsusedintheablationstudyinTab.3,includingVQGAN[5]andRQ-VAE[7]variants,aretrainedusingthesamesetupforfaircomparisons.Fig.12comparesthetrainingcurvesofFID,IS,LPIPS,andCLIPscoreofSPAEandVQGAN.Asshown,within40%ofthetrainingsteps,SPAEshowsbetterFIDthanthefinalVQGANcheckpoint.TheCLIPscorekeepsimprovingasthetrainingproceeds,whiletheLPIPSsaturatesquiteearly.DAdditionalQualitativeExamplesTokenpyramidvisualization.Fig.13showstokenizationandreconstructionsamplesbya6-layerSPAEfromImageNetvalidationset.Keyconceptsarecapturedinthefirstfewlayers,whereasthelaterlayersfocusonthevisualappearance.Inthecoffeemachineexample,manykeywordsarepresenttodescribevariousaspectsfromthestovetothethermometer.Intheparrotcase,asingleunifiedconceptisrepeatedlyhighlighted.Coarse-to-finereconstruction.Fig.14showsreconstructionsamplesbySPAE-8fromImageNetvalidationset.Wecomparethereconstructedimagesfromlayer5tolayer8todemonstratethecoarse-to-fineprogress.Conditionalimageinterpolation.Tothebestofourknowledge,therehavebeennosuccessfulattemptsthatdemonstrategenericimagegenerationcapabilityusingafrozenLLM.Tothisend,wedefineaverysimplesetuptoexploretheinterpolationcapabilityofLLM,wheretheconditionsare19


integersfrom1to9.Thetargetimagesarecreatedwithdifferentpixel-spacetransformationsdetailedin.AsshowninFig.15,images1-4and6-9arefedascontexttoproduceimage5,wherethemodelinterpolatesthevariableproperty.Fig.16showsgeneratedsamplesat256√ó256resolutionunderthesamesetup.Conditionalimagedenoising.WeusePARdecodingtoproducethefirst5tokenlayerswithtask-specificconditions,followedbytask-agnosticPNARdecodingtofillinlayer6.Fig.17visualizestheinputpairsfortheimage-to-imagegenerationexamplesinFigs.7and9,withmoreexamplesinFig.18.Underthein-contextdenoisingsetup,theLLMgeneratesnovelimagesbasedontheprovidedcontext,wheremultipledifferentgenerationscanbeobtained.Multimodaloutputs.Fig.19showsataskrequiringasingleLLMtooutputbothimageandtext,whereitfirstinpaintsthecenterregionofanimageusingin-contextdenoisingandthencreatesmultiplecaptionsfortheoutputimage.Image-to-videodenoising.Fig.20showsanimage-to-videoexamplewiththeframepredictiontaskusingprogressivein-contextdenoising.TheinputisoneframetokenizedbytheimageSPAE,whiletheoutputisa16-framecliptokenizedbythevideoSPAE.Wefollowthesametwo-stageprocedureasimage-to-imagegeneration,withmorestepsineachstagetoaccountforthelongersequence.Duetothesequencelengthlimit,onlyfoursamplescanbefitintothecontext,whichlimitsLLM‚Äôsperformanceforthistask.20


(a)Manykeywordsarepresenttodescribevariousaspectsfromthestovetothethermometer.
Original
Original
(b)Asingleunifiedconceptisrepeatedlyhighlighted.Figure13.Examplesofmulti-layerimagetokenizationandreconstructionbya6-layerSPAE.Forvisualiza-tionpurposesonly,weusedarkercellstoshowtokenswithhigherCLIPscoresregardingtheoriginalimage.Fornon-Englishsub-wordtokens,weshowautomatictranslationforreferenceinitalicfontsbelowtheoriginaltoken.Weshowtokensinallsixlayers,alongwithreconstructedimagesfromthelasttwolayers.21


Color
1
52346789Condition:ContextContextGenerationFigure15.Examplesofconditionalimageinterpolationofdifferentimagetransformations.22
OriginalLayer 5Layer 6Layer 7Layer 8Figure14.Examplesofcoarse-to-fineimagereconstructionbySPAE-8.Thetop5layersreconstructanoisyimage.Theappearancedetailsgraduallygetrefinedasmoretokenlayersareaggregatedbythestreamingaveragequantizationprocess.
BrightnessContrastSaturation


‚Ä¶
‚Ä¶
Figure16.Examplesofconditionalimageinterpolationat256x256resolution.TheLLMisprovidedwitheightconditionimagesfortheinterpolationfollowingthesetupinFig.15.23
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ContextContext
Genera&on


(a)Outpaintingthebottomhalf.Twogeneratedimagesareshown.
Stage 1: ARLayer 1-5Task-speciÔ¨Åc
Stage 1: ARLayer 1-5Task-speciÔ¨Åc
Stage 1: ARLayer 1-5Task-speciÔ¨Åc
Stage 1: ARLayer 1-5Task-speciÔ¨Åc
Stage 1: ARLayer 1-5Task-specific
Generation
Generation
Corruption: 50%20%(d)Spatialtranslationtotheright.
Corrup&on: 50%20%Outpainting
Genera)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondition
Corruption: 50%20%(c)Inpaintingthecenterregion.
Corrup&on: 50%20%(e)Clockwiserotationby90degrees.Figure17.Examplesofconditionalimagedenoising.Allinputsamplesforthein-contextlearningarepresentedfortheexamplesinFigs.7and9.TheLLMgeneratesnovelimagesbasedontheprovidedcontext.Multipledifferentgenerationscanbeobtainedfromthesamesetofcontextsamples.24
Genera)ons
Genera,on
Corruption: 50%20%(b)Deblurfromagaussianfilter.
Stage 2: NARLayer 6Task-agnosticContextCondi)on


Stage 2: NARLayer 6Task-agnosticContextCondition
Stage 1: ARLayer 1-5Task-specific
Stage 1: ARLayer 1-5Task-specific
Stage 1: ARLayer 1-5Task-specific
Generation
Generation
Stage 2: NARLayer 6Task-agnos<cContextCondition
Stage 2: NARLayer 6Task-agnos<cContextCondition
Corruption: 50%20%(a)Inpaintingthecenterregion.
Corruption: 50%20%A dog with a long, curly coat of fur.A small dog with a big smile.A small dog with long hair looks up at the camera.
Genera,on
Corrup&on: 50%20%(b)Outpaintingthebottomhalf.Figure18.Moreexamplesofconditionalimagedenoising.TheLLMgeneratesnovelimagesbasedontheprovidedcontextimagepairs.
CaptionFigure19.ExamplesofmultimodaloutputsfromtheLLM.TheLLMgeneratesanovelimagewithmultiplecaptionsbasedontheprovidedcontext.25


Condi&on
Genera&onFigure20.Examplesofimage-to-videodenoising:frameprediction.Wefollowthesametwo-stagegenerationprocedureasinimage-to-imagetasks.Duetothesequencelengthlimit,onlyfoursamplescanbefitintothecontext.Thegeneratedvideoclipappearvisuallydifferentfromthecontextsamples,especiallyaroundthereflectionsofthebowl.26
Stage 2: NAR; Layer 6; Task-agnos=c
Corrup&on35%11%Stage 1: AR; Layer 1-5; Task-speciÔ¨Åc
Context


References[1]JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,andAndrewZisserman.AshortnoteaboutKinetics-600.arXiv:1808.01340,2018.15[2]HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.MaskGIT:Maskedgenerativeimagetransformer.InCVPR,2022.14[3]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scalehierarchicalimagedatabase.InCVPR,2009.18[4]LiDeng.Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb].IEEESignalProcessingMagazine,29(6):141‚Äì142,2012.15[5]PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.InCVPR,2021.15,19[6]TakuKudoandJohnRichardson.Sentencepiece:Asimpleandlanguageindependentsubwordtokenizeranddetokenizerforneuraltextprocessing.InEMNLP,2018.17[7]DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan.Autoregressiveimagegenerationusingresidualquantization.InCVPR,2022.14,19[8]HaoLiu,WilsonYan,andPieterAbbeel.Languagequantizedautoencoders:Towardsunsupervisedtext-imagealignment.arXiv:2302.00902,2023.15,18[9]OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal.Matchingnetworksforoneshotlearning.InNeurIPS,2016.15,18[10]LijunYu,YongCheng,KihyukSohn,Jos√©Lezama,HanZhang,HuiwenChang,AlexanderGHauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal.MAGVIT:Maskedgenerativevideotransformer.InCVPR,2023.15[11]NeilZeghidour,AlejandroLuebs,AhmedOmran,JanSkoglund,andMarcoTagliasacchi.Soundstream:Anend-to-endneuralaudiocodec.IEEE/ACMTrans.onAudio,Speech,andLanguageProcessing,30:495‚Äì507,2021.1427