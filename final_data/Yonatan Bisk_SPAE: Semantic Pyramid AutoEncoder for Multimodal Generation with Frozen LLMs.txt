∗WorkpartiallydoneduringaresearchinternshipatGoogleResearch.37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).arXiv:2306.17842v3 [cs.CV] 28 Oct 2023
SPAE:SemanticPyramidAutoEncoderforMultimodalGenerationwithFrozenLLMs
LijunYu‡†∗YongCheng†ZhiruoWang‡VivekKumar†WolfgangMacherey†YanpingHuang†DavidA.Ross†IrfanEssa†YonatanBisk‡Ming-HsuanYang†KevinMurphy†AlexanderG.Hauptmann‡LuJiang†‡†Google,‡CarnegieMellonUniversityAbstractInthiswork,weintroduceSemanticPyramidAutoEncoder(SPAE)forenablingfrozenLLMstoperformbothunderstandingandgenerationtasksinvolvingnon-linguisticmodalitiessuchasimagesorvideos.SPAEconvertsbetweenrawpixelsandinterpretablelexicaltokens(orwords)extractedfromtheLLM’svocabulary.Theresultingtokenscaptureboththesemanticmeaningandthefine-graineddetailsneededforvisualreconstruction,effectivelytranslatingthevisualcontentintoalanguagecomprehensibletotheLLM,andempoweringittoperformawidearrayofmultimodaltasks.Ourapproachisvalidatedthroughin-contextlearningexperimentswithfrozenPaLM2andGPT3.5onadiversesetofimageunderstandingandgenerationtasks.OurmethodmarksthefirstsuccessfulattempttoenableafrozenLLMtogenerateimagecontentwhilesurpassingstate-of-the-artperformanceinimageunderstandingtasks,underthesamesetting,byover25%.1IntroductionLargelanguagemodels(LLMs)empoweredbyTransformers[38]haveachievedremarkableprogressinaddressingabroadspectrumofNaturalLanguageProcessing(NLP)tasks[4,8,28,2].Withthecontinuousincreasesinmodelsizeandtrainingdata,LLMsaregraduallybecomingmoreversatileandagnostictospecifictasks,unlockingnewcapabilitiesinsolvingcomplexAItasks[42],likequestionanswering,codegeneration,reasoning,mathematicsproblem-solving,andunderstandinghumor,amongvariousotherapplications[2,28].LLMscapturerichconceptualknowledgeabouttheworldintheirlexicalembeddings.Thisraisesaquestion:ifprovidedwiththeappropriatevisualrepresentationsasinput,arefrozenLLMscapableofsolvingtasksinvisualmodalities?Veryrecently,therehavebeennotableadvancementsinextendingthecapabilitiesoffrozenLLMstotackleimageunderstandingandretrievaltasks[21,27].However,generatingadifferentmodalityusingafrozenLLMthathasnotbeenexplicitlytrainedonthatmodalityhasproventobechallengingandhashadlittlesuccess.TofacilitateLLMsforsuchcross-modaltasks,weproposetolearnavectorquantizertomapanimage,orsomeothernon-linguistic(“foreign”)modality,tothetokenspaceofafrozenLLM.ThiseffectivelytranslatestheimageintoalanguagethattheLLMcancomprehend,enablingustoleveragethegenerativeabilitiesoftheLLMtoperformimageunderstandingandgenerationtaskswithouthavingtotrainonimage-textpairs.Specifically,ournewapproachisthat,givenanimageprompt,convertittoatokenspacewithourlearnedencoder,usetheLLMtogeneratesuitablelexicaltokens,andconvertbacktopixelspacewithourlearneddecoder.


Top 5 layers (341 tokens)
branch, nest, eagle, …, bird, veld, climb, …
Decoder
Encoder
❄4 layers5 layers6 layersReconstructed image
CLIP
Seman&c loss
Top 4 layers (85 tokens)branch, nest, eagle, …
Top 6 layers (597 tokens)branch, nest, eagle, …, bird, veld, climb, …,wings, 观, ودي, …
Figure1.FrameworkoftheproposedSPAEmodel.Animageisencodedintoapyramidoflexicaltokenscapturingsemanticconceptsandappearancedetailsnecessaryforreconstruction.WeintroduceanovelSemanticPyramidAutoEncoder(SPAE)thatproducesalexicalwordsequencethat(1)carriesrichsemantics,and(2)retainsfinedetailsforsignalreconstruction.IncontrasttothemajorityofVQ-VAEapproaches[37],ourencodermapstoaninterpretablediscretelatentspace,i.e.,words.AsdepictedinFig.1,SPAEtokenshaveamulti-scalerepresentationarrangedinapyramidstructure.Theupperlayersofthepyramidcomprisesemantic-centralconcepts,whilethelowerlayersprioritizeappearancerepresentationsthatcapturesthefinedetailsforimagereconstruction.Thisdesignenablesustodynamicallyadjustthetokenlengthtoaccommodatevarioustasks,suchasusingfewertokensforunderstandingtasksandmoretokensforgenerationtasks.Weverifytheplausibilityofourapproachinanextremesettingofin-contextlearning[4],withoutanyparameterupdatestotheLLM.OurSPAEmodelistrainedstandalone,withoutbackpropagatingthroughanylanguagemodel.Weevaluateourapproachonimageunderstandingtasksincludingimageclassification,imagecaptioning,andvisualquestionanswering.WeshowcaseapromisingdirectiontoimagegenerationwithLLMsbyutilizingin-contextdenoisingtechniques.OurmethodisLLM-agnosticandhasbeentestedwithPaLM2[2]andGPT-3.5[28],suggestingcompatibilitywitharbitraryLLMs.Themaincontributionsofthisworkaresummarizedasfollows:•Thisisthefirstsuccessfulmethod,tothebestofourknowledge,thatusesafrozenlanguagemodel,trainedsolelyonlanguagetokens,todirectlygenerateimagecontentthroughin-contextlearning.•WeintroduceanewSPAEtokenizerproducinginterpretablerepresentationsofsemanticconceptsandfine-graineddetailsintheformofmultilinguallinguistictokenswithadjustablelengths.•Weevaluateourmethodonvisualunderstandingandgenerationtasks,andnotably,ourapproachoutperformsthebest-publishedfew-shotimageclassificationaccuracy[27]byanabsolute25%underthesamein-contextsetting.2RelatedWorkMultimodalgenerationwithLLMs.AdvanceshavebeenmadetoexpandthecapabilitiesofLLMsbeyondlanguage.Forexample,VisualChatGPT[43]usesChatGPTtogeneratepromptsandexecutesmultimodaltasksthroughanothermodel,e.g.,generatingimagefromtextpromptsbyStableDiffusion[32].FROMAGe[21]feedsCLIP[30]embeddingstoOPT[49]forimageunderstandingandretrieval.However,itrequiresbackpropagationthroughtheLLManddoesnotsupportimagesynthesis.ThisworkenablesastandalonefrozenLLMtounderstandandgenerateothermodalitieswhichareunseenintraining.Tokenizationviavectorquantization.VQ-VAE[37]compressesdataintoadiscretelatentspacedefinedbyacodebookviavectorquantization.VQGAN[14]enhancesthereconstructionqualitywithadversarialandperceptualobjectives.Thesediscretelatentquantities,oftenreferredtoastokens,arewidelyusedtolearngenerativetransformermodelsforimage[32,7],video[45,15,39],image-video[46],andaudio[3,9].OurSPAEmodelisbuiltupontheVQGANframeworkandapplicabletodifferentmodalities.2
Layer 4Layer 3Layer 2Layer 1Layer 5Layer 6
❄LLM codebook


2(cid:25)+1,w′j−(cid:24)w′
2(cid:25)+1)|(i,j)∈([1,hl]×[1,wl])∩Z2},(1)whereh′=hD
hl,andw′=wD
wlarethedownsampleratios.Foreachembeddingzatposition(x,y),weobtainitsdiscretetokenssequentiallyfromlayer1toD.Atlayerl,if(x,y)∈P(l),thequantizerassignsdiscretetokenkl=argmink∈T∥zl−e(k)∥22,3
Tokenizationintolexicalrepresentations.ThecodebooksintypicalVQGANsarelearnedjointlywiththeencoderanddecoderstacks,whicharenotdirectlyinterpretablevianaturallanguages.LQAE[27]replacesthelearnedcodebookwithfrozenwordembeddingsfromBERT[12]toconnectwithanEnglishvocabulary.However,theLQAEtokensseldomcontainsemanticconceptsinanimage,andthereconstructionqualityisworsethanthatwithalearnedcodebook.OurSPAEquantizesaninputsampleintosemanticallyrelatedtokensinamultilingualvocabularywhilepreservingthehighreconstructionqualityofaVQGANforgenerativetasks.Inaddition,SPAEtokensareorganizedinamulti-layercoarse-to-finepyramidforflexibleusageindifferenttasks.Few-shotlearningwithLLMs.In-contextlearning[4,8,2]facilitatesLLMsforfew-shotlearningviathetextinterfacewithoutparameterupdates.ThisapproachiscommonlyemployedtoassesstheperformanceofLLMsonnumerousNLPbenchmarks,e.g.,classificationandquestionanswering[41],mathematicalreasoning[24],andcodegeneration[44],whichyieldscompetitiveresultstotheirfine-tunedcounterparts.However,existingfew-shotvision-languageunderstandingandgenerationframeworks[1,21]stillrequireLLMparameterupdates.Incontrast,ourworkinheritsthein-contextlearningabilityfromfrozenLLMs.3MethodOurgoalistomodelanimage,orsomeothernon-linguisticmodality(e.g.,videooraudio),asalanguagesequencethatLLMscancomprehend.SemanticPyramidAutoEncoder(SPAE)generatesalexicalwordsequencewithdynamicallyadjustablelengththatcarriesrichsemanticsandretainsfinedetailsforsignalreconstruction.ToworkwithafrozenLLMviain-contextlearning,weintroduceaprogressivein-contextdenoisingmethodtofacilitateimagegeneration.WeusetheimagemodalityinthissectiontointroduceourSPAEmodelin2D,andlatershowcasetheresultsofa3Dvariantwiththevideomodalityinourexperiments.3.1SemanticPyramidAutoEncoderOurSPAEmodelextendstheVQ-VAE[37]framework,whichcomprisesanencoder,aquantizer,andadecoder.TheCNNencodermapsanimageI∈RH×W×3intocontinuousembeddingsZ∈Rh×w×c.Eachelementz∈Zisthenpassedthroughthequantizer,whichassignsittotheclosestentryinacodebook,resultinginthequantizedembedding.LetˆZrepresentthequantizedembeddingsfortheentireimage.TheCNNdecoderreceivesˆZasinputandgeneratesthereconstructedimageˆI.BelowwehighlightthedesigndifferencesinSPAE.AsillustratedinFig.1,SPAEgenerateslexicaltokensarrangedinapyramidstructure,whichcontainssemanticconceptsintheupperlayersandappearancewithprogressivelyrefineddetailsinthelowerlayers.Weintroduceasemanticlosstoencouragetheusageofconceptuallyrelevanttokens.Frozenlanguagecodebook.Togeneratelexicaltokens,weutilizeapretrainedLLMcodebookC={(k,e(k))|k∈T}andfreezeitduringtraining,whereTisasubsetoftheLLMvocabulary.Here,e(·)producesthetextembeddingforasub-wordkwhichmaybeobtainedfromanylayeroftheLLM.Sincethecodebookisalignedwiththelanguagevocabulary,weusetheterms“token”and“word”interchangeably.Tokenpyramid.TheSPAEquantizerproducesDlayersoftokenswherethetokensatlayerlaredenotedaskl∈Thl×wl.PriorworksuseResidualQuantization(RQ)togeneratemulti-layertokens[22,47].Inthesemethods,tokensfromalllayershaveuniformshapesanddonotcarryspecificsemanticmeanings.Incontrast,weproposeapyramidtokenstructurebyenforcingtheconstrainthl≤hl+1∧wl≤wl+1.Thepyramidstructureispurposefullydesignedtoconcentratesemanticswithinthewithintheupperlayersofthepyramid.Thisdesignallowsforrepresentingsemanticconceptswithnotablyfewertokens,e.g.,asfewasfivetokensforunderstandingtasks.Thehightokenefficiencystemsfromthepyramidstructure,asaconventionallayerwithoutpyramidstructuresneedsaminimumofhwtokens(e.g.,256)torepresenttheimage.Tokenefficiencyiscrucialforin-contextlearningasitenablestheaccommodationofmoreexampleswithinthecontext.AdilationsubsamplerP(l)isused,whichselectsthepositionsforquantizationatlayerlasP(l)={(h′i−(cid:24)h′


|p|P|p|i=1fT(pi(k)),(5)wherepisalistofprompttemplates,suchas"aphotoof...".Duringtraining,weextracttheimagefeaturefI(I)andcomputethedot-productsimilarityass′(I,k)=fI(I)·f′T(k).Thesimilarityscoreisthennormalizedtoaccountforthevaryingscalesacrossdifferentimages.s(I,k)=s′(I,k)−minjs′(I,j)
Pk∈Texp(−∥zl−e(k)∥22),(7)wherewerandomlysamplesemanticallysimilartargetcodescforeachlayerembeddinginthefirstD′layers.Appearanceloss.Usinganimprovedobjectivefrom[45],theappearancelossiscalculatedas:Lappearance(θe,θd;I)=∥I−ˆI∥22+βPDl=1∥Z−sg(ˆZ≤l)∥22+λLGAN+ηLPerceptual+ϕLLeCAM,(8)whereLGAN,LPerceptual,andLLeCAMaretheVQGAN[15],perceptual[19],andLeCAM[34]losses.Inaddition,sg(x)isthestop-gradientoperation.Theappearancelossisappliedtoboththeencoderθeanddecoderparametersθd,excludingthefrozencodebookembedding.Tostabilizethetrainingandbalancebetweenappearanceandsemantics,weaddadynamicweightforthesemanticguidancelossasw=sg(cid:16)Lappearance(I)
wherezlisthecurrentlayerembedding,calculatedfromzl=z+Pl−1i=11(x,y)∈P(i)(z−e(ki)).(2)Thequantizedembeddingreconstructedwiththefirstllayersisgivenbytheaverageoftheexistingtokenembeddingsasˆz≤l=Pli=11(x,y)∈P(i)e(ki)
ˆl+1e(kl+1),ˆl=Pli=11(x,y)∈P(i).RQ[22,47]isapplicablebutyieldsworseresultsinthiscontext,asrevealedbyourablationstudies.Thiscanbeattributedto(1)varyingscalesofembeddingsinresiduallayers,potentiallydividingthecodebookintomultipleparts,and(2)misalignmentinthesummationofwordembeddings,whichundermineslearningsemanticallymeaningfultokensinlaterlayers.Semanticloss.WeencouragethesemanticsimilaritybetweentheimageIandeachlexicaltokenkdenotedbys(I,k).Duringtraining,webuildper-layercandidatetokenpoolsasCl(I)={k∈T|s(I,k)≥ρl},(4)whereρlisathreshold.Wesetρl≥ρl+1toallowdeeperlayerstohavealargerpoolofcandidatetokenswhilesacrificingsomesemantics.Todefinethesimilarityscore,thispaperemploysapretrainedCLIPmodel[29].Inmoredetails,letfIandfTbeapairofimageandtextCLIPembeddingfunctions.Weprecomputethetextfeatureforeachtokenk∈Tasf′T(k)=1
maxjs′(I,j)−minjs′(I,j).(6)WedefinethesemanticlossfortheencoderparametersθeasLsemantic(θe;I)=El∈[1,D′]EzlEc∈Cl(I)−logexp(−∥(zl−e(c)∥22)
Pli=11(x,y)∈P(i).(3)UsingtheinputofˆZ≤lfromtokensuptolayerl,thedecodercanprogressivelyreconstructtheimagewithdynamictokenlengths,resultingingraduallyimprovedqualitywithrefinedappearancedetails.WetermthisapproachasStreamingAverageQuantization(SAQ)duetoitsresemblancetocomputingtheaverageonstreamingdata,whereˆz≤l+1=ˆz≤l+1
Lsemantic(I)(cid:17).ThetotaltraininglossexcludingtheGANdiscriminatorisLSPAE(θe,θq)=EIhLappearance(θe,θq;I)+αwLsemantic(θe;I)i.(9)4


20%InputOutputQueryOriginalFigure2.Anexampleoftheconditionalimagedenoisingtaskforhighresolutionsynthesis.Thecontextcomprisesimagesrandomlycorruptedinthetokenspace.3.2ProgressiveIn-ContextDecodingWhileourmethodismoreeffectivewhenbackpropagatingthroughLLMsbyprompt[23]oradaptertuning[17,18],thisworkfocusesonverifyingtheplausibilityinanextremesettingofin-contextlearning[4].WedemonstratethatLLMsarecapableofperformingnewtasksinforeignmodalitieswithoutanyparameterupdates.Specifically,asetofKexamples{(ui,vi)}Ki=1arefedtotheLLMtolearnanewtaskandansweraqueryˆuwithˆv∼PLLM(·|ˆu;{(ui,vi)}Ki=1).(10)Samplingˆvbyasingle-passautoregressivedecodingissuboptimalduetothedistributionalshiftintherepresentationandthepresenceofexceptionallylongsequences,e.g.,animageisquantizedintoover500tokens.Tothisend,weuseaprogressivedecodingmethod.WegeneralizeEq.(10)intoamulti-passprocess,wheretheLLMlearnstogenerateonesegmentofthetargetsequenceatatime.Thesegmentgeneratedfromthet-thpassisˆvt∼PLLM(·|[ˆu,ˆv<t′];{([ui,vi<t′],vit)}Ki=1),(11)where[·,·]indicatesconcatenation.t′controlsthelengthofprevioussegmentstoconditionon,withtwocommoncases:(1)aprogressiveautoregressive(PAR)processwitht′=t,whereeachdecodedsegmentconditionsonallpreviouslydecodedones;(2)aprogressivenon-autoregressive(PNAR)processwitht′=0tosampleeachsegmentindependently,whichgreatlyreducesthesequencelengthrequirementfortheLLM.Inpractice,weusePARtogeneratethefirstfewtokenlayersgiventask-specificconditions,followedbyPNARtogeneratetheremainingtokenlayersconditionedonthepreviouslayersinanunconditionallatentrefinementprocess.Thelearningcapacityofanin-contextsetupisfarfromsufficientforamodalitythathasnotbeenseenduringtraining.Sofar,therehavebeennosuccessfulattemptsintheliteraturedemonstratingthatafrozenLLMcangenerateimagecontent.Forlow-resolutionimages,LLMscanproduceimagesdirectlyusingin-contextlearning,aswillbedemonstratedwith32×32MNISTimages[11].Forhigherresolutions,thecontextlengthrestrictsthenumberofexamples.Forinstance,acontextwindowof8ktokenscanonlyholdlessthanadozen128×128images.Therefore,weoperateinadenoisingsubspacetosynthesisbeyond32×32resolution.Fig.2illustratesoneexample,withdetaileddefinitionsintheAppendix.4ExperimentalResults4.1ExperimentalSettingsToverifythecompatibilitywithdifferentLLMs,wetraintwovariantsofSPAE,namelySPAEPaLMandSPAEGPT.TheSPAEPaLMcodebookistakenfromtheinputembeddinglayerofaPaLM2-Scheckpointwitha65kvocabularyofthemostfrequentsentencepieces.ThePaLM2-LAPI[2]isusedforin-contextlearningwithSPAEPaLM.SPAEGPTusesabyte-pairencodingvocabularywith99kUTF-8tokens(https://github.com/openai/tiktoken),whereweobtainthecontextualtokenembeddingsfromOpenAItext-embedding-ada-002(https://platform.openai.com/docs/models/embeddings).Forafaircomparisonwithpriorworks[27],weuseSPAEGPTwiththeGPT3.5text-davinci-003API(https://platform.openai.com/docs/models/gpt-3-5).WeconfigureSPAEtoencodea128×128imageintoatokenpyramidof6layerswhereeachlayerhas2k×2ktokensandk=[0,1,2,3,4,4].Additionally,wetrainavideo-basedSPAEmodelontheKinetics-600dataset[5],andfurtherdetailscanbefoundintheAppendix.Weapplysemanticguidancelosstothefirstfivelayers,withthresholdsof0.98,0.95,0.9,0.85,and0.8.TheCLIPwithaViT-L/14[13]visionbackboneisused.Weuse80prompttemplatesfromthezero-shotImageNet5
Context: corruption 50%


5-WayClassificationFrozen[35]-0.914.534.733.833.833.332.826.26LQAE[27]1:256GPT3.51.015.735.936.531.936.445.929.04SPAEGPT(ours)2:5GPT3.54.363.063.460.661.962.162.153.91
5-way 5-shot
Table1.Few-shotclassificationaccuracyonthemini-ImageNetbenchmarks.SPAEGPTandSPAEPaLMaretrainedusingdifferentvocabulariesandembeddingsources,withdifferentprompttemplatesforin-contextlearning.TheyshowthebroadcompatibilityofSPAEbutarenotforacomparisonbetweentheLLMs.ThebestperformancewithGPTisinitalicswhiletheoverallbestisinbold.
SPAEPaLM(ours)2:5PaLM223.664.268.069.963.462.060.258.76SPAEPaLM(ours)3:21PaLM220.265.173.774.366.467.066.361.86
# Layers: # Tokens02550751001: 12: 53: 214: 855: 341
2-way 1-shot
2-WayClassificationFrozen[35]-1.733.76666636563.751.3LQAE[27]1:256GPT3.51.535.268.269.868.568.765.953.97SPAEGPT(ours)2:5GPT3.55.377.284.486.079.477.277.169.51
classificationsetuptoprecomputetheCLIPtextembeddingsforthevocabulary.Inaddition,weusetheAdam[20]optimizerwithlossweightsα=1,β=0.33,λ=0.1,η=0.1,ϕ=10−4andalearningrateof10−4followingalinearwarmup/cooldownandrootsquaredecayschedule.Followingthepriorwork[27],SPAEistrainedontheImageNetILSVRC2012[10]dataset.Wetrainwithabatchsizeof256for450ksteps.FurtherdetailscanbefoundintheAppendix.4.2MainEvaluationFew-shotimageclassification.Weevaluatethein-contextimageunderstandingcapabilitywithafrozenLLMonthemini-ImageNet[40]few-shotclassificationbenchmark.Asetoftokenizedimagesandclasslabelsarefedtothelanguagemodelascontextforclassificationofanewimage.Following[35,27],weevaluate14settingscontrolledbyfourfactorsregardingthecontentofeachtestcase:(1)taskinduction:whetherincludingapreambletospecifytheoutputspace;(2)numberofways:thenumberofcategories;(3)numberofinnershots:thenumberofuniqueexamplesforeachcategory;(4)numberofrepeats:thenumberoftimesthateachuniqueexampleisrepeated.
5-way 1-shot
SPAEPaLM(ours)2:5PaLM232.284.088.588.485.183.682.477.74SPAEPaLM(ours)3:21PaLM227.984.892.592.684.885.285.479.03
5-way 3-shot
2-way 5-shot
TaskInduction✓✓✓✓✓✓AvgMethod#LayersInnerShots1135111:#TokensRepeats0000135
2-way 3-shot
AverageFigure3.Few-shotclassificationaccuracyonmini-ImageNetusingdifferentSPAEPaLMlayers.WecompareSPAEwiththestate-of-the-artmethodsFrozen[35]andLQAE[27].AsshowninTab.1,SPAEGPTconsistentlyoutperformsLQAE,bothusingthesameGPT3.5modelandin-contextformat,whileusingonly2%ofitstokens.Fig.3showstheperformancetrendwhenusingdifferentnumberofSPAEPaLMlay-ersacrosssixsettingswithtaskinductionand0repeats.SPAEPaLMwith3layersachievesthebestperformancewhichbalancesbetweensuf-ficientsemanticsandanimagesequencelengththatisoptimalforLLMin-contextlearning.Overall,SPAEPaLMyields+25%and+32%averageaccuracyimprovementoverthestate-of-the-artonthe2-wayand5-waybenchmarksinTab.1.Reconstructionquality.WecomparetheimageandvideoreconstructionqualityusingthetokensproducedbySPAEandtheVQGANbaselineusedinstate-of-the-artimage[7,25,6]andvideogeneration[45].WeuseFID[16],InceptionScore(IS)[33],andLPIPS[48]tocomparewiththeimageVQGANfromMaskGIT[7]ontheImageNetvalidationset,andFVD[36]tocomparethe3D-VQGANfromMAGVIT[45]ontheKinetics-600validationset.TheresultsarepresentedinTab.2.WhileSPAEmayhavemorelossyreconstructioncomparedtoVQGANwhenusingasimilarnumberoftokens,thisiscompensatedbygoingintodeeperlayers.AtthebottomofTab.2,weshowcasethescalabilityofourmodelbytrainingontheImageNet-21kdatasetwith13MimagesandlistthecomparablevariantfromLDM[32]asareference.Tokenpyramidvisualization.WevisualizethetokensproducedbySPAEinFig.4,whereweshowtherawpyramidorhistogramoftokenswithtopfrequenciesforthefirstfourlayers,withreconstructedimagesfromlayer5and6.Wehavethefollowingfindings.6


Figure4.Examplesofpyramidimagetokenizationandreconstructionbya6-layerSPAE.Weshowtherawpyramidorhistogramofmostfrequenttokensforthefirstfourlayers,andreconstructedimagesfromlayer5and6.Inthepyramid,weusedarkercellstoshowtokenswithhigherCLIPsimilaritytotheoriginalimage.Fornon-Englishsub-wordtokens,weshowautomatictranslationforreferenceinitalicfontsbelowtheoriginaltoken.CircledtokensarementionedinSection4.2.SeefullpyramidvisualizationsintheAppendix.Table2.ComparisonofreconstructionqualitybetweenSPAEandVQGANbaselinesusedinstate-of-the-artimage[7,25,6]andvideo[45]generationmodels.
Visualquestionanswering.Tab.3providesquantitativeresultsonthevisualquestionanswering(VQA)task.WecomparewiththebaselineFrozen[35]methodontheReal-Fast-VQA[35]benchmarkforfew-shotlearning.Asshown,SPAEconsistentlyoutperformsFrozen.UnlikeFrozen,SPAEtrainingdoesnotrequirebackpropagationthroughtheLLM.4.3QualitativeStudiesThissectionexploresthecapabilityofafrozenPaLM2,trainedsolelyonlanguagetokens,inperformingmultimodaltasksusingin-contextlearning.Weadoptatwo-stagedecodingprocessforimagegeneration.Instageone,weuseARdecodingtoproducethefirst5SPAElayerswithtask-specificconditions.Stagetwoisatask-agnosticNARdecodingprocessforlayer6conditionedonthefirst5layers.ImagetotextandVQA.Weexaminetwotasksinvolvingvisual-textreasoning(1)imagecaption-ingonCOCO[26]captions;and(2)visualquestionanswering(VQA)onCOCO-QA[31].Forboth7
SPAE(ours)5:3419.49109.460.1752.286:5974.41133.030.126.35
InnerShots135
128×128VQGAN1:2565.48119.690.136.79
Original
ResolutionMethodImageVideo#Layers(ImageNetILSVRC2012[10])(Kinetics-600[5]):#TokensFID↓IS↑LPIPS↓FVD↓
OriginalLayer 1-4Layer 6Layer 5Layer 6Layer 5Layers 1-4OriginalLayers 1-4Layer 6Layer 5
First,theSPAEtokensareorganizedinapyramidstructure,witheverylayercomprisingsemanticallyrelatedtokenstotheimage.Thefewtokensinthetoplayersseemtocapturetheprimarythemeoftheimage.Forinstance,inFig.4,thetokenpresso(highlightedinorange)representstheespressomachineandothertokenslikeblenderrefertorelatedregions.Layer3andLayer4revealadditionaldetailsaboutlocalizedobjects.Forexample,thetokenThermoreferstothethermometerinthetop-leftregion,whilestoveappearsinthebottom-rightarea.Inadditiontonouns,relatedverbsalsoshowup,includingpouring,refill,spill,andbrew.Second,itisworthnotingthattheCLIPmodelhasanEnglish-onlyvocabulary.However,thankstothemultilingualvocabulariesandembeddingsfromtheLLM,SPAE’ssemanticguidanceisabletomaptosimilarconceptsinotherlanguages,suchaskoffieinDutchandkaffeinDanishascorrespondingtermstotheconceptofcoffee.Third,similartoRQtokens[22],SPAEtokenscanreconstructtheimagewithprogressivelyrefineddetailswhenmorelayers,andthustokens,areutilized.Fig.4showsLayer5beginstoproduceareasonablereconstructionwhileLayer6furtherenhancesthelevelofdetailandsmoothness.Table3.Few-shotVQAperformanceonReal-Fast-VQA.
Frozen[35]7.810.110.5SPAEPaLM(ours)14.315.915.1
VQGAN(LDM[32],OpenImages)1:2565.15144.55--SPAE(ours,ImageNet-21k)6:5973.08173.790.19-
256×256VQGAN1:2564.04163.950.21-SPAE(ours)6:5973.60168.500.19-


an image of {}Genera)onan image of 1+7
Baseline: A man and a woman are sitting on a bench in a park.SPAE L1: A man is holding a baby in his arms.SPAE L2: A group of people are standing in a line.SPAE L3: A group of people in costumes at a Halloween party.SPAE L4: A group of people are dressed up in costumes for Halloween.SPAE L5: a group of people dressed in costumes at a partySPAE L6: a table with a bowl of fruit and a vase of flowers
Baseline: A man is standing on a rock in the middle of a river.SPAE L1: A man is standing on a rock in the middle of a river.SPAE L2: A man is wearing a coat and a hat.SPAE L3: A man is holding a small dog.SPAE L4: A teddy bear is sitting on a bed.SPAE L5: A teddy bear is sitting on a bed.SPAE L6: A teddy bear is sitting on a bed.
Q: what is the young boy riding in the empty parking lotA: Baseline: bikeSPAE: skateboardQ: how many different wines are lined up in glasses on an outdoor tableA: SPAE: 5Q: what bear walking through tall grass A: Baseline: siberianSPAE: grizzlyQ: how many computer screens are displayed with one imageA: SPAE: 3Figure5.Qualitativesamplesofimage-to-textgeneration:imagecaptioningandVQA.WecomparebetweendifferentlayersofSPAE(L1-L6)andabaselinemodelwithoutsemanticguidanceorpyramidSAQ.
Baseline: A man in a suit standing in front of a white wall.SPAE L1: A man in a red jacket and black pants standing on a snowy mountain.SPAE L2: A man in a red jacket skiing down a snowy mountain.SPAE L3: A man skiing down a snowy mountain.SPAE L4: A person skiing down a snowy mountain.SPAE L5: A person skiing down a mountain.SPAE L6: A person skiing down a mountain.Baseline: A group of people are standing in a field.SPAE L1: A group of people are standing in a room.SPAE L2: A kitchen with a stove, sink, and refrigerator.SPAE L3: A kitchen with a stove, sink, and refrigerator.SPAE L4: A kitchen with a stove, sink, and refrigerator.SPAE L5: A kitchen with a stove, sink, and cabinets.SPAE L6: A kitchen with a sink, stove, and refrigerator.
an image of the number of continents in the world
an image of the square root of 4
Context
an image of the last digit of 5*7
SPAE: A pizza with pepperoni and cheese on a white plate.SPAE: A man in a suit and tie standing next to a woman in a wedding dress.
Query
❄ LLMFigure6.Examplesoftext-to-imagegenerationonMNISTusingSPAEwithafrozenPaLM2model.WeuseSPAEtotokenize50handwrittenimagesasthecontextandaskPaLM2,anLLMtrainedsolelyontexttokens,toanswercomplexqueriesthatrequiregeneratingdigitimagesthroughSPAEastheoutput.tasks,weprovide10uniquetrainingexamplesasprompts.InthecaseofVQA,10differentanswersarepresentedtoforma10-way1-shotsetup.WecompareSPAEtoabaselinemodeltrainedwiththesamefrozenlanguagecodebookbutwithouttheproposedsemanticguidanceorpyramidSAQ.AsshowninFig.5,whenfedwithbaselinetokens,theLLMrandomlyhallucinatesacaptionorguessesananswersimplybasedonthequestion.SimilarhallucinationcanhappenifweonlyusethefirsttwolayersofSPAEorfivewordstorepresentanimage,asitprovidesinsufficientcontextforcaptioning.Reasonablecaptionsstarttoappearwith4layersor85words,whilecomplexscenesmaystillneedthefull6layersof597words.LLMgeneratingMNISTimages.Fig.6showsafewimagegenerationexamplesonMNIST[11].ThefrozenLLMlearnsabouthandwrittendigitimagesthrough50contextsamplestokenizedbySPAEtrainedonMNIST.Eachsampleconsistsofapreamble"animageofk"andthelexicaltokensrepresentinganimageofdigitk.ThenwecanasktheLLMtoanswerquestionswithdigitimages.Specifically,withaqueryof"animageof1+7",wecanuseprogressiveARdecodingwiththeLLMtoproduceatokensequencethatcanbedecodedintoanimageof8bySPAE.Wetestwithcomplexquestionsrequiringmathematicalreasoningorcommonsenseknowledge,andtheLLMisabletorespondcorrectly.Inaddition,thegenerateddigitimagesappeardifferentfromallcontextsamples.Thisdemonstratesthecross-modalreasoningcapabilityenabledbySPAEandafrozenLLM,withimagesgeneratedoverthetext-onlyinterface.Conditionalimagedenoising.Tothebestofourknowledge,therehavebeennosuccessfulattemptsthatdemonstrategenericimagegenerationcapabilityusingafrozenLLM.Tothisend,wedefineasimplerdenoisingsetuptoexplorethecapabilityofLLMs.Fig.7demonstratestheconditionalimagedenoisingtasks,e.g.,imageoutpainting,deblur,inpainting,locationtranslation,rotation,etc.Notethat,inordertogenerateimagesforeachtask,weutilize10pairsofnoisyexampleswithcorruptionratesrangingfrom50%to20%,asdiscussedinSection3.2.Thefullcontext,whichisomittedinFig.7,canbefoundintheAppendix.8
SPAE: A train is stopped at a station.


Figure7.Examplesofconditionalimagedenoising.Wecomparedifferentdecodingstridesforbothstages.Yellowandblueboxesindicatetheselectedresults.TheLLMisprovidedwithtenpairsofnoisyexampleslikeFig.2,whicharedeferredtotheAppendix.
VQGAN
Input
Stage 2: PNARTask-agnos1c=>Layer 6
Figure9.Comparisononconditionalimagedenoisingwithdifferenttokeniz-ers.AllmodelsusethesamedecodingsetupwiththesametenpairsofpromptimagesavailableintheAppendix.ThetoprowsofFig.7comparethegenerationfromdiffer-entdecodingstrideswiththesamesetofcontextexamples.Single-stepdecodingwithinfinitystridefailstoproduceareasonableimage,whichvalidatestheproposedprogressivegenerationapproach.InFig.9,wequalitativelycompareSPAEwithbaselinemeth-odsVQGANandLQAEusingthesamein-contextdenoisingprocedure.Asshown,VQGANfailstoproducereasonableimages,inpartbecausemanywordsintheLLMoutputareoutofitsvocabulary.LQAEonlyproducesvagueobjectcon-toursbutcannotrecoveranyvisualdetails.Onthecontrary,SPAEcangeneratereasonableimages.Conditionalvideodenoisingandothertasks.Duetospaceconstraints,weshowtheexamplesintheAppendix.4.4AblationStudiesTheresultsinTab.4andFig.8verifytheeffectivenessoftheproposeddesignsinSPAE,asevaluatedbyreconstructionquality(FID,IS,LPIPS)andsemanticrelevance(CLIPscore,few-shotclassificationaccuracy).Wehavethefollowingfindings.First,simplyusingafrozencodebooknegativelyaffectsthereconstructionresults,butwithsemanticguidanceitperformscomparablywiththeoriginalVQGANwhileproducingmeaningfullexicalwords.Second,RQhurtsreconstructionqualitywithafrozencodebook.ThisisdifferentfromRQ’sstandardsetup[22]wherethecodebookislearned.Third,SAQimprovesbothqualityandsemanticsimilarity,wherethepyramidenablesrepresentationwithmuchfewertokens.Thisallowsforaccommodatingmoreexampleswithinthefixedandconstrainedin-contextlength.Finally,per-layersemanticthresholdsbenefitunderstandingandthedynamicsemanticlossweighthelpsreconstruction.Theperceptuallossleveragesatrainednetworkwithaccesstoclassificationlabels,butremovingitresultsinasurprisingimprovementinclassificationaccuracywhilegreatlyhurtingthereconstruction.9
SPAELQAE
OriginalBaseline VQ+ frozen codebook+ frozen codebook+ seman7c guidance+ frozen codebook+ semantic guidance+ 2-layer RQ+ frozen codebook+ semantic guidance+ 2-layer SAQ+ frozen codebook+ semantic guidance+ 6-layer pyramid SAQFigure8.AblationexampleswithreconstructedimageandsemantictokensformodelslistedinTab.4.Fornon-pyramidtokens,weshowa4×4cropfromthefirstlayercorrespondingtotheregionindicatedbytheblackbox.Forpyramidtokens,weusethethirdlayerwhichconsistsof4×4tokens.
Stride∞2561286432168421Stage 1: PAROutpain1ng=>Layer 1-5


5ConclusionOurworkunveilstheuntappedpotentialoffrozenLargeLanguageModels(LLMs)intacklingmultimodalunderstandingandgenerationtasksinvolvingimagesandvideos,withoutrequiringexplicittrainingonthesemodalities.Thisisachievedbyanewmethod,SPAE,whichconvertsbetweenvisualcontentandlexicaltokensofvariablelength,imbuedwithrichsemanticmeaning.OurfindingsshowthegreatpotentialofharnessingthevastknowledgeandreasoningcapabilitiesofLLMsinthefieldofcomputervision,transcendingthelimitationsoflanguage-onlytasks.Limitations.Moretokensarerequiredtoachievethesamelevelofreconstructionwhenusingthefrozenlanguagecodebook,comparedtotheexistingVQGANmodelswithlearnedcodebooks.Thecapabilityofin-contextlearningissignificantlyconstrainedbytheacceptablesequencelength.Althoughourresultssuggesttheplausibilityofimagegeneration,theresolution,quality,anddiversityisfarfromtherecenttext-to-imagemodelstrainedonlargeimageandtextdata.Broaderimpact.OurpapershowcasestheuntappedpotentialoffrozenLLMsinmultimodalunderstandingandgenerationtasksinvolvingimagesandvideos,withoutrequiringexplicittrainingonthesemodalities.Asaninitialresearchproof-of-concept,wefocusonin-contextlearning,whichhaslimitationsinlearningcontextandconstrainedcapabilities.Consequently,thereisstillasubstantialgaptotherecentspecializedmodelsfortext-to-image(e.g.,StableDiffusion)orimage-to-textthathavebeenspecificallytrainedusingbillionsoftext-imagepairs.Thepotentialimpactofourresearchliesinitsinfluenceonfuturestudies,specificallyintheareaofintegratingvisionmodalitiesintotheLLMs.Forinstance,ourworkcanbeextendedtoexplorefinetuningoradaptertuningofLLMsonlarge-scaletext-imagedatasets.Futureresearchinthesedirectionsmayimplicateethicalissuesaroundfairnessandtransparency.Wehavefoundthatthegeneratedtokensoccasionallyincludeslangtermsorwordsthatcreateinappropriateconnotationsrelatedtothesubjectdepictedintheimageorvideo.Suchconcernsmustbethoroughlyconsideredandeffectivelyaddressedpriortodeployingthismethodinreal-worldapplications.Acknowledgmentsanddisclosureoffunding.Theauthorswouldliketothankanonymousreviewersandareachairstheirinsightfulcomments,andtoSiamakShakeri,SergeyIoffe,JayYagnik,andBoqingGongfortheirvaluablefeedbackandconstructivediscussions.ThisprojectisfundedinpartbyCarnegieMellonUniversity’sMobility21NationalUniversityTransportationCenter,whichissponsoredbytheUSDepartmentofTransportation.10
+2-layerSAQ1:25612.3093.330.210.161356.62:5125.08125.270.140.1595-
Table4.Ablationstudiesoncodebook,trainingobjective,quantizationmethod,andtokenstructure.Classifica-tionaccuracyisevaluatedunderthemini-ImageNet5-way1-shotsetup.
BaselineVQ1:2565.48119.690.13n/a19.6+frozencodebook1:2567.44101.390.170.146419.5+semanticloss1:2565.17124.410.130.151846.2
noper-layerthresholds6:5974.33122.250.110.165059.4(layer3)nodynamicsemanticweight6:5979.0085.140.190.184765.1(layer3)noperceptualloss6:59740.4733.410.200.199469.5(layer3)
+6-layerpyramidSAQ(SPAE)1:1---0.187952.02:5---0.186864.23:21---0.181565.14:85---0.171158.55:3419.49109.460.170.160446.36:5974.41133.030.120.1577-
+2-layerRQ[22]1:25611.9489.010.220.159556.22:5126.05113.930.150.1547-
Method#LayersFID↓IS↑LPIPS↓CLIP↑Classification:#TokensAccuracy↑


References[1]Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal.Flamingo:avisuallanguagemodelforfew-shotlearning.InNeurIPS,2022.3[2]RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal.PaLM2technicalreport.arXiv:2305.10403,2023.1,2,3,5[3]ZalánBorsos,RaphaëlMarinier,DamienVincent,EugeneKharitonov,OlivierPietquin,MattSharifi,DominikRoblek,OlivierTeboul,DavidGrangier,MarcoTagliasacchi,etal.AudioLM:alanguagemodelingapproachtoaudiogeneration.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,2023.2[4]TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.InNeurIPS,2020.1,2,3,5[5]JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,andAndrewZisserman.AshortnoteaboutKinetics-600.arXiv:1808.01340,2018.5,7[6]HuiwenChang,HanZhang,JarredBarber,AJMaschinot,JoseLezama,LuJiang,Ming-HsuanYang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal.Muse:Text-to-imagegenerationviamaskedgenerativetransformers.InICML,2023.6,7[7]HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.MaskGIT:Maskedgenerativeimagetransformer.InCVPR,2022.2,6,7[8]AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.PaLM:Scalinglanguagemodelingwithpathways.arXiv:2204.02311,2022.1,3[9]AlexandreDéfossez,JadeCopet,GabrielSynnaeve,andYossiAdi.Highfidelityneuralaudiocompression.arXiv:2210.13438,2022.2[10]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scalehierarchicalimagedatabase.InCVPR,2009.6,7[11]LiDeng.Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb].IEEESignalProcessingMagazine,29(6):141–142,2012.5,8[12]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.BERT:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.InNAACL,2019.3[13]AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth16x16words:Transformersforimagerecognitionatscale.InICLR,2020.5[14]PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.InCVPR,2021.2[15]SongweiGe,ThomasHayes,HarryYang,XiYin,GuanPang,DavidJacobs,Jia-BinHuang,andDeviParikh.Longvideogenerationwithtime-agnosticVQGANandtime-sensitivetransformer.InECCV,2022.2,4[16]MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter.Ganstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium.InNeurIPS,2017.6[17]NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly.Parameter-efficienttransferlearningfornlp.InICLR,2019.5[18]EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.Lora:Low-rankadaptationoflargelanguagemodels.InICLR,2021.5[19]JustinJohnson,AlexandreAlahi,andLiFei-Fei.Perceptuallossesforreal-timestyletransferandsuper-resolution.InECCV,2016.4[20]DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXiv:1412.6980,2014.6[21]JingYuKoh,RuslanSalakhutdinov,andDanielFried.Groundinglanguagemodelstoimagesformulti-modalgeneration.InICML,2023.1,2,3[22]DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan.Autoregressiveimagegenerationusingresidualquantization.InCVPR,2022.3,4,7,9,1011


[23]BrianLester,RamiAl-Rfou,andNoahConstant.Thepowerofscaleforparameter-efficientprompttuning.InEMNLP,2021.5[24]AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,VinayRamasesh,AmbroseSlone,CemAnil,ImanolSchlag,TheoGutman-Solo,etal.Solvingquantitativereasoningproblemswithlanguagemodels.InNeurIPS,2022.3[25]JoseLezama,TimSalimans,LuJiang,HuiwenChang,JonathanHo,andIrfanEssa.Discretepredictor-correctordiffusionmodelsforimagesynthesis.InICLR,2023.6,7[26]Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,andCLawrenceZitnick.Microsoftcoco:Commonobjectsincontext.InECCV,2014.7[27]HaoLiu,WilsonYan,andPieterAbbeel.Languagequantizedautoencoders:Towardsunsupervisedtext-imagealignment.arXiv:2302.00902,2023.1,2,3,5,6[28]OpenAI.GPT-4technicalreport.arXiv:2303.08774,2023.1,2[29]AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.InICML,2021.4[30]AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.Zero-shottext-to-imagegeneration.InICML,2021.2[31]MengyeRen,RyanKiros,andRichardZemel.Exploringmodelsanddataforimagequestionanswering.InNeurIPS,2015.7[32]RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.High-resolutionimagesynthesiswithlatentdiffusionmodels.InCVPR,2022.2,6,7[33]TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improvedtechniquesfortraininggans.InNeurIPS,2016.6[34]Hung-YuTseng,LuJiang,CeLiu,Ming-HsuanYang,andWeilongYang.Regularizinggenerativeadversarialnetworksunderlimiteddata.InCVPR,2021.4[35]MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill.Multimodalfew-shotlearningwithfrozenlanguagemodels.InNeurIPS,2021.6,7[36]ThomasUnterthiner,SjoerdvanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,andSylvainGelly.Towardsaccurategenerativemodelsofvideo:Anewmetric&challenges.arXiv:1812.01717,2018.6[37]AaronVanDenOord,OriolVinyals,etal.Neuraldiscreterepresentationlearning.InNeurIPS,2017.2,3[38]AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.InNeurIPS,2017.1[39]RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,Moham-madTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan.Phenaki:Variablelengthvideogenerationfromopendomaintextualdescription.arXiv:2210.02399,2022.2[40]OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal.Matchingnetworksforoneshotlearning.InNeurIPS,2016.6[41]AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.SuperGLUE:Astickierbenchmarkforgeneral-purposelanguageunderstandingsystems.InNeurIPS,2019.3[42]JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,etal.Emergentabilitiesoflargelanguagemodels.TMLR,2022.1[43]ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang,ZechengTang,andNanDuan.VisualChatGPT:Talking,drawingandeditingwithvisualfoundationmodels.arXiv:2303.04671,2023.2[44]PengchengYin,Wen-DingLi,KefanXiao,AbhishekRao,YemingWen,KensenShi,JoshuaHowland,PaigeBailey,MicheleCatasta,HenrykMichalewski,etal.Naturallanguagetocodegenerationininteractivedatasciencenotebooks.arXiv:2212.09248,2022.3[45]LijunYu,YongCheng,KihyukSohn,JoséLezama,HanZhang,HuiwenChang,AlexanderGHauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal.MAGVIT:Maskedgenerativevideotransformer.InCVPR,2023.2,4,6,7[46]LijunYu,JoséLezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,YongCheng,AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal.Languagemodelbeatsdiffusion–tokenizeriskeytovisualgeneration.arXiv:2310.05737,2023.212


[47]NeilZeghidour,AlejandroLuebs,AhmedOmran,JanSkoglund,andMarcoTagliasacchi.Soundstream:Anend-to-endneuralaudiocodec.IEEE/ACMTrans.onAudio,Speech,andLanguageProcessing,30:495–507,2021.3,4[48]RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InCVPR,2018.6[49]SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.OPT:Openpre-trainedtransformerlanguagemodels.arXiv:2205.01068,2022.213


Figure11.ComparisonbetweenRQandSAQ.Weshowa2-layerquantizationprocessina2-dimensionalspaceasanexample.Atlayerl,weuseblueforthecurrentremainderembeddingszl,greenforcurrentpost-quantizationembeddingse(kl),andorangeforthere-constructedembeddingsuptolayerlasˆz≤l.Streamingaveragequantization.Fig.11comparesourproposedStreamingAverageQuantization(SAQ)withResidualQuantization(RQ)[7,11].Atlayer2,theSAQremainderembeddingz2=2z−e(k1)isatamoresim-ilarscaletoz,comparedtotheRQremainderz−e(k1).Wefindthatthescaleconsistencypro-motesbetterutilizationofthefrozenlanguagecodebookdespitealargenumberoflayersbeingused.Duetothepyramidstructure,quantizationinthefirstfewlayersmaybeskippedforthosepositionsnotselectedbythedilationsubsam-pler.Consideringthescaleconsistencyacrossquantizationlayers,theuseofSAQismoreap-propriateinthiscase.In-contextdenoising.Taketheimage-to-imagetaskinFig.2asanexample.Theprovidedcontextareimagesrandomlycorruptedinthetokenspacebyϵ(·;r),wherethecorruptionratiorfollowsacosineschedule[2].(ui,vi)∼(cid:16)ϵ(cid:0)T(mask(I));ri(cid:1),ϵ(cid:0)T(I);ri(cid:1)(cid:17),I∈M(12)whereT(·)representstheSPAEtokenizerandMisasmallsetofrawimages.mask(·)zerosoutpixelsoftherealimagetocreatetheconditionimage,suchasmaskingoutthebottomhalffor37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).arXiv:2306.17842v3 [cs.CV] 28 Oct 2023
Figure10.Dilationsubsamplervisualization.WepresentadditionaldetailsabouttheSPAEmodelinthissection.Tokenpyramid.Fig.10showsanexampleofthedilationsubsamplerdefinedbyEq.(1).Weselectevenlydistributedpositionsineachlayertoformthetokenpyramidwithmonotonicallyincreasinglayersizes.
𝑙: ℎ!×𝑤!1: 1×12: 2×23: 4×44: 8×8
𝑧̂=𝒆
𝒛−𝒆
𝑘"
𝑘"
𝑘"
𝒆
𝒆
𝒆
AppendixOverviewThissupplementarydocumentprovidesadditionaldetailstosupportourmainmanuscript,organizedasfollows:•AppendixApresentsmoredetailsonthemethod,includingSPAEarchitecturedesigns.•AppendixBprovidesadditionalimplementationdetails,includingavideoSPAEvariant.•AppendixCincludesmorequantitativeevaluationresults.•AppendixDshowsmorequalitativeexamplesofmodelgenerations.AMethodDetails
𝑘"SAQ[skipped]𝒛𝒛Layer 1Layer 2
2𝒛−𝒆
𝒛𝒆
𝒛𝒆
𝑘!+𝒆
𝑘!+𝒆(𝑘")𝒆
SPAE:SemanticPyramidAutoEncoderforMultimodalGenerationwithFrozenLLMsSupplementaryMaterials
RQSAQ𝑧̂=12
𝑘!
𝑘!
𝑘!
𝑘!


out-painting.ThequeryˆuisalwayssampledfromMwithoutnoiseϵ.Toensurethegenerationisnotsimplycopyingthecontext,weenforceaminimalcorruptionrateof20%suchthatnoidenticalimagefromthecontextmatchestherealtargetimage.BImplementationDetailsB.1SPAETrainingImageSPAE.AnimageSPAEencodesa128×128imageinto16×16embeddings.FollowingtheVQGAN[5]architecture,weuse128basefilterswithchannelmultipliers[1,2,2,4]and2residualblocksateachscale,whichresultsin59Mparametersintotal.ImageSPAE-8.InadditiontotheprimarySPAEmodelwithsixpyramidlayersstudiedinthemainpaper,wealsotrainanSPAE-8modelwitheightlayerstoconductamorein-depthanalysisofthecoarse-to-finereconstructionprocess.Thetwoextralayerseachcontain16×16tokens.Thesemanticlossisstillappliedonthefirst5layersasintheprimarymodel.MNISTSPAE.WetrainanotherSPAEontheMNIST[4]datasetwiththesamearchitecturesetup.Wepadthehandwrittendigitimagesfrom28×28to32×32pixels,whicharethenencodedinto4×4embeddings.Eachimageisrepresentedby37tokensorganizedinfourlayers,withsizesof1×1,2×2,4×4,and4×4.WereplacetheCLIPimageembeddingwiththeCLIPtextembeddingofthelabelforthesemanticloss.Themodelistrainedfor10kstepswithabatchsizeof256.Forin-contextgeneration,ARdecodingwithastrideof4isusedtoproduceall37tokens.VideoSPAE.WeinitializeavideoSPAEbyVQGANinflation[10]fromapretrainedimageSPAE,whichencodes16framesat128×128resolutioninto4×16×16embeddings.AvideoSPAEconsistsof176Mparameters.Thepyramidlayerscontain1×1×1,1×2×2,1×4×4,2×8×8,4×16×16,and4×16×16tokens.ThevideoembeddingisobtainedastheaverageCLIPembeddingforallframes.ThemodelistrainedontheKinetics-600[1]datasetwhichcontains384kvideos.Wetrainwithabatchsizeof512for130ksteps,whichtakes5.8kTPUv4-hours.B.2LLMPromptingTogenerateprompts,weutilizeSPAEtoquantizeanimage,oranothernon-linguisticmodality,intoapyramidoflexicaltokens.Subsequently,weflattenthetokensbyconcatenatingthemlayer-by-layer,followingarasterscan,andresultingina1-Dstring.Thisstring,representingtheimage,isreferredtoastheSPAEstringinthefollowingprompts.Weusetask-specificprompttemplatestofacilitateanswergenerationwithLLMs.TheLLMoutputisalwaysparsedbyremovingleadingandtrailingwhitespaceornewlinecharacters.ImageclassificationwithGPT3.5.WeusethesameprompttemplateasLQAE[8]tointeractwithGPT3.5.Fora2-way1-shotclassificationbetweenclasslionandvase,thepromptisForeachofthefollowinginputoutputpairs,outputisoneof[‘lion’,‘vase’]###Input:<SPAEstringfromalionimage>Output:lion###Input:<SPAEstringfromavaseimage>Output:vase###Input:<SPAEstringfromthequeryimage>Output:Weusegreedydecodingtogetamaximumof7tokensfromGPT3.5.ImageclassificationwithPaLM2.WeusetheoriginalminiImageNet[9]formatwithPaLM2.Thepromptlookslike15


Answerwith"lion"or"vase".<SPAEstringfromalionimage>Thisisalion<SPAEstringfromavaseimage>Thisisavase<SPAEstringfromthequeryimage>Whatisthis?#Onlyusedin5-way3/5-shotsetupsThisisaWeusegreedydecodingtogetamaximumof4tokensfromPaLM2.Imagecaptioning.Weusegreedydecodingtogetamaximumof20tokensbeforethefirstnewlinecharacterwiththefollowingprompt:Generateacaptionsentencebasedonwordsdescribinganimage.Q:<SPAEstringfromimage1>A:<Captionforimage1>Q:<SPAEstringfromimage2>A:<Captionforimage2>Q:<SPAEstringfromthequeryimage>A:Visualquestionanswering.Weusegreedydecodingtogetamaximumof4tokensbeforethefirstnewlinecharacterwiththeprompttemplateasAnswerwithasingleword.C:<SPAEstringfromimage1>Q:<Questionforimage1>A:<Answerforimage1>C:<SPAEstringfromimage2>Q:<Questionforimage2>A:<Answerforimage2>C:<SPAEstringfromthequeryimage>Q:<Questionforthequeryimage>A:Image/videogenerationwithPARdecoding.Forimageorvideogenerationtasks,theconditioncanbeatextstringoranSPAEstringofaconditionimage.SupposeweusePARdecodingwithastrideof4tokens.Atthe4thstep,thepromptlookslikeLearnanewlanguageandpredictthe4tokensfollowingtheexamples.C:<conditionforimage1>Q:<SPAEstring(token1-12)forimage1>A:<SPAEstring(token13-16)forimage1>C:<conditionforimage2>Q:<SPAEstring(token1-12)forimage2>A:<SPAEstring(token13-16)forimage2>16


C:<conditionforthequery>Q:<SPAEstring(token1-12)forthegeneratedimagefromprevioussteps>A:WeusePaLM2togenerate8predictedsequencesforthenext4tokens,startingwithatemperatureT0=0.Weusethesentencepiece[6]tokenizertotokenizetheoutputstring.Ifallpredictionsareshorterthan4tokens,weretrytheLLMpredictionwithahighertemperature.Atthei-thretry,thetemperatureisgivenbyTi=ψiXj=12j(13)whereψ=0.01isused.Image/videogenerationwithPNARdecoding.WeusePNARdecodingtogenerateSPAElayer6conditionedonlayer1-5.Withastrideof16,thepromptatthe3rdsteplookslikePredicttheoutputsfollowingtheexamples.Q:<SPAEstringfromlayer1-5forimage1>A:<SPAEstringfromlayer6(token33-48)forimage1>Q:<SPAEstringfromlayer1-5forimage2>A:<SPAEstringfromlayer6(token33-48)forimage2>Q:<SPAEstringfromlayer1-5forthegeneratedimagefromARdecoding>A:WeusePaLM2togenerate8predictedsequencesforthenext16tokens.Ifthesentencepieceparsingfails,weretrywiththesametemperaturescheduleasinPARdecoding.B.3CorruptionFunctionsPixel-spacetransformation.Weusepixel-spacetransformationintheconditionalimageinterpola-tiontaskswiththefollowingsetups:•Brightness:[±0.8,±0.6,±0.4,±0.2].•Contrast:[±0.8,±0.6,±0.4,±0.2].•Saturation:[±0.4,±0.3,±0.2,±0.1].•Color(RGB):[(0.6,1.4,1),(0.7,1.3,1),(0.8,1.2,1),(0.9,1.1,1),(1.1,0.9,1),(1.2,0.8,1),(1.3,0.7,1),(1.4,0.6,1)]Overflowpixelsareclippedto[0,255].Token-spacepermutationnoise.Randompermutationisusedinthein-contextdenoisingsetupforconditionalimagedenoisingtasks.Specifically,wereplaceafractionoftokenseachwitharandomtokensampledfromtheentire65kvocabularytosatisfyagivencorruptionrate.Thecorruptionratesforthe10examplesare[0.5,0.47,0.44,0.41,0.38,0.35,0.32,0.29,0.26,0.23].Thepermutationnoisepresentsacontextdistributionwithexpectationattherealimage,butdoesnotcontainthegroundtruthtokenstopreventinformationleakage.CAdditionalQuantitativeResultsFew-shotimageclassificationwithdifferentSPAElayers.Tab.5presentthefew-shotmini-ImageNetclassificationperformancewitheachSPAEPaLMlayer.ThesedetailedquantitativenumbersaccompanythefindingsfromFig.3.Asshown,Layer3achievesthebestoverallperformanceaswellasinmostofthesetups,whichbalancesbetweenthelevelofdetailsandtheburdenoftheLLM.17


Model#LayersFID↓IS↑LPIPS↓CLIP↑Relative:#TokensCLIP↑
SPAEdisjointPaLM2:5PaLM224.879.884.583.780.878.578.472.93SPAEdisjointPaLM3:21PaLM221.481.489.287.982.681.780.674.98
Few-shotimageclassificationwithSPAEdisjoint.FollowingthepreviousworkofLQAE[8],wetrainourSPAEontheImageNettrainingsplit[3]andpresentthecomparativeresultsinthemainpaper.ThereisapossibilityofoverlapbetweenthetrainingsplitofImageNetandthemini-ImageNetdatasetusedinthefew-shotclassificationtask[9].Sincefewstudieshaveinvestigatedthisbefore,wepresenttheresultsoftrainingSPAEontheImageNettrainingsplitafterexcludingthe20classesusedinthefew-shotmini-ImageNetclassificationtask.ThiscreatesaevenmorechallengingsettingasthevisualclasseshaveneverbeenseenduringthetrainingofthetokenizerortheLLMs.AsdemonstratedinTab.5,wepresenttheresultsoftrainingourtokenizeronthedisjointeddata,referredtoasSPAEdisjoint.Asexpected,weobserveaslightdecreaseinperformance,sincebothSPAEandLLMsneedtogeneralizetothetestclassesthatareoutsidethetrainingdatadistribution.Despitethefactthatthebaselineistrainedonunlabeledimagessampledfromthemini-ImageNettestclasses,SPAEdisjointPaLMstilldemonstratesasignificantimprovementoverthestate-of-the-artbaselineonthe2-waybenchmarks.TokenqualitywithmoreSPAElayers.Tab.6showstheper-layerreconstructionqualityandsemanticrelevanceoftokensfromtheSPAE-8modelincomparisontothedefaultmodel.Withmoretokenlayers,themodelgainslargercapacityforbothsemanticandappearance,wheretheappearancegetspushedintodeeperlayers.Atlayer1to6,SPAE-8yieldsconsistentlyhigherCLIPscoresthanSPAE.Atthelastthreelayers,SPAE-8alsohasbetterreconstructionqualitythanthelasttwolayers18
Table5.Few-shotclassificationaccuracyonthemini-ImageNetbenchmarks.-meansvalueunavailableduetoaninfeasiblesequencelength.
5-WayClassificationSPAEPaLM1:1PaLM226.852.050.949.951.948.447.946.83SPAEPaLM2:5PaLM223.664.268.069.963.462.060.258.76SPAEPaLM3:21PaLM220.265.173.774.366.467.066.361.86SPAEPaLM4:85PaLM216.158.567.269.164.066.467.458.39SPAEPaLM5:341PaLM212.146.355.967.243.346.3--SPAEPaLM6:597PaLM212.135.7------
SPAE-81:1---0.20510.80182:5---0.20460.79943:21---0.20120.78344:85---0.18960.72895:34143.4249.780.320.17090.64126:5978.93116.120.180.16670.62137:8534.78135.010.130.16470.61198:11093.89140.550.110.16340.6058
Table6.ReconstructionqualityandsemanticrelevanceofSPAE-8tokens.
TaskInduction✓✓✓✓✓✓AvgMethod#LayersInnerShots1135111:#TokensRepeats0000135
2-WayClassificationSPAEPaLM1:1PaLM234.877.281.280.374.073.271.570.31SPAEPaLM2:5PaLM232.284.088.588.485.183.682.477.74SPAEPaLM3:21PaLM227.984.892.592.684.885.285.479.03SPAEPaLM4:85PaLM222.881.191.490.482.684.384.776.76SPAEPaLM5:341PaLM221.277.488.079.184.874.076.171.51SPAEPaLM6:597PaLM221.873.870.862.464.862.158.659.19


Figure12.TrainingcurvesofSPAEincomparisontoVQGAN.Metricsarepresentedregardingreconstructionquality(FID,IS,LPIPS)andsemanticrelevance(CLIP).ofSPAE.Theseresultssuggestthepotentialofbetterreconstructionqualityandsemanticrelevancefromusingmoretokenlayers.Trainingefficiency.AllmodelsusedintheablationstudyinTab.3,includingVQGAN[5]andRQ-VAE[7]variants,aretrainedusingthesamesetupforfaircomparisons.Fig.12comparesthetrainingcurvesofFID,IS,LPIPS,andCLIPscoreofSPAEandVQGAN.Asshown,within40%ofthetrainingsteps,SPAEshowsbetterFIDthanthefinalVQGANcheckpoint.TheCLIPscorekeepsimprovingasthetrainingproceeds,whiletheLPIPSsaturatesquiteearly.DAdditionalQualitativeExamplesTokenpyramidvisualization.Fig.13showstokenizationandreconstructionsamplesbya6-layerSPAEfromImageNetvalidationset.Keyconceptsarecapturedinthefirstfewlayers,whereasthelaterlayersfocusonthevisualappearance.Inthecoffeemachineexample,manykeywordsarepresenttodescribevariousaspectsfromthestovetothethermometer.Intheparrotcase,asingleunifiedconceptisrepeatedlyhighlighted.Coarse-to-finereconstruction.Fig.14showsreconstructionsamplesbySPAE-8fromImageNetvalidationset.Wecomparethereconstructedimagesfromlayer5tolayer8todemonstratethecoarse-to-fineprogress.Conditionalimageinterpolation.Tothebestofourknowledge,therehavebeennosuccessfulattemptsthatdemonstrategenericimagegenerationcapabilityusingafrozenLLM.Tothisend,wedefineaverysimplesetuptoexploretheinterpolationcapabilityofLLM,wheretheconditionsare19


integersfrom1to9.Thetargetimagesarecreatedwithdifferentpixel-spacetransformationsdetailedin.AsshowninFig.15,images1-4and6-9arefedascontexttoproduceimage5,wherethemodelinterpolatesthevariableproperty.Fig.16showsgeneratedsamplesat256×256resolutionunderthesamesetup.Conditionalimagedenoising.WeusePARdecodingtoproducethefirst5tokenlayerswithtask-specificconditions,followedbytask-agnosticPNARdecodingtofillinlayer6.Fig.17visualizestheinputpairsfortheimage-to-imagegenerationexamplesinFigs.7and9,withmoreexamplesinFig.18.Underthein-contextdenoisingsetup,theLLMgeneratesnovelimagesbasedontheprovidedcontext,wheremultipledifferentgenerationscanbeobtained.Multimodaloutputs.Fig.19showsataskrequiringasingleLLMtooutputbothimageandtext,whereitfirstinpaintsthecenterregionofanimageusingin-contextdenoisingandthencreatesmultiplecaptionsfortheoutputimage.Image-to-videodenoising.Fig.20showsanimage-to-videoexamplewiththeframepredictiontaskusingprogressivein-contextdenoising.TheinputisoneframetokenizedbytheimageSPAE,whiletheoutputisa16-framecliptokenizedbythevideoSPAE.Wefollowthesametwo-stageprocedureasimage-to-imagegeneration,withmorestepsineachstagetoaccountforthelongersequence.Duetothesequencelengthlimit,onlyfoursamplescanbefitintothecontext,whichlimitsLLM’sperformanceforthistask.20


(a)Manykeywordsarepresenttodescribevariousaspectsfromthestovetothethermometer.
Original
Original
(b)Asingleunifiedconceptisrepeatedlyhighlighted.Figure13.Examplesofmulti-layerimagetokenizationandreconstructionbya6-layerSPAE.Forvisualiza-tionpurposesonly,weusedarkercellstoshowtokenswithhigherCLIPscoresregardingtheoriginalimage.Fornon-Englishsub-wordtokens,weshowautomatictranslationforreferenceinitalicfontsbelowtheoriginaltoken.Weshowtokensinallsixlayers,alongwithreconstructedimagesfromthelasttwolayers.21


Color
1
52346789Condition:ContextContextGenerationFigure15.Examplesofconditionalimageinterpolationofdifferentimagetransformations.22
OriginalLayer 5Layer 6Layer 7Layer 8Figure14.Examplesofcoarse-to-fineimagereconstructionbySPAE-8.Thetop5layersreconstructanoisyimage.Theappearancedetailsgraduallygetrefinedasmoretokenlayersareaggregatedbythestreamingaveragequantizationprocess.
BrightnessContrastSaturation


…
…
Figure16.Examplesofconditionalimageinterpolationat256x256resolution.TheLLMisprovidedwitheightconditionimagesfortheinterpolationfollowingthesetupinFig.15.23
………………ContextContext
Genera&on


(a)Outpaintingthebottomhalf.Twogeneratedimagesareshown.
Stage 1: ARLayer 1-5Task-speciﬁc
Stage 1: ARLayer 1-5Task-speciﬁc
Stage 1: ARLayer 1-5Task-speciﬁc
Stage 1: ARLayer 1-5Task-speciﬁc
Stage 1: ARLayer 1-5Task-specific
Generation
Generation
Corruption: 50%20%(d)Spatialtranslationtotheright.
Corrup&on: 50%20%Outpainting
Genera)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondi)on
Stage 2: NARLayer 6Task-agnos<cContextCondition
Corruption: 50%20%(c)Inpaintingthecenterregion.
Corrup&on: 50%20%(e)Clockwiserotationby90degrees.Figure17.Examplesofconditionalimagedenoising.Allinputsamplesforthein-contextlearningarepresentedfortheexamplesinFigs.7and9.TheLLMgeneratesnovelimagesbasedontheprovidedcontext.Multipledifferentgenerationscanbeobtainedfromthesamesetofcontextsamples.24
Genera)ons
Genera,on
Corruption: 50%20%(b)Deblurfromagaussianfilter.
Stage 2: NARLayer 6Task-agnosticContextCondi)on


Stage 2: NARLayer 6Task-agnosticContextCondition
Stage 1: ARLayer 1-5Task-specific
Stage 1: ARLayer 1-5Task-specific
Stage 1: ARLayer 1-5Task-specific
Generation
Generation
Stage 2: NARLayer 6Task-agnos<cContextCondition
Stage 2: NARLayer 6Task-agnos<cContextCondition
Corruption: 50%20%(a)Inpaintingthecenterregion.
Corruption: 50%20%A dog with a long, curly coat of fur.A small dog with a big smile.A small dog with long hair looks up at the camera.
Genera,on
Corrup&on: 50%20%(b)Outpaintingthebottomhalf.Figure18.Moreexamplesofconditionalimagedenoising.TheLLMgeneratesnovelimagesbasedontheprovidedcontextimagepairs.
CaptionFigure19.ExamplesofmultimodaloutputsfromtheLLM.TheLLMgeneratesanovelimagewithmultiplecaptionsbasedontheprovidedcontext.25


Condi&on
Genera&onFigure20.Examplesofimage-to-videodenoising:frameprediction.Wefollowthesametwo-stagegenerationprocedureasinimage-to-imagetasks.Duetothesequencelengthlimit,onlyfoursamplescanbefitintothecontext.Thegeneratedvideoclipappearvisuallydifferentfromthecontextsamples,especiallyaroundthereflectionsofthebowl.26
Stage 2: NAR; Layer 6; Task-agnos=c
Corrup&on35%11%Stage 1: AR; Layer 1-5; Task-speciﬁc
Context


References[1]JoaoCarreira,EricNoland,AndrasBanki-Horvath,ChloeHillier,andAndrewZisserman.AshortnoteaboutKinetics-600.arXiv:1808.01340,2018.15[2]HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.MaskGIT:Maskedgenerativeimagetransformer.InCVPR,2022.14[3]JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scalehierarchicalimagedatabase.InCVPR,2009.18[4]LiDeng.Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb].IEEESignalProcessingMagazine,29(6):141–142,2012.15[5]PatrickEsser,RobinRombach,andBjornOmmer.Tamingtransformersforhigh-resolutionimagesynthesis.InCVPR,2021.15,19[6]TakuKudoandJohnRichardson.Sentencepiece:Asimpleandlanguageindependentsubwordtokenizeranddetokenizerforneuraltextprocessing.InEMNLP,2018.17[7]DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan.Autoregressiveimagegenerationusingresidualquantization.InCVPR,2022.14,19[8]HaoLiu,WilsonYan,andPieterAbbeel.Languagequantizedautoencoders:Towardsunsupervisedtext-imagealignment.arXiv:2302.00902,2023.15,18[9]OriolVinyals,CharlesBlundell,TimothyLillicrap,DaanWierstra,etal.Matchingnetworksforoneshotlearning.InNeurIPS,2016.15,18[10]LijunYu,YongCheng,KihyukSohn,JoséLezama,HanZhang,HuiwenChang,AlexanderGHauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal.MAGVIT:Maskedgenerativevideotransformer.InCVPR,2023.15[11]NeilZeghidour,AlejandroLuebs,AhmedOmran,JanSkoglund,andMarcoTagliasacchi.Soundstream:Anend-to-endneuralaudiocodec.IEEE/ACMTrans.onAudio,Speech,andLanguageProcessing,30:495–507,2021.1427