This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
End-to-End Speech Recognition: A Survey
Rohit Prabhavalkar, Member, IEEE, Takaaki Hori, Senior Member, IEEE, Tara N. Sainath, Fellow, IEEE, Ralf Schl¨uter, Senior Member, IEEE, and Shinji Watanabe, Fellow, IEEE
Abstract—In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain- specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.
Index Terms—end-to-end, automatic speech recognition.
I. INTRODUCTION The classical1 statistical architecture decomposes an auto- matic speech recognition (ASR) system into four main compo- nents: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes’ decision rule [1], [2], [3]. Classical acoustic modeling is based on hidden Markov models (HMMs) to account for speaking rate variation. Within the classical approach, deep learning has been introduced into acoustic and language mod- eling. In acoustic modeling, deep learning has replaced Gaus- sian mixture distributions (hybrid HMM [4], [5]) or augmented the acoustic feature set (e.g., non-linear discriminant/tandem approach [6], [7]). In language modeling, deep learning has re- placed count-based approaches [8], [9], [10]. However, in these early attempts at introducing deep learning, the classical ASR architecture was unmodified. Classical state-of-the-art ASR systems today are composed of many separate components and knowledge sources: especially speech signal preprocessing; methods for robustness with respect to recording conditions; phoneme inventories and pronunciation lexica; phonetic clus- tering; handling of out-of-vocabulary words; various methods for adaptation/normalization; elaborate training schedules with different objectives including sequence discriminative training, etc. The potential of deep learning, on the other hand, initiated successful approaches to integrate formerly separate modeling steps, e.g., by integrating speech signal pre-processing and feature extraction into acoustic modeling [11], [12].
More consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR archi- tectures based on hidden Markov models (HMM) with more integrated joint neural network model structures [13], [14], [15], [16]. These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing – akin to how statistical approaches to natural language processing have come to replace more linguistically oriented models. For these all-neural approaches recently the term end-to-end (E2E) [14], [17], [18], [19] has been established. Therefore, first of all an attempt to define the term end-to-end in the context of ASR is due in this survey. According to the Cambridge Dictionary, the adjective “end-to-end” is defined as: “includ- ing all the stages of a process” [20]. We therefore propose the following definition of end-to-end ASR: an integrated ASR model that enables joint training from scratch; avoids separately obtained knowledge sources; and, provides single- pass recognition consistent with the objective to optimize the i.e., usually label (word, task-specific evaluation measure, character, subword, etc.) error rate. While this definition suffices for the present discussion, we note that such an idealized definition hides many nuances involved in the term E2E and lacks distinctiveness; we elaborate on some of these nuances in Sec. II to discuss the various connotations of the term E2E in the context of ASR.
What are potential benefits of E2E approaches to ASR? The primary objective when developing an ASR systems is to minimize the expected word error rate; secondary objectives are to reduce time and memory complexity of the resulting decoder, and – assuming a constrained development budget – genericity, and ease of modeling. First of all, an integrated ASR system, defined in terms of a single neural network structure supports genericity of modeling and may allow for faster development cycles when building ASR systems for new languages or domains. Similarly, ASR models defined by a single neural network structure may become more ‘lean’ compared to classical modeling, with a simpler decoding process, obviating the need to integrate separate models. The resulting reduction in memory footprint and power consump- tion supports embedded ASR applications [21], [22]. Further- more, end-to-end joint training may help to avoid spurious optima from intermediate training stages. Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available. Also, secondary knowledge sources may themselves be erroneous; avoiding these may improve models trained directly from data, provided that sufficient amounts of task- specific training data are available.
1 The term “classical” here refers to the former, long-term, state-of-the-art ASR architecture based on the decomposition into acoustic and language model, and with acoustic modeling based on hidden Markov models.
With the current surge of interest in E2E ASR models and an
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
1


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research. The goal of this survey is to provide an in-depth overview of the current state of research on E2E ASR systems, covering all relevant aspects of E2E ASR, with a contrastive discussion of the different E2E and classical ASR architectures.
This survey of E2E speech recognition is structured as fol- lows. Sec. II discusses the nuances in the term E2E as it applies to ASR. Sec. III describes the historical evolution of E2E speech recognition, with specific focus on the input-output alignment and an overview of prominent E2E ASR models. Sec. IV discusses improvements of the basic E2E models, including E2E model combination, training loss functions, context, encoder/decoder structures and endpointing. Sec. V provides an overview of E2E ASR model training. Decoding algorithms for the different E2E approaches are discussed in Sec. VI. Sec. VII discusses the role and integration of (separate) language models in E2E ASR. Sec. VIII reviews experimental comparisons of the different E2E as well as classical ASR approaches. Sec. IX provides an overview of applications of E2E ASR. Sec. X investigates future directions of E2E research in ASR, before concluding in Sec. XI. Finally, this survey paper also includes comparative we note that discussions between novel E2E models and classical HMM- based ASR approaches in terms of various aspects; most sections end with a summarization of the relationship between E2E models and HMM-based ASR approaches in relation to the topics covered within the respective sections.
c) Training from Scratch: The E2E property can also be interpreted with respect to the training process itself, by re- quiring training from scratch, avoiding external knowledge like prior alignments or initial models pre-trained using different criteria or knowledge sources. However, note that pre-training and fine-tuning strategies are also relevant, if the model has explicit modularity, including self-supervised learning [25] or joint training of front-end and speech recognition models [26]. Especially in case of limited amounts of target task training data, utilizing large pretrained models is important to obtain performant E2E ASR systems.
d) Avoiding Secondary Knowledge Sources: For ASR, standard secondary knowledge sources are pronunciation lex- ica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [27]. Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost. Therefore, in an E2E approach, these would be avoided. Standard joint training of an E2E model requires using a single kind of training data, which in case of ASR would be transcribed speech audio data. However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available. One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data jointly without introducing secondary (pretrained) models and/or training objectives [28], [29].
II. DISTINCTIVENESS OF THE TERM E2E
As noted in Sec. I the term E2E provides an idealized definition of ASR systems, and can benefit from a more detailed discussion based on the following perspectives.
a) Joint Modeling: In terms of ASR, the E2E property can be interpreted as considering all components of an ASR system jointly as a single computational graph. Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which may also mean dropping the classical separation of ASR into an acoustic model and a language model. However, in practice E2E ASR systems are often combined with external language models trained on text- only data, which weakens the end-to-end nature of the system to some extent.
b) Joint Training: In terms of model training, E2E can be interpreted as estimating all parameters, of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate2. However, the term lacks distinctiveness here, as classical and/or modular ASR model architectures also support joint training with a single objective.
2 Note that this does not necessarily require Bayes Risk training, as standard training criteria like cross entropy, maximum mutual information and max- imum likelihood in case of classical ASR models asymptotically guarantee optimal performance in the sense of Bayes decision rule, also [23], [24].
e) Direct Vocabulary Modeling: Avoiding pronunciation lexica and corresponding subword units leave E2E recognition vocabularies to be derived from whole word or character representations. Whole word models [30], according to Zipf’s law [31], would require unrealistically high amounts of tran- scribed training data for large vocabularies, which might not be attainable for many tasks. On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [32], might be seen as secondary approaches outside the E2E objective, even more so if acoustic data is considered for subword derivation [33], [34], [35], [36].
f) Generic Modeling: Finally, E2E modeling also re- quires genericity of the underlying modeling: task-specific constraints are learned completely from data, in contrast to task-specific knowledge which influences the modeling of the system architecture in the first place. For example, the monotonicity constraint in ASR may be learned completely from data in an end-to-end fashion (e.g., in attention-based approaches [16]), or it may directly be implemented, as in classical HMM structures. However, model constraints may be considered by way of regularization in E2E ASR model training, and can thus provide an alternative way to introduce task-specific knowledge.
g) Single-Pass Search: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision. This is in line with Bayes’ decision rule, which exactly requires a single global decision integrating all available knowledge sources, which is supported
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
2


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
by both classical ASR models as well as E2E models. On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring.
All in all, we need to conclude that a) “E2E” does not provide a clear distinction between classical and novel, so- called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling.
A. Encoder and Decoder Modules
Irrespective of the alignment modeling approach, following the notation introduced in [41], it is useful to view all E2E ASR models as being composed of an encoder module and a decoder module. The encoder module, denoted H(X), maps an input acoustic frame sequence, X, of length T ′ into a higher-level representation, H(X) = (h1, · · · , hT ) of length T (typically T ≤ T ′). Note that the encoder output is independent of the hypothesized label sequence. The decoder module models the label sequence posterior on top of the encoder output:
III. A TAXONOMY OF E2E MODELS IN ASR Before we derive a taxonomy of E2E ASR modeling approaches, we first introduce our notation. We denote the input speech utterance as X, which we assume has been pa- rameterized into D-dimensional acoustic frames (e.g., log-mel features) of length T ′: X = (x1, · · · , xT ′), where xt ∈ RD. We denote the corresponding word sequences as C, which can be decomposed into a suitable sequence of labels of length L: C = (c1, · · · , cL), where each label cj ∈ C. Our description is agnostic to the specific representation used for decomposing the word sequence into labels; popular choices include char- acters, words, or sub-word sequences (e.g., BPE [32], word- pieces [37]).
P (C|X) = P (cid:0)C(cid:12)
(cid:12)H(X)(cid:1)
Thus, we may distinguish different approaches based upon how the output label sequence distribution (including potential latent variables resulting from the alignment modeling) are de- composed into individual label (and alignment) contributions; these may occur per output label position, per encoder frame position, or combinations thereof: (cid:12)H(X)(cid:1)
P (cid:0)C[, A](cid:12)
=
L (cid:89)
P (cid:0)ci[, ai](cid:12)
(cid:12)ci−1 1
[, ai−1 1
], vi(ci−1
1
[, ai−1 1
], H(X))(cid:1)
ASR may be viewed as a sequence classification problem which maps a variable length input, X, into an output, C, of unknown length. Following Bayes’ decision rule, any statistical approach to ASR must determine how to model the word sequence posterior probability, P (C|X). Thus, a natural taxonomy of E2E ASR modeling can be based on the various strategies for modeling this word sequence posterior: i.e., how the alignment problem between input and output sequence is handled; and, how sequence modeling is decomposed to the level of individual input vectors xt′ and/or output labels cl. We find that it is useful to distinguish implicit and explicit modeling approaches, based on the modeling of the sequence- to-sequence alignment:
a) Explicit Alignment Modeling: does not necessarily refer to the determination of a single unique alignment, but instead introduces an explicit alignment modeled as a latent variable, A:
P (C|X) =
(cid:88)
P (C, A|X)
A
i=1
where the notation mi−1 to the sequence the variables m; and, of vi(ci−1 ], H(X)) denotes a context-vector that provides 1 the connection between encoder output, H(X), and the la- bel output position, i. In general the context vector may depend on the label context (and possibly the latent vari- able context, for explicit alignment modeling approaches). Apart from the underlying alignment model and corresponding output label decomposition, decoder modules differ in terms of the assumptions on their label context ci−1 (and their latent variable context ai−1 ), which correspond to different conditional independence assumptions, and by their access to the encoder output. For example, the local posterior may only depend on a single encoder frame output (i.e., with the context vector being reduced to a single encoder frame’s output): , H(X)(cid:1) = hti(X). As we shall see in detail in the vi following sections, the simplest case of an encoder frame- level decomposition (with L = T , and ti = i) corresponds to CTC [13]; AED models [16] and their variants maintain the full dependency of the context vector.
corresponds
1
i − 1 previous instances of
[, ai−1 1
1
1
(cid:0)ci−1
1
b) Implicit Alignment Modeling: does not introduce a latent alignment variable, but models the label sequence pos- terior P (C|X) directly.
Explicit alignment modeling approaches can mainly be distinguished by their choice of latent variable; these can be encoded in terms of valid emission paths in corresponding finite state automata (FSA) [38] which relate the input and output sequences – the approach taken in our article. Typically, latent variables in explicit alignment modeling in transducer label set E2E models introduce extensions to the output with different forms of continuation labels (including, but not limited to so-called blank labels).3
3 For example, these extensions may also include explicit duration variables, leading to segmental models [39]. Such models can be rewritten into equiv- alent transducer models [40], and vice-versa.
Finally, different E2E models can also be distinguished by the specific modeling choices that are involved in the design of the neural network used to implement the encoder and the decoder. These might involve feed-forward neural networks, convolutional neural networks, recurrent neural networks (ei- ther uni-directional or bi-directional) [42], attention [43], and various combinations thereof (e.g., transformers [44] or conformers [45]). These modeling choices and corresponding training methods can be applied across E2E ASR models and therefore do not enter the taxonomy of E2E ASR models discussed here. However, specific choices will be discussed as part of the exemplary E2E ASR models presented in Sec. VIII and Sec. IX and.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
3


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
4
B. Explicit Alignment Modeling Approaches
Early E2E modeling approaches modeled alignments explic- itly through a latent variable, which is marginalized out (pos- sibly, approximately) during training and inference. Examples of this family of approaches include connectionist temporal classification (CTC) [13], the recurrent neural network trans- ducer (RNN-T) [14], the recurrent neural aligner (RNA) [46], and the hybrid auto-regressive transducer [47] (HAT). As will be discussed in subsequent sections, the latter modeling approaches in this family represent increasingly sophisticated modeling of alignments, with fewer independence assumptions and are thus increasingly powerful. A common feature of all explicit alignment models discussed in this section is that they introduce an additional blank symbol, denoted ⟨b⟩, and define an output probability distribution over symbols in the set Cb = C ∪ {⟨b⟩}. The interpretation of the ⟨b⟩ symbol varies slightly between each of these models, as we discuss in greater details below. For now, it suffices to say that given a specific training example, (X, C), each of these models defines a set of valid alignments, denoted by A(T,C), and define the conditional distribution P (C|X) by marginalizing over all valid alignment sequences:
s
s
Time
e
<b>
e
e
<b>
<b>
<b>
<b>
<b>
e
<b>
e
s
<b>
e
<b>
<b>
<b>
e
Fig. 1. Example alignment sequence for a CTC model with the target sequence C = (s, e, e) (right), alongside a (non-deterministic) finite state automaton (FSA) [38] (left) representing the set of all valid alignment paths.
Softmax
Encoder H(X)
P (C|X) =
(cid:88)
P (C|A, H(X))P (A|H(X))
=
A
(cid:88)
P (A|H(X))
(1)
Fig. 2. A representation of the CTC model consisting of an encoder which maps the input speech into a higher-level representation, and a softmax layer which predicts frame-level probabilities over the set of output labels and blank.
A∈A(T =|H(X)|,C)
where, by definition P (C|A, H(X)) = 1 if and only if A ∈ A(T,C) and 0 otherwise.4 We discuss the specific formulations of each of these models in the subsequent sections.
marginalizing over all possible CTC alignments as:
PCTC(C|X) =
(cid:88)
P (A|H(X))
A∈ACTC
(X,C)
1) Connectionist Temporal Classification (CTC): Connec- tionist Temporal Classification (CTC) was proposed by Graves et al. [13] as a technique for mapping a sequence of input tokens to a corresponding sequence of output tokens. CTC ex- plicitly models alignments between the encoder output, H(X), and the label sequence, C, by introducing a special “blank” la- bel, denoted by ⟨b⟩: Cb = C ∪ {⟨b⟩}. An alignment, A ∈ C∗ b , is thus a sequence of labels in C or ⟨b⟩.5 Given a specific training example, (X, C), we denote the set of all valid alignments, ACTC (X,C) = {A = (a1, a2, . . . , aT )}, such that each at ∈ Cb with the additional constraint that A is identical to C after first collapsing consecutive identical labels, and then removing all blank symbols. For example, if T = 10, and C = (s, e, e), then A = (s, ⟨b⟩ , ⟨b⟩ , e, e, ⟨b⟩ , e, e, ⟨b⟩ , ⟨b⟩) ∈ ACTC (X,C), as illustrated in Figure 1. As can be seen in this example, repeated labels in the output can be represented by intervening blanks. Following Eq. (1), CTC defines the posterior probability of the label sequence C conditioned on the input, X, by
=
(cid:88)
T (cid:89)
P (at|at−1, · · · , a1, H(X))
A∈ACTC
(X,C)
t=1
=
(cid:88)
T (cid:89)
P (at|ht)
A∈ACTC
(X,C)
t=1
Critically, as can be seen in Eq. (2), CTC makes a strong independence assumption that the model’s output at time t is conditionally independent of the outputs at other timesteps, given the local encoder output at time t.
Thus, a CTC model consists of a neural network that models the distribution P (at|X), at each step as shown in Figure 2. The encoder is connected to a softmax layer with |Cb| targets representing the individual probabilities in Eq. (2): P (at = c|X) = P (at = c|H(X)), which comprises the decoder module for CTC. Thus, at each step, t, the model consumes a single encoded frame ht and outputs a distribution over the labels; in other words, the model “outputs” a single label either blank, ⟨b⟩, or one of the targets in C.
(2)
4 This is equivalent to the assumption that the mapping from an alignment 5 S∗ denotes a Kleene A to a label sequence C is unique, by definition. closure: the set of all possible sequences composed of tokens in the set S.
2) Recurrent Neural Network Transducer (RNN-T): The Recurrent Neural Network Transducer (RNN-T) [14], [48] was proposed by Graves as an improvement over the basic CTC
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
5
Prediction Network
Encoder H(X)
Joint Network
Softmax
then A = (⟨b⟩ , s, ⟨b⟩ , ⟨b⟩ , ⟨b⟩ , e, e, ⟨b⟩ , ⟨b⟩ , ⟨b⟩) ∈ ARNNT (X,C). Note that, unlike the CTC model, repeated labels in the output require no special treatment as illustrated in Figure 4, where, i1 = i2 = 0; i3 = i4 = 1; i10 = 3; etc.
We may then define the posterior probability P (C|X) as
before:
PRNNT(C|X) =
(cid:88)
P (A|H(X))
A∈ARNNT
(X,C)
=
(cid:88)
T +L (cid:89)
P (aτ |aτ −1, . . . , a1, H(X))
A∈ARNNT
(X,C)
τ =1
=
(cid:88)
T +L (cid:89)
P (aτ |ciτ , ciτ −1, . . . , c0, hτ −iτ )
Fig. 3. An RNN-T Model [14], [48] consists of an encoder which transforms the input speech frames into a high-level representation, and a prediction- network which models the sequence of non-blank labels that have been output previously. The prediction network output, pit , represents the output after producing the previous non-blank label sequence c1, . . . , cit . The joint network produces a probability distribution over the output symbols (augmented with blank) given the prediction network state and a specific encoded frame.
=
A∈ARNNT
(X,C)
(cid:88)
A∈ARNNT
(X,C)
τ =1
T +L (cid:89)
τ =1
P (aτ |piτ , hτ −iτ )
(3)
Time
s
<sos>
e
<b>
e
<b>
e
<b>
e
s
<b>
where, P = (p1, · · · , pL) represents the output of the predic- tion network depicted in Figure 3 which summarizes the se- quence of previously predicted non-blank labels, implemented as another neural network: pj = N N (·|c0, . . . , cj−1), where c0 is a special start-of-sentence label, ⟨sos⟩. Thus, as can be seen in Eq. (2), RNN-T reduces some of the independence assumptions in CTC since the output at time t is conditionally dependent on the sequence of previous non-blank predictions, but is independent of the specific choice of alignment (i.e., the choice of the frames at which the non-blank tokens were emitted).
Fig. 4. Example alignment sequence (right) for an RNN-T model with the target sequence C = (s, e, e). Horizontal transitions in the image correspond to blank outputs. The FSA (left) represents the set of all valid RNN-T alignment paths.
model [13], by removing some of the conditional indepen- dence assumptions that we discussed previously. The RNN- T model, which is depicted in Figure 3, is best understood by contrasting it against the CTC model. As with CTC, the RNN-T model augments the output symbols with the blank symbol, and thus defines a distribution over label sequences in Cb. Similarly, as with CTC, the model consists of an encoder which processes the input acoustic frames X to generate the encoded representation H(X) = (h1, · · · , hT ).
Unlike CTC, however, the blank symbol in RNN-T has a slightly different interpretation; for each input encoder frame, ht, the RNN-T model outputs a sequence of zero or more symbols in C which are terminated by a single blank symbol. Thus, we may define the set of all valid alignment se- quences in RNN-T as: ARNNT (X,C) = {A = (a1, a2, · · · , aT +L)}, the set of all sequences of T + L symbols in C∗ b , which are identical to C after removing all blanks. Finally, for a given output position τ , let iτ denote the number of non- blank labels in the partial sequence (a1, · · · , aτ −1). Thus, the number of blanks in the partial sequence (a1, · · · , aτ −1) is τ − iτ − 1. For example, if T = 7, and C = (s, e, e),
Our presentation of RNN-T alignments considers the “canonical” case. In principle, however, the model can encode the same set of conditional independence assumptions in RNN-T (i.e., the model structure), while considering alter- native alignment structures as in the work of [49]. In their work, Moritz et al., represent valid frame-level alignments as an arbitrary graph. This formulation, for example, allows for the use of “CTC-like” alignments in the RNN-T model (i.e., outputting a single label – blank, or non-blank – at each frame) while conditioning on the set of previous non-blank symbols as in the RNN-T model.
3) Recurrent Neural Aligner (RNA): The recurrent neural aligner (RNA) was proposed by Sak et al. [46]. The RNA model generalizes the RNN-T model by removing one of its conditional independence assumptions. The model, depicted in Figure 5, is best understood by considering how it differs from the RNN-T model. As with CTC and RNN-T, the RNA model defines a probability distribution over blank augmented labels in the set Cb, where ⟨b⟩ has the same semantics as in the CTC model: at each frame the model can only output a single label – either blank, or non-blank – before advancing to the next frame; unlike CTC (but as in RNN- T) the model only outputs a single instance of each non- blank label. More specifically, the set of valid alignments, ARNA (X,C) = (a1, · · · , aT ), in the RNA model consist of length T sequences in C∗ b with exactly T − L blank symbols, and which are identical to C after removing all blanks. Thus, the blank
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
Prediction Network
Softmax
Joint Network
Encoder H(X)
Fig. 5. An RNA Model [46] resembles the RNN-T model [14], [48] in terms of the model structure. However, this model is only permitted to output a single label – either blank, or non-blank – in a single frame. Unlike RNN-T, the prediction network state in the RNA model, qt−1, depends on the entire alignment sequence at−1, . . . , a1. The joint network produces a probability distribution over the output symbols (augmented with blank) given the prediction network state and a specific encoded frame.
s
e
e
Time
<sos>
e
<b>
<b>
s
e
<b>
<b>
Fig. 6. Example alignment sequence (right) for an RNA model with the target sequence C = (s, e, e). Horizontal transitions in the image correspond to blank outputs; diagonal transitions correspond to outputting a non-blank symbol. The FSA (left) represents the set of valid alignments for the RNA model. Although the FSA is identical to the corresponding FSA for RNN-T in Figure 4, the semantics of the ⟨b⟩ label are different in the two cases.
symbol has a different interpretation in RNA and the RNN- T models: in RNN-T, outputting a blank symbol advances the model to the next frame; in RNA, however, the model advances to the next frame after outputting a single blank or non-blank label. Restricting the model to output a single non- blank label at each frame improves computational efficiency and simplifies the decoding process, by limiting the number of model expansions at each frame (in constrast to RNN-T decoding). For example, if T = 8, and C = (s, e, e), then A = (⟨b⟩ , s, ⟨b⟩ , e, ⟨b⟩ , ⟨b⟩ , e, ⟨b⟩) ∈ ARNA (X,C) as illustrated in Figure 6.
The RNA posterior probability, P (C|X), is defined as:
PRNA(C|X) =
(cid:88)
P (A|H(X))
A∈ARNA
(X,C)
=
(cid:88)
T (cid:89)
P (at|at−1, . . . , a1, H(X))
where, as before it denotes the number of non-blank sym- bols in the partial alignment sequence (a1, . . . , at−1), and qt−1 = NN(·|at−1, · · · , a1) represents the output of a neu- ral network which summarizes the entire partial alignment sequence, where NN(·) represents a suitable neural network (an LSTM in [46]). Thus, RNA removes the one remaining conditional independence assumption of the RNN-T model, by conditioning on the sequence of previous non-blank labels as well as the alignment that generated them. However, this comes at a cost: the exact computation of the log-likelihood in Eq. (3) (and corresponding gradients) is intractable. Instead, RNA makes two simplifying assumption to ensure tractable training: by assuming that the model can only output a single label at each frame; and utilizing a straight-through estimator for the alignment [50]. The latter constraint – allowing only a single label (blank or non-blank) at each frame – has also been explored in the context of the monotonic RNN-T model [51]. Finally, we note that the work in [52] further generalizes the RNA model by employing two RNNs when defining the state: a slow RNN (which corresponds to the sequence of previously predicted non-blank labels), and a fast RNN (which also conditions on the frames at which the non-blank labels were output).
C. Implicit Alignment Modeling Approaches
One of the main benefits of the explicit alignment ap- proaches such as CTC, RNN-T, or RNA is that they result in ASR models that are easily amenable to frame-synchronous decoding6 In this section, we discuss the attention-based encoder-decoder (AED) models (also known as, listen-attend- and-spell (LAS)) [15], [16], [53], which employs the attention mechanism [43] to implicitly identify and model the portions of the input acoustics which are relevant to each output unit. These models were first popularized in the context of machine translation [54]. Unlike explicit alignment modeling approaches, attention-based encoder-decoder models use an attention mechanism [43] to learn a correspondence between the entire acoustic sequence and the individual labels. Such models support label-synchronous decoding, meaning that during inference, each hypothesis in the beam is expanded by 1 label.
In the explicit alignment approaches presented in Sec- tion III-B, during inference, the model continues to output symbols until it has processed the final frame at which point the decoding process is complete; similarly, during training, the forward-backward algorithm aligns over all possible align- ment sequences. Since an AED model processes the entire acoustic sequence at once, the model needs a mechanism by which it can indicate that it is done emitting all output symbols. This is achieved by augmenting the set of outputs with an end-of-sentence symbol, ⟨eos⟩, so that the output vocabulary consists of the set Ceos = C ∪ {⟨eos⟩}. Thus, the AED model, depicted in Figure 7, consists of an en- coder network – which encodes the input acoustic frame
A∈ARNA
(X,C)
t=1
=
(cid:88)
A∈ARNA
(X,C)
T (cid:89)
t=1
P (at|qt−1, ht)
(4)
6 By frame-synchronous decoding, we refer to the ability of the model to produce output label for each input frame of speech. Models such as CTC, RNN-T, or RNA, support frame-synchronous decoding.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
6


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
Attention
Decoder
Softmax
Encoder H(X)
e
<eos>
e
Time
s
Fig. 8. Unlike models such as RNN-T or CTC, AED models do not have explicit alignment. However, it is possible to interpret the attention weights αt,i for a particular output symbol ci as an alignment weight which is represented above for the target sequence C = (s, e, e, ⟨eos⟩). In this representation, the size of the circle and the darkness level are proportional to the corresponding attention weights; thus the total probability mass is the same for each row. As illustrated above, the first few frames correspond to the first symbol c1 = s, while the latter frames correspond to the second ‘e’: c3 = e.
Fig. 7. An attention-based encoder decoder (AED) model [15], [16], [53]. The output distribution is conditioned on the decoder state, si (which summarizes the previously decoded symbols), and the context vector, vi (which summarizes the encoder output based on the decoder state). In the seminal work of Chan et al., [16], for example, this is accomplished by concatenating the two vectors, as denoted by the (cid:76) symbol in the figure.
sequence, X = (x1, . . . , xT ′), into a higher-level representa- tion H(X) = (h1, . . . , hT ) – and an attention-based decoder which defines the probability distribution over the set of output symbols, Ceos. Thus, given a paired training example, (X, C), we denote by Ce = (c1, . . . , cL, ⟨eos⟩), the ground-truth symbol sequence of length (L + 1) augmented with the ⟨eos⟩ symbol. AED models compute the conditional probability of the output sequence augmented with the ⟨eos⟩ symbol as:
P (Ce|X) = P (Ce|H(X))
=
L+1 (cid:89)
P (ci|ci−1, . . . , c0 = ⟨sos⟩ , H(X))
=
i=1 L+1 (cid:89)
=
i=1 L+1 (cid:89)
P (ci|si, vi)
i=1
where, vi corresponds to a context vector, which summarizes the relevant portions of the encoder output, H(X), given the sequence of previous predictions ci−1, . . . , c0; and, si corresponds to the corresponding decoder state after outputting the sequence of previous symbols, which is produced by updating the decoder state based on the previous context vector and output label:
si = Decoder(vi−1, si−1, ci−1)
The symbol c0 = ⟨sos⟩ is a special start-of-sentence symbol which serves as the first input to the attention-based decoder before it has produced any outputs. As can be seen in Eq. (5), an important benefit of AED models over models such as CTC or RNN-T is that they do not make any independence assumptions between model outputs and the input acoustics,
(5)
and are thus more general than the implicit alignment models, while being considerably easier to train and implement since we do not have to explicitly marginalize over all possible alignment sequences. However, this comes at a cost: previ- ously generated context vectors (which are analogous to the decoded partial alignment in explicit alignment models) are not revised as the decoding proceeds. Stated another way, while the encoder processing H(X) might be bi-directional, the decoding process in AED models reveals a left-right asymmetry [55].
1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci−1. The attention function, atten(ht, si) ∈ R, then defines a score between the model state after outputting i − 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: αt,i =
1) Computing the Context Vector in AED Models: As we mentioned before, the context vector, vi, is computed by employing the attention mechanism [43]. The central idea behind these approaches is to define a state vector si which corresponds to the state of the model after outputting c1, . . . , ci−1. The attention function, atten(ht, si) ∈ R, then defines a score between the model state after outputting i − 1 previous symbols, and each of the encoded frames in H(X). These scores can then be normalized using the softmax function to define a set of weights corresponding to each ht as: αt,i =
the weight αt,i represents the relevance of a particular encoded frame ht when outputting the next symbol ci, after the model has already output the symbols c1, . . . , ci−1, as illustrated in Figure 8. The context vector summarizes the encoder output based on the computed attention weights:
vi =
(cid:88)
αt,iht
t
A number of possible attention mechanisms have been explored in the literature: the most common forms are called ‘content-based attention’, which include dot-product atten- tion [16] and additive attention [43]. The content-based atten- tion computes the attention score atten(ht, si) based on the relevance between ht and si. However, the score does not consider location information, i.e., it is determined by only the content, independent of the position. This can lead to incorrect attention weights with a large discrepancy against the previous steps. Thus, location-based attention atten(si, fi,j) has been
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
7


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
proposed [15], where fi,j is a convolutional feature vector extracted from αi−1, the attention weights in the previous step. The hybrid attention, i.e., a combination of the content- and location-based attentions, has also been investigated in [15], showing a higher accuracy than the separate ones. Besides, other location-based methods use a Gaussian (mixture) model estimated with si to obtain attention weights [56], [57]. Transformer model [44] uses only content-based dot-product attention, but also takes location information into account through positional encoding. Apart from the specific choice of the attention mechanism, a common technique to improve performance involves the use of multiple independent attention heads – v1 i – which are then concatenated together to obtain the final context vector vi = (cid:2)v1 in the so-called multi-head attention approach [44], or indeed by stacking together multiple attention-based layers in the transformer decoder presented by Vaswani et al. [44].
i , . . . , vK
i ; . . . ; vK i
D. From Implicit to Explicit Alignment Modeling
independence assumptions, are extremely powerful, often outperforming explicit alignment E2E approaches such as CTC, or RNN- T [41]. However, these models also have some significant disadvantages, most notably that the models are typically non- streaming: i.e., the models must process all acoustic frames before they can generate any output hypotheses. A somewhat related issue is that the models are extremely sensitive to the length of the acoustic sequences, which requires special processing to be able to decode long-form audio [58]. There is a body of work that lies in between these two extremes: models such as the neural transducer [59], or those based on monotonic alignments [60] and its variants (e.g., monotonic chunkwise alignments (MoChA) [61], monotonic infinite look- back (MILK) [62] etc.) use an explicit alignment model, while also utilizing an attention mechanism that allows the model to examine local acoustics in order to refine predictions. In other words, this corresponds to a class of streaming AED models. Generally speaking, these models are motivated by the observation that speech (unlike tasks such as machine translation) exhibits a ‘local’ relationship between the encoded frames (assuming that the encoder is uni-directional) and the output units; thus, unlike the general AED model which computes the context vector, vi, as a sum over all input frames ht, the various proposed models constrain this sum to be computed over a subset of frames to allow for streaming decoding. In the context of our presentation, it is easiest to think of these models as consisting of an underlying alignment (whether known or unknown) which can be used to perform streaming inference.
AED models, which make no conditional
(cid:3),
(analogous to ⟨eos⟩ in the AED model); inference in the model terminates when the model has output the end-of-chunk sym- bol in the final chunk H W T W . If the alignments of the ground- truth output sequence, C, with respect to the W -length chunks are unknown, then it is possible to train the system by using a rough initial alignment where symbols are distributed equally among the T W chunks, followed by iterative refinement by computing the most likely output alignments given the current model parameters [59] similar to forced-alignments in HMM- based systems. An alternate approach [63] consists of using a separate system (e.g., a classical hybrid system) to get initial alignments (e.g., word-level alignments), which can be used to assign sub-word units to the individual chunks.
An alternative approach, proposed by Raffel et al. [60], modifies the vanilla AED model by explicitly introducing an alignment module which scans the encoder frames, H(X), from left-to-right to identify whether the current frame should be used to emit any outputs (modeled as a Bernoulli random variable). If a frame, τ , is selected, then the model produces an output based on the local encoder frame, hτ . The process is then repeated starting from the currently selected frame, thus allowing multiple outputs to be generated at the same frame. This results in a model with hard monotonic alignments labels since the between the input speech and the output models are constrained to generate outputs in a streaming fash- ion. A Monotonic Chunkwise Attention (MoChA) model [61] improves upon the work of Raffel et al., by allowing the model to generate the next output using a context vector computed using attention over a local window of frames to the left of the selected frame τ : hτ −W +1, . . . , hτ . Thus, the MoChA model consists of a two-level process – identifying frames where output should be produced following [60], followed by an AED model over frames to the left of the selected frame. A refinement to the MoChA model, proposed by Arivazhagan et al. [62] – the monotonic infinite lookback (MILK) attention model – computes the context vector over all frames to the left of the selected frame τ (i.e., h1, . . . , hτ ) at each step. Another two-fold approach to enable streaming operation is presented in [64] under the term of triggered attention, where a CTC-network is used to trigger, i.e. control the activation of an AED model with a limited decoder delay. We also direct interested readers to studies of various attention variants: Merboldt et al. [65] compare a number of local monotonic attention variants; Zeyer et al. [66] discuss segmental attention variants; Zeyer et al. [67] study the related decoding and the relevance of segment length modeling, leading to improved generalization towards long sequences. Segmental attention models are related to transducer models [68]. However, seg- mental E2E ASR models are not limited to be realized based on the attention mechanism and may not only be related to a direct HMM [39], but have also been shown to be equivalent to neural transducer modeling [40], thus even providing a clear relation between duration modeling and blank probabilities.
The Neural Transducer (NT) [59] explicitly partitions the in- put encoder frames into T W non-overlapping chunks of length W : H W T W = [hT W +1, . . . , hT W W ], (cid:7), and ht = 0 if t > T . Unlike the AED where T W = (cid:6) T model which examines all encoded frames when computing the context vector, is restricted to process a single chunk at a time; the model only advances to the next chunk when it outputs a special end-of-chunk symbol
1 = [h1, . . . , hW ]; · · · ; H W
W
the NT model
Relationship to Classical ASR
In classical ASR models, these frame-level alignments can be modeled with HMMs while using generative GMMs or
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
8


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
neural networks to model the output distribution of acous- tic frames; frame-level alignments to train neural network acoustic models may be obtained by force-alignment from a base GMM-HMM systems, but direct sequence training not requiring initial alignments is also possible [69].
E2E models introduce alternative alignment modeling ap- proaches to ASR. While the attention mechanism provides a qualitatively novel approach to map acoustic observation sequences to label sequences, transducer approaches [13], [14], [46], [70] handle the alignment problem in a way that can be interpreted to be similar to HMMs with a specific model topology, including marginalization over alignments [71], [72], [73]. CTC models can also be employed in an HMM-like fash- ion during decoding [74]. Moreover, transducer approaches are equivalent to segmental models/direct HMM [40].
is needed to predict the current word, e.g., "one dollar and fifty cents" → $1.50. To combine RNN-T and AED, the authors propose to produce a first-pass result with RNN-T, that is then rescored with AED in the second-pass. To reduce computation, the authors share the encoder between RNN-T and AED. The authors find that RNN-T + AED provides a 17–22% relative improvement in word error rate over RNN-T alone on a voice search task. Other flavors of streaming 1st-pass following by attention-based 2nd-pass rescoring, such as deliberation [88], have also been explored. One of the issues with such rescoring approaches is that any potential improvements are limited to the lattice produced by the 1st-pass system. To address this, methods which run a 2nd-pass beam search have also been explored, particularly in the context of streaming ASR – e.g. cascaded encoder [89], Y-architecture [90] and Universal ASR [91].
Another prominent feature of E2E systems besides the alignment property is their direct character-level modeling avoiding phoneme-based modeling and pronunciation lexica [19], [75], [74], [16], [76], [77], [78], [79], [80], [81], [82], with some even heading for whole-word modeling [76], [30]. However, character-level modeling also is viable with classical hybrid HMM architectures [83] and has been shown to work even with standard HMM models w/o neural networks [84].
IV. ARCHITECTURE IMPROVEMENTS TO BASIC E2E MODELS
In this section, we describe various algorithmic changes to vanilla E2E models which are critical in order to obtain improved performance over classical ASR systems. First, we describe various ways of combining different complementary E2E models to improve performance. Next, we introduce ways to incorporate context into these models to improve performance on rare proper noun entities. We then describe improved encoder and decoder architectures that take better advantage of the many cores on specialized architectures such as tensor processing units (TPUs) [85]. Finally, we discuss how to improve the latency of the model through an integrated E2E endpointer.
A. Combinations of Models
Different end-to-end models are complementary, and there have been numerous attempts at combining these methods. For example, Watanabe et al. [86] find that attention-based models perform poorly on long or noisy utterances, mainly because the model has too much flexibility in predicting alignments when presented with the entire input utterance. In contrast, models such as CTC – which have left-to-right constraints during decoding – perform much better in these cases. They propose to employ a multi-task learning strategy with both CTC and attention-based losses, which provides a 5–14% relative improvement in word error rate over attention- based models on Wall Street Journal (WSJ) and Chime tasks. Pang et al. [87] explore combining the benefits of RNN-T and AED. Specifically, RNN-T decodes utterances in a left- to-right fashion, which works well for long utterances. On the other hand, since AED sees the entire utterance, it often shows improvements for utterances where surrounding context
B. Incorporating Context
Contextual biasing to a specific domain, including a user’s song names, app names and contact names, is an impor- tant component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in E2E models because these models typically retain only a small list of candidates during beam search, and tend to perform poorly when recognizing words that are seen infrequently during training (typically named entities), which is the main source of biasing phrases. There have been a few approaches in the literature to incorporate context.
One approach, known as shallow-fusion contextual bias- ing [92], constructs a stand-alone weighted finite state trans- ducer (FST) representing the biasing phrases. The scores from the biasing FST are interpolated with the scores of the E2E model during beam search, with special care taken to ensure we do not over- or under-bias phrases. An alternate approach proposes to inject biasing phrases into the model in an all- neural fashion. For example, Pundak et al. [93] represent a set of biasing phrases by embedding vectors. These vectors are fed as additional input to an attention-based model, which can then choose to attend to the phrases and hence boost the chances of predicting the phrases. Kim and Metze [94] propose to bias towards dialog context. In addition, Bruguier et al. [95] extend [93], by leveraging phonemic pronunciations for the biasing phrases when constructing phrase embeddings. Finally, Delcroix et al. [96] use an utterance-wise context vector like an i-vector computed by a pooling across frame-by-frame hidden state vectors obtained from a sub network (this sub-network is called a sequence-summary network).
C. Encoder and Decoder Structure
There have been improvements to encoder architectures of E2E models over time. The first end-to-end models used long short-term memory recurrent neural networks (LSTMs), for both the encoder and decoder. The main drawback of these sequential models is that each frame depends on the computation from the previous frame, and therefore multiple frames cannot be batched in parallel.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
9


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
With the improvement of hardware, specifically on-device Edge Tensor Processing Units (TPUs), with thousands of cores, architectures that can better take advantage of the hardware, have been explored. Such architectures include convolution-based architectures, such as ContextNet [97]. The use of self-attention to replace the sequential recurrence in LSTMs was explored in Transformers for ASR [98], [99], [100]. Finally, combining self-attention with convolution, known as Conformer [45], or multi-layer perceptron [101], was also explored. Both Transformer and Conformer have shown competitive performance to LSTMs on many tasks [102], [103].
and extensive data augmentation. Part of the appeal of end- to-end models is that in- dependence between the input frames. Given a training set T = {(Xn, Cn)}N n=1, the training criterion L to be minimized can be written as: L = − (cid:80)N n=1 log P (Cn|Xn) (which is equivalent to maximizing the total conditional log-likelihood).
they do not assume conditional
A. Alignment in Training
E2E models such as RNN-T and CTC introduce an addi- tional blank token ⟨b⟩ for alignment. Therefore optimization implies marginalizing across all alignments, as follows:
On the decoder side, research for transducer models has shown that a large LSTM decoder can be replaced with a sim- ple embedding lookup table, that attends to only a few previous tokens from the model [47], [104], [105], [106], [107]. This demonstrates that most of the power of the E2E model is in the encoder, which has been a consistent theme of both E2E as well as classical hybrid HMM models. However, improved decoder modeling may also be effective depending on the specific downstream task. Research has shown that extended decoder architectures enable pre-training and adaptation of the decoder using extensive text-only data, leading to accuracy gains [108], [109]. For example, one approach separates RNN- T’s prediction network into separate blank and vocabulary prediction (LM) components, where the LM component can be trained with text data [108]. In addition, in line with the growing interest in large language models in recent years, research has also begun on solving multiple tasks, including speech recognition, using only an auto-regressive, GPT-style decoder [110], [111].
D. Integrated Endpointing
An important characteristic of streaming speech recognition systems is that they must endpoint quickly, so that the ASR result can be finalized and sent to the server for the appro- priate action to be performed. Endpointing is typically done with an external voice-activity detector. Since endpointing is both an acoustic and language model decision, recent works in streaming RNN-T models [112], [113] have investigated predicting a microphone closing token ⟨eos⟩ at the end of the utterance – e.g., “What’s the weather ⟨eos⟩”. Following the notation from Section III, this is done by including an ⟨eos⟩ token as part of the set of class labels C and encouraging the model to predict this token to terminate decoding. These models have shown improved latency and WER trade-off by having the endpointing decision predicted as part of the model. Furthermore, [114], [115] explored using the CTC blank symbol for endpoint detection.
V. TRAINING E2E MODELS
In general, training of E2E models follows deep learn- ing schemes [116], [117], with specific consideration of the sequential structure and the latent alignment problem to be handled in ASR. E2E ASR models may be trained end-to- end, notwithstanding potential elaborate training schedules
Lex = −
N (cid:88)
(cid:88)
log P (Cn, An|Xn)
n=1
An
This requires the forward-backward algorithm [118], [119] for efficient computation of the training criterion and its gradient, with minor modifications for CTC, RNN-T, and RNA models, as well as classical (full-sum) hybrid ANN/HMMs correspond- ing to the differences in alignments defined in each of these models. In comparison, AED models are based on implicit alignment modeling approaches, and the training criterion does not have a latent variable A for explicit alignment as:
Lim = −
N (cid:88)
log P (Cn|Xn)
n=1
We refer the interested reader to the individual papers for further details on the training algorithms [13], [14], [15], [16], [46], [48], [53], [71], [120]. As shown in Section III-A, in both explicit and implicit alignment cases, P (C|X) is factorized with respect to input time t and output position i, respectively, and the factorized distribution is conditioned on the label context ci−1 , except for CTC. For example, in the AED case: 1 log P (C|X) = (cid:80)L ). During training, we use a teacher-forcing technique where the ground truth history is used as a label context.
i=1 log P (ci|X, ci−1
1
As part of the training procedure, all E2E as well as classical hidden Markov models for ASR provide mechanisms to solve the underlying sequence alignment problem - either explicitly via corresponding latent variables, as in CTC, RNN-T or RNA, and also hybrid ANN/HMM, or implicitly, as in AED models. Also, the distinction between speech and silence needs to be considered, which may be handled explicitly by introducing silence as a latent label (hybrid ANN/HMM), or implicitly by not labeling silence at all, as currently is the standard in virtually all E2E models.
E2E models also may take advantage of hierarchical training schedules. These schedules may comprise several separate training passes and explicit, initially generated alignments that are kept fixed for some Viterbi-style [121], [122], [123] train- ing epochs before re-enabling E2E-style full-sum training that marginalizes over all possible alignments. Such an alternative approach is employed by Zeyer et al. [52], where an initial full-sum RNN-T model is used to generate an alignment and continue with framewise cross-entropy training. This greatly simplifies the training process by replacing the summation over all possible alignments in Eq. (4) by a single term cor- responding to the alignment sequence generated. Recently, a
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
10


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
similar procedure has been introduced in [124] also employing E2E models, only. In this work, CTC is used to initialize the training and to generate an initial alignment, followed by intermediate Viterbi-style RNN-T training and final full-sum fine tuning, which improved convergence compared to full- sum-only training approaches.
It is interesting to note that in contrast to the RNN-T and RNA label-topologies, CTC does not require alignments with single label emissions per label position. However, training CTC models eventually does lead to single label emissions per hypothesized label. An analysis of this property of CTC training which is usually called peaky behavior can be found in [125] and references therein. Laptev et al. [126] even introduces a CTC variant without non-blank loop transitions.
B. Training with External Language Models
E2E ASR models generally are normalized on sequence level. Therefore, sequence training with the maximum mutual information criterion [127] is the same as standard cross en- tropy/conditional likelihood training. However, once external language models are included in the training phase, sequence normalization needs to be included explicitly, leading to MMI sequence discriminative training. This has been exploited as a further approach to combine E2E models with external language models trained on text-only data during the training phase itself [128], [129], [130].
that consider MWER in terms of reinforcement learning [140], [141]. Optimal Completion Distillation (OCD) [81] proposes to minimize the total edit distance using an efficient dynamic programming algorithm. Finally, another body of research with sequence training introduce a separate external language model at training time [142], which can also be done efficiently via approximate lattice recombination [129] and also lattice-free approaches [130].
D. Pretraining
All E2E models as well as classical hidden Markov models for ASR provide holistic models that in principle enable train- ing from scratch. However, many strategies exist to initialize and guide the training process to reach optimal performance and/or to obtain efficient convergence by applying pretrain- ing and model growing [143], [144]. Supervised layer-wise pretraining has been successfully applied for classical [5], [145], as well as attention-based ASR models [146], which can be combined with intermediate sub-sampling schemes [147], and model growing [148]. Pretraining approaches utilizing untranscribed audio, large-scale semi-supervised data and/or multilingual data [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160] would deserve a self-contained survey and they are applicable for hybrid DNN/HMM and E2E approaches likewise – they will not be further discussed here.
C. Minimum Word Error Rate Training
E. Training Schedules and Curricula
Since the objective of speech recognition is to minimize word error rate (WER), there has been a growing number of research studies that incorporate this into the objective function by minimizing the model-based expectation of the number of word errors, as follows:
Lmwer =
N (cid:88)
(cid:88)
W(Cn, C ′
n)P (C ′
n|Xn)
n=1
C′ n
where W(Cn, C ′ n) is the word error count in a hypothesis C ′ n given a reference Cn, and n is an index which iterates over the entire training set. These methods, known as sequence or discriminative training, have shown great improvements for classical ASR [131], [132], [133], [134], [135], and have since been explored in E2E models. Typically these losses are constructed by running in ‘beam-search’ mode rather than teacher-forcing mode, and construct a loss from the errors made from the candidate hypotheses in the beam. Thus, this type of training first requires training the model to optimize P (C|X) in order to initialize the model with a good set of parameters to run a beam search. However, also direct approaches have been introduced that avoid this separation to train discriminatively from scratch [69], [136].
Papers that explore penalizing word errors include, Mini- mum Word Error Rate (MWER) training [137], where the loss function is constructed such that the expected number of word errors are minimized. Further work includes MWER for RNN- T and self-attention-T [138], as well as MWER using prefix search instead of n-best [139]. Also, there have been studies
Dedicated training schedules have been developed to guide the optimization process and as part of that reach proper alignment behavior explicitly or implicitly [52], [124], [147]. Many approaches exist for learning rate control [161], [162]: NewBob [163], [164] and enhancements [162]; global ver- sus parameter-wise learning rate control (exponential decay, power decay, etc.) [165]; learning rate warm-up [44]; warm restarts/cosine annealing [166]; weight decay versus gradually decreasing batch size [167]; fine-tuning [168] or population- based training [169]; etc. For a survey of meta learning cf. [170].
Sequence learning approaches also consider curriculum learning [171], [172], e.g., by considering short sequences first [173], [174]; interim increase of sub-sampling [147] initially more sub-sampling; or, for multi-speaker ASR training sort mixed speech by SNR and start with speakers of balanced energy and mixed gender [175].
F. Optimization and Regularization
Optimization usually is based on stochastic gradient descent [176], with momentum [177], [178], and a number of corre- sponding adaptive approaches, most prominently Adam [179] and variants thereof [145], [179], [180].
Investing more training epochs seems to provide improve- ments [52, Table 8], and also averaging over epochs has been reported to help [102]. For a discussion of the double descent effect and its relation to the amount of training data, label noise and early stopping cf. [181].
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
11


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
Regularization strongly contributes to training performance: e.g., L2 and weight decay [182], [166]; weight noise [183]; adaptive mean L1/L2 [184]; gradient noise [185]; dropout [186], [187], [188], layer dropout [189], [190], [191]; dropcon- nect [192]; zoneout [193]; smoothing of attention scores [15]; label smoothing [194]; scheduled sampling [195]; auxiliary loss [194], [196]; variable backpropagation through time [197], [198]; mixup [199]; increased frame rate [180]; or, batch normalization [200].
in low-resource scenarios. However, their generation often is costly and may even introduce errors, like pronunciations from a lexicon not reflecting the actual pronunciations observed. Therefore, for large enough training resources, secondary knowledge sources might become obsolete [209], or even harmful, in case of erroneous information introduced [210], [211].
G. Data Augmentation
Training of E2E ASR models also benefit from data aug- mentation methods, which might also be viewed as regu- larization methods. However, their diversity and impact on performance justifies a separate overview.
Most data augmentation methods perform data perturbation by exploiting certain dimensions of speech signal variation: speed perturbation [201], [202], vocal tract length perturbation [201], [203], frequency axis distortion [201], sequence noise injection [204], SpecAugment [205], or semantic mask [206]. Also, text-only data may be used to generate data using text- to-speech (TTS) on feature [207] or signal level [208]. In a comparison of the effect of TTS-based data augmentation on different E2E ASR architectures in [208], AED seemed to be the only architecture that appeared to benefit significantly from the TTS data.
In a recent study [174] and corresponding follow-up work [180], many of the regularization and data augmentation methods listed here have been exploited jointly leading to state-of-the-art performance on the Switchboard task for a single-headed AED model.
Relationship to Classical ASR
E2E systems attempt to define ASR models that integrate all knowledge sources into a single global joint model that does not utilize secondary knowledge sources and avoids the classical separation into acoustic and language models. These global joint models are completely trained from scratch using a single global training criterion based on a single kind of (transcribed) training data and thus require less ASR domain- specific knowledge provided sufficient amounts of training data are available.
While standard hybrid ANN/HMM training for ASR using frame-wise cross entropy already is discriminative, it is not yet sequence discriminative, requires prior alignments and also lacks consideration of an (external) language model during training. However, these potential shortcomings may be remedied by using sequence discriminative training criteria [127] and lattice-free training approaches [69].
In contrast to strict E2E systems, the classical ASR ar- chitecture includes the use of secondary knowledge sources beyond the primary training data, i.e. (transcribed) speech audio for acoustic model training, and textual data for language model training. Most prominently, this includes the use of a pronunciation lexicon and the definition of a phoneme set. Secondary resources like pronunciation lexica may be helpful
Classical ASR models usually are trained successively, with knowledge derived from models trained earlier injected into later training stages, e.g. in the form of HMM state alignments. However, such approaches from classical ASR might also be interpreted as specific training schedules. Initializing deep learning models using HMM alignments obtained from acous- tic models based on mixtures of Gaussians may be interpreted in this way, with the Gaussian mixtures serving as an initial shallow model. In classical ASR, also approaches training deep neural networks from scratch while avoiding interme- diate training of Gaussians has been proposed [212], [213], [214], also in combination with character-level modeling [83]. Another step towards more integrated training of classical systems has been to apply discriminative training criteria avoiding intermediate (usually lattice-based) representations of competing word sequences [215], [69], [216], [217], [136].
The training of classical ASR systems usually applies sec- ondary objectives to solve subtasks like phonetic clustering. The classification and regression trees (CART) approach is used to cluster triphone HMM states [27], [218]. More re- cent approaches proposed clustering within a neural network modeling framework, while still retaining secondary clustering objectives [219], [213]. However, also in E2E approaches secondary objectives are used, most prominently for subword generation, e.g. via byte-pair encoding [32]. Also, available pronunciation lexica can be utilized indirectly for assisting subword generation for E2E systems [35], [36], which are shown to outperform byte-pair encoding. Within classical ASR systems, phonetic clustering also can be avoided completely by modeling phonemes in context directly [220].
It is interesting to observe that specifically attention-based encoder-decoder models require larger numbers of training epochs to reach high performance, e.g. for a comparison of systems trained on Switchboard 300h cf. Table 5 in [221]. Also, attention-based encoder-decoder models have been shown to suffer from low training resources [222], [223], which can be improved by a number of approaches, including regularization techniques [174] as well as data augmentation using SpecAugment [224] and text-to-speech (TTS) [29]. SpecAugment also is shown to improve classical hybrid HMM models [225]. TTS on the other hand considerably improved attention-based encoder-decoder models trained on limited resources, but did not reach the performance of other E2E approaches or hybrid HMM models, which in turn were not considerably improved by TTS [208]. Multilingual approaches also help improve ASR development for low resource tasks, again both for classical [226], as well as for E2E systems [227], [228].
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
12


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
13
VI. DECODING E2E MODELS
B. Beam Search
This section describes several decoding algorithms for end- to-end speech recognition. The basic decoding algorithm of end-to-end ASR tries to estimate the most likely sequence ˆC among all possible sequences, as follows:
The beam search algorithm is introduced to approximately consider a subset of possible hypotheses ˜C among all possible hypotheses U ∗ during decoding, i.e., ˜C ⊂ U ∗ . A predicted output sequence ˆC is selected among a hypothesis subset ˜C instead of all possible hypotheses U ∗, i.e.,
ˆC = arg max C∈U ∗
P (C|X)
The following section describes how to obtain the recognition result ˆC.
A. Greedy Search
ˆC = arg max C∈ ˜C
P (C|X)
The beam search algorithm is to find a set of possible hy- potheses ˜C, which can include promising hypotheses efficiently by avoiding the combinatorial explosion encountered with all possible hypotheses U ∗.
(6)
The Greedy search algorithm is mainly used in CTC, which
ignores the dependency of the output labels as follows:
ˆA =
T (cid:89)
t=1
(cid:18)
arg max
at
P (at|X)
(cid:19)
where at is an alignment token introduced in Section III-B1. The original character sequence is obtained by converting alignment token sequence ˆA to the corresponding token se- quence ˆC. The argmax operation can be performed in parallel over input frame t, yielding fast decoding [13], [229], although the lack of the output dependency causes relatively poor performance than the attention and RNN-T based methods in general.
CTC’s fast decoding is further boosted with transformer [44], [98], [102] and its variants [45], [103] since their entire computation across the frames is parallelized [190], [230]. For example, the non-autoregressive models, including Im- puter [231], Mask-CTC [230], Insertion-based modeling [232], Continuous integrate-and-fire (CIF) [233] and other variants [234], [235] have been actively studied as an alternative non- autoregressive model to CTC. [235] shows that CTC greedy search and its variants achieve 0.06 real-time factor (RTF)7 by using Intel(R) Xeon(R) Silver 4114 CPU, 2.20GHz. The paper also shows that the degradation of the non-autoregressive models from the attention/RNN-T methods with beam search is not extremely large (19.7% with self-conditioned CTC [234] versus 18.5 and 18.9% with AED and RNN-T, respectively). The greedy search algorithm is also used as approximate decoding for both implicit and explicit alignment modeling approaches, including AED, RNA, CTC, and RNN-T, as follows:
There are two major beam search categories: 1) frame synchronous beam search and 2) label synchronous beam search. The major difference between them is whether it performs hypothesis pruning for every input frame t or every output token i. The following sections describe these two algorithms in more detail.
C. Label Synchronous Beam Search
Suppose we have a set of partial hypotheses up to (i − 1)th token ˜C1:i−1. A set of all possible partial hypotheses up to ith token C1:i is expanded from ˜C1:i−1 as follows:
C1:i = {( ˜C1:i−1, ci = c)}c∈U (7) The number of hypotheses |C1:i| would be | ˜C1:i−1| × |U|, at most. The beam search algorithm prunes the low probability score hypotheses from C1:i and only keeps a certain number (beam size ∆) of hypotheses at i among C1:i. This pruning step is represented as follows:
˜C1:i = NBESTC1:i∈C1:i P (C1:i|X), where | ˜C1:i| = ∆ (8)
Note that NBEST(·) is an operation to extract top ∆ hypothe- ses in terms of the probability score P (C1:i|X) computed from an end-to-end neural network, or a fusion of multiple scores described in Section VII-B.
In the label synchronous beam search, the length of the out- put sequence (N ) is unknown. Therefore, during this pruning process, we also add the hypothesis that reaches the end of an utterance (i.e., predict the end of sentence symbol ⟨eos⟩) to a set of hypotheses ˜C in Eq. (6) as a promising hypothesis.
ˆci = arg max
ci
P (ci| ˆC1:i−1, X) for i = 1, . . . , N
ˆat = arg max
at
P (at| ˆA1:t−1, X) for t = 1, . . . , T
The greedy search algorithm does not consider alternate hypotheses in a sequence compared with the beam search algorithm described below. However, it is known that the degradation of the greedy search algorithm is not very large trained in [16], [46], especially when the model matched conditions8.
is well
7 The ratio of the actual decoding time to the duration of the input speech. 8 On the other hand, in the AED models, increasing the search space does not consistently improve the speech recognition performance [77], [236] – a fact also observed in neural machine translation [237].
The label synchronous beam search does not explicitly depend on the alignment information; thus, it is often used in implicit alignment modeling approaches, including AED. Due to this nature, sequence hypotheses of the same length might cover a completely different number of encoder frames, unlike the frame synchronous beam search, as pointed out by [40]. As a result, we observe that the scores of very short and long segment hypotheses often become the same range, and the beam search wrongly selects such hypotheses. [86] shows an example of such extreme cases, resulting in large deletion and insertion errors for short and long-segment hypotheses, respectively. Thus, the label synchronous beam search requires heuristics to limit the output sequence length to avoid ex- tremely long/short output sequences. Usually, the minimum and maximum length thresholds are determined proportionally
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
to the input frame length |X| with tunable parameters ρmin and ρmax as Lmin = ⌊ρmin|X|⌋, Lmax = ⌊ρmax|X|⌋. Although these are quite intuitive ways to control the length of a hypothesis, the minimum and maximum output lengths depend on the token unit or type of script in each language. Another heuristic is to provide an additional score related to the output length or attention weights – e.g., a length penalty, and a coverage term [77], [238]. The end-point detection [239] is also used to estimate the hypothesis length automatically. [236] redefines the implicit length model of the attention decoder to take into account beam search, resulting in consistent behavior without degradation for increasing beam sizes.
Note that there are several studies on applying label syn- chronous beam search to explicit alignment modeling ap- proaches. For example, label synchronous beam search al- gorithms for CTC are realized by marginalizing all possible alignments for each label hypothesis [13]. [240] extends CIF [233] to produce label-level encoder representation and realizes label synchronous beam search in RNN-T.
E. Block-wise Decoding
Another beam search implementation uses a fixed-length block unit for the input feature. In this block processing, we can use the future context inside the block by using the non- causal encoder network based on the BLSTM, output-delayed unidirectional LSTM, or transformer (and its variants). This future context information avoids the degradation of the fully causal network. In this setup, the chunk size becomes the trade-off of controlling latency and accuracy. This technique is used in both RNN-T [100], [250], [251] and AED [61], [252], [253], [254]. Block-wise processing is especially important for implicit alignment modeling approaches, including AED, since it can provide block-wise monotonic alignment constraint between the input feature and output label, and realize block- wise streaming decoding.
F. Model Fusion during Decoding
Similar to the classical HMM-based beam search, we com- bine various scores obtained from different modules, including the main end-to-end ASR and LM scores.
D. Frame Synchronous Beam Search
In contrast to the label synchronous case in Eq. (8), the frame synchronous beam search algorithm performs pruning at every input frame t, as follows:
1) Synchronous Score Fusion: The most simple score fu- sion is performed when the scores of multiple modules are synchronized. In this case, we can simply add the multiple scores at each frame t or label i. The most well-known score combination is LM shallow fusion.
˜C1:i(t) = NBESTC1;i(t) P (C1;i(t)|X), where | ˜C1:i(t)| = ∆
where C1;i(t) is an i(t)-length label sequence obtained from the alignment A1:t, which is introduced in Sec. III-B. P (C1;i(t)|X) is obtained by summing up all possible align- ments A1:t ∈ A(X,C1;i(t)). Unlike the label synchronous beam search, frame synchronous beam search depends on explicit alignment A; thus, it is often used for explicit alignment including CTC, RNN-T, and RNA. modeling approaches, C1:i(t) is an expanded partial hypotheses up to input frame t, similar to Eq. (7).
LM shallow fusion: As discussed in Sec. VII, various neural LMs can be integrated with end-to-end ASR. The most simple integration is based on LM shallow fusion [255][256][257], as discussed in Sec. VII-B1, which (log-) linearly adds the LM score Plm(C1:i) to E2E ASR scores P (C1:i|X) during beam search in Eq. (8) as follows:
log P (C1:i|X) → log P (C1:i|X) + γ log Plm(C1:i)
where γ is a language model weight. Of course, we can combine other scores, such as the length penalty and coverage terms, as discussed in Sec. VI-C.u
Compared with the label synchronous algorithm, the frame synchronous algorithm needs to handle additional output to- ken transitions inside the beam search algorithm. The frame synchronous algorithm can be easily extended in online and/or streaming decoding, thanks to the explicit alignment informa- tion with input frame and output token.
Classical approaches to beam search for HMM, but also CTC and RNN-T variants, are based on weighted finite state transducers (WFST) [38], [74], [241] or lexical prefix trees [106], [242], [243]. They are categorized as frame synchronous beam search. These methods are often combined with an N- gram language model or a full-context neural language model [244], [245]. RNN-T [14], [246] and CTC prefix search [247] can deal with a neural language model by incorporating the language model score in the label transition state. Interestingly, triggered attention approaches [248], [249] allow us to use implicit alignment modeling approaches, including AED, in frame-synchronous beam search together with CTC and neural LM, which applies on-the-fly rescoring to the hypotheses given by CTC prefix search using the AED and LM scores.
2) Asynchronous Score Fusion: If we combine the frame- dependent score functions, P (at|·), used in explicit alignment modeling approaches, e.g., CTC, RNN-T, and label-dependent score functions, P (ci|·), used implicit alignment modeling approaches, e.g., AED, language model, we have to deal with the mismatch between the frame and label time indices t and i, respectively. In the time-synchronous beam search, this fusion is per- formed by incorporating the language model score in the label transition state [70], [22], [258]. [247] also combines a word-based language model and token-based CTC model by incorporating the language model score triggered by the word delimiter (space) symbol.
In the label-synchronous beam search, we first compute the label-dependent scores from the frame-dependent score function by marginalizing all possible alignments given a hypothesis label sequence. CTC/attention joint decoding [86] is a typical example, where the CTC score is computed by marginalizing all possible alignments based on the CTC forward algorithm [229]. This approach eliminates the wrong
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
14


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
15
alignment issues and difficulties of finding the correct end of sentences in the label-synchronous beam search [86].
Note that the model fusion method during beam search can realize simple one-pass decoding, while it limits the time unit of the models to be the same or it requires additional dynamic programming to adjust the different time units, especially for the label-synchronous beam search. This dynamic program- ming computation becomes significantly large when the length of the utterance becomes larger and requires some heuristics to reduce the computational cost [259].
G. Lexical Constraint during Score Fusion
Classically, we use a word-based language model to cap- ture the contextual information with the word unit, and also consider the word-based lexical constraint for ASR. However, end-to-end ASR often uses a letter or token unit and it causes further unit mismatch during beam search. As described in previous sections, the classical approach of incorporating the lexical constraint from the token unit to the word unit is based on a WFST. This method first makes a TLG transducer composed of the token (T), word lexicon (L), and word-based language transducers (G) [74]. This TLG transducer has been used for both CTC [74] and attention-based [53] models.
HMM-based or CTC systems. To tackle this issue, there are several studies of modifying these models by limiting the output dependencies in the fixed length (i.e., finite-history) [47], [267], or keeping the original RNN-T structure but merging the similar hypotheses during beam search [107].
I. Vectorization across both Hypotheses and Utterances
We can accelerate the decoding process by vectorizing multiple hypotheses during the beam search, where we replace the score accumulation steps for each hypothesis with vector- matrix operations for the vectorized hypotheses. This has been studied in RNN-T [22], [258], [268] and attention-based [259] models. This modification leverages the parallel computing capabilities of multi-core CPUs, GPUs and TPUs, resulting in significant speedups, while enabling multiple utterances to be processed simultaneously in a batch. Major deep neural net- work and end-to-end ASR toolkits support this vectorization. For example, Tensorflow9 [269], and FAIRESEQ10 [270] pro- vide a vectorized beam search interface for a generic sequence to sequence task, and it can be used for attention-based end-to- end ASR. End-to-end ASR toolkits including ESPnet11 [259], ESPRESSO12[261], LINGVO [271], and, RETURNN13 [272] also support the vectorized beam search algorithm.
Another approach used in the time synchronous beam search is to insert the word-based language model score triggered by the word delimiter (space) symbol [75]. To synchronize the word-based language model with a character-based end-to-end ASR, [260] combines the word and character-based LMs with the prefix tree representation, while [239], [261] uses look- ahead word probabilities to predict next characters instead of using the character-based LM. The prefix tree representation is also used for the sub-word token unit case [262], [263].
H. Multi-pass Fusion
The previous fusion methods are performed during the beam search, which enables a one-pass algorithm. The popular alternative methods are based on multi-pass algorithms where we do not care about the synchronization and perform n-best or lattice scoring by considering the entire context within an ut- terance. [16] uses the N-best rescoring techniques to integrate a word-based language model. [55] combines forward and backward searches within a multi-pass decoding framework to combine bidirectional LSTM decoder networks. Recently two- pass algorithms of switching different end-to-end ASR systems have been investigated, including RNN-T → AED [264]; CTC → AED [265], [266]. This aims to provide streamed output in the first pass and re-scoring with AED in the second pass to refine the previous output, thus satisfying a real-time interface requirement while providing high recognition performance.
In addition to the N-best output in the above discussion, there is a strong demand for generating a lattice output for better multi-pass decoding thanks to richer hypothesis information in a lattice. The lattice output can also be used for spoken term detection, spoken language understanding, and word posteriors. However, due to the lack of Markov assumptions, RNN-T and AED cannot merge the hypothesis and cannot generate a lattice straightforwardly, unlike the
Relationship to Classical ASR
One of the most prominent properties shared between E2E and classical statistical ASR systems is the use of a single- pass decoding strategy, which integrates all knowledge sources involved (models, components), before coming to a final decision [123]. This includes the use of full label context dependency both for E2E systems [229], [51], [77], [273], [174], [262], [274], [275], as well as classical systems via full- context language models [276], [244], [245], [277]. In classical ASR systems, even HMM alignment path summation may be retained in search [278]. Both E2E as well as classical ASR systems employ beam search in decoding. However, compared to classical search approaches, E2E beam search usually is highly simplified with very small beam sizes around 1 to 100 [15], [16], [77], [147]. Very small beam sizes also partly mask a length bias exhibited by E2E attention-based encoder- decoder models [279], [280], thus trading model errors against search errors [281]. An overview of approaches to handle the length bias beyond using small beam sizes in ASR is presented in [236].
Many classical ASR search paradigms are based on mul- tipass approaches that successively generate search space representations applying increasingly complex acoustic and/or language models [282], [283], [243]. However, multipass strategies also are employed using E2E models, which how- ever softens the E2E concept. Decoder model combination is pursued in a two-pass approach, while even retaining latency constraints as in [87]. Further multipass approaches include E2E adaptation approaches [284], [285], [286], [287].
9 https://www.tensorflow.org/api docs/python/tf/contrib/seq2seq/BeamSearchDecoder 10 https://github.com/pytorch/fairseq/blob/master/fairseq/sequence generator.py 12 https://github.com/freewym/espresso 13 https://github.com/rwth-i6/returnn
11 https://github.com/espnet/espnet
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
VII. LM INTEGRATION
This section discusses language models (LMs) used for E2E ASR. Hybrid ASR systems have long been using a pretrained LM [2], whereas most end-to-end (E2E) ASR systems employ a single E2E model that includes a network component acting as an LM.14 For example, the prediction network of RNN- T and the decoder network of AED models take on the role of a LM covering label back-histories. Therefore, E2E ASR does not seem to require external LMs. Nevertheless, many studies have demonstrated that external LMs help improve the recognition accuracy in E2E ASR.
There are presumably three reasons that E2E ASR still
requires an external LM:
a) Compensation for poor generalization: E2E models need to learn a more complicated mapping function than classical modular-based models such as acoustic models. Con- sequently, E2E models tend to face overfitting problems if the amount of training data is not sufficient. Pretrained LMs potentially compensate for the less generalized predictions made by E2E models.
b) Use of external text data: E2E models need to be trained using paired speech and text data, while LMs can be trained with only text data. Generally, text data can be collected more easily than the paired data. The training speed of an LM is also faster than that of E2E models for the same number of sentences. Accordingly, the LM can be improved more effectively with external text data, providing additional performance gain to the ASR system.
c) Domain adaptation: Domain adaptation helps im- prove recognition accuracy when the E2E model is applied to a specific domain. However, domain adaptation of the E2E model requires a certain amount of paired data in the target domain. Also, when multiple domains are assumed, it may be costly to maintain multiple E2E models for the domains the system supports. If a pretrained LM for the target domain is available, it may more easily improve recognition accuracy for domain-specific words and speaking styles without updating the E2E model.
This section reviews various types of LMs used for E2E ASR and fusion techniques to integrate LMs into E2E models.
A. Language Models
The LMs provide a prior probability distribution, P (C). If the sentence, C, can be decomposed into a sequence of tokens such as characters, subwords, and single words, the probability distribution can be computed based on the chain rule as:
P (C) =
L+1 (cid:89)
P (ci|c0:i−1)
i=1
where ci denotes the i-th token of C, and c0:i−1 represents token sequence c0, c1, . . . , ci−1, assuming c0 = ⟨sos⟩ and cL+1 = ⟨eos⟩.
Most LMs are designed to provide the conditional probabil- ity P (ci|c0:i−1), i.e., they are modeled to predict the next token
given a sequence of the preceding tokens. We briefly review such LMs focusing on the different techniques to represent each token, ci, and back-history, c0:i−1.
1) N-gram LM: N -gram LMs have long been used for ASR [2]. Early E2E systems in [53], [74], [77] also employed an N -gram LM. The N -gram models rely on the Markov assumption that the probability distribution of the next token depends only on the previous N −1 tokens, i.e., P (ci|c0:i−1) ≈ P (ci|ci−N +1:i−1), where N is typically 3 to 5 for word-based models and higher for sub-word and character-based models. The maximum likelihood estimates of N -gram probabilities are determined based on the counts of N sequential tokens in the training data set as: ci
where, K(·) denotes the count of each token sequence. Since the data size is finite, it is important to apply a smoothing technique to avoid estimating the probabilities based on zero or very small counts for rare token sequences. Those techniques compensate the N -gram probabilities with lower order models, e.g., (N − 1)-gram models, according to the magnitude of the count [288]. However, since the N -gram probabilities still rely on the discrete representation of each token and the history, they suffer from data sparsity problems, leading to poor generalization.
The advantage of the N -gram models is their simplicity, although they underperform state-of-the-art neural LMs. In the training, the main step is to just count the N tuples in the data set, which is required only once. During decoding, the LM probabilities can be obtained very quickly by table lookup or can be attached to a decoding graph, e.g., WFST, in advance. 2) FNN-LM: The feed-forward neural network (FNN) LM was proposed in [9], which estimates N -gram probabilities using a neural network. The network accepts N − 1 tokens, and predicts the next token as:
P (ci|ci−N +1:i−1) = softmax(Wohi + bo)
hi = tanh(Whei + bh) ei = concat(E(ci−N +1), . . . , E(ci−1))
where Wo and Wh are weight matrices, and bo and bh are bias vectors. E(y) provides an embedding vector of c, and concat(·) operation concatenates given vectors 15. This model first maps each input token to an embedding space, and then obtains hidden vector, hi, as a context vector representing the previous N −1 tokens. Finally, it outputs the probability distri- bution of the next token through the softmax layer. Although this LM still relies on the Markov assumption, it outperforms classical N -gram LMs described in the previous section. The superior performance of FNN-LM is primarily due to the distributed representation of each token and the history. The LM learns to represent token/context vectors such that semantically similar tokens/histories are placed close to each other in the embedding space. Since this representation has a better smoothing effect than the count-based one used for N - gram LMs, FNN-LM can provide a better generalization than
14 In the simplest case of a CTC model as in Fig. 2, the included LM component however is limited to a label prior without label context.
15 We omit the optional direct connection from the embedding layer to the softmax layer in [9] for simplicity.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
16


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
N -gram LMs for predicting the next token. Neural network- based LMs basically utilize this type of representation.
3) RNN-LM: A recurrent neural network (RNN) LM was introduced to exploit longer contextual information over N − 1 previous tokens using recurrent connections [289]. Unlike FNN-LM, the hidden vector is computed as: hi = recurrence(ei, hi−1) ei = E(ci−1)
where, recurrence(ei, hi−1) represents a recursive function, which accepts previous hidden vector hi−1 with input ei, and outputs next hidden vector hi. In the case of simple (Elman- type) RNN, the function can be computed as
recurrence(e, h) = tanh(Whe + Wrh + bh)
where, Wr is a weight matrix for the recurrent connection, which is applied to the previous hidden vector h. This recurrent loop makes it possible to hold the history information in the hidden vector without limiting the history to N − 1 tokens. the history information decays exponentially as However, tokens are processed with this recursion. Therefore, currently stacked LSTM layers are more widely used for the recurrent network, which have separate internal memory cells and gating mechanisms to keep long-range history information [290]. With this mechanism, RNN-LMs outperform other N -gram- based models in many tasks.
4) ConvLM: Convolutional neural networks (ConvLM) have also been applied to LMs [291], [292], [293]. ConvLM [292] replace the recurrent connections used in RNN-LMs with gated temporal convolutions. The hidden vector is com- puted as hi =h′ i ⊗ σ(gi) h′ i =ei−k+1:i ∗ W + b gi =ei−k+1:i ∗ V + c
where ⊗ is element-wise multiplication, ∗ is a temporal convolution operation, and k is the patch size. σ(gi) represents a gating function of convoluted activation h′ i, and is modeled as a sigmoid function. W and V are matrices for convolution and b and c are bias vectors. The convolution and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set.
5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h′ hi = FFN(h′ i h′ i = MHA(ei, e1:i, e1:i) + ei
where FFN(·) and MHA(·, ·, ·) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network.
the next
B. Fusion Approaches
There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations.
1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9)
where γ is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains.
2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state.
3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
17


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
18
only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data.
4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log Pφ(C|X) − γφ log Pφ(C) + γτ log Pτ (C)
where subscripts φ and τ indicate the source and target domains, respectively. γφ and γτ are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density Pφ(X|C) because Pφ(X|C) ∝ Pφ(C|X)/Pφ(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM Pτ (C).
The density ratio method [303] trains an internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305].
C. Use of Large-scale Pretrained LMs
In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309].
Relationship to Classical ASR
The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling.
the language model
training objective,
i.e.
VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS
This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16
Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable
16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md.
readers who want
know the
to
update
can
also
check
of
these and
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
swb
HMM
AED
AED
chm
AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021
AED
AED
AED
AED
AED
AED
AED
AED
IX. DEPLOYMENT OF E2E MODELS
Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment.
The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time.
Fig. 9. E2E ASR performance improvement in the switchboard task.
AED
test_clean
ContextNet Transducer
transducer
AED
AED
AED
transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021
HMM
CTC
test_other
AED
AED
AED
AED
AED
AED
transducer
In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier.
Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a 2-pass cascaded encoder to run a 2nd-pass beam search [89]; and, a neural LM re-scorer to help improve accuracy long-tail named entities. This model is the best ASR system that Google has released to date, both in terms of quality and latency.
Fig. 10. E2E ASR performance improvement in the Librispeech task.
amount of unlabeled in-domain speech data (e.g., Libri-light 60K hours [322]).
Relationship to Classical ASR
Speech recognition research has always been pushed by international evaluation campaigns (e.g. as lead by NIST) and corresponding benchmark tasks. The competition between classical and E2E approaches is nicely reflected in the widely used Librispeech [317] and Switchboard [316] tasks, showing that E2E models gain momentum. As shown in Figure 10, on Librispeech, the current best-published classical hybrid systems range around 2.3% (test-clean) and 4.9% (test-other) word error rate [323], [222], while there already are a number of E2E systems providing similar performance [224], [205], [320], [206], with some E2E systems clearly outperforming former state-of-the-art results with word error rates down to 1.8% (test-clean) and 3.7% (test-other) [324] with similar results reported in [45], [97]. Merging insights from classical HMM-based and monotonic RNN-T provided similarly well results with a limited training budget [124]. Finally, when trained on Switchboard 300h, the current best result, obtained with an E2E system [180] is 5.4% compared to 6.6% word error rate for the best hybrid system result [325] on the HUB5’00 Switchboard test set, in Figure 9
X. AREAS FOR FUTURE WORK
Currently, E2E models dominate the academic debate on ASR. However, at least partly, this is not (yet?) reflected in the corresponding commercial deployment of E2E ASR architectures. E2E models are not yet the perfect match for all ASR conditions and further research is needed to take full advantage of the benefits of E2E modeling.
E2E models seem to perform really well when training data is abundant, while not scaling well to low-resource conditions. Similarly, domain change requires a flexible exchange of lan- guage models, which is natural for classical ASR models based on a separation of acoustic and language models. Ongoing research on the use of external language models in E2E models and internal language model estimation already is promising, but can be expected to see further improvements.
Top E2E ASR systems usually require orders of magnitude more training epochs than comparable classical ASR systems, and further research into efficient and robust optimization and training schedules is needed.
The high level of integration of E2E models also involves a loss in modularity, which might support the explainability and reusability of models. Also, more efficient training schedules might take advantage of modularity. One assumed advantage of E2E models is that everything is trained from data and secondary knowledge sources (e.g. pronunciation lexica and phoneme sets) are avoided. However, rare events, like rare
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
19


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
words in ASR still provide a challenge, which needs further research.
With the missing separation of acoustic and language mod- els, the question arises of how to exploit text-only resources in E2E model training - do we foresee solutions beyond training data generation using TTS? We note that a number of recent works have explored approaches to combine speech and text modalities by attempting to implicitly or explicitly map them into a shared space [159], [335], [336], [337], [338], [339], [340], [341]. Furthermore, high-performance E2E solutions exist for both discriminative problems like ASR, as well as generative problems like TTS, how can both be exploited jointly to support semi-supervised training based on text-only and/or audio-only data on top of transcribed speech audio [28], [342]?
recognition; multilingual ASR; adaptation to new application domains, and speakers; etc.), which we do not cover due to space limitations.
ACKNOWLEDGMENT
The authors would like to thank Julian Dierkes, Yifan Peng, Zolt´an T¨uske, Albert Zeyer, and Wei Zhou for their help on refining our manuscript.
REFERENCES
[1] T. Bayes, “An Essay Towards Solving a Problem in the Doctrine of Chances,” Philosophical Transactions of the Royal Society of London, vol. 53, pp. 370–418, 1763.
[2] F. Jelinek, Statistical Methods for Speech Recognition. Cambridge,
MA: MIT Press, 1997.
For AED architectures, we observe a length bias, which complicates the decoding process. Although many heuristics are known to tackle length bias in AED, we are still missing a well-founded explanation for it, as well as a corresponding remedy of the original model.
[3] L. R. Rabiner, “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,” Proc. of the IEEE, vol. 77, no. 2, pp. 257–286, Feb. 1989.
[4] H. A. Bourlard and N. Morgan, Connectionist Speech Recognition: a Hybrid Approach. Norwell, MA: Kluwer Academic Publishers, 1993. [5] F. Seide, G. Li, and D. Yu, “Conversational Speech Transcription Using Context-Dependent Deep Neural Networks,” in Proc. Interspeech, Florence, Italy, Aug. 2011, pp. 437–440.
Other open research problems include speaker adaptation and robustness to recording conditions, especially in mismatch situations. The E2E principle also provides a promising candi- date to solve multichannel ASR by providing an E2E solution jointly tackling the source separation, speaker diarization and speech recognition problem [343], [26].
Finally, we need to investigate, if E2E is a suitable guiding principle, and how different E2E ASR models relate to each other as well as to classical ASR approaches. The most important guiding principle of ASR research and development has been performance, and ASR has been boosted strongly by widely used benchmark tasks and international evaluation campaigns. With the current diversity of classical and E2E models, we also need to resolve the question of what con- stitutes state-of-the-art in ASR today, and can we expect a common state-of-the-art ASR architecture in the future?
[6] V. Fontaine, C. Ris, and H. Leich, “Nonlinear Discriminant Analysis for Improved Speech Recognition,” in Proc. Eurospeech, Rhodes, Greece, Sep. 1997, pp. 1–4.
[7] H. Hermansky, D. Ellis, and S. Sharma, “Tandem connectionist Feature Extraction for Conventional HMM Systems,” in Proc. IEEE ICASSP, vol. 3, Istanbul, Turkey, Jun. 2000, pp. 1635–1638.
[8] M. Nakamura and K. Shikano, “A Study of English Word Category Prediction Based on Neural Networks,” in Proc. IEEE ICASSP, Glas- glow, UK, May 1989, pp. 731–734.
[9] Y. Bengio, R. Ducharme, and P. Vincent, “A Neural Probabilistic Language Model,” in Proc. NIPS, vol. 13, Denver, CO, Nov. 2000, pp. 932–938.
[10] H. Schwenk and J.-L. Gauvain, “Connectionist Language Modeling for Large Vocabulary Continuous Speech Recognition,” in Proc. IEEE ICASSP, Orlando, FL, May 2002, pp. 765–768.
[11] Z. T¨uske, P. Golik, R. Schl¨uter, and H. Ney, “Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR,” in Proc. Interspeech, Singapore, Sep. 2014, pp. 890–894.
[12] T. N. Sainath, R. J. Weiss, K. W. Wilson, A. Narayanan, M. Bacchiani, and A. Senior, “Speaker Location and Microphone Spacing Invariant Acoustic Modeling from Raw Multichannel Waveforms,” in Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 30–36.
XI. CONCLUSIONS
In this work, we presented a detailed overview of end-to- end approaches to ASR. Such models, which have grown in popularity over the last few years, propose to use highly inte- grated neural network components which allow input speech to be converted directly into output text sequences through character-based output units. Thus, such models eschew the classical modular ASR architecture consisting of an acoustic model, a pronunciation model, and a language model, in favor of a single compact structure, and rely on the data to learn effectively. These design choices enable the deployment of highly accurate on-device speech recognition models (see Section IX), but also come with a number of downsides which are still areas of active research (see Section X).
interested readers to Li’s excellent contemporaneous overview article on end-to-end ASR [344], which offers a complementary perspective to our own. In particular, readers of [344] may find a more detailed exposition on the choice of encoder structure, and the applications of E2E approaches to allied ASR areas (e.g., multi-speaker
Finally, we direct
[13] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Connection- ist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, Pittsburgh, PA, Jun. 2006, pp. 369–376.
[14] A. Graves, “Sequence Transduction with Recurrent Neural Networks,” in Proc. ICML, Edinburgh, Scotland, Jun. 2012, Workshop on Repre- sentation Learning, arXiv:1211.3711.
[15] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-Based Models for Speech Recognition,” in Proc. NIPS, vol. 28, Laval, Que`ebec, Canada, Dec. 2015, pp. 577–585.
[16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition,” in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4960–4964.
[17] P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar, “An End-to- End Discriminative Approach to Machine Translation,” in Proc. ACL, Sydney, Australia, Jul. 2006, p. 761–768.
[18] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural Language Processing (Almost) from Scratch,” Journal of Machine Learning Research, vol. 12, pp. 2493–2537, 2011. [19] A. Graves and N. Jaitly, “Towards End-to-End Speech Recognition with Recurrent Neural Networks,” in Proc. ICML, Beijing, China, Jun. 2014, pp. 1764–1772.
[20] “Cambridge Dictionary,” https://dictionary.cambridge.org/dictionary/
english/end-to-end, accessed: 2020-02-21.
[21] R. Pang, T. N. Sainath, R. Prabhavalkar, S. Gupta, Y. Wu, S. Zhang, and C.-c. Chiu, “Compression of End-to-End Models,” in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 27–31.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
20


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[22] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shang- guan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-y. Chang, K. Rao, and A. Gruenstein, “Streaming End-to-End Speech Recognition for Mobile Devices,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6381– 6385.
[23] R. Schl¨uter and H. Ney, “Model-based MCE Bound to the True Bayes’ Error,” IEEE Signal Processing Letters, vol. 8, no. 5, pp. 131–133, May 2001.
[24] H. Ney, “On the Relationship between Classification Error Bounds and Training Criteria in Statistical Pattern Recognition,” in Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA), Puerto de Andratx, Spain, Jun. 2003, pp. 636–645.
[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,” in Proc. NeurIPS, Vancouver, BC, Canada, Dec. 2020, pp. 12 449– 12 460.
[45] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution- Augmented Transformer for Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 5036–5040.
[46] H. Sak, M. Shannon, K. Rao, and F. Beaufays, “Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping,” in Proc. Interspeech, vol. 8, Stockhol, Sweden, Aug. 2017, pp. 1298–1302.
[47] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid Autore- gressive Transducer (HAT),” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6139–6143.
[48] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech Recognition with Deep Recurrent Neural Networks,” in Proc. IEEE ICASSP, Vancouver, BC, Canada, May 2013, pp. 6645–6649.
[49] N. Moritz, T. Hori, S. Watanabe, and J. Le Roux, “Sequence Trans- duction with Graph-Based Supervision,” in Proc. IEEE ICASSP, Sin- gapore, May 2022, pp. 7212–7216.
[26] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, “MIMO- Speech: End-to-End Multi-Channel Multi-Speaker Speech Recogni- tion,” in Proc. IEEE ASRU. Sentosa, Singapore: IEEE, Dec. 2019, pp. 237–244.
[27] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classication and
Regression Trees. Belmont, CA: Taylor & Francis, 1984.
[28] A. Tjandra, S. Sakti, and S. Nakamura, “Listening While Speaking: Speech Chain by Deep Learning,” in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec. 2017, pp. 301–308.
[29] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and ˇCernock´y, “Semi-Supervised Sequence-to-Sequence ASR Using J. Unpaired Speech and Text,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3790–3794, arXiv:1905.01152.
[30] H. Soltau, H. Liao, and H. Sak, “Neural Speech Recognizer: Acoustic- to-Word LSTM Model for Large Vocabulary Speech Recognition,” in Proc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1610.09975. [31] G. K. Zipf, Human Behavior and the Principle of Least Effort. Boston,
MA: Addison-Wesley Press, 1949.
[32] R. Sennrich, B. Haddow, and A. Birch, “Neural Machine Translation of Rare Words with Subword Units,” in Proc. ACL, Berlin, Germany, Aug. 2015, pp. 1715–1725.
[50] Y. Bengio, N. L´eonard, and A. Courville, “Estimating or Propagating Gradients through Stochastic Neurons for Conditional Computation,” Aug. 2013, arXiv:1308.3432.
[51] A. Tripathi, H. Lu, H. Sak, and H. Soltau, “Monotonic Recurrent Neural Network Transducer and Decoding Strategies,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 944–948.
[52] A. Zeyer, A. Merboldt, R. Schl¨uter, and H. Ney, “A New Training Pipeline for an Improved Neural Transducer,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2812–2816.
[53] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, “End-to-End Attention-Based Large Vocabulary Speech Recognition,” in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 4945–4949. [54] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, “Google’s Neural Machine Translation System: Bridging the Gap Be- tween Human and Machine Translation,” Oct. 2016, arXiv:1609.08144. [55] M. Mimura, S. Sakai, and T. Kawahara, “Forward-Backward Attention Decoder,” in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2232– 2236.
[33] W. Chan, Y. Zhang, Q. Le, and N. Jaitly, “Latent Sequence Decompo- sitions,” in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1610.03035. [34] H. Liu, Z. Zhu, X. Li, and S. Satheesh, “Gram-CTC: Automatic unit selection and target decomposition for sequence labelling,” in Proc. ICML, ser. Proceedings of Machine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 2188–2197, arXiv:1703.00096.
[35] H. Xu, S. Ding, and S. Watanabe, “Improving End-to-End Speech Recognition with Pronunciation-Assisted Sub-Word Modeling,” in Proc. IEEE ICASSP, Brighton, UK, Sep. 2019, pp. 7110–7114. [36] W. Zhou, M. Zeineldeen, Z. Zheng, R. Schl¨uter, and H. Ney, “Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition,” in Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2886–2890. [37] M. Schuster and K. Nakajima, “Japanese and Korean Voice Search,” in Proc. IEEE ICASSP, Kyoto, Japan, Mar. 2012, pp. 5149–5152. [38] M. Mohri, F. Pereira, and M. Riley, “Weighted Finite-State Transducers in Speech Recognition,” Computer Speech & Language, vol. 16, no. 1, pp. 69–88, 2002.
[56] A. Graves, “Generating Sequences with Recurrent Neural Networks,”
Aug. 2013, arXiv:1308.0850.
[57] J. Hou, S. Zhang, and L.-R. Dai, “Gaussian Prediction Based At- tention for Online End-to-End Speech Recognition,” in Proc. In- terspeech, Stockholm, Sweden, Aug. 2017, pp. 3692–3696, DOI: 10.21437/Interspeech.2017-751.
[58] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen, A. Narayanan, H. Liao, S. Zhang, A. Kannan, R. Prabhavalkar, Z. Chen, T. Sainath, and Y. Wu, “A Comparison of End-to-End Models for Long- Form Speech Recognition,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 889–896.
[59] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio, “An Online Sequence-to-Sequence Model Using Partial Conditioning,” in Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 5067–5075.
[60] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, “Online and Linear-Time Attention by Enforcing Monotonic Alignments,” in Proc. ICML, Sydney, Australia, Aug. 2017, pp. 2837–2846.
[61] C.-C. Chiu and C. Raffel, “Monotonic Chunkwise Attention,” in Proc.
[39] E. Beck, M. Hannemann, P. Doetsch, R. Schl¨uter, and H. Ney, “Segmental Encoder-Decoder Models for Large Vocabulary Automatic Speech Recognition,” in Proc. Interspeech, Hyderabad, India, Sep. 2018.
ICLR, Vancouver, Canada, Apr. 2018, arXiv:1712.05382.
[62] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz, R. Pang, W. Li, and C. Raffel, “Monotonic Infinite Lookback Attention for Simultaneous Machine Translation,” in Proc. ACL, Florence, Italy, Jun. 2019, pp. 1313–1323.
[40] W. Zhou, A. Zeyer, A. Merboldt, R. Schl¨uter, and H. Ney, “Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept,” in Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2891–2895. [41] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and N. Jaitly, “A Comparison of Sequence-to-Sequence Models for Speech Recognition,” in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp. 939–943.
[63] T. N. Sainath, C.-C. Chiu, R. Prabhavalkar, A. Kannan, Y. Wu, P. Nguyen, and Z. Chen, “Improving the Performance of Online Neural Transducer Models,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5864–5868.
[64] N. Moritz, T. Hori, and J. Le Roux, “Triggered Attention for End-to- End Speech Recognition,” in Proc. IEEE ICASSP, Brighton, England, May 2019, pp. 5666–5670.
[42] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural
Computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[43] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” in Proc. ICLR, San Diego, CA, May 2015, arXiv:1409.0473.
[65] A. Merboldt, A. Zeyer, R. Schl¨uter, and H. Ney, “An Analysis of Local Monotonic Attention Variants,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1398–1402.
[66] A. Zeyer, R. Schl¨uter, and H. Ney, “A Study of Latent Monotonic
Attention Variants,” Mar. 2021, arXiv:2103.16710.
[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is All You Need,” in Proc. NIPS, Los Angeles, CA, Dec. 2017, pp. 5998–6008.
[67] A. Zeyer, R. Schmitt, W. Zhou, R. Schl¨uter, and H. Ney, “Monotonic Segmental Attention for Automatic Speech Recognition,” in Proc. IEEE SLT, Doha, Qatar, Jan. 2023, arXiv:2210.14742.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
21


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[68] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, and Z. Wen, “Synchronous Transformers for End-to-End Speech Recognition,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, arXiv:1912.02958.
[69] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, “Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI,” in Proc. Inter- speech. San Francisco, CA: ISCA, Sep. 2016, pp. 2751–2755, DOI: 10.21437/Interspeech.2016-595.
[70] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2Letter: An End- to-End Convnet-Based Speech Recognition System,” Sep. 2016, arXiv:1609.03193.
[71] P. Haffner, “Connectionist Speech Recognition with a Global MMI Algorithm,” in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp. 1929–1932.
[72] A. Zeyer, E. Beck, R. Schl¨uter, and H. Ney, “CTC in the Context of Generalized Full-Sum HMM Training,” in Proc. Interspeech, Stock- holm, Sweden, Aug. 2017, pp. 944–948.
[73] T. Raissi, W. Zhou, S. Berger, R. Schl¨uter, and H. Ney, “HMM vs. CTC for Automatic Speech Recognition: Comparison Based on Full- Sum Training from Scratch,” in Proc. IEEE SLT, Doha, Qatar, Jan. 2023, arXiv:2210.09951.
[74] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-End Speech Recognition Using Deep RNN Models and WFST-Based Decoding,” in Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 167–174. [75] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, “Deep Speech: Scaling up End-to-End Speech Recognition,” Dec. 2014, arXiv:1412.5567.
[76] L. Lu, X. Zhang, and S. Renals, “On Training the Recurrent Neural Network Encoder-Decoder for Large Vocabulary End-to-End Speech Recognition,” in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 5060–5064.
[77] J. Chorowski and N. Jaitly, “Towards Better Decoding and Language Model Integration in Sequence to Sequence Models,” in Proc. Inter- speech, Stockhol, Sweden, Aug. 2017, pp. 523–527.
[78] Y. Zhang, W. Chan, and N. Jaitly, “Very Deep Convolutional Networks for End-to-End Speech Recognition,” in Proc. IEEE ICASSP, New Orleans, LA, Mar. 2017, pp. 4845–4849.
[79] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, “Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder based Speech Recognition,” in Proc. Interspeech, Stockholm, Sweden, Aug. 2017, arXiv:1704.01631.
[80] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, “Multi- Modal Data Augmentation for End-to-End ASR,” in Proc. Interspeech, Hyderabad, India, Mar. 2018, pp. 2394–2398.
[89] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab- havalkar, E. Variani, and T. Strohman, “Cascaded Encoders for Uni- fying Streaming and Non-Streaming ASR,” in Proc. IEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5629–5633.
[90] A. Tripathi, J. Kim, Q. Zhang, H. Lu, and H. Sak, “Transformer Trans- ducer: One Model Unifying Streaming and Non-Streaming Speech Recognition,” Oct. 2020, arXiv:2010.03192.
[91] J. Yu, W. Han, A. Gulati, C.-C. Chiu, B. Li, T. N. Sainath, Y. Wu, and R. Pang, “Universal ASR: Unify and Improve Streaming ASR with Full-Context Modeling,” Oct. 2020, arXiv:2010.06030.
[92] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, and R. Pang, “Shallow-Fusion End-to-End Contextual Biasing,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1418–1422.
[93] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao, “Deep Context: End-to-end Contextual Speech Recognition,” in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 418–425.
[94] S. Kim and F. Metze, “Dialog-Context Aware End-to-End Speech Recognition,” in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 434– 440.
[95] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, “Phoebe: Pronunciation-Aware Contextualization for End-to-End Speech Recog- nition,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6171– 6175.
[96] M. Delcroix, S. Watanabe, A. Ogawa, S. Karita, and T. Nakatani, “Auxiliary Feature Based Adaptation of End-to-End ASR Systems,” in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 2444–2448.
[97] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu, “ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 3610–3614. [98] L. Dong, S. Xu, and B. Xu, “Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5884–5888. [99] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, “Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7829–7833.
[100] C.-F. Yeh, J. Mahadeokar, K. Kalgaonkar, Y. Wang, D. Le, M. Jain, K. Schubert, C. Fuegen, and M. L. Seltzer, “Transformer-Transducer: End-to-Snd Speech Recognition with Self-Attention,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 7829–7833.
[101] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, “Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding,” in Proc. ICML. Baltimore, MD: PMLR, Jul. 2022, pp. 17 627–17 643.
[81] S. Sabour, W. Chan, and M. Norouzi, “Optimal Completion Distillation for Sequence Learning,” in Proc. ICLR, New Orleans, LA, May 2019, arXiv:1810.01398.
[82] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu, “Improving Attention Based Sequence-to-Sequence Models for End-to- End English Conversational Speech Recognition,” in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 761–765.
[83] D. Le, X. Zhang, W. Zheng, C. F¨ugen, G. Zweig, and M. L. Seltzer, “From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid Speech Recognition,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 457–464.
[84] S. Kanthak and H. Ney, “Context-Dependent Acoustic Modeling Using Graphemes for Large Vocabulary Speech Recognition,” in Proc. IEEE ICASSP, Orlando, FL, May 2002, pp. 845–848.
[85] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, A. Borchers et al., “In-Datacenter Performance Analysis of a Tensor Processing Unit,” in Proc. of the 44th Annual International Symposium on Computer Architecture, Toronto, Ontario, Canada, Jun. 2017, pp. 1–12.
[86] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid CTC Attention Architecture for End-to-End Speech Recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[87] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li, M. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C. Chung- Cheng, “Two-Pass End-to-End Speech Recognition,” in Proc. Inter- speech, Brighton, UK, May 2019, pp. 2773–2777.
[88] K. Hu, T. N. Sainath, R. Pang, and R. Prabhavalkar, “Deliberation Model Based Two-Pass End-to-End Speech Recognition,” in Proc. IEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 7799–7803.
[102] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang, “A Comparative Study on Transformer vs RNN in Speech Applications,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 449–456.
[103] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. Garcia-Romero, J. Shi, J. Shi, S. Watanabe, K. Wei, W. Zhang, and Y. Zhang, “Recent Developments on ESPNET Toolkit Boosted by Conformer,” in Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5874–5878.
[104] R. Botros, T. Sainath, R. David, E. Guzman, W. Li, and Y. He, “Tied & Reduced RNN-T Decoder,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 4563–4567.
[105] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “RNN- Transducer with Stateless Prediction Network,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7049–7053.
[106] W. Zhou, S. Berger, R. Schl¨uter, and H. Ney, “Phoneme Based Neural Transducer for Large Vocabulary Speech Recognition,” in Proc. IEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5644–5648. [107] R. Prabhavalkar, Y. He, D. Rybach, S. Campbell, A. Narayanan, T. Strohman, and T. N. Sainath, “Less is More: Improved RNN-T Decoding Using Limited Label Context and Path Merging,” in Proc. IEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5659–5663. [108] X. Chen, Z. Meng, S. Parthasarathy, and J. Li, “Factorized Neural Transducer for Efficient Language Model Adaptation,” in Proc. IEEE ICASSP, Singapore, May 2022, pp. 8132–8136, arXiv:2110.01500.
[109] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi, J. Emond, T. Strohman, B. Ramabhadran, W. R. Huang et al., “Modular Hybrid Autoregressive Transducer,” in Proc. IEEE SLT, Doha, Qatar, Jan. 2023, pp. 197–204, https://arXiv:2210.17049.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
22


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[110] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and F. Wei, “VioLA: Unified Codec Language Models for Speech Recog- nition, Synthesis, and Translation,” May 2023, arXiv:2305.16107.
[111] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Bor- sos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov et al., “AudioPaLM: A Large Language Model That Can Speak and Listen,” Jun. 2023, arXiv:2306.12925.
[112] S.-Y. Chang, B. Li, and G. Simko, “A Unified Endpointer Using Multitask and Multidomain Training,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 100–106.
[113] B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and Y. Wu, “Towards Fast and Accurate Streaming End-To-End ASR,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6069–6073.
[114] T. Yoshimura, T. Hayashi, K. Takeda, and S. Watanabe, “End-To- End Automatic Speech Recognition Integrated with CTC-Based Voice Activity Detection,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6999–7003.
[115] Y. Fujita, T. Wang, S. Watanabe, and M. Omachi, “Toward Stream- ing ASR with Non-Autoregressive Insertion-Based Model,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3740–3744.
[135] G. Heigold, R. Schl¨uter, H. Ney, and S. Wiesler, “Discriminative Training for Automatic Speech Recognition: Modeling, Criteria, Opti- mization, Implementation, and Performance,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 58–69, Nov. 2012.
[136] W. Michel, R. Schl¨uter, and H. Ney, “Comparison of Lattice-Free and Lattice-Based Sequence Discriminative Training Criteria for LVCSR,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 1601–1605, arXiv:1907.01409.
[137] R. Prabhavalkar, T. N. Sainath, Y. Wu, P. Nguyen, Z. Chen, C.-C. Chiu, and A. Kannan, “Minimum Word Error Rate Training for Attention- Based Sequence-to-Sequence Models,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 4839–4843.
[138] C. Weng, C. Yu, J. Cui, C. Zhang, and D. Yu, “Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 966–970, DOI: 10.21437/Interspeech.2020-1221.
[139] M. K. Baskar, L. Burget, S. Watanabe, M. Karafi´at, T. Hori, and J. H. ˇCernock`y, “Promising Accurate Prefix Boosting for Sequence- to-Sequence ASR,” in Proc. IEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 5646–5650.
[116] Y. Bengio, “Practical Recommendations for Gradient-Based Training
of Deep Architectures,” Jun. 2012, arXiv:1206.5533.
[117] J. Schmidhuber, “Deep Learning in Neural Networks: An Overview,”
[140] A. Tjandra, S. Sakti, and S. Nakamura, “Sequence-to-Sequence ASR Optimization via Reinforcement Learning,” in Proc. IEEE ICASSP. Calgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5829–5833.
Neural Networks, vol. 61, pp. 85–117, Jan. 2015, arXiv:1404.7828.
[118] L. Baum, “An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistic Functions of Markov Processes,” Inequalities, vol. 3, pp. 1–8, 1972.
[141] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, “Sequence Training of Encoder-Decoder Model Using Policy Gradient for End-to-End Speech Recognition,” in Proc. IEEE ICASSP. Calgary, Alberta, Canada: IEEE, Apr. 2018, pp. 5839–5843.
[119] L. Rabiner and B.-H. Juang, “An Introduction to Hidden Markov Mod- els,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 3, no. 1, pp. 4–16, 1986.
[120] Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, “Neural Network- Gaussian Mixture Hybrid for Speech Recognition or Density Estima- tion,” in Proc. NIPS, vol. 4, Colorado, Dec. 1991, pp. 175–182.
[142] W. Michel, R. Schl¨uter, and H. Ney, “Early Stage LM Integration Using Local and Global Log-Linear Combination,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 3605–3609, arXiv:2005.10049. [143] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A Fast Learning Algorithm for Deep Belief Nets,” Neural Computation, vol. 18, no. 7, pp. 1527– 1554, Jul. 2006.
[121] R. E. Bellman, Dynamic Programming.
Princeton, NJ: Princeton
University Press, 1957.
[122] A. Viterbi, “Error Bounds for Convolutional Codes and an Asymptoti- cally Optimal Decoding Algorithm,” IEEE Transactions on Information Theory, vol. 13, pp. 260–269, 1967.
[123] H. Ney, “The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 263–271, 1984.
[124] W. Zhou, W. Michel, R. Schl¨uter, and H. Ney, “Efficient Training of Neural Transducer for Speech Recognition,” in Proc. Interspeech, Incheon, Korea, Sep. 2022, arXiv:2204.10586.
[144] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy Layer- Wise Training of Deep Networks,” in Proc. NIPS, Barcelona, Spain, Dec. 2006, pp. 153–160.
[145] A. Zeyer, P. Doetsch, P. Voigtlaender, R. Schl¨uter, and H. Ney, “A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic Modeling in Speech Recognition,” in Proc. IEEE ICASSP, New Orleans, LA, Mar. 2017, pp. 2462–2466.
[146] A. Zeyer, T. Alkhouli, and H. Ney, “RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recogni- tion,” in Proc. ACL, Melbourne, Australia, Jul. 2018, pp. 128–133.
[125] A. Zeyer, R. Schl¨uter, and H. Ney, “Why does CTC Result in Peaky
Behavior?” May 2021, arXiv:2105.14849.
[126] A. Laptev, S. Majumdar, and B. Ginsburg, “CTC Variations Through New WFST Topologies,” in Proc. Interspeech, Incheon, Korea, sep 2022, DOI: 10.21437/interspeech.2022-10854.
[127] X. He, L. Deng, and W. Chou, “Discriminative Learning in Sequential Pattern Recognition – A Unifying Review for Optimization-Oriented Speech Recognition,” IEEE Signal Processing Magazine, vol. 25, no. 5, pp. 14–36, 2008.
[128] M. Zeineldeen, A. Glushko, W. Michel, A. Zeyer, R. Schl¨uter, and H. Ney, “Investigating Methods to Improve Language Model Inte- gration for Attention-Based Encoder-Decoder ASR Models,” in Proc. Interspeech, Brno, Czechia, Aug. 2021, pp. 2856–2860.
[129] N.-P. Wynands, W. Michel, J. Rosendahl, R. Schl¨uter, and H. Ney, “Efficient Sequence Training of Attention Models using Approxima- tive Recombination,” in Proc. IEEE ICASSP, Singapore, May 2022, arXiv:2110.09245.
[130] Z. Yang, W. Zhou, R. Schl¨uter, and H. Ney, “Lattice-Free Sequence Discriminative Training for Phoneme-based Neural Transducers,” in Proc. IEEE ICASSP, Rhodes, Greece, Jun. 2023, arXiv:2212.04325.
[131] V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young, “MMIE Training of Large Vocabulary Recognition Systems,” Speech Commu- nication, vol. 22, no. 4, pp. 303–314, 1997.
[147] A. Zeyer, K. Irie, R. Schl¨uter, and H. Ney, “Improved Training of End-to-End Attention Models for Speech Recognition,” in Proc. Interspeech, Hyderabad, India, Sep. 2018, pp. 7–11.
[148] A. Zeyer, A. Merboldt, R. Schl¨uter, and H. Ney, “A Comprehensive Analysis on Attention Models,” in Proc. NIPS, Montreal, Canada, Dec. 2018.
[149] Y. Chung, C. Wu, C. Shen, H. Lee, and L. Lee, “Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder,” in Proc. Interspeech, San Fran- cisco, CA, Sep. 2016, arXiv:1603.00982.
[150] Y.-C. Chen, S.-F. Huang, H.-y. Lee, Y.-H. Wang, and C.-H. Shen, “Au- dio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 9, pp. 1481–1493, 2019, DOI: 10.1109/TASLP.2019.2922832.
[151] S. Scanzio, P. Laface, L. Fissore, R. Gemello, and F. Mana, “On the Use of a Multilingual Neural Network Front-End,” in Proc. Interspeech, Brisbane, Australia, Sep. 2008, pp. 2711–2714.
[152] Z. T¨uske, J. Pinto, D. Willett, and R. Schl¨uter, “Investigation on Cross- and Multilingual MLP features under matched and mismatched acoustical conditions,” in IEEE International Conference on Acoustics, Speech, and Signal Processing, Vancouver, Canada, May 2013, pp. 7349–7353.
[132] D. Povey and P. Woodland, “Improved Discriminative Training Tech- niques for Large Vocabulary Continuous Speech Recognition,” in Proc. IEEE ICASSP, Salt Lake City, UT, May 2001, pp. 45–48.
[153] S. Zhou, S. Xu, and B. Xu, “Multilingual End-to-End Speech Recog- nition with a Single Transformer on Low-Resource Languages,” Jun. 2018, arXiv:1806.05059.
[133] R. Schl¨uter, W. Macherey, B. M¨uller, and H. Ney, “Comparison of Discriminative Training Criteria and Optimization Methods for Speech Recognition,” Speech Communication, vol. 34, no. 3, pp. 287–310, May 2001, EURASIP Best Paper Award.
[134] B. Kingsbury, “Lattice-Based Optimization of Sequence Classifica- tion Criteria for Neural-Network Acoustic Modeling,” in Proc. IEEE ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3761–3764.
[154] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, “Massively Multilingual Adversarial Speech Recognition,” in Proc. NAACL-HLT, Minneapolis, MN, Jun. 2019, arXiv:1904.02210.
[155] W. Hou, Y. Dong, B. Zhuang, L. Yang, J. Shi, and T. Shinozaki, “Large-scale end-to-end multilingual speech recognition and language identification with multi-task learning,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1037–1041.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
23


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[156] V. Pratap, A. Sriram, P. Tomasello, A. Hannun, V. Liptchinsky, G. Syn- naeve, and R. Collobert, “Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters,” in Proc. Interspeech, Shanghai, China, Oct. 2020, arXiv:2007.03001.
[176] B. Polyak, “Some Methods of Speeding up the Convergence of Iteration Methods,” USSR Computational Mathematics and Mathe- matical Physics, vol. 4, no. 5, pp. 1–17, 1964, DOI: 10.1016/0041- 5553(64)90137-5.
[157] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, M. Ma, and J. Bai, “Scaling End-to-End Models for Large-Scale Multilingual ASR,” in Proc. IEEE ASRU, 2021, pp. 1011– 1018.
[158] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li, M. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen, Q. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, “BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1519–1532, oct 2022, arXiv:2109.13226.
[177] Y. Nesterov, “A method of solving a convex programming problem k2 ),” Soviet Mathematics Doklady, vol. 27,
with convergence rate O( 1 pp. 372–376, 1983.
[178] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the Importance of Initialization and Momentum in Deep Learning,” in Proc. ICML, Atlanta, GA, Jun. 2013, pp. 1139–1147.
[179] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimiza-
tion,” in Proc. ICLR, San Diego, CA, May 2015, arXiv:1412.6980.
[180] Z. T¨uske, G. Saon, and B. Kingsbury, “On the Limit of English Con- versational Speech Recognition,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 2062–2066.
[159] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno, A. Bapna, and H. Zen, “MAESTRO: Matched Speech Text Repre- sentations through Modality Matching,” in Proc. Interspeech, Incheon, South Korea, Sep. 2022, arXiv:2204.03409.
[160] A. Radford, J. W. Kim, C. McLeavey, P. Mishkin, T. Xu, G. Brockman, and I. Sutskever, “Introducing Whisper - Robust Speech Recognition via Large-Scale Weak Supervision,” Sep. 2022. [Online]. Available: https://openai.com/blog/whisper/
[161] T. P. Vogl, J. Mangis, A. Rigler, W. Zink, and D. Alkon, “Acceler- ating the Convergence of the Back-Propagation Method,” Biological Cybernetics, vol. 59, no. 4, pp. 257–263, 1988.
[181] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever, “Deep Double Descent: Where Bigger Models and More Data Hurt,” in Proc. ICLR, virtual, Apr. 2020, arXiv:1912.02292.
[182] A. Krogh and J. Hertz, “A Simple Weight Decay Can Improve Generalization,” in Neural Information Processing Systems (NIPS), Denver, CO, Dec. 1991, pp. 950–957.
[183] A. F. Murray and P. J. Edwards, “Enhanced MLP Performance and Fault Tolerance Resulting from Synaptic Weight Noise during Train- ing,” IEEE Transactions on Neural Networks, vol. 5, no. 5, pp. 792– 802, Sep. 1994.
[184] A. Graves, “Practical Variational Inference for Neural Networks,”
[162] N. S. Keskar and G. Saon, “A Nonmonotone Learning Rate Strategy for SGD Training of Deep Neural Networks,” in Proc. IEEE ICASSP. Queensland, Australia: IEEE, Apr. 2015, pp. 4974–4978.
[163] S. Renals, N. Morgan, H. Bourlard, C. Wooters, and P. Kohn, “Connec- tionist Speech Recognition: Status and Prospects,” ICSI, 1991, Tech. Rep. TR-OI-070.
[164] D. Johnson, D. Ellis, C. Oei, C. Wooters, and P. Faerber, “QuickNet,” ICSI, Berkeley, 2004. [Online]. Available: http://www.icsi.berkeley. edu/Speech/qn.html
[165] A. Senior, G. Heigold, M. Ranzato, and K. Yang, “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition,” in Proc. IEEE ICASSP. Vancouver, BC, Canada: IEEE, May 2013, pp. 6724–6728.
[166] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regularization,” in Proc. ICLR, New Orleans, LA, May 2019, arXiv:1711.05101. [167] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le, “Don’t Decay the Learning Rate, Increase the Batch Size,” in Proc. ICLR, New Orleans, LA, May 2018, arXiv:1711.00489.
[168] J. Howard and S. Ruder, “Universal Language Model Fine-Tuning for Text Classification,” in Proc. ACL, Melbourne, Australia, Jun. 2018, pp. 328–339.
[169] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Donahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan, C. Fer- nando, and K. Kavukcuoglu, “Population Based Training of Neural Networks,” Nov. 2017, arXiv:1711.09846.
[170] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta- Learning in Neural Networks: A Survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PP, pp. 1–20, 2021.
[171] J. L. Elman, “Learning and Development in Neural Networks: The Importance of Starting Small,” Cognition, vol. 48, no. 1, pp. 71–99, 1993, DOI: 10.1016/0010-0277(93)90058-4.
[172] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum Learning,” in Proc. ICML, Montreal, Quebec, Canada, Jun. 2009, p. 41–48.
[173] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catan- zaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. Engel, L. Fan, C. Fougner, T. Han, A. Hannun, B. Jun, P. LeGresley, L. Lin, S. Narang, A. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao, D. Yogatama, J. Zhan, and Z. Zhu, “Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,” in Proc. ICML, New York City, NY, Jun. 2016, pp. 173–182.
[174] Z. T¨uske, G. Saon, K. Audhkhasi, and B. Kingsbury, “Single Headed Attention Based Sequence-to-Sequence Model for State-of-the-Art Results on Switchboard,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 551–555.
[175] W. Zhang, X. Chang, Y. Qian, and S. Watanabe, “Improving End- to-End Single-Channel Multi-Talker Speech Recognition,” IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 28, pp. 1385– 1394, 2020.
Advances in Neural Information Processing Systems, vol. 24, 2011.
[185] A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens, “Adding Gradient Noise Improves Learning for Very Deep Networks,” Nov. 2015, arXiv:1511.06807.
[186] G. E. Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. R. Salakhutdinov, “Improving Neural Networks by Preventing Co- Adaptation of Feature Detectors,” Jul. 2012, arXiv:1207.0580. [187] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” in Advances in Neural Information Processing Systems (NIPS), vol. 25, Lake Tahoe, NV, Dec. 2012.
[188] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,” in Proc. ICML, New York City, NY, Jun. 2016, pp. 1050–1059.
[189] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep Net- works with Stochastic Depth,” in European Conference on Computer Vision, Amsterdam, Netherlands, Oct. 2016, pp. 646–661.
[190] N.-Q. Pham, T.-S. Nguyen, J. Niehues, M. M¨uller, and A. Waibel, “Very Deep Self-Attention Networks for End-to-End Speech Recogni- tion,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 66–70. [191] J. Lee and S. Watanabe, “Intermediate Loss Regularization for CTC- Based Speech Recognition,” in Proc. IEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 6224–6228.
[192] L. Wan, M. Zeiler, S. Zhang, Y. Le Cun, and R. Fergus, “Regularization of Neural Networks using DropConnect,” in Proc. ICML, 2013, pp. 1058–1066.
[193] D. Krueger, T. Maharaj, J. Kram´ar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, A. Courville, and C. Pal, “Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,” in Proc. ICLR, Toulon, France, Apr. 2017, arXiv:1606.01305.
[194] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink- ing the Inception Architecture for Computer Vision,” in IEEE Conf. on Computer Vision and Pattern Recognition, Las Vegas, NV, Jun. 2016, pp. 2818–2826.
[195] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,” Proc. NIPS, vol. 28, Dec. 2015.
[196] T. Trinh, A. Dai, T. Luong, and Q. Le, “Learning Longer-Term Depen- dencies in RNNs with Auxiliary Losses,” in Proc. ICML, Stockholm, Sweden, Jul. 2018, pp. 4965–4974.
[197] R. J. Williams and J. Peng, “An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories,” IEEE Neural Computation, vol. 2, no. 4, pp. 490–501, 1990.
[198] S. Merity, N. S. Keskar, and R. Socher, “An Analysis of Neural Language Modeling at Multiple Scales,” Mar. 2018, arXiv:1803.08240. [199] L. Meng, J. Xu, X. Tan, J. Wang, T. Qin, and B. Xu, “MixSpeech: Data Augmentation for Low-resource Automatic Speech Recognition,” in Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 7008–7012.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
24


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[200] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” in Proc. ICML, Lille, France, Jul. 2015, pp. 448–456.
[201] N. Kanda, R. Takeda, and Y. Obuchi, “Elastic Spectral Distortion for Low Resource Speech Recognition with Deep Neural Networks,” in Proc. IEEE ASRU, Olomouc, Czech Republic, Dec. 2013, pp. 309– 314.
[202] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio Augmentation for Speech Recognition,” in Proc. Interspeech, Dresden, Germany, Sep. 2015.
[203] N. Jaitly and G. E. Hinton, “Vocal Tract Length Perturbation (VTLP) Improves Speech Recognition,” in Proc. ICML, vol. 117, Jun. 2013, p. 21.
Label Units for Encoder-Decoder-Attention Models,” Nov. 2020, arXiv:2005.09336. [222] C. L¨uscher, E. Beck, K.
Irie, M. Kitza, W. Michel, A. Zeyer, R. Schl¨uter, and H. Ney, “RWTH ASR Systems for LibriSpeech: Hybrid vs Attention,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 231–235.
[223] D. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. Le, “Improved Noisy Student Training for Automatic Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 2817–2821.
[224] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 2613–2617.
[204] G. Saon, Z. T¨uske, K. Audhkhasi, and B. Kingsbury, “Sequence Noise Injected Training for End-to-End Speech Recognition,” in Proc. IEEE ICASSP, Brighton, England, May 2019, pp. 6261–6265.
[205] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu, “SpecAugment on Large Scale Datasets,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6879–6883.
[206] C. Wang, Y. Wu, Y. Du, J. Li, S. Liu, L. Lu, S. Ren, G. Ye, S. Zhao, and M. Zhou, “Semantic Mask for Transformer Based End-to-End Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 971–975.
[225] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schl¨uter, and H. Ney, “The RWTH ASR System for TED-LIUM Release 2: Improving Hybrid HMM with SpecAugment,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7839–7843.
[226] J. Cui, B. Kingsbury, B. Ramabhadran, A. Sethy, K. Audhkhasi, X. Cui, E. Kislal, L. Mangu, M. Nussbaum-Thom, M. Picheny, Z. T¨uske, P. Golik, R. Schl¨uter, H. Ney, M. J. F. Gales, K. M. Knill, A. Ragni, H. Wang, and P. Woodland, “Multilingual Representations for Low Resource Speech Recognition and Keyword Search,” in Proc. IEEE ASRU, Scottsdale, AZ, Dec. 2015, pp. 259–266.
[207] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo, and K. Takeda, “Back-Translation-Style Data Augmentation for End- to-End ASR,” in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018, pp. 426–433.
[208] N. Rossenbach, M. Zeineldeen, B. Hilmes, R. Schl¨uter, and H. Ney, “Comparing the Benefit of Synthetic Training Data for Various Au- tomatic Speech Recognition Architectures,” in Proc. IEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2104.05379.
[209] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan, D. Rybach, V. Schogol, P. Nguyen, B. Li, Y. Wu, Z. Chen, and C.-C. Chiu, “No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5859–5863, DOI: 10.1109/ICASSP.2018.8462380.
[210] C. Wooters and A. Stolcke, “Multiple-Pronunciation Lexical Modeling in a Speaker Independent Speech Understanding System,” in Proc. ICSLP, Yokohama, Japan, Sep. 1994, pp. 1363–1366.
[211] I. McGraw, I. Badr, and J. R. Glass, “Learning Lexicons From Speech Using a Pronunciation Mixture Model,” IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 21, no. 2, pp. 357–366, 2012. [212] A. Senior, G. Heigold, M. Bacchiani, and H. Liao, “GMM-Free DNN Acoustic Model Training,” in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 5602–5606, DOI: 10.1109/ICASSP.2014.6854675.
[213] G. Gosztolya, T. Gr´osz, and L. T´oth, “GMM-Free Flat Start Sequence- Discriminative DNN Training,” in Proc. Interspeech, N. Morgan, Ed. San Francisco, CA: ISCA, Sep. 2016, pp. 3409–3413, DOI: 10.21437/Interspeech.2016-391.
[214] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, “Flat-Start Single-Stage Discriminatively Trained HMM-Based Models for ASR,” IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 26, no. 11, pp. 1949–1961, 2018.
[215] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and G. Zweig, “The IBM 2004 Conversational Telephony System for Rich Transcrip- tion,” in Proc. IEEE ICASSP, Philadelphia, PA, Mar. 2005, pp. 205– 208.
[216] H. Hadian, D. Povey, H. Sameti, J. Trmal, and S. Khudanpur, “Improving LF-MMI Using Unconstrained Supervisions for ASR,” in Proc. IEEE SLT, Athens, Greece, Dec. 2018, pp. 43–47, DOI: 10.1109/SLT.2018.8639684.
[217] N. Kanda, Y. Fujita, and K. Nagamatsu, “Lattice-Free State-Level Min- imum Bayes Risk Training of Acoustic Models,” in Proc. Interspeech, B. Yegnanarayana, Ed. Hyderabad, India: ISCA, Sep. 2018, pp. 2923– 2927, DOI: 10.21437/Interspeech.2018-79.
[227] O. Adams, M. Wiesner, S. Watanabe, and D. Yarowsky, “Massively Multilingual Adversarial Speech Recognition,” in Proc. NAACL, Min- neapolis, MN, Jun. 2019, pp. 96–108.
[228] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. Ramabhadran, Y. Wu, A. Bapna, Z. Chen, and S. Lee, “Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 2130–2134.
[229] A. Graves, “Connectionist Temporal Classification,” in Supervised Sequence Labelling with Recurrent Neural Networks. Heidelberg, Germany: Springer, 2012, ch. Connectionist Temporal Classification, pp. 61–93.
[230] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 3655–3659.
[231] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, “Imputer: Sequence Modelling via Imputation and Dynamic Programming,” in Proc. ICML. PMLR, Jul. 2020, pp. 1403–1413.
[232] Y. Fujita, S. Watanabe, M. Omachi, and X. Chang, “Insertion-Based Modeling for End-to-End Automatic Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 3660–3664.
[233] L. Dong and B. Xu, “Cif: Continuous Integrate-and-Fire for End-to- End Speech Recognition,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6079–6083.
[234] J. Nozaki and T. Komatsu, “Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3735– 3739.
[235] Y. Higuchi, N. Chen, Y. Fujita, H. Inaguma, T. Komatsu, J. Lee, J. Nozaki, T. Wang, and S. Watanabe, “A Comparative Study on Non- Autoregressive Modelings for Speech-to-Text Generation,” in Proc. IEEE ASRU, Cartagena, Colombia, Dec. 2021, arXiv:2110.05249.
[236] W. Zhou, R. Schl¨uter, and H. Ney, “Robust Beam Search for Encoder- Decoder Attention Based Speech Recognition without Length Bias,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 1768–1772.
[237] P. Koehn and R. Knowles, “Six Challenges for Neural Machine Transla- tion,” in First Workshop on Neural Machine Translation. Vancouver, BC, Canada: Association for Computational Linguistics, Aug. 2017, pp. 28–39.
[238] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, “Modeling Coverage for Neural Machine Translation,” in Proc. ACL, Berlin, Germany, May 2016, pp. 76–85.
[218] S. J. Young and P. C. Woodland, “The Use of State Tying in Continuous Speech Recognition,” in Proc. Eurospeech, Berlin, Germany, Dec. 1993, pp. 2203–2206.
[239] T. Hori, J. Cho, and S. Watanabe, “End-to-End Speech Recogni- tion with Word-Based RNN Language Models,” in Proc. IEEE SLT. Athens, Greece: IEEE, Dec. 2018, pp. 389–396.
[219] S. Wiesler, G. Heigold, M. Nußbaum-Thom, R. Schl¨uter, and H. Ney, “A Discriminative Splitting Criterion for Phonetic Decision Trees,” in Proc. Interspeech, Makuhari, Japan, Sep. 2010, pp. 54–57, one of shortlist for Best Student Paper Award.
[220] T. Raissi, E. Beck, R. Schl¨uter, and H. Ney, “Towards Consistent
[240] K. Deng and P. C. Woodland, “Label-Synchronous Neural Transducer
for End-to-End ASR,” Jul. 2023, arXiv:2307.03088.
[241] T. Hori and A. Nakamura, Speech Recognition Algorithms Using San Rafael, CA: Morgan &
Weighted Finite-State Transducers. Claypool Publishers, 2013.
Hybrid HMM Acoustic Modeling,” Apr. 2021, arXiv:2104.02387.
[221] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schl¨uter, and H. Ney, “A Systematic Comparison of Grapheme-Based vs. Phoneme-Based
[242] R. Haeb-Umbach and H. Ney, “Improvements in Beam Search for 10000-Word Continuous-Speech Recognition,” IEEE Transactions on Speech and Audio Processing, vol. 2, no. 2, pp. 353–356, 1994.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
25


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[243] H. Ney and S. Ortmanns, “Progress in Dynamic Programming Search for LVCSR,” Proc. of the IEEE, vol. 88, no. 8, pp. 1224–1240, Aug. 2000, http://dx.doi.org/10.1109/5.880081.
[244] T. Hori, Y. Kubo, and A. Nakamura, “Real-Time One-Pass Decoding with Recurrent Neural Network Language Model for Speech Recog- nition,” in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6364– 6368.
[265] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen, L. Xie, and X. Lei, “WeNet: Production Oriented Streaming and Non- Streaming End-to-End Speech Recognition Toolkit,” Brno, Czechia, pp. 4054–4058, Sep. 2021.
[266] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei, “U2++: Unified Two-Pass Bidirectional End-to-End Model for Speech Recognition,” Dec. 2021, arXiv:2106.05642.
[245] E. Beck, W. Zhou, R. Schl¨uter, and H. Ney, “LSTM Language Models for LVCSR in First-Pass Decoding and Lattice-Rescoring,” Jul. 2019, arXiv:1907.01030.
[267] M. Zapotoczny, P. Pietrzak, A. Lancucki, and J. Chorowski, “Lattice Generation in Attention-Based Speech Recognition Models,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 2225–2229.
[246] G. Saon, Z. T¨uske, and K. Audhkhasi, “Alignment-Length Syn- chronous Decoding for RNN Transducer,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7804–7808.
[268] J. Kim, Y. Lee, and E. Kim, “Accelerating RNN Transducer Inference via Adaptive Expansion Search,” IEEE Signal Processing Letters, vol. 27, pp. 2019–2023, 2020.
[247] A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng, “First- Pass Large Vocabulary Continuous Speech Recognition Using Bi- Directional Recurrent DNNs,” Dec. 2014, arXiv:1408.2873.
[248] N. Moritz, T. Hori, and J. Le Roux, “Triggered Attention for End-to- Brighton, UK:
End Speech Recognition,” in Proc. IEEE ICASSP. IEEE, May 2019, pp. 5666–5670.
[249] N. Moritz, T. Hori, and J. Le, “Streaming Automatic Speech Recogni- tion with the Transformer Model,” in Proc. IEEE ICASSP. Barcelona, Spain: IEEE, May 2020, pp. 6074–6078.
[250] M. Jain, K. Schubert, J. Mahadeokar, C.-F. Yeh, K. Kalgaonkar, A. Sri- ram, C. Fuegen, and M. L. Seltzer, “RNN-T for Latency Controlled ASR with Improved Beam Search,” Nov. 2019, arXiv:1911.01629.
[269] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: A system for Large- Scale Machine Learning,” in Proc. OSDI, Savannah, GA, Nov. 2016, pp. 265–283.
[270] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, “FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling,” in Proc. NAACL, Minneapolis, MN, Jun. 2019, pp. 48–53. [271] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan, T. Sainath, Y. Cao, C.-C. Chiu et al., “Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling,” Feb. 2019, arXiv:1902.08295.
[251] L. Lu, C. Liu, J. Li, and Y. Gong, “Exploring Transformers for Large- Scale Speech Recognition,” in Proc. Interspeech, Shanghai, China, Oct. 2020, pp. 5041–5045.
[252] T. Wang, Y. Fujita, X. Chang, and S. Watanabe, “Streaming End-to- End ASR Based on Blockwise Non-Autoregressive Models,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3755–3759.
[253] H. Miao, G. Cheng, P. Zhang, T. Li, and Y. Yan, “Online Hybrid CTC/Attention Architecture for End-to-End Speech Recognition,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 2623–2627, DOI: 10.21437/Interspeech.2019-2018.
[272] P. Doetsch, A. Zeyer, P. Voigtlaender, I. Kulikov, R. Schl¨uter, and H. Ney, “RETURNN: The RWTH Extensible Training Framework for Universal Recurrent Neural Networks,” in Proc. IEEE ICASSP. New Orleans, LA: IEEE, Mar. 2017, pp. 5345–5349.
[273] A. Hannun, A. Lee, Q. Xu, and R. Collobert, “Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3785–3789.
[274] M. Li, M. Liu, and H. Masanori, “End-to-End Speech Recognition with Adaptive Computation Steps,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6246–6250.
[254] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming Transformer ASR with Blockwise Synchronous Beam Search,” in Proc. IEEE SLT. Shenzhen, China: IEEE, Jun. 2021, pp. 22–29.
[255] K. Hwang and W. Sung, “Character-level language modeling with hierarchical recurrent neural networks,” in Proc. IEEE ICASSP. New Orleans, LA: IEEE, Mar. 2017, pp. 5720–5724.
[256] T. Hori, S. Watanabe, Y. Zhang, and W. Chan, “Advances in Joint CTC- Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,” in Proc. Interspeech, Stockhol, Sweden, Aug. 2017, pp. 949–953.
[257] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and R. Prabhavalkar, “An Analysis of Incorporating an External Lan- guage Model into a Sequence-to-Sequence Model,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 5824–5828, DOI: 10.1109/ICASSP.2018.8462682.
[275] P. Bahar, N. Makarov, A. Zeyer, R. Sch¨uter, and H. Ney, “Exploring a Zero-Order Direct HMM Based on Latent Attention for Automatic Speech Recognition,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7854–7858.
[276] Z. Huang, G. Zweig, and B. Dumoulin, “Cache Based Recurrent Neural Network Language Model Inference for First Pass Speech Recognition,” in Proc. IEEE ICASSP, Florence, Italy, May 2014, pp. 6354–6358.
[277] J. Jorge, A. Gim´enez, J. Iranzo-S´anchez, J. Civera, A. Sanchis, and A. Juan, “Real-Time One-Pass Decoder for Speech Recognition Using LSTM Language Models,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3820–3824.
[278] W. Zhou, R. Schl¨uter, and H. Ney, “Full-Sum Decoding for Hybrid HMM Based Speech Recognition Using LSTM Language Model,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7834–7838.
[258] G. Saon, Z. T¨uske, D. Bolanos, and B. Kingsbury, “Advancing RNN Transducer Technology for Speech Recognition,” in Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5654–5658, arXiv:2103.09935.
[259] H. Seki, T. Hori, S. Watanabe, N. Moritz, and J. Le Roux, “Vectorized Beam Search for CTC-Attention-Based Speech Recognition,” in Proc. Interspeech, Brighton, UK, May 2019, pp. 3825–3829.
[260] T. Hori, S. Watanabe, and J. R. Hershey, “Multi-Level Language Modeling and Decoding for Open Vocabulary End-to-End Speech Recognition,” in Proc. IEEE ASRU. Okinawa, Japan: IEEE, Dec. 2017, pp. 287–293.
[261] Y. Wang, T. Chen, H. Xu, S. Ding, H. Lv, Y. Shao, N. Peng, L. Xie, S. Watanabe, and S. Khudanpur, “Espresso: A Fast End-to-End Neural Speech Recognition Toolkit,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 136–143.
[279] P. Sountsov and S. Sarawagi, “Length Bias in Encoder Decoder Models and a Case for Global Conditioning,” in Proc. EMNLP, Austin, TX, Nov. 2016, pp. 1516–1525.
[280] K. Murray and D. Chiang, “Correcting Length Bias in Neural Machine Translation,” in Proc. WMT, Brussels, Belgium, Oct. 2018, pp. 212– 223.
[281] F. Stahlberg and B. Byrne, “On NMT Search Errors and Model Errors: Cat Got Your Tongue?” in Proc. EMNLP. Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 3354–3360. [282] N. Deshmukh, A. Ganapathiraju, and J. Picone, “Hierarchical Search for Large-Vocabulary Conversational Speech Recognition: Working Toward a Solution to the Decoding Problem,” IEEE Signal Processing Magazine, vol. 16, no. 5, pp. 84–107, 1999.
[283] L. Nguyen and R. Schwartz, “Single-Tree Method for Grammar- Directed Search,” in Proc. IEEE ICASSP, vol. 2, Phoenix, AZ, Mar. 1999, pp. 613–616.
[262] Z. T¨uske, K. Audhkhasi, and G. Saon, “Advancing Sequence-to- Sequence Based Speech Recognition,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3780–3784.
[263] J. Drexler and J. Glass, “Subword Regularization and Beam Search Decoding for End-to-End Automatic Speech Recognition,” in Proc. IEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 6266–6270.
[264] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li, M. Visontai, Q. Liang, T. Strohman, Y. Wu, I. McGraw, and C.-C. Chiu, “Two-Pass End-to-End Speech Recognition,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 2773–2777.
[284] L. Sarı, N. Moritz, T. Hori, and J. Le Roux, “Unsupervised Speaker Adaptation using Attention-based Speaker Memory for End-to-End ASR,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 2– 6.
[285] F. Weninger, J. Andr´es-Ferrer, X. Li, and P. Zhan, “Listen, Attend, Spell and Adapt: Speaker Adapted Sequence-to-Sequence ASR,” in Proc. Interspeech. Graz, Austria: ISCA, Sep. 2019, pp. 3805–3809. [286] Z. Meng, Y. Gaur, J. Li, and Y. Gong, “Speaker Adaptation for Attention-Based End-to-End Speech Recognition,” in Proc. Inter- speech. Graz, Austria: ISCA, Sep. 2019, pp. 241–245.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
26


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[287] N. Tomashenko and Y. Est`eve, “Evaluation of Feature-Space Speaker Adaptation for End-to-End Acoustic Models,” in Proc. LREC. Miyazaki, Japan: ELRA, May 2018, pp. 3163–3170.
[288] S. F. Chen and J. Goodman, “An Empirical Study of Smoothing Techniques for Language Modeling,” in Proc. ACL, Santa Cruz, CA, Jun. 1996, pp. 310–318.
[289] T. Mikolov, M. Karafi´at, L. Burget, J. ˇCernock`y, and S. Khudanpur, “Recurrent Neural Network Based Language Model,” in Proc. Inter- speech, Makuhari, Japan, Sep. 2010, pp. 1045–1048.
[290] M. Sundermeyer, R. Schl¨uter, and H. Ney, “LSTM Neural Networks for Language Modeling,” in Proc. Interspeech, Portland, OR, Sep. 2012, pp. 194–197.
[291] N.-Q. Pham, G. Kruszewski, and G. Boleda, “Convolutional Neural Network Language Models,” in Proc. EMNLP, Austin, TX, Nov. 2016, pp. 1153–1162.
[292] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language Modeling the 34th JMLR.
with Gated Convolutional Networks,” in Proceedings of International Conference on Machine Learning-Volume 70. org, 2017, pp. 933–941.
[293] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and R. Collobert, “Fully Convolutional Speech Recognition,” Feb. 2018, arXiv:1812.06864.
[294] T. Likhomanenko, G. Synnaeve, and R. Collobert, “Who needs words? lexicon-free speech recognition,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3915–3919, arXiv:1904.04479.
[295] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, “Character- Level Language Modeling with Deeper Self-Attention,” in Proc. AIII, vol. 33, Honolulu, Hawaii, Feb. 2019, pp. 3159–3166.
[296] K. Irie, A. Zeyer, R. Schl¨uter, and H. Ney, “Language Modeling with Deep Transformers,” in Proc. Interspeech, Graz, Austria, Sep. 2019, pp. 3905–3909.
[297] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and R. Salakhutdinov, “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,” in Proc. ACL, Florence, Italy, Jul. 2019, pp. 2978–2988.
[310] A. Zeyer, A. Merboldt, W. Michel, R. Schl¨uter, and H. Ney, “Lib- rispeech Transducer Model with Internal Language Model Prior Cor- rection,” in Proc. Interspeech, Brno, Czech Republic, Apr. 2021, pp. 2052–2056.
[311] L. R. Bahl, F. Jelinek, and R. L. Mercer, “A Maximum Likelihood Approach to Continuous Speech Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 5, no. 2, pp. 179–190, Mar. 1983.
[312] J. Makhoul and R. Schwartz, “State of the Art in Continuous Speech Recognition,” Proc. NAS, vol. 92, no. 22, pp. 9956–9963, Oct. 1995. [313] D. Klakow and J. Peters, “Testing the Correlation of Word Error Rate and Perplexity,” Speech Communication, vol. 38, no. 1, pp. 19–28, 2002.
[314] M. Sundermeyer, H. Ney, and R. Schl¨uter, “From Feedforward to Re- current LSTM Neural Networks for Language Modeling,” IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 517–529, Mar. 2015.
[315] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, “Minimum Word Error Training of Long Short-Term Memory Recurrent Neural Network Language Models for Speech Recognition,” in Proc. IEEE ICASSP, Shanghai, China, Mar. 2016, pp. 5990–5994.
[316] J. Godfrey, E. Holliman, and J. McDaniel, “SWITCHBOARD: Tele- phone Speech Corpus for Research and Development,” in Proc. IEEE ICASSP, vol. 1, San Francisco, CA, Mar. 1992, pp. 517–520 vol.1.
[317] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an ASR Corpus Based on Public Domain Audio Books,” in Proc. IEEE ICASSP, Queensland, Australia, Apr. 2015, pp. 5206–5210.
[318] A. Zeyer, P. Bahar, K. Irie, R. Schl¨uter, and H. Ney, “A Comparison of Transformer and LSTM Encoder Decoder Models for ASR,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 8–15.
[319] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,” IEEE/ACM Trans. Audio, Speech, and Language Processing, vol. 19, pp. 3451–3460, 2021.
[298] P. Werbos, “Backpropagation Through Time: What It Does and How to Do It,” Proc. of the IEEE, vol. 78, no. 10, pp. 1550–1560, 1990, DOI: 10.1109/5.58337.
[299] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, “Compressive Transformers for Long-Range Sequence Modelling,” Advances in Neural Information Processing Systems, vol. 33, pp. 6154– 6158, 2020.
[320] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, “End-to-End ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,” in Proc. ICML, Jul. 2020, arXiv:1911.08460.
[321] E. G. Ng, C.-C. Chiu, Y. Zhang, and W. Chan, “Pushing the Limits of Non-Autoregressive Speech Recognition,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 3725–2729.
[300] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin, F. Bougares, H. Schwenk, and Y. Bengio, “On Using Monolingual Corpora in Neural Machine Translation,” Jun. 2015, arXiv:1503.03535. [301] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold Fusion: Training Seq2Seq Models Together with Language Models,” in Proc. Inter- speech, Hyderabad, India, Sep. 2018, pp. 387–391.
[322] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazare, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, “Libri-Light: A Benchmark for ASR with Limited or no Supervision,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 7669–7673.
[302] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and L. Xie, “Component Fusion: Learning Replaceable Language Model Com- ponent for End-to-End Speech Recognition System,” in Proc. IEEE ICASSP. Brighton, UK: IEEE, May 2019, pp. 5361–5635.
[323] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang, C. Fuegen, G. Zweig, and M. L. Seltzer, “Transformer-Based Acoustic Modeling for Hybrid Speech Recognition,” in Proc. IEEE ICASSP, Barcelona, Spain, May 2020, pp. 6874–6878.
[303] E. McDermott, H. Sak, and E. Variani, “A Density Ratio Approach to Language Model Fusion in End-To-End Automatic Speech Recogni- tion,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 434– 441.
[324] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, “E-branchformer: Branchformer with enhanced merging for speech recognition,” in Proc. IEEE SLT, Doha, Qatar, Jan. 2023, arXiv:2210.00077.
[304] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen, R. Zhao, J. Li, and Y. Gong, “Internal Language Model Estimation for Domain-Adaptive End-to-End Speech Recognition,” in Proc. IEEE SLT, Shenzhen , China, Dec. 2020, pp. 243–250.
[305] W. Zhou, Z. Zheng, R. Schl¨uter, and H. Ney, “On Language Model Inte- gration for RNN Transducer based Speech Recognition,” in Proc. IEEE ICASSP, Singapore, May 2022, pp. 8407–8411, arXiv:2110.06841.
[306] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre- Training of Deep Bidirectional Transformers for Language Understand- ing,” in Proc. ACL, Florence, Italy, Jul. 2019, pp. 4171–4186.
[307] A. Radford,
J. Wu, R. Child, D. Luan, D. Amodei, Unsuper- [Online]. https://cdn.openai.com/better-language-models/language
and I. vised Multitask Available: models are unsupervised multitask learners.pdf
Sutskever,
“Language Models openAI
are blog.
Learners,”
2019,
[308] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, “Masked
Language Model Scoring,” in Proc. ACL, Jul. 2020, pp. 2699–2712.
[325] M. Kitza, P. Golik, R. Schl¨uter, and H. Ney, “Cumulative Adaptation for BLSTM Acoustic Models,” in Interspeech, Graz, Austria, Sep. 2019, pp. 754–758.
[326] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. Li, J. Chorowski, and M. Bacchiani, “State-of-the-Art Speech Recognition with Sequence-to-Sequence Models,” in Proc. IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018, pp. 4774–4778.
[327] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo, D. Kim, S. Jung, J. Lee, M. Han, and C. Kim, “Attention Based On- Device Streaming Speech Recognition with Large Speech Corpus,” in Proc. IEEE ASRU, Sentosa, Singapore, Dec. 2019, pp. 956–963. [328] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov, Z. Wang, L. He, S. Zhao et al., “Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability,” in Proc. Interspeech, Shanghai, China (virtual), Oct. 2020, pp. 3590– 3594, arXiv:2007.15188.
[309] S. Kim, S. Dalmia, and F. Metze, “Gated Embeddings in End-to-End Speech Recognition for Conversational-Context Fusion,” in Proc. ACL, Florence, Italy, Jul. 2019, pp. 1131–1141.
[329] R. Hsiao, D. Can, T. Ng, R. Travadi, and A. Ghoshal, “Online Automatic Speech Recognition with Listen, Attend and Spell Model,” IEEE Signal Processing Letters, vol. 27, pp. 1889–1893, 2020.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
27


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
[330] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, “Emformer: Efficient Memory Transformer based Acoustic Model for Low Latency Streaming Speech Recognition,” in Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 6783–6787. [331] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, “Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large- Scale Dataset,” in Proc. IEEE ICASSP. Toronto, Ontario, Canada: IEEE, Jun. 2021, pp. 5904–5908.
[332] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier, S.-y. Chang, W. Li, R. Alvarez, Z. Chen, C.-C. Chiu, D. Garcia, A. Gruenstein, K. Hu, M. Jin, A. Kannan, Q. Liang, I. McGraw, C. Peyser, R. Prabhavalkar, G. Pundak, D. Rybach, Y. Shangguan, Y. Sheth, T. Strohman, M. Visontai, Y. Wu, Y. Zhang, and D. Zhao, “A Streaming On-Device End-To-End Model Surpassing Server-Side Conventional Model Quality and Latency,” in Proc. IEEE ICASSP, Barcelona, Spain, may 2020, pp. 6059–6063.
Rohit Prabhavalkar Rohit Prabhavalkar received his PhD in Computer Science and Engineering from The Ohio State University, USA, in 2013. Follow- ing his PhD, Rohit joined the Speech Technologies group at Google where he is currently a Staff Re- search Scientist. At Google, his research has focused primarily on developing compact acoustic models which can run efficiently on mobile devices, and on developing improved end-to-end automatic speech recognition systems. Rohit has co-authored over 50 refereed papers, which have received two best paper awards (ASRU 2017; ICASSP 2018). He currently serves as a member of the IEEE Speech and Language Processing Technical Committee (2018–2024), and as an Associate Editor of the IEEE/ACM Transactions on Audio, Speech, and Language Processing.
PLACE PHOTO HERE
[333] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan, S.-Y. Chang, R. Pang, Y. He, J. Qin, W. Han, Q. Liang, Y. Zhang, T. Strohman, and Y. Wu, “A Better and Faster End-to-End Model for Streaming ASR,” in Proc. IEEE ICASSP, Toronto, Ontario, Canada, Jun. 2021, pp. 5634–5638.
[334] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach, C. Allauzen, E. Variani, J. Qin, Q.-N. Le-The, S.-Y. Chang, B. Li, A. Gulati, J. Yu, C.-C. Chiu, D. Caseiro, W. Li, Q. Liang, and P. Rondon, “An Efficient Streaming Non-Recurrent On-Device End- to-End Model with Improvements to Rare-Word Modeling,” in Proc. Interspeech, Brno, Czechia, Sep. 2021, pp. 1777–1781.
[335] A. Bapna, Y.-A. Chung, N. Wu,
, A. Gulati, Y. Jia, J. H. Clark, M. Johnson, J. Riesa, A. Conneau, and Y. Zhang, “SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training,” Oct. 2021, arXiv:2110.10329.
[336] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau, “mSLAM: Massively Mul- tilingual Joint Pre-Training for Speech and Text,” Feb. 2022, arXiv:2202.01374.
Takaaki Hori received his PhD degree in system and information engineering from Yamagata Uni- versity, Yonezawa, Japan, in 1999. From 1999 to 2015, he had been engaged in researches on speech recognition and spoken language processing at Cy- ber Space Laboratories and Communication Science Laboratories in Nippon Telegraph and Telephone (NTT) Corporation, Japan. From 2015 to 2021, he was a Senior Principal Research Scientist at Mitsubishi Electric Research Laboratories (MERL), USA. He is currently a Machine Learning Re- searcher at Apple. His research interests include automatic speech recognition, spoken language understanding, and language modeling. He served as a member of the IEEE Speech and Language Processing Technical Committee (2020–2022).
PLACE PHOTO HERE
[337] Y. Tang, H. Gong, N. Dong, C. Wag, W. Hsu, J. Gu, A. Baevski, X. Li, A. Mohamed, M. Auli, and J. Pino, “Unified Speech-Text Pre-training for Speech Translation and Recognition,” in Proc. ACL, Dublin, Ireland, May 2022, pp. 1488–1499, arXiv:2204.05409.
[338] Y.-A. Chung, C. Zhu, and M. Zeng, “SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding,” in Proc. NAACL, Jun. 2021, pp. 1897–1907, arXiv:2010.02295.
[339] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li, Y. Zhang, Z. Wei, Y. Qian, J. Li, and F. Wei, “SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing,” in Proc. ACL, Dublin, Ireland, May 2022, pp. 5723–5738, arXiv:2110.07205.
[340] S. Thomas, H. J. Kuo, B. Kingsbury, and G. Saon, “Towards Reducing the Need for Speech Training Data to Build Spoken Language Under- standing Systems,” in Proc. IEEE ICASSP, Singapore, May 2022, pp. 7932–7936, arXiv:2203.00006.
[341] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman, “JOIST: A joint speech and text streaming model for ASR,” in Proc. IEEE SLT, Doha, Qatar, Jan. 2023, arXiv:2210.07353.
Tara Sainath received her PhD in Electrical Engi- neering and Computer Science from MIT in 2009. The main focus of her PhD work was in acoustic modeling for noise robust speech recognition. After her PhD, she spent 5 years at the Speech and Language Algorithms group at IBM T.J. Watson Re- search Center, before joining Google Research. She has served as a Program Chair for ICLR in 2017 and 2018. Also, she has co-organized numerous special sessions and workshops, including Interspeech 2010, ICML 2013, Interspeech 2016 and ICML 2017. In addition, she is a member of the IEEE Speech and Language Processing Technical Committee (SLTC) as well as the Associate Editor for IEEE/ACM Transactions on Audio, Speech, and Language Processing.
PLACE PHOTO HERE
[342] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and J. Le Roux, “Cycle-Consistency Training for End-to-End Speech Recognition,” in Proc. IEEE ICASSP, Brighton, UK, May 2019, pp. 6271–6275.
[343] T. Ochiai, S. Watanabe, T. Hori, and J. R. Hershey, “Multichannel End-to-End Speech Recognition,” in Proc. ICML. Sydney, Australia: PMLR, Aug. 2017, pp. 2632–2641.
[344] J. Li, “Recent Advances in End-to-End Automatic Speech Recogni- tion,” APSIPA Trans. on Signal and Information Processing, vol. 11, no. 1, Nov. 2021, DOI: 10.1561/116.00000050, arXiv:2111.01690.
Ralf Schl ¨uter Ralf Schl¨uter received his Dr.rer.nat. degree in Computer Science in 2000 and habilitated in Computer Science in 2019, both at RWTH Aachen University. In May 1996, Ralf Schl¨uter joined the Computer Science Department at RWTH Aachen University, where he currently is Lecturer and Academic Director, leading the Automatic Speech Recognition Group at the Chair Computer Science 6 – Machine Learning and Human Language Tech- nology. In 2019, Ralf also joined AppTek GmbH Aachen as Senior Researcher. His research interests cover sequence classification, specifically all aspects of automatic speech recognition, decision theory, stochastic modeling, and signal analysis. Ralf served as Subject Editor for Speech Communication (2013-2019).
PLACE PHOTO HERE
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
28


This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and
content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283
Shinji Watanabe is an Associate Professor at Carnegie Mellon University, Pittsburgh, PA. He re- ceived his B.S., M.S., and Ph.D. (Dr. Eng.) degrees from Waseda University, Tokyo, Japan. He was a research scientist at NTT Communication Science Laboratories, Kyoto, Japan, from 2001 to 2011, a visiting scholar at Georgia institute of technology, Atlanta, GA, in 2009, and a senior principal research scientist at Mitsubishi Electric Research Laborato- ries (MERL), Cambridge, MA USA from 2012 to 2017. Before Carnegie Mellon University, he was an associate research professor at Johns Hopkins University, Baltimore, MD, USA, from 2017 to 2020. His research interests include automatic speech recognition, speech enhancement, spoken language understanding, and machine learning for speech and language processing. He is an IEEE and ISCA Fellow.
PLACE PHOTO HERE
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/
29