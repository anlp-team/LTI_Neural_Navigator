29.41
14.12
6.50
#stepsinoutput
1step
56.47
Table1:ShareofFinQANeterrorsduetotheselectionofwrongoperatorsoroperandswhenappliedtotheFinQAdataset(Chenetal.,2021b),brokendownbythenumberofstepsintheoutput.Notethatthenumbersarebasedonprogramaccuracy,whichaccountsforeachtypeoferrorseparately,resultingintheeachrowsummingupto1.1973),especiallywhenthenumberofreasoningstepsgrows(Chenetal.,2021b).InthecontextofquantitativeQA,compositionalgeneralizationreferstothemodel’sabilitytogeneralizetonewcompositionsofpreviouslyseenelements.Asanexample,ifthemodelhasencounteredtrainingex-amplesthatdemonstratecalculationsfor“growthrate”and“percentchange”,wewouldlikeittobeabletocomeupwithareasonablehypothesisastohowtocalculate“percentgrowth”or“rateofchange”.Table1demonstrateshowthischal-lengebecomesmoredifficultasthenumberofreasoningstepsgrows.Forquestionsthatrequirelongerchainsofreasoning,themodellearnsspu-riouspatternsandunsuccessfullytriestoleveragethesememorizedpatternstosolvenewproblems.TheTablealsoshowsthatasthenumberofstepsgrows,generatingthewrongoperatorbecomesamoredominantmistakethanselectingthewrongoperand.Notonlyisthiserrormoredominant,butitcanalsohaveamoredestructiveimpactonthechainofreasoning,asitcanderailthemodel’shiddenrepresentationsfromthatpointonward.Asanexample,ouranalysisoftheFinQANetmodel(Chenetal.,2021b)outputshowedthatifthemodelgeneratesanincorrectoperator,itisabout30%morelikelytocommitothererrorsinthefollowing
46.75
46.75
%wrongoperands
39.07
Usingcounterfactualcontrasttoimprovecompositionalgeneralizationformulti-stepquantitativereasoningArminehNourbakhshLanguageTechnologiesInstitute,CarnegieMellonUniversityJ.P.MorganAIResearchanourbak@cs.cmu.eduSameenaShahJ.P.MorganAIResearchsameena.shah@jpmorgan.comCarolynRoséLanguageTechnologiesInstitute,CarnegieMellonUniversitycprose@cs.cmu.eduAbstractInquantitativequestionanswering,composi-tionalgeneralizationisoneofthemainchal-lengesofstateoftheartmodels,especiallywhenlongersequencesofreasoningstepsarerequired.InthispaperweproposeCounter-Comp,amethodthatusescounterfactualsce-nariostogeneratesampleswithcompositionalcontrast.Insteadofadataaugmentationap-proach,CounterCompisbasedonmetriclearn-ing,whichallowsfordirectsamplingfromthetrainingsetandcircumventstheneedforad-ditionalhumanlabels.Ourproposedauxiliarymetriclearninglossimprovestheperformanceofthreestateoftheartmodelsonfourrecentlyreleaseddatasets.Wealsoshowhowtheap-proachcanimproveOODperformanceonun-seendomains,aswellasunseencompositions.Lastly,wedemonstratehowthemethodcanleadtobettercompositionalattentionpatternsduringtraining.1IntroductionEnterprisedocumentssuchasreports,forms,andanalyticalarticlesoftenincludequantitativedataintabularform.Thedatainthesetablescanbeself-contained,butmorecommonlythesurround-ingtextprovidesmorecontextthatisnecessarytounderstandthecontent.Answeringquestionsoverthesehybridtabular/textcontextsrequiresreason-ingthatcombinesverbalandquantitativeseman-tics.Questionansweringoverquantitativetabu-lar/textdatahasgainedrecenttractionwiththereleaseofdatasetssuchasFinQA(Chenetal.,2021b),TAT-QA(Zhuetal.,2021),andHiTab(Chengetal.,2022).Table2showsanexampleofaquestionthatrequiresquantitativereasoningtoderivetheanswer.Giventhequestionandthetabularcontext,theoutputisasingle-stepprogramthatleadstothefinalanswerof-20.Amajorchallengethatstateoftheartmodelsfaceiscompositionalgeneralization(Montague,
52.00
8.00
2steps
53.64
7.28
3steps
%wrongoperator(s)
40.00
%wrongorderofoperands
>=4steps
14930 Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics Volume 1: Long Papers, pages 14930–14943 July 9-14, 2023 ©2023 Association for Computational Linguistics


2019
2018
Question
Outputprogram
29
60
80
2019revenuewas$80M.2020revenuewas$60M.
30
2020
Figure1:Ahigh-levelillustrationofourproposedmethod.Theinputexample(anchor)isprocessedbycross-attentionandrecurrentmodulestoproducetheoutputstepbystep.InadditiontoaregularCross-Entropyloss,CounterCompaddsanauxiliarytripletlossbasedonpositiveandnegativeexamples.Notethattheanchorandpos/negexamplesareallprocessedthroughtheRNNbeforecalculatingthetripletloss,aprocesswhichwehavenotillustratedduetospacelimitations.Alsonotethatmultiplepos/negexamplesaresampledateachstep.stepscomparedtowhenthemodelgeneratesanincorrectoperand.Inthispaper,weproposeCounterComp,anap-proachthatcanenhancecompositionallearninginmulti-stepquantitativeQA.Wetakeinspirationfromthesymboliccompositionofarithmeticopera-tions,andtheircorrespondencetonaturallanguagephrases.Buildingontheworkonattentionalign-mentsfrompreviousstudies,weproposeanauxil-iarymetriclearninglossthatisfocusedonspecificcomponentsoftheinputandoutput.Oursamplingstrategyisbasedoncounterfactualscenarios.Thismeansthatthemodellearnsproperrepresentationsforeachcomponentbasedonwhat-ifscenarios.Tothebestofourknowledge,thisisthefirststudythatsuccessfullyappliescomponent-wisecounter-factualsamplingasametriclearningstrategy.Weshowhow,whenstateoftheartmodelsareaug-mentedwithourauxiliarymetriclearningloss,theyexhibitbetterperformanceincaseswheremulti-stepreasoningisrequired.CounterCompoutper-formscurrentbaselinesonfourrecentlyreleaseddatasets,andshowstrongerperformanceonOODsamples.2RelatedworkThetypicalarchitectureofaquantitativeQAmodeliscomposedofaretrieverandagenerator(JurafsyandMartin,2021).Theretrieveridentifiesthepar-ticularcontextwheretheanswermightbefound.Sincethecontextcanbeamixoftablecellsand
Verbalizedfacts
35
Metric($M)
Answer
subtract(80,60)
Revenue
Operatingexpenses
20
Tabularcontext
Table2:ExampleofaquantitativeQAproblemovertabulardata.sentences,oftenatabularencoder(Herzigetal.,2020)orverbalizer(Chenetal.,2021b)isusedtoconvertthecellsintoanaturallanguagesequence.Theretrievedcontextisreferredtoasretrievedfacts.Next,thegeneratorusesthequestionalongwiththefactstogeneratetheoutputinastepbystepfashion.Inmulti-stepQA,thegeneratorof-tencombinesarecurrentmodulewithanattentionmechanism(Chenetal.,2021b),asillustratedintophalfofFigure1.Theoutputcanbeassessedintermsofprogramaccuracyaswellasexecutionaccuracy.Ourstudyisfocusedonimprovingprogramaccuracybyen-couragingcompositionalgeneralizationinthegen-erator.Therearetwocommonapproachestoimproving
70
Whatwasthenetchangeinrevenuefrom2019to2020?
14931


compositionalgeneralization.Attentionalignmentmodelsencourageexplicitalignmentsbetweennat-urallanguageutterances(e.g.“rateofchange”)andcorrespondingsymbolicmathoperations(e.g.sub-tractionfollowedbydivision).Methodsinformedbycounterfactualsusewhat-ifscenariostogeneral-izetoawidervarietyofcompositionsandreducetheeffectofmemorization.2.1AttentionalignmentsYinetal.(2021)showedthatadditionalsupervi-sioncanbeusedtopromoteexplicitalignmentsbetweencomponentsintheinputandintheoutput.Theyaddedaregularizationlossthatencouragesthecross-attentionmoduletoadjustitsattentionweightsaccordingtogoldalignments.Usingasfewas16examples,theirmodelwasabletoim-provegeneralizationinasemanticparsingtask.CompAQT(Nourbakhshetal.,2022)extendedthisideatomulti-stepquantitativeQA.Insteadofus-ingadditionalsupervision,itusednaturallanguageheuristicstocreatenoisyalignmentlabelsbetweeninputtokensandoutputsymbols.Theadditionalalignmentlossimprovedtheperformanceofthreebaselinemodelsonmulti-stepreasoningtasksforfourdatasets.2.2MethodsinformedbycounterfactualsThesuccessofalignment-basedmethodsislimitedbythefactthatbyheavilydiscouragingmemoriza-tion,theyunderperforminsettingswheremem-orizationcanbehelpful(Orenetal.,2020).Tostrikeabalancebetweenmemorizationandgener-alization,oneapproachistogeneratenewtrainingexamplesthatcoverimportantsemanticgapsinthetrainingdata.Thisisreminiscentofhowadver-sarialtrainingcanhelpbetterdefinethesemanticcontoursofcompositionalrepresentations(Zhangetal.,2022).Contrastiveormetriclearningmeth-odspursueasimilargoal,butinsteadofgeneratingnewsamples,theyleverageexistingsampleswithinthetrainingset(Jainetal.,2021).Counterfactualdataaugmentation(CAD)meth-odsstrivetoachievethisbygeneratingnewsam-plesusingwhat-ifscenarios(Zmigrodetal.,2019;Liuetal.,2021;Chenetal.,2021a).Thiscanbedonebyalteringaminimallysufficientsetofto-kensintheinputsuchthattheoutputclasschanges(Kaushiketal.,2020).Therearetwomainchal-lengestocreatingthesesamples.First,itisdifficulttoidentifytheminimalsetoftokensnecessarytoaltertheoutput.Second,thereisnoguaranteethatacounterfactualsampleexistsinthetrainingset.Toaddressthesechallenges,somestudiesemployhumanlabelers(Kaushiketal.,2020)orathirdpartymodel(Huangetal.,2020).IndomainslikesemanticparsingandquantitativeQAwheretheoutputissymbolic,analternativeapproachlever-agesthestructureoftheoutputtoavoidtheneedforhumanlabelers.Lietal.(2022)achievethisbyinterveningontheoperands.Supposethataques-tionstates“Whatwasthenetchangeinrevenuefrom2019to2020?”andtheretrieverproducestwo(verbalized)tablecells:“2019revenuewas$80M”and“2020revenuewas$60M”.Theoutputprogramforthisquestionwouldbe:subtract(80,60).Giventhenumericnatureoftheoperands,it’spossibletogeneratenewscenariossuchas“Whatif2019revenuewas$90?”withtheupdatedoutputsubtract(90,60).Employingthismethod,Lietal.(2022)augmenttheTAT-QAdataset(Zhuetal.,2021)intoanewdatasetnamedTAT-HQA.Theyalsoenhancetheverbalreasoningcapacityoftheirmodelbyofferingthecounterfactualscenarioasanaturallanguageprompt.Theirmodel,namedLearningtoImagine(L2I),outperformsstateoftheartmodels.Asmentionedintheprevioussection,modelsthatstrugglewithcompositionalgeneralizationsuf-ferfromerrorsinoperatorselection,whereasL2Iisfocusedontheselectionofoperands.Inthispaper,weproposeCounterComp,amethodthatfo-cusesoncounterfactualsamplingforcomponentsthatindicateoperators1.Usingnaturallanguageconstraintsfrompreviousstudies,wefirstfindcom-ponentsthatcorrespondtooperatorsversusthosethatcorrespondtooperands.Next,weuseanauxil-iarymetriclearninglosswithpositiveandnegativesampleschosenbasedonthosecomponents.Thishelpsusavoidthecomplexitiesassociatedwithadataaugmentationapproach,suchastheneedforcreationofadditionalhumanlabels.Thenextsec-tionlaysoutourproblemdefinitioninmoredetail.3ProblemformulationLetusconsidertheexampleprovidedbyTable3.SupposeQisthequestion,representedasasequenceoftokensq1,···,qN(i.e.“what”,“was”,“the”,···,“2020”,“?”).Fistheevidenceobtainedbytheretriever,madeupofasequenceoftokensf1,···,fM(i.e.“2019”,
1PleaserefertoAppendixCforastudyontheuseofCounterCompforoperatorsversusoperands.
14932


“revenue”,“was”,···,“$60M”).Theconcatenationofthesetwosequences,i.e.Q||F,formstheinputtothegenerator.Thegener-atorencodesQ||FusinganeurallanguagemodelsuchasRoBERTa(Liuetal.,2019),resultinginanembeddingmatrixU∈Rdenc×(N+M).ConsistentwithChenetal.(2021b),werepre-senttheoutputSasasequenceofstepss1,···,sL.Eachstepslcanbeanoperator(suchasaddordivide),oranoperand.SimilartoChenetal.(2021b),ourprogramsaremodeledasright-expandingbinarytreeswitheachoperatorhavingexactlytwooperands.Ifnecessary,oneormoreoperandsaresettoNONE,whereNONEisaspecialconstant.Lispre-definedasthemaximumnumberofstepsallowed.IntheexamplefromTable3,Sis:subtract,80,60,NONE,NONE,NONE.Togeneratethelthoutputstepsl,thegeneratorappliesacross-attentionmoduletoU,resultingintheattentionweightmatrixAl∈R1×(N+M)andtheattentionoutputXl∈RK.Arecurrentmodulethengeneratesthehiddenvectorhl,whichisusedtoproducetheoutputstepsl.hl=RNN(hl−1,Xl)sl=NN(hl)(1)whereNNcanbeanyneuralmodulethatprojectshlontothesimplexsl∈RK,fromwhichslcanbesampled:sl=argmaxksl,k.OurgoalistoencouragehltobesensitivetothecompositionoftheinputQ||Fwithregardstothecurrentoutputstepsl.Thismeansthathlneedstocaptureproperalignmentsbetweenimportanttermsintheinputandtherelevantoperator/operandintheoutput.Toachievethis,wepursueametriclearningap-proachwherepositiveandnegativesamplesaregeneratedaccordingtocounterfactualscenarios.3.1CounterfactualsamplesGivenatrainingexample([Q||F](i),S(i)),wede-fineaninterventiontargetQ(i)asasubsequenceofthequestiontokens,i.e.Q(i)={q(i)n;n∈N(i)}whereN(i)⊆{1,2,···,N}.Supposethatchangingtheinterventiontargetaf-fectsasinglestepintheoutputprogramS(i)=s(i)l,whichwenametheinterventionoutcome.Notethatduetoourfocusonthegenerationofoperators,welimittheinterventionoutcometoanoperator.Sincetheoutputiscomposedofoneoperatorfol-lowedbytwooperandsfollowedbyanotheropera-torandsoon,lisselectedfromalimitedindexset:l∈{1,4,7,···,L−3}.IntheexamplefromTable3,thepossibleindiceswillbe1and4,representingtheoperatorssubtractandNONE.Giventhisdefinition,it’spossibletominepos-itiveandnegativeexamplesfortheithtraininginstance.Apositiveexample([Q||F](i)pos,S(i)pos)isaninstanceforwhich,despiteapossibleinterven-tioninthetarget,theoutcomeremainsthesame,i.e.Q(i)pos̸=Q(i)andS(i)pos=S(i).Anegativeex-ample([Q||F](i)neg,S(i)neg)isaninstanceforwhichaninterventioninthetargetleadstoachangeintheoutcome,i.e.Q(i)neg̸=Q(i)andS(i)neg̸=S(i).Thisallowsustodefineatripletlossthaten-couragesh(i)ltoremainclosetoh(i)l,posandfarfromh(i)l,negwithamarginofα(i):L(i)triplet=max{||h(i)l−h(i)l,pos||22−||h(i)l−h(i)l,neg||22+α(i),0}(2)Figure1illustratesthesamplingprocessforonetrainingexample.Notethatthismetriclearningapproachwillonlybevalidifcausalassumptionswithregardstotheinterventiontargetarevalid,i.e.thechangeinS(i)negisinfacttheresultoftheinterventioninQ(i)negandnotachangeinanyotherpartoftheinput.Inadataaugmentationsetting,thiscanbeachievedbykeepingtheinputfixedandperturbingasmallsegmentthatfunctionsastheinterventiontargetsimilarto(Kaushiketal.,2020).However,asdiscussedinSection2.2.thisrequiresadditionalmanuallabortoannotatetheperturbedexamples.Inthenextsection,wedescribehowweim-posecertainconstraintsontheinterventiontargettoachievethisinaself-supervisedsetting2.4MethodologyOurgoalistoidentifypotentialpositiveandnega-tivesamplesfortheanchor([Q||F](i),S(i)).Sup-posetheanchoristheoneshowninthetopthreerowsofTable3.Bolditalicizedtokensareredun-dantbetweenthequestionandthefact,e.g.“rev-enue”,“2019”,and“2020”.Thosetermsareoftenusedbytheretrievertofindthecorrectfacts.Theyarealsousedbythegeneratortofindthecorrectorderofoperands.
2Notethattheterm“self-supervised”isusedinthiscontexttorefertothesamplingstrategy,i.e.noadditionallabelingisneededtogeneratethepositiveandnegativesamples.
14933


Variables
(#0
indicatescomponentsthatareuniquetothequestion(candidatesforintervention).Thesetermsoftenindicateanoperator.Reditalicizedtextindicatestermsthatareuniquetothefacts.Thesetermsoftenindicateoperands.Bolditalicizedtextindicatestermsthataresharedbe-tweenthequestionandfacts.Thesetermsoftenindicatemetrics.4.1RuntimeoptimizationTherearetworuntimechallengestothisproposedapproach:1)Samplingcanbecostlyiftheentiretrainingsethastobescannedforeachbatch.Thismeansanonlinesamplingstrategycannotbeused.Ontheotherhand,anofflinestrategyintroducesalargeoverhead.Ahybridapproachisneeded.2)CalculatingtheeditdistancemetricisacostlyoperationwithO(n2)steps.Tosolvethefirstproblem,webuildtwoindicespriortotraining.Oneindexgroupsthesamplesbytheirsequenceofoutputoperators.Thisindexcanbeusedtosamplepositiveexamples.Theotherindexincludesalltrainingexamples,andforeachexample,itincludesthefulllistofone-stepperturbationsappliedtoitsoutputopera-tors.Bygeneratingallpossibleperturbations,weareabletofindothersampleswhoseoutputsmatchtheperturbedsequence(i.e.negativesamples).For
revenuefrom
(80,60)NONE(NONE,NONE)
subtract
subtract
subtract
(35,30)NONE(NONE,NONE)
Question
Question
Question
{q(i)1,q(i)3,q(i)4,q(i)5,q(i)6,q(i)8,q(i)10}:whatthenetchangeinfromtos(i)4:NONE
wastherateofchangeof
Facts
Facts
Facts
(65,60)divide
,65)
{q(i)1,q(i)3,q(i)4,q(i)5,q(i)5,q(i)6,q(i)12}:whattherateofchangeoftos(i)4:divide4
2019?
2019revenuewas$80M.2020revenuewas$60M.
Candidateinterventionspans
to
2020?
2020?
from
Therearealsotermsthatareuniquetotheques-tion,i.e.“What”,“thenetchangeto”,“from”,and“to”(highlightedinblue).InCompAQT,theau-thorsshowedthatthesecanbeusedasindicatorsfortheoperators.Lastly,therearetermsthatareuniquetothefacts,i.e.“was$80M”and“was$60M”(reditalicizedtokens).Thesecanbeusedasindicatorsfortheoperands.Weusetheseheuris-ticstoguideoursamplingstrategy.Weflagallspansinthequestionthatdonotover-lapwiththefacts,i.e.underlinedbluesegments.Thosespansserveascandidateinterventionspans.IntheexamplefromTable3,thisresultsinfourcandidates:“What”,“thenetchangein”,“from”,and“to”.Next,weseekapositiveandanegativeexamplewithinthetrainingset.Apositiveexampleisasampleinwhich,despitepossiblechangesinthequestion,theoperatorsintheoutputremainconsis-tentwiththeoperatorsintheanchor.Table3showsonesuchexample.Severaltermshavebeenalteredinthequestion.However,wewouldonlyfocusonthechangesinthecandidatespans.Here,“was”haschangedto“is”,“netchange”haschangedto“difference”,“from”to“between”and“to”to“and”.Thisresultsinatoken-levelLevenshteindistanceof5(foureditsandoneinsertion)(YujianandBo,2007).Weignorethechangefrom“revenue”to“operatingexpenses”andfrom“2019”to“2018”,becausethosechangeshaveoccurredoutsideofourcandidatespansandonlycorrespondtooperands.Anegativeexampleisasampleinwhichexactlyoneoutputoperatorisaltered,deleted,oradded.Table3showsonesuchexample.Here,theoutputincludesanewoperatordivide.Thequestionhasalsobeenalteredwithatoken-levelLevenshteindistanceof4.ThegivenpositiveandnegativeexamplecannowbepluggedintoEquation2.Insteadofafixedmargin,weusetheeditdistancesmentionedbe-foretodynamicallyadjustthemargin.LetNLD(i)posandNLD(i)negbethenormalized,token-levelLev-enshteineditdistancebetweentheanchorandthepositiveexample,andthenegativeexample,respec-tively.Wesetthemarginto:α(i)=1−|NLD(i)neg−NLD(i)pos|Thisencouragesalargermarginforcaseswheretheanchorisequallysimilartothepositiveandthenegativeexamples,andthemodelmighthaveahardertimepickinguponthenuancesofeachcomponent.
{q(i)1,q(i)2,q(i)3,q(i)4,q(i)5,q(i)8,q(i)10}:whatisthedifferenceinbetweenands(i)4:NONE5
2018and
Program
Program
Program
2019to
Anchor
operatingexpensesbetween
wasthenetchangein
Table3:Exampleofpositiveandnegativesamplingusingcounterfactualcomponents.Blueunderlinedtext
operatingincomefrom2018to
Positivesample
2018operatingexpenseswere$35M.2020operatingexpenseswere$30M.
Q(i)S(i)
Q(i)negS(i)negeditdist
thenetchangein
2018incomefromoperatingactivitieswas$65M.2019incomefromoperatingactivitieswas$60M.
Q(i)posS(i)poseditdist
Negativesample
What
What
What
Whatisthedifferencein
14934


asequencewithnoperators,allpossibleperturba-tionscanbegeneratedinO(n×K)time,whereKisthenumberofpossibleoperators3.Giventhepre-generatedpositiveandnegativepools,wecanalsocalculateandcacheeditdis-tancesaheadoftime.However,inpractice,werealizedthatwecoulddosoduringtrainingwithlittleadditionalcost.Thisisbecausetheeditdis-tanceisappliedatthetoken-level4,andislimitedtocandidatespans,renderingitrelativelyfast.Thedecisionastowhetherdistancesshouldbecachedorcalculatedontheflydependsontheaveragesizeofeachpoolversusthenumberoftrainingsteps.ThealgorithmoutlinedinAppendixBsumma-rizesourapproach.5Experiments5.1DatasetsWeusethehybridCompAQTdataset,whichiscomposedoffourpreviouslyreleaseddatasets,namelyFinQA(Chenetal.,2021b),TAT-QA(Zhuetal.,2021),HiTab(Chengetal.,2022),andMULTIHIERTT(Zhaoetal.,2022).TheauthorsfilteredthesefourdatasetsdowntoQApairsthatrequiresingleormulti-stepquantitativereasoning.TheyalsoprocessedthetablesandoutputsinallfourdatasetstomatchtheFinQAformat.5.2BaselinesWeapplyourproposedauxiliarylosstothreebase-lines:1)FinQANet,originallydevelopedfortheFinQAdataset(Chenetal.,2021b).2)TAGOP,originallydevelopedfortheTAT-QAdataset(Zhuetal.,2021).3)Pointer-VerbalizerNetwork(PVN),originallyproposedbyNourbakhshetal.(2022).WealsoapplytheCompAQTlosstoeachmodelasasecondarybaselineinordertodeter-minehowCounterCompcomparestoanattention-alignmentstrategy.5.3SamplingsuccessrateAnotherpossibleconcernisthatoursamplingstrat-egymightbelimited,inthatpositiveandnegativesamplesmightnotalwaysbeavailableinthetrain-ingset,orthatlimitedavailabilityofsamplesmightbiasthetrainingprocess.Toremediatetheproblemofunavailablesamples,whenapositivesampleis
5PleaserefertoAppendixAforresultsusingretrievedfacts.
3SincewefollowChenetal.(2021b),inallofourexperi-mentsK=10.4Sincewe’reusingalanguagemodelthatusesword-piecetokenization,ineffecttheruntimeisatsubwordlevel.missing,weusetheanchorasthepositivesample,andwhenanegativesamplecannotbefound,weuseauniformlysampledinstancefromthebatch.Table4showssomestatisticsaboutthesuccessrateofthesamplingalgorithm.“%Failure”identi-fiestheshareoftrainingexamplesforwhicheitherapositiveoranegativeexamplewasmissing.Un-surprisinglythisneverhappensforsingle-steppro-grams,isveryrarefortwo-stepprograms,andwiththeexceptionofHiTab,happensinlessthan10%ofthecasesforlongerprograms.TheTablealsoshowstheaveragenumberofpositiveandnegativeexamplesfoundforeachanchor.Again,HiTabhasthelowestnumberofavailablesamples,makingitthemostchallengingdataset.InSection6.4,wedemonstratehow,evenincaseswithfewpossiblesamples,themodelisabletogeneralizetounseenexamples.5.4SettingsSincewearefocusedonthegenerator,intheex-perimentsdiscussedinthissectionwewillusegoldfactsandencodetheinputusingRoBERTa-large(Liuetal.,2019)5.WerunthebaselineswithandwithouttheadditionalLtripletfor50epochswithalearningrateof5e−5,theAdamopti-mizer(KingmaandBa,2015)withβ1=0.9andβ2=0.999.Ateachstep,wesample(withreplace-ment)5positiveandnegativepairsperanchor,andaddtheaverageauxiliarytripletlosstothemainmodellosswithaweightofλ.Afteragridsearchwithastep-sizeof0.1,wesetλto0.4forallex-periments.Allexperimentswereconductedon8NVIDIAT4GPUSwith16GBsofmemory.6ResultsandanalysisTable5showstheprogramaccuracyofbase-lines(toprowofeachcell)comparedtheaddi-tionofCounterComploss(bottomrowofeach-cell).Amongthebaselinemodels,TAGOPisnotdesignedtogeneratemulti-stepprograms.There-foreweonlyapplyittotheTAT-QAdataset,whichhasasetofpre-determinedoperations(e.g.changeratio).WealsoapplythePVNmodeltothecom-bineddataset,butsinceFinQANetoutperformsitonallbenchmarks,wewillcontinuetouseFin-QANetasthereferencebaselinemodelforthere-mainingexperimentsinthissection.AsTable5shows,CounterCompconsistently
14935


1055
73.33
Avg.#possamples
Avg.#possamples
Avg.#possamples
16.86
67.91*
Programaccuracy
1808
15.01
45.01
42.51*
39.25*
913
62.00
7.8
630
190
190
60.88+CounterComp
70.78
75.45*
34.94
43.73
42.73
34.70
34.16
Table4:Thefailurerateofsamplingfromeachdataset(whennopositiveornonegativesamplecanbefoundforagivenanchor),aswellastheaveragenumberofpositiveandnegativesamplesfoundforeachanchor.outperformsthebaselinesandthemarginisoftenhigherforlongerprograms.Onenotableexcep-tionistheTAT-QAdataset.Asmentionedbefore,thedatasetisnotdesignedforopen-endedmulti-stepreasoningandincludesalimitedsetofpossi-bleoperations.ThereforemethodsthatencouragememorizationmightachievehigherperformanceonTAT-QA.HiTabisanotherchallengingdataset,butdespitelowperformanceonlongersequences,CounterCompoffersanimprovementoverthebase-line.
40.07
61.33
29
38.87
Topattendedtokensduringthegenerationofdivide
78.68
TAGOP
MULTIHIERTT
MULTIHIERTT
66
65.11
36.91*
41
Avg.#negsamples
Avg.#negsamples
Avg.#negsamples
71.58*
66.90
Dataset
Dataset
0.2
0.2
15.91
70.01
1step
1step
4
16.63
45.38
TAT-QA
TAT-QA
TAT-QA
30.76
13.54
66.19
annual,per
70.56
Model
Model
66.26
40.82+CounterComp
45.67*
29.8
0
0
0
0
0
30
1457
34.73
221
18.44*
32.23+CounterComp
3+steps
3+steps
65.87
46.07
36.86*
189
Overall
75.12
35.85
29.94
38.94+CompAQT
+CounterComp
59.58
335
68.14
30.12+CompAQT
Combined
Combined
92
554
8.2
63.45
Table5:Programaccuracyforprogramgenerationusingbaselinemodelsv.s.usingCompAQTloss,v.s.usingCounterComploss.*indicatesthatagain/lossissig-nificantatp<0.005comparedtothebaseline,usingthepaired-bootstraptestproposedbyBerg-Kirkpatricketal.(2012)forb=103.6.1AuxiliarytripletlossversusauxiliaryattentionalignmentlossThemiddlerowofeachcellinTable5showstheprogramaccuracywhenCompAQTlossisaddedinsteadofCounterComploss.Aspreviouslyde-scribed,CompAQTimposesanauxiliaryattentionalignmentlosssuchthattokensrelatedtoopera-torsreceivemoreattentionduringthegenerationofoperators.Eventhoughthisleadstoimprove-mentsoverthebaselines,CounterCompoutper-formsCompAQTinallexperiments.ThismightbeduetothefactthattheregularizingeffectofCom-pAQTlossisnotasstrongastherepresentationlearningimpactofCounterComp.DespitethefactthatCounterCompwasnotde-signedasanattentionalignmentmodel,itdoeshaveanimpactonhowattentionpatternsevolveduringtraining.Table6showsthetop-attendedinputtokensduringthegenerationofadivideoperatorinvariouscontexts.Forasingulardivi-sionoperation,FinQANetattendstotokenssuchas“year”whereasCounterCompencouragesthemodeltoattendtomorerelevanttokenssuchas“net”and“change”.Asubtractionfollowedbyadivisionoftenindicatesapercentagecalculation,ascapturedbybothmodels.Anadditionfollowedbyadivisionoftenindicatesanaveragecalculation.Again,CounterCompisabletocapturerelevanttokensbuttheFinQANetbaselineseemstoattendtosomememorizedtokenssuchas“annual”.
divide
58.60+CompAQT
66.60
66.00*
79.13*
1.3
65.14
%Failure
%Failure
%Failure
295
25.14
64.88
39.56
share,percent
326
46.12
30.36
share,year
69.97+CounterComp
74.49*
63.76
63.76
56.64+CompAQT
40.28
42.35
17.35
Table6:Topattendedtokensduringthegenerationofthedivisionoperatorinvarioussequences.ThedatasetusedforthisexperimentisFinQA.6.2FixedversusadaptivemarginThelastrowofTable5showstheperformanceoftheFinQANetmodelonthecombineddatasetusingtheCounterComplosswithafixedmarginof1.Theperformancesuffers,especiallyasthenumberofstepsgrows.Thisfurtherdemonstratestheimportanceoftheadaptivemarginα(i)thattakestheeditdistanceintoaccount.
2.5
64.31*
30.00*
FinQANet
FinQANet
FinQANet
FinQANet
FinQANet
FinQANet
40.85*
24
34.06
38.99
PVN
70.71+CompAQT
average,per
59.21+CounterComp
subtractdivide
64.89
32.61*
2steps
2steps
HiTab
HiTab
3254
16.77
adddivide
958
61.82*(Fixedmargin)
41.51
70.00
net,change
63.80
43.88+CounterComp
ratio,percent
FinQA
FinQA
43.25+CompAQT
17.39
68.44+CompAQT
638
61.20*
75.63
533
73.74+CounterComp
14936


Evidence
Didtheshareofsecuritiesratedaaa/aaaincreasebetween2008and2009?
1)theaaa/aaashareof2009is14%2)theaaa/aaashareof2008is19%
Whatistheamountofcreditlinesthathasbeendrawninmillionsasofyear-end2016?
multiply(746,554)
+CompAQT
1)Wehaveothercommittedanduncommittedcreditlinesof$746million2)$554millionofthesecreditlineswereavailableforuseasofyear-end2016
Question
22.80
MULTIHIERTT
41.64
subtract(27.5,27.3)
subtract(27.5,27.3)
70.32
FinQANet+CounterComp
TAT-QA
multiply(554,const_1000000)
Model
36.94
Table7:FourexamplesfromtheFinQAdataset,showingCounterComp’ssuccessandfailureincapturingcomposi-tionalexpressions.Notethatsomenumbershavebeentruncatedtosavespace.
+CounterComp
1)thegrossmarginpctof2004is27.3%2)thegrossmarginpctof2003is27.5%
Whatpercentageofamountsexpensedin2009camefromdiscretionarycompanycontributions?
22.97
35.33
42.00
divide(3.8,35.1),multiply(#0,const_100)
divide(3.8,35.1),multiply(#0,const_100)
Table8:OODperformanceofFinQANetvariationswhentrainedontheFinQAdatasetandtestedonotherdatasets,ortestedonunseenoperatorcompositionsintheFinQAdevset.6.3QualitativeexamplesTable7showsfourqualitativeexamplesfromtheFinQAdataset.ThefirsttworowsshowhowCoun-terCompenablestheFinQANetmodeltorepresentconceptssuchas“decline”and“percentage”moreaccurately.ThethirdexampleshowshowCounter-Compisabletodeterminethedifferencebetweenacalculationquestionandayes/noquestion.Thelastrowshowsafailureexamplethatwasalsore-portedinChenetal.(2021b).Here,CounterCompdoesnotimprovetheperformanceofFinQANet.Thisparticularexamplerequiresdomainexpertisetoaddress,whichgoesbeyondadirectmappingbe-tweencomponentsinthequestionandtheevidence.Thishighlightstheneedformethodsthatallowdo-mainexpertisetoberepresentedmoreeffectively(Chenetal.,2021b).6.4Compositionalv.s.OODgeneralizationInarecentstudyJoshiandHe(2022)showedthatcurrentapproachestocounterfactualdataaugmen-tationdonotnecessarilyleadtobettergeneral-izationtoout-of-distribution(OOD)samples.TotestwhetherthisholdsforCounterComp,wecon-ducttwostudies.First,wetrainFinQNetwithandwithoutCounterComplossontheFinQAdataset,thentestitontheotherthreedatasets.Notethatthefourdatasetsarebasedondifferentdomains.FinQAandTAT-QAarebothbasedonfinancialreports,butwhileFinQAwasderivedfromUSfil-ings,TAT-QAisbasedoninternationalfilingsandthereforecoversawidervarietyofmetrics.HiTabandMULTIHIERTTarebothbasedonothertypesofcorporatereportswithhighlycomplextabularstructures.ThefirstthreecolumnsofTable8showaslightimprovementwhenCounterCompisusedinthissetting.Incontrast,usingCompAQTlossslightlyhurtstheperformance,demonstratingCounter-Comp’shigherOODgeneralizationpotential.Next,weselectasubsetofsamplesfromtheFinQAdevsetthathaveunseencompositionscom-paredtothetrainingset.Thismeansthatthepar-ticularcombinationofoperationswereneverseenduringtraining.Asthelastcolumnofthetableshows,CounterCompoutperformsthebaselinebymorethan7points.Thisfurtherdemonstrateshowimprovingrepresentationlearningatthecompo-nentlevelcanenhancegeneralizationtounseencontexts.7ConclusionInthispaper,wepresentedCounterComp,amethodthatleveragescounterfactualcontrasttoenablemetriclearningforquantitativeQA.WeshowhowusingtheauxiliaryCounterComplosscanimprovecompositionalgeneralizationinmulti-stepreasoningtasks,especiallyasthenumberofstepsgrows.Duetoruntimechallenges,weproposedahy-bridoffline/onlinesamplingstrategythatusespre-definedindicesforeasierlookupoperations.Thisallowsustocapturesamplesthathaveacontrastofoneoperatorwiththeanchor.Infuturestudies,wehopetocapturecontrastivesampleswithlongerperturbationchains.Wealsohopetoexaminetheeffectivenessofcounterfactualcompositionalcon-trastinotherdomainssuchassemanticparsingandquestionansweringovermultimodalinput.Lastly,wehopetoextendtheuseofCounter-Comptoenhancetheperformanceoftheretriever,usingtheheuristicsintroducedinSection4(i.e.by
35.28
FinQA(unseenprograms)
FinQANet
FinQANet
73.53
Goldprogram
1)amountsexpensedfor2009was$35.12)expenseincludesadiscretionarycompanycontributionof$3.8
65.74
subtract(27.5,27.3),divide(#0,27.5)
HiTab
39.88
subtract(14,19),subtract(14,#0)
Programaccuracyontestdataset
greater(14,19)
greater(14,19)
divide(3.8,35.1)
Whatwasthegrossmargindeclineinfiscal2004from2003?
22.71
subtract(746,554)
14937


focusingoncomponentsinthequestionthatover-lapwiththefacts).ThiscanresultinaquantitativeQApipelinethatispoweredbycompositionalcon-trastinanend-to-endfashion.8LimitationsAspreviouslymentioned,ourstudyisfocusedonthegeneratorcomponentofaQApipelineandig-norestheretrievaltask.Intheexperimentspre-sentedinthepaper,wehaveusedgoldfactstoreporttheresults.ForcertaindatasetssuchasHiTabandMULTIHIERTTwhichweredesignedforcomplextabularstructures,thismightsimplifytheend-to-endchallenge.Infuturestudies,wehopetoexplorewhetherCounterCompcanenhancetheperformanceofretrievers.Thedatasetsusedinourexperimentswerecu-ratedusingenterprisedocumentssuchasfinancialreportsorothercorporatedisclosures.QuantitativeQAoverthesereportsofteninvolvesmulti-stepreasoningthatislimitedtolineararithmeticopera-tionssuchasaddition,division,averaging,etc.Acompletelyopen-domainQAenginemightneedtocovermorecomplexoperators.Lastly,wedesignedCounterComptolever-ageexistingdatabysamplingfromthetrainingset.Nevertheless,combiningCounterCompwithaugmentation-focusedmethodssuchasCADmightleadtomorerobustmodels.ReferencesTaylorBerg-Kirkpatrick,DavidBurkett,andDanKlein.2012.Anempiricalinvestigationofstatisticalsig-nificanceinNLP.InProceedingsofthe2012JointConferenceonEmpiricalMethodsinNaturalLan-guageProcessingandComputationalNaturalLan-guageLearning,pages995–1005,JejuIsland,Korea.AssociationforComputationalLinguistics.HaoChen,RuiXia,andJianfeiYu.2021a.Reinforcedcounterfactualdataaugmentationfordualsentimentclassification.InProceedingsofthe2021Confer-enceonEmpiricalMethodsinNaturalLanguageProcessing,pages269–278,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.ZhiyuChen,WenhuChen,ChareseSmiley,SameenaShah,IanaBorova,DylanLangdon,ReemaMoussa,MattBeane,Ting-HaoHuang,BryanRoutledge,andWilliamYangWang.2021b.Finqa:Adatasetofnumericalreasoningoverfinancialdata.ProceedingsofEMNLP2021.ZhoujunCheng,HaoyuDong,ZhiruoWang,RanJia,JiaqiGuo,YanGao,ShiHan,Jian-GuangLou,andDongmeiZhang.2022.HiTab:Ahierarchicaltabledatasetforquestionansweringandnaturallanguagegeneration.InProceedingsofthe60thAnnualMeet-ingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages1094–1110,Dublin,Ireland.AssociationforComputationalLinguistics.JonathanHerzig,PawelKrzysztofNowak,ThomasMüller,FrancescoPiccinno,andJulianEisenschlos.2020.TaPas:Weaklysupervisedtableparsingviapre-training.InProceedingsofthe58thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages4320–4333,Online.AssociationforComputa-tionalLinguistics.Po-SenHuang,HuanZhang,RayJiang,RobertStan-forth,JohannesWelbl,JackRae,VishalMaini,DaniYogatama,andPushmeetKohli.2020.Reducingsen-timentbiasinlanguagemodelsviacounterfactualevaluation.InFindingsoftheAssociationforCom-putationalLinguistics:EMNLP2020,pages65–83,Online.AssociationforComputationalLinguistics.ParasJain,AjayJain,TianjunZhang,PieterAbbeel,JosephGonzalez,andIonStoica.2021.Contrastivecoderepresentationlearning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages5954–5971,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.NitishJoshiandHeHe.2022.Aninvestigationofthe(in)effectivenessofcounterfactuallyaugmenteddata.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages3668–3681,Dublin,Ireland.AssociationforComputationalLinguistics.DanJurafsyandJamesH.Martin.2021.Speechandlanguageprocessing,3edition,chapter23.DivyanshKaushik,EduardHovy,andZacharyLipton.2020.Learningthedifferencethatmakesadiffer-encewithcounterfactually-augmenteddata.InInter-nationalConferenceonLearningRepresentations.DiederikP.KingmaandJimmyBa.2015.Adam:Amethodforstochasticoptimization.In3rdInter-nationalConferenceonLearningRepresentations,ICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings.MoxinLi,FuliFeng,HanwangZhang,XiangnanHe,FengbinZhu,andTat-SengChua.2022.Learningtoimagine:Integratingcounterfactualthinkinginneuraldiscretereasoning.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages57–69,Dublin,Ireland.AssociationforComputationalLin-guistics.QiLiu,MattKusner,andPhilBlunsom.2021.Counter-factualdataaugmentationforneuralmachinetrans-lation.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationfor
14938


ComputationalLinguistics:HumanLanguageTech-nologies,pages187–197,Online.AssociationforComputationalLinguistics.YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:Arobustlyoptimizedbertpretrainingap-proach.ArXiv,abs/1907.11692.RichardMontague.1973.TheProperTreatmentofQuantificationinOrdinaryEnglish,pages221–242.SpringerNetherlands,Dordrecht.ArminehNourbakhsh,CathyJiao,SameenaShah,andCarolynRosé.2022.Improvingcompositionalgen-eralizationformulti-stepquantitativereasoninginquestionanswering.InProceedingsofthe2022Con-ferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1916–1932.AssociationforCom-putationalLinguistics.InbarOren,JonathanHerzig,NitishGupta,MattGard-ner,andJonathanBerant.2020.Improvingcompo-sitionalgeneralizationinsemanticparsing.InFind-ingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages2482–2495,Online.AssociationforComputationalLinguistics.PengchengYin,HaoFang,GrahamNeubig,AdamPauls,EmmanouilAntoniosPlatanios,YuSu,SamThomson,andJacobAndreas.2021.Compositionalgeneralizationforneuralsemanticparsingviaspan-levelsupervisedattention.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages2810–2823,Online.AssociationforComputationalLinguistics.LiYujianandLiuBo.2007.Anormalizedlevenshteindistancemetric.IEEETransactionsonPatternAnal-ysisandMachineIntelligence,29(6):1091–1095.LeZhang,ZichaoYang,andDiyiYang.2022.TreeMix:Compositionalconstituency-baseddataaugmentationfornaturallanguageunderstanding.InProceedingsofthe2022ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages5243–5258,Seattle,UnitedStates.AssociationforComputationalLinguistics.YilunZhao,YunxiangLi,ChenyingLi,andRuiZhang.2022.MultiHiertt:Numericalreasoningovermultihierarchicaltabularandtextualdata.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages6588–6600,Dublin,Ireland.AssociationforComputationalLinguistics.FengbinZhu,WenqiangLei,YouchengHuang,ChaoWang,ShuoZhang,JianchengLv,FuliFeng,andTat-SengChua.2021.TAT-QA:Aquestionansweringbenchmarkonahybridoftabularandtextualcon-tentinfinance.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLin-guisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages3277–3287,Online.AssociationforComputationalLinguistics.RanZmigrod,SabrinaJ.Mielke,HannaWallach,andRyanCotterell.2019.Counterfactualdataaugmenta-tionformitigatinggenderstereotypesinlanguageswithrichmorphology.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages1651–1661,Florence,Italy.Asso-ciationforComputationalLinguistics.
14939


Programaccuracy
67.52
IPIi=1L(i)33:model.backward(L)
1step
Table10showstheprogramaccuracyofCoun-terCompversusthenewmethodwhenappliedtoeachdataset.Asexpected,TAT-QAistheonlydatasetresponsivetotheperturbationofoperands.Allotherdatasetssufferfromanexclusivefocusonoperands.ForHiTabandMULTIHIERTT,theoperandstrategyalsounderperformscomparedtothebaselineFinQANetperformance(seeTable5).
61.18
Model
22.79
1:Trainingdata:{([Q||F](i),S(i))}Ii=12:Parameters:λ3:Model:model//Createtheindicesforposandnegsamples4:pos_index←{}5:neg_index←{}6:fori∈{1,...,I}do7:O(i)←s(i)1,s(i)4,···,s(i)L−38:add_to_index(pos_index,O(i),i)//pistheperturbedoutputandlisthelocationoftheperturbation9:forp,l∈possible_perturbations(O(i))do10:j←find_matching_sample(p)11:add_to_index(neg_index,O(i),(j,l))12:endfor13:endfor//Train(singleepoch,non-batchversion)14:fori∈{1,...,I}do15:forj∈{1,2,···,5}do//Basicmodelloss16:L(i)←loss(model.forward([Q||F](i)),S(i))//Pos/negsampling17:O(i)←s(i)1,s(i)4,···,s(i)L−318:pos_sample←sample(pos_index[O(i)]\i)19:neg_sample,l←sample(neg_index[O(i)])//Findcandidateinterventionspans20:Q(i)←find_intrvntn_span(i)21:Q(i)pos←find_intrvntn_span(pos_sample)22:Q(i)neg←find_intrvntn_span(neg_sample)//Calculateeditdistancesandloss23:NLD(i)pos←norm_edit_dist(Q(i),Q(i)pos)24:NLD(i)neg←norm_edit_dist(Q(i),Q(i)neg)25:α(i)=1−|NLD(i)neg−NLD(i)pos|26:pos_dist(i)j=||h(i)l−h(i)l,pos||2227:neg_dist(i)j=||h(i)l−h(i)l,neg||2228:L(i)tripletj=max{pos_dist(i)j−neg_dist(i)j+α(i),0}29:endfor30:L(i)=(1−λ)L(i)+λ
3+steps
Overall
58.30+CounterComp
6ThiscouldbebecausewefailedtogenerateaTAT-HQAdatasetthatwascomparabletotheoneusedintheoriginalpaper.
Table9:AblationresultsontheFinQANetmodel,ap-pliedtotheFinQAdatasetwithretrievedfacts.BTrainingalgorithmAlgorithm1detailsourpre-indexing,sampling,andtrainingprocesses.Notethatthealgorithmisasimplifiedversionofourimplementation,e.g.itfollowsabasicSGDinsteadofabatchSGDprocess,andshowstheprocessforonlyoneepoch.CCounterCompforoperatorsversusoperandsCounterCompintervenesonoperators,whereasoperandsprovideanotherpossibleinterventiontar-get.AsmentionedinSection2,LearningtoImage(L2I)(Lietal.,2022),whichfocusesoncounterfac-tualscenariosforoperands,wasabletooutperformTAGOPbyalargemargin.L2IwasevaluatedonTAT-QA,adatasetwithalimitedsetofpossiblemulti-stepoperations,resultinginthechallengeofcompositionalgeneralizationbeingmainlyfocusedonoperands.SincewewerenotabletorecreatetheresultsreportedintheoriginalL2Ipaper6,in-steadweevaluateanoperand-focusedapproachviaametriclearningmethodsimilartoCounterComp.Givenananchor,wegeneratenewsamplesusingtheoperationslaidoutintheL2Ipaper(i.e.SWAP,ADD,MINUS,etc.),whereoneormoreoperandsareperturbedinrandom.Weapplythesamepertur-bationinthefacts.Thiseffectivelyeliminatesthe"imagination"componentbutprovidesabaselinethatismorecomparabletoCounterComp.Thesesamplesareusedaspositiveexamples,whereasnegativeexamplesarerandomlysampledfromthebatch.
57.03
20.56
5P5j=1L(i)tripletj31:endfor32:L=1
FinQANet
58.87
2steps
Algorithm1Trainingalgorithm
64.13
AResultsonretrievedfactsTable9showstheperformanceofFinQANetversusFinQANet+CounterComponretrievedfactsfromtheFinQAdataset.Similartogoldfacts,Counter-Compimprovesprogramaccuracy,especiallyonmulti-stepoutput.
14940


CounterComp(operands)
68.98
MULTIHIERTT
70.01
TAT-QA
Model
28.88
70.80
CounterComp(operators)
40.85
32.61
HiTab
74.49
37.67
Table10:ProgramaccuracyofCounterCompversusasamplingstrategyfocusedonoperands.FinQANetwasusedforallexperiments.
FinQA
14941


ACL2023ResponsibleNLPChecklist
TheResponsibleNLPChecklistusedatACL2023isadoptedfromNAACL2022,withtheadditionofaquestiononAIwritingassistance.
AForeverysubmission:(cid:3)3A1.Didyoudescribethelimitationsofyourwork?8(cid:3)3A2.Didyoudiscussanypotentialrisksofyourwork?6.3,6.4,8(cid:3)3A3.Dotheabstractandintroductionsummarizethepaper’smainclaims?1(cid:3)7A4.HaveyouusedAIwritingassistantswhenworkingonthispaper?Leftblank.B(cid:3)3Didyouuseorcreatescientiﬁcartifacts?5.1,5.2(cid:3)3B1.Didyoucitethecreatorsofartifactsyouused?5.1,5.2(cid:3)3B2.Didyoudiscussthelicenseortermsforuseand/ordistributionofanyartifacts?5.1,5.2(cid:3)3B3.Didyoudiscussifyouruseofexistingartifact(s)wasconsistentwiththeirintendeduse,providedthatitwasspeciﬁed?Fortheartifactsyoucreate,doyouspecifyintendeduseandwhetherthatiscompatiblewiththeoriginalaccessconditions(inparticular,derivativesofdataaccessedforresearchpurposesshouldnotbeusedoutsideofresearchcontexts)?5.1,5.2(cid:3)7B4.Didyoudiscussthestepstakentocheckwhetherthedatathatwascollected/usedcontainsanyinformationthatnamesoruniquelyidentiﬁesindividualpeopleoroffensivecontent,andthestepstakentoprotect/anonymizeit?Thedataisbasedonpubliclyavailablecorporatereports.(cid:3)3B5.Didyouprovidedocumentationoftheartifacts,e.g.,coverageofdomains,languages,andlinguisticphenomena,demographicgroupsrepresented,etc.?5.1,5.2(cid:3)3B6.Didyoureportrelevantstatisticslikethenumberofexamples,detailsoftrain/test/devsplits,etc.forthedatathatyouused/created?Evenforcommonly-usedbenchmarkdatasets,includethenumberofexamplesintrain/validation/testsplits,astheseprovidenecessarycontextforareadertounderstandexperimentalresults.Forexample,smalldifferencesinaccuracyonlargetestsetsmaybesigniﬁcant,whileonsmalltestsetstheymaynotbe.1C(cid:3)3Didyouruncomputationalexperiments?5(cid:3)3C1.Didyoureportthenumberofparametersinthemodelsused,thetotalcomputationalbudget(e.g.,GPUhours),andcomputinginfrastructureused?5
14942


(cid:3)3C2.Didyoudiscusstheexperimentalsetup,includinghyperparametersearchandbest-foundhyperparametervalues?5(cid:3)3C3.Didyoureportdescriptivestatisticsaboutyourresults(e.g.,errorbarsaroundresults,summarystatisticsfromsetsofexperiments),andisittransparentwhetheryouarereportingthemax,mean,etc.orjustasinglerun?6(cid:3)7C4.Ifyouusedexistingpackages(e.g.,forpreprocessing,fornormalization,orforevaluation),didyoureporttheimplementation,model,andparametersettingsused(e.g.,NLTK,Spacy,ROUGE,etc.)?Nopackagesused.D(cid:3)7Didyouusehumanannotators(e.g.,crowdworkers)orresearchwithhumanparticipants?Leftblank.(cid:3)D1.Didyoureportthefulltextofinstructionsgiventoparticipants,includinge.g.,screenshots,disclaimersofanyriskstoparticipantsorannotators,etc.?Noresponse.(cid:3)D2.Didyoureportinformationabouthowyourecruited(e.g.,crowdsourcingplatform,students)andpaidparticipants,anddiscussifsuchpaymentisadequategiventheparticipants’demographic(e.g.,countryofresidence)?Noresponse.(cid:3)D3.Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’reusing/curating?Forexample,ifyoucollecteddataviacrowdsourcing,didyourinstructionstocrowdworkersexplainhowthedatawouldbeused?Noresponse.(cid:3)D4.Wasthedatacollectionprotocolapproved(ordeterminedexempt)byanethicsreviewboard?Noresponse.(cid:3)D5.Didyoureportthebasicdemographicandgeographiccharacteristicsoftheannotatorpopulationthatisthesourceofthedata?Noresponse.
14943