SigMoreFunSubmissiontotheSIGMORPHONSharedTaskonInterlinearGlossingTaiqiHe∗,LindiaTjuatja∗,NateRobinson,ShinjiWatanabe,DavidR.Mortensen,GrahamNeubig,LoriLevinLanguageTechnologiesInstituteCarnegieMellonUniversity{taiqih,ltjuatja,nrrobins,swatanab,dmortens,gneubig,lsl}@cs.cmu.eduAbstractInoursubmissiontotheSIGMORPHON2023SharedTaskoninterlinearglossing(IGT),weexploreapproachestodataaugmentationandmodelingacrosssevenlow-resourcelanguages.Fordataaugmentation,weexploretwoap-proaches:creatingartificialdatafromthepro-videdtrainingdataandutilizingexistingIGTresourcesinotherlanguages.Onthemodelingside,wetestanenhancedversionofthepro-videdtokenclassificationbaselineaswellasapretrainedmultilingualseq2seqmodel.Ad-ditionally,weapplypost-correctionusingadictionaryforGitksan,thelanguagewiththesmallestamountofdata.Wefindthatourtokenclassificationmodelsarethebestperforming,withthehighestword-levelaccuracyforAra-pahoandhighestmorpheme-levelaccuracyforGitksanoutofallsubmissions.Wealsoshowthatdataaugmentationisaneffectivestrategy,thoughapplyingartificialdatapretraininghasverydifferenteffectsacrossbothmodelstested.1IntroductionThispaperdescribestheSigMoreFunsubmissiontotheSIGMORPHON2023SharedTaskoninterlin-earglossing.Giveninputtextinatargetlanguage,thetaskistopredictthecorrespondinginterlineargloss(usingLeipzigglossingconventions).IGTisanimportantformoflinguisticannotationforthemorphologicalanalysisoflanguages,andalsoservesasanextremelyvaluableresourceforlan-guagedocumentationandeducationforspeakersoflow-resourcelanguages.Thereweretwotracksforthissharedtask,Track1(closed)andTrack2(open).ForTrack1,sys-temscouldonlybetrainedoninputsentencesandglosses;inTrack2,systemscouldmakeuseofthemorphologicalsegmentationoftheinputaswellasany(non-IGT)externalresources.SincetheTrack2settingbettermatchesthelong-termre-
∗Theseauthorscontributedequallysearchgoalsofourteam,weonlyparticipateinthisopentrack.Inoursubmission,weinvestigatetwodifferentapproaches.First,weattemptdataaugmentationbyeithercreatingourownartificialglossdatabymanipulatingtheexistingtrainingdata,orbyuti-lizingexistingresourcescontainingIGTinotherlanguages(§2).Second,weexploretwodifferentmodelsforglossgeneration(§3).Thefirstbuildsoffthetokenclassificationbaseline,whilethesec-ondusesapretrainedmultilingualseq2seqmodel.Finally,wealsoattempttopost-correctmodeloutputswithadictionary.WeapplythistoGitk-sanandfindthatthis,combinedwithourotherapproaches,resultsinthehighestmorpheme-levelaccuracyforGitksaninTrack2.2DataAugmentationOnemajorchallengeforthissharedtaskisthescaleofdataprovided.Allofthelanguageshavelessthan40klinesoftrainingdata,andallbutArapahohavelessthan10k.Thesmallestdataset(Gitk-san)hasonly31linesofdata.Thus,oneobviousmethodtotryisdataaugmentation.Morespecif-ically,wetrypretrainingourmodelsondifferentformsofaugmenteddatabeforetrainingthemontheoriginaltargetlanguagedata.Weexploredtwoformsofdataaugmentation.First,wegeneratedartificialglossdatainthetar-getlanguagebyswappingwordsintheexistingtrainingdata.Second,weutilizeddatafromODIN(LewisandXia,2010;Xiaetal.,2014)toseeiftransferlearningfromdatainotherlanguagescanhelpimproveperformance.2.1ArtificialDataAchallengeourteamfacedwithrespecttodataaugmentationisfiguringouthowtoobtainaddi-tionaldatawhenwedonothavemuchknowledgeofthelanguages’grammaticalsystems,alongwiththefactthattheselanguagesaregenerallyfrom
209 Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 209–216 July 14, 2023 ©2023 Association for Computational Linguistics


digitallyunder-resourcedlanguagefamilies.Fur-thermore,wewantedoursolutiontobeeasilyim-plementedandrelativelylanguageagnosticduetotimeconstraintsandpracticalusabilityforre-searchersworkingonavarietyoflanguages.Thus,oneavenueofdataaugmentationwetriedwasbycreatingartificialdatafromtheprovidedtrainingdata.Thisrequiresnorule-writingorknowledgeofthegrammarofthelanguage,andthuscouldbeappliedquicklyandeasilytoallofthelanguagesinthesharedtask.Weusedanaiveword-swappingmethodtoran-domlyswapmorphemesthatoccurinsimilarcon-textstocreatenewsentences.Todothis,foreachglossline,wereplaceeachwordstem(thathasaglosslabelaffix)with“STEM”tocreateaskeletongloss.Wenaivelydetermineifalabelisastembycheckingifitisinlowercase.Wedonotdothistowordsthatdonothaveaffixesas(withtheexceptionofUspanteko)wedonothaveaccesstopartsofspeech,anddonotwanttoswapwordsthatwouldcreateanungrammaticalsequence.Wecreateadictionarymappingeachskeletonwordglosstopossibleactualglosses,andmapeachactualglosstopossiblesurfaceforms(wemakenoassumptionsthatthesemappingsareone-to-one).Wethenrandomlysamplekrandomskeletonglosses(inthiscase,weusedkequaltoroughlythreetimestheamountoftrainingdata)andran-domlyfillinwordsthatmatchtheformatofskele-tonwordspresentintheline.(1)to(3)belowillustrateanexampleinthisprocess.Wecreateaskeletongloss(2)fromtheGitksansentencein(1)byreplacingtheallwordstemsthathaveanaffixwith“STEM”inboththesegmentationandglosstiers—inthiscase,only’witxw-itappliestothisstep.Thentocreatetheartificialdatain(3),wereplacetheskeletonwordandcorrespondingglosswithanotherwordfromthetrainingdatathathasthesameskeletonform,inthiscasehahla’lst-it.(1)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSP’witxw-itcome-SX(2)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPSTEM-itSTEM-SX(3)iiCCNJnee-dii-tNEG-FOC-3.InaawhodimPROSPhahla’lst-itwork-SXWhilethismethodmaycreateasomewhatun-naturalinputsurfacesequence(asweareunabletocapturephonologicalchangesinthesurfaceformandcorrespondingtranslationsmaybenonsensi-cal),thismethodguaranteesthatthestructureoftheglossisanaturallyoccurringsequence(asweonlyuseglossskeletonsthatarepresentinthein-put).However,alimitationofthismethodisthatitdoesnotextendtoout-of-vocabularytokensorunseenglossstructures.Furthermore,aswecannotgenerateagold-standardtranslationfortheartifi-cialdata,wedonotmakeuseofatranslationintraining.2.2ODINAnotherpotentialavenuefordataaugmentationistransferlearningfromdatainotherlanguages,whichhasbeenshowntobeaneffectivemethodtoimproveperformanceinlow-resourcesettings(Ruderetal.,2019).TheavailableresourceweutilizeisODIN,ortheOnlineDatabaseforInterlinearText(LewisandXia,2010;Xiaetal.,2014).ODINcontains158,007linesofIGT,covering1,496languages.Weusethe2.1versionofODINdataandconvertthedatasettothesharedtaskformat,andfilteroutlanguageswithfewerthanfiveglossedsentences.However,thereremainssignificantnoiseinthedatasetthatcouldcausesignificantalignmentis-suesforthetokenclassificationmodels.ThereforeweopttoonlytraintheByT5modelsonODIN,inthehopethatthismodelislesssensitivetoalign-menterrors.Indeed,wefindthattheByT5modelfinetunedfirstonODINreceivesaperformanceboostwhenfinetunedagainonthesharedtaskdata.3ModelsWeexploretwomodelsforglossgeneration.Thefirstoneisbuiltuponthetokenclassificationbase-linewithsomeimprovements,andwetreatthismodelasourinternalbaseline.Thesecondmodelwedeploytestswhetherwecanachievecompeti-tiveperformancebyfinetuningapretrainedcharac-terbasedmultilingualandmultitaskmodel,ByT5.Forthismodel,weperformminimalpreprocess-inganduserawsegmentedmorphemesandfreetranslationsifavailable.3.1TokenClassificationTransformerWeusethebaselineTrack2modelprovidedbytheorganizersasastartingpoint.Theoriginalimple-mentationrandomlyinitializesatransformermodelfromthedefaultHuggingfaceRoBERTabasecon-figuration,andusesatokenclassificationobjective
210


1https://en.wikipedia.org/wiki/Lezgin_alphabets
withcross-entropyloss,whereeachglossistreatedasadistincttoken.Themorphemesandfreetrans-lationsaretokenizedbyspaceanddashes,withpunctuationspre-separated.TheyareconcatenatedandseparatedbytheSEPtokenandareusedastheinputstothemodel.WemodifytheoriginalTrack2baselinemodeltoobtainabetterbaseline.WeusepretrainedweightsfromXLM-RoBERTabase(Conneauetal.,2020),insteadofrandomlyinitializingtheweights.Wealsoslightlymodifythemorphemetokenizertoenforcethatthenumberofmorphemetokensmatchesthenumberofoutputglosstokensexactly.Additionally,weintroducetheCOPYtokentoreplacetheglossifitmatchesthecorrespondingmorphemeexactly.AnexamplefromNatuguisshowningloss(4):(4)67COPY.COPYmnc-xbe-1MINIMzloCOPYSkulCOPYWebelievethiswouldimproveperformancebyremovingtheneedtomemorizeglossedcode-switchingandpropernouns,thoughitisonlyef-fectiveifthecode-switchedlanguageisthesameasthematrixlanguage(e.g.Arapaho),andwouldhavenoeffectifthesourcelanguageusesadif-ferentorthographyoriscode-switchedtoanotherlanguage,wheretheglosswouldnotmatchedthemorphemeformexactly.Thismethodalsocom-pressesallpunctuationmarkersintoonetoken,buttheusefulnessofthissideeffectislessclear.Sinceweareusingpretrainedweights,itisthennaturaltoexploreintegratingthepretrainedtok-enizer.SinceXLM-RoBERTawasnottrainedonanyofthesourcelanguages,itmakesthemostsensetoonlyusethepretrainedtokenizertotok-enizefreetranslations,iftheyareavailable,andextendthevocabularytoincludemorphemes.3.2FinetunedByT5Multi-taskandmulti-lingualpretrainedlargelan-guagemodelshavebeenshowntobeeffectiveformanytasks.Weexplorewhethersuchmodelscanbeusedeffectivelyforglossing.Weconductex-perimentswithbothmT5(Xueetal.,2021)andByT5(Xueetal.,2022),butByT5ispreferredbecauseittakesrawtexts(bytesorcharacters)asinputsandintheoryshouldbemoreeffectiveforunseenlanguages.Weuseapromptbasedmultilin-gualsequencetosequenceobjectiveforbothmod-els.Theprompttemplateis:“Generateinterlin-earglossfrom[sourcelanguage]:[segmentedmorphemes]withits[matrixlanguage]trans-lation:[freetranslation]Answer:”.Datafromalllanguagesaremixedtogetherandshuffled,withnoupordownsampling.Afterinitialexperi-ments,wefindByT5outperformsmT5acrossalllanguages,andthereforeweonlyconductsubse-quentexperimentsonByT5andreportthosere-sults.Uponinitialexperiments,wealsofindtheresultsforLezgitobelowerthanexpected.Wehypothe-sizethatthefactthatthedataareinCyrillicscriptcausesthisdeficiency,sinceByT5wastrainedonfarlessCyrillicdatathandataintheLatinscript.Thereforewecreateanautomaticromanizationtool,sourcedfromWikipedia1andintegratedintheEpitranpackage(Mortensenetal.,2018),andconvertallLezgidatatoLatinscriptforByT5fine-tuning.AfterinspectingtheoutputsoftheByT5models,wefindcaseswherepunctuationsareattachedtothepreviousglosses,insteadofbeingseparatedbyaspaceasisstandardinthetrainingsets.Thisisprobablyduetothefactthatthemodelwaspre-trainedonuntokenizeddataandthisbehaviorispreserveddespitefinetuningontokenizeddata.Wethereforeuseasimpleregularexpressionbasedtok-enizertofixtheinconsistencies.WenoticethattheprocedureonlygivesperformanceboostonGitk-san,Lezgi,Uspanteko,andNatugu,andsoweonlyapplytheproceduretothoselanguages,leavingtherestoftheoutputsunchanged.4DictionaryPost-correction:GitksanOneofthekeychallengesforextremelylowre-sourcelanguagesistheintegrationofstructuredlinguisticdatainotherforms,suchasadictionary,intomachinelearningpipelines.Wetestasimplepost-correctionmethodfromapre-existingdictio-naryonGitksanonly,duetoitsuniquecombinationoflowresourceandeasilyobtainabledictionaryinmachinereadableform.Weusethedictionarycom-piledbyForbesetal.(2021),withoutconsultingthemorphologicalanalyzersthattheyalsoprovided.Atinferencetime,ifamorphemeisunseendur-ingtraining,wesearchfortheexactforminthedictionary.Wealsoexpandthesearchtoallsubse-quencesofmorphemeswithintheenclosingword,plusthepreviouswholewordincaseswhereapar-ticleisincludedinthedictionaryform.Thefirst
211


matcheddefinitionisusedastheglossandifnoneofthesearchyieldsanexactmatch,wefallbacktothemodelprediction.Weonlyapplythismethodtothetokenclassificationmodelsbecausethealign-mentbetweenmorphemesandglossesisdirectlyestablished,whereastheseq2seqmodelsdonotguaranteethatthenumberofglossesmatchesthenumberofmorphemes.5ResultsandDiscussionTables1and2showoursystems’performance(aswellastheoriginalbaseline)onthetestdatawithre-specttoword-andmorpheme-levelmicro-averagedaccuracy,respectively.Overall,thetokenclassifica-tionmodeltrainedfirstontheartificiallygeneratedaugmenteddataperformthebest,withthemodeltrainedonthesharedtaskdataonlynotfarbehind.Meanwhile,ByT5modelsperformworse,withthemodelfinetunedfirstonODINtrailingourbestmodelbyafewpercentagepoints,whilethemodelfinetunedfirstonaugmenteddataperformsworsethanthebaseline.5.1DataAugmentationOverall,wefinddataaugmentationtobeuseful.Withartificiallygenerateddata,weseetheeffectsareperhapsgreatestforthemid-resourcelanguages(ddo,lez,ntu,nyb,usp),whilethehighestandlowestresourcedlanguagesdidnotreceivemuchbenefitfrompretrainingontheartificialdata.Wethinkthisisperhapsbecausethereisa“sweetspot”withrespecttotheamountofdatathatisrequiredtotrainamodel.Ifthereisenoughdataalready,inthecaseofArapaho,thenthenoisinessofartificialdatawouldout-weightthebenefitoftrainingonthem.Ontheotherendofthescale,Gitksanperhapsneedsmoresyntheticdatafordataaugmentationtoyieldmeaningfulimprovements.ForByT5models,artificiallygenerateddataseemtohavetheoppositeeffect,whereperfor-manceissignificantlydegraded.Aspeculationforthiseffectisthefactthepretrainedmodelismoresemanticallyaware,andsincetheartificiallygen-eratedsentencescouldbenonsensical,themodelcouldbecomeconfused.Ontheotherhand,pre-trainingonODINyieldsimprovementsforthema-jorityofthelanguages2.ThisisencouragingsincewedidnotperformmuchpreprocessingforODIN,
2TsezistheonlylanguagethatappearedinODIN(68sentences).Wedidnotremoveitfromthecorpusbutthisshouldhavelittleinfluenceontheperformancebecausethesizeofthedatasetisverysmall.andthereisdefinitelystillroomtomakethedatacleanerandmoreinternallyconsistent,whichinturnshouldresultinabettermodel.5.2ChoiceofHyperparametersWefindthechoiceofhyperparametersofthetokenclassificationmodelstobenecessarilylanguageanddatasetspecific.ArapahoandGitksaninpar-ticularneedspecialattention,wherethenumberoftrainingepochsneedtobeadjustedfortheveryhighandlowdatasize.WealsodevelopedmostoftheoptimizationonthetokenclassificationmodelonArapaho.However,wedidnothavetimetopropagatethechanges(usingpretrainedtokenizer,savingthelastmodelinsteadofthemodelwiththelowestvalidationloss)totherestoflanguagessinceinitialexperimentshowedthatpretrainedtokeniz-ersdidnotimproveontheotherlanguages.How-ever,afterthesubmissiondeadlineisconcluded,weranmoreexperimentsanddiscoveredthataddingpretrainedtokenizersrequiresmoretrainingsteps,andthetrainingisbettercontrolledbyspecifyingthetrainingstepsinsteadofepochs.Wedonotin-cludethoselatestexperimentsinthispaper,butourtokenclassificationmodelshavethepotentialtoperformbetterwithmorehyperparametertuning.5.3In-VersusOut-of-VocabularyErrorsOnedimensionoferroranalysisweinvestigatedwaswhatproportionofoursystems’errorscomefrommorphemesorwordsthatareeitherinoroutofthetrainingdatavocabulary.Wecountamor-phemeorwordasin-vocabularyifthesurfaceformanditscorrespondingglossco-occurinthepro-videdtrainingdata(notincludingthedevelopmentdata,asourmodelsareonlytrainedonthetrainset).NotethatthereisamuchlargerproportionofOOVwordsasopposedtomorphemesduetothefactthatanunseenwordcanbecomposedofdifferentcombinationsofseenmorphemes.Table3showstheproportionofmorphemesandwordsthatareout-of-vocab(OOV)withinthetestset.Whilenearlyallthelanguageshavelessthan10%oftheirmorphemesclassifiedasOOV,GitksannotablyhasarelativelylargeportionofOOVtestdata,with≈45%ofmorphemesand≈78%ofwordsbeingOOV.Tables4and5showourmodels’performancesonin-versesout-of-vocabtokensatthemorphemeandwordlevels,respectively.Whilewewouldintuitivelyexpectthatword-levelOOVaccuracybeaboutthesameorworsethanmorpheme-levelOOV
212


Table2:Morpheme-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselinewithartificialpretraininganddictionarypost-correctionisthehighestGitksanaccuracyreportedoutofallsharedtasksubmissions.
xlmr-base91.3684.3547.47/52.8280.1788.3585.8480.0880.42xlmr-aug89.3488.1546.89/52.3982.3685.5389.4983.0881.48
Table3:ProportionofmorphemesandwordsthatareOOVwithinthetestset.accuracy,thisisnotthecaseduetothefactthatalargeportionofout-of-vocabwordsareformedwithin-vocabmorphemes.Formostlanguages,withtheexceptionofGitksan,thereappearstobeatrade-offbetweenbetterin-vocabmorphemeperformancewithXLMRandperformanceout-of-vocabwithByT5.6RelatedWorkTherehavebeenavarietyofapproachestotheprob-lemof(semi-)automaticallygeneratinginterlineargloss.BaldridgeandPalmer(2009)investigatetheefficacyofactivelearningforthetaskofinterlinearglossing,usingannotationtimerequiredbyexpertandnon-expertannotatorsastheirmetric.Thesys-temtheyusetogenerateglosslabelsuggestionsisastandardmaximumentropyclassifier.Arule-basedapproachbySnoeketal.(2014)utilizesanFSTtogenerateglossesforPlainsCree,focusingonnouns.Samardži´cetal.(2015)viewthetaskofglossingsegmentedtextasatwo-stepprocess,firsttreatingitasastandardPOStaggingtaskandthenaddinglexicalglossesfromadictio-nary.TheydemonstratethismethodonaChintangcorpusofabout1.2millionwords.Anumberofotherworksfocusingoninterlinearglossingutilizeconditionalrandomfield(CRF)models.MoellerandHulden(2018)testthreedifferentmodelsonaverysmallLezgidataset(<3000words):aCRF(thatoutputsBIOlabelswiththecorrespondingglosspercharacterinthein-put),asegmentationandlabellingpipelinethatuti-lizesaCRF(forBIOlabels)andSVM(forglossla-bels),andanLSTMseq2seqmodel.TheyfindthattheCRFthatjointlyproducestheBIOlabelsandtagsproducedthebestresults.McMillan-Major(2020)utilizestranslationsintheirtrainingdatabycreatingtwoCRFmodels,onethatpredictsglossfromthesegmentedinputandanotherthanpre-
ModelarpddogitlezntunybuspAVG
ModelarpddogitlezntunybuspAVG
byt5-base78.8680.3214.8460.72b76.6776.7377.2166.48byt5-aug73.2762.374.1738.6055.1169.2570.8553.38byt5-odin80.5682.7920.5763.7777.9782.5975.7269.14
baseline91.1185.3425.3351.8249.0388.7182.4867.69
baseline85.4475.7116.4134.5441.0884.3076.5559.14
xlmr-base85.8773.7727.86/34.11a74.1582.9980.6173.4772.14xlmr-aug82.9280.0724.74/31.2577.7778.7285.5377.5173.39
aWereportbefore/afterdictionarybasedpost-correctionforGitksan.bWetrainedthismodelwithoutromanizingLezgi.Table1:Word-levelaccuracyofoursubmittedsystems.Bestperformanceperlanguageinthetableisbolded.TheXLMRbaselineisthehighestArapahoaccuracyreportedoutofallsharedtasksubmissions.
byt5-base78.8275.7712.5944.1062.4078.9774.2560.99byt5-aug72.1057.932.6026.2435.6270.0167.7347.46byt5-odin80.8178.2412.7450.0063.3985.3073.2563.39
Morph0.0430.0090.4500.0560.0340.0190.070
arpddogitlezntunybusp
Word0.2420.1550.7810.1690.2140.0840.200
213


byt5-aug74.7658.243.4240.2736.5471.2770.56
35.48
Table4:Morpheme-levelaccuracyoveralltokensofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.
24.67
12.31
xlmr-base95.2085.1282.8984.7990.8787.4686.05
23.24
45.07
xlmr-aug93.7283.8594.0587.6489.2490.8191.11
29.33
24.10
49.79
byt5-odin91.9387.6663.1073.7885.9387.6083.46
59.51
23.54
byt5-odin83.4778.5518.4262.9064.3886.8575.23
18.26
22.41
49.17
Table5:Word-levelaccuracyofoursubmittedsystems,splitbyin-versusout-of-vocab.CellshighlightedingrayindicateOOVaccuracy.dictsfromthetranslation,andthenusesheuristicstodeterminewhichmodeltoselectfromforeachmorpheme.BarrigaMartínezetal.(2021)usedaCRFmodeltoachieve>90%accuracyforgloss-ingOtomiandfindthatitworksbetterthananRNN,whichiscomputationallymoreexpensive.Otherworks,includingoursystems,haveturnedtoneuralmethods.Kondratyuk(2019)leveragespretrainedmultilingualBERTtoencodeinputsentences,thenapplyadditionalword-levelandcharacter-levelLSTMlayersbeforejointlydecod-inglemmasandmorphologytagsusingsimplese-quencetagginglayers.Furthermore,theyshowthattwo-stagetrainingbyfirsttrainingonalllan-guagesfollowedbytrainingonthetargetlanguageismoreeffectivethantrainingthesystemonthetargetlanguagealone.AnapproachbyZhaoetal.(2020),likeMcMillan-Major(2020),makesuseoftranslationsavailableinparallelcorpora,butdosobyusingamulti-sourcetransformermodel.Theyalsoincorporatelengthcontrolandalignmentdur-inginferencetoenhancetheirmodel,andtesttheirsystemonArapaho,Tsez,andLezgi.7ConclusionInoursharedtasksubmission,weexploredataaug-mentationmethodsandmodelingstrategiesforthetaskofinterlinearglossinginsevenlow-resourcelanguages.Ourbestperformingmodelsareto-kenclassificationmodelsusingXLMR.Wedemon-stratethatpretrainingonartificialdatawithXLMRisaneffectivetechniqueforthemid-resourcetestlanguages.Additionally,inourerroranalysiswefindthatwemayhaveactuallyundertrainedourtokenclassificationmodels,andthusoursystemsmayhavethepotentialtoperformbetterwithad-ditionalhyperparametertuning.WhileourByT5modelsdidnotperformaswellasourothersys-tems,weshowthatpretrainingonODINdataiseffective,despitethisdatabeingverynoisy.Finally,wealsodemonstrateimprovementsbyutilizingadictionarytopost-correctmodeloutputsforGitk-san.
0.00
0.00
0.00
0.00
11.24
0.82
12.86
14.52
23.60
4.97
56.36
45.65
21.14
43.37
1.61
47.52
46.94
29.69
13.67
xlmr-base95.9378.1895.2384.2493.1485.8586.27
5.79
28.04
2.00
17.00
0.41
16.08
14.67
19.35
Modelarpddogitlezntunybusp
Modelarpddogitlezntunybusp
7.49
28.09
28.09
44.81
3.23
3.23
54.44
2.33
byt5-aug87.2268.6910.7146.0665.1374.5981.44
30.20
xlmr-aug92.9888.9484.7487.1087.8891.1789.31
48.70
40.00
28.63
8.67
2.60
2.60
9.68
214


AcknowledgementsThisworkwassupportedbyNSFCISERIgrantnumber2211951,FromAcousticSignaltoMor-phosyntacticAnalysisinoneEnd-to-EndNeuralSystem.ReferencesJasonBaldridgeandAlexisPalmer.2009.Howwelldoesactivelearningactuallywork?Time-basedeval-uationofcost-reductionstrategiesforlanguagedocu-mentation.InProceedingsofthe2009ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages296–305,Singapore.AssociationforCompu-tationalLinguistics.DiegoBarrigaMartínez,VictorMijangos,andXi-menaGutierrez-Vasques.2021.Automaticinterlin-earglossingforOtomilanguage.InProceedingsoftheFirstWorkshoponNaturalLanguageProcessingforIndigenousLanguagesoftheAmericas,pages34–43,Online.AssociationforComputationalLin-guistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzmán,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.ClarissaForbes,GarrettNicolai,andMiikkaSilfverberg.2021.AnFSTmorphologicalanalyzerforthegitksanlanguage.InProceedingsofthe18thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages188–197,Online.AssociationforComputationalLinguistics.DanKondratyuk.2019.Cross-linguallemmatizationandmorphologytaggingwithtwo-stagemultilin-gualBERTfine-tuning.InProceedingsofthe16thWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages12–18,Florence,Italy.AssociationforComputationalLinguistics.WilliamD.LewisandFeiXia.2010.DevelopingODIN:AMultilingualRepositoryofAnnotatedLanguageDataforHundredsoftheWorld’sLanguages.Liter-aryandLinguisticComputing,25(3):303–319.AngelinaMcMillan-Major.2020.Automatingglossgenerationininterlinearglossedtext.ProceedingsoftheSocietyforComputationinLinguistics,3(1):338–349.SarahMoellerandMansHulden.2018.Automaticglossinginalow-resourcesettingforlanguagedoc-umentation.InProceedingsoftheWorkshoponComputationalModelingofPolysyntheticLanguages,pages84–93.DavidR.Mortensen,SiddharthDalmia,andPatrickLittell.2018.Epitran:PrecisionG2Pformanylan-guages.InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).SebastianRuder,MatthewEPeters,SwabhaSwayamdipta,andThomasWolf.2019.Transferlearninginnaturallanguageprocessing.InProceed-ingsofthe2019conferenceoftheNorthAmericanchapteroftheassociationforcomputationallinguistics:Tutorials,pages15–18.TanjaSamardži´c,RobertSchikowski,andSabineStoll.2015.Automaticinterlinearglossingastwo-levelsequenceclassification.NoamShazeerandMitchellStern.2018.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InProceedingsofthe35thInternationalConferenceonMachineLearning,volume80ofProceedingsofMachineLearningResearch,pages4596–4604.PMLR.ConorSnoek,DorothyThunder,KaidiLõo,AnttiArppe,JordanLachler,SjurMoshagen,andTrondTrosterud.2014.ModelingthenounmorphologyofPlainsCree.InProceedingsofthe2014WorkshopontheUseofComputationalMethodsintheStudyofEndangeredLanguages,pages34–42,Baltimore,Maryland,USA.AssociationforComputationalLinguistics.FeiXia,WilliamLewis,MichaelWayneGoodman,JoshuaCrowgey,andEmilyM.Bender.2014.En-richingODIN.InProceedingsoftheNinthInter-nationalConferenceonLanguageResourcesandEvaluation(LREC’14),pages3151–3157,Reykjavik,Iceland.EuropeanLanguageResourcesAssociation(ELRA).LintingXue,AdityaBarua,NoahConstant,RamiAl-Rfou,SharanNarang,MihirKale,AdamRoberts,andColinRaffel.2022.ByT5:Towardsatoken-freefuturewithpre-trainedbyte-to-bytemodels.Transac-tionsoftheAssociationforComputationalLinguis-tics,10:291–306.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483–498,On-line.AssociationforComputationalLinguistics.XingyuanZhao,SatoruOzaki,AntoniosAnastasopou-los,GrahamNeubig,andLoriLevin.2020.Auto-maticinterlinearglossingforunder-resourcedlan-guagesleveragingtranslations.InProceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages5397–5408,Barcelona,Spain(On-line).InternationalCommitteeonComputationalLin-guistics.
215


AHyperparameterSettingsWeuseAdafactor(ShazeerandStern,2018)astheoptimizeracrossallexperiments,withthede-faultschedulerfromHuggingFaceTransformers,abatchsizeof32forRoBERTabasedmodelsandabatchsizeof4withagradientaccumulationstepof8forByT5basedmodels.Wetrainthetokenclassificationmodelsfor40epochsexceptforAra-paho,onwhichwetrain20epochs,andGitksan,onwhichwetrain2,000steps.WetraintheByT5basedmodelsfor20epochsonallofthedatamixedtogether.
216