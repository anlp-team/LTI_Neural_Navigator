3 2 0 2
g u A 8 2
]
Y C . s c [
1 v 1 0 6 4 1 . 8 0 3 2 : v i X r a
Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery Fernando Diaz∗ Google/MILA Montreal, Quebec, Canada diazf@acm.org
Golnoosh Farnadi∗ HEC/Universite de Montreal/MILA Montreal, Quebec, Canada farnadiq@mila.quebec
Rebecca Salganik Universite de Montreal/MILA Montreal, Quebec, Canada rebecca.salganik@umontreal.edu
Abstract
1 Introduction
As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.
CCS Concepts • Computing methodologies → Structured outputs; Batch learn- ing.
Keywords
Graph Neural Networks, Algorithmic Fairness, Individual Fairness
ACM Reference Format: Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi. 2018. Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discov- ery. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2018 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX
The proliferation of market activity on digital platforms has acted as a catalyst for research in recommendation, search, and information retrieval [36]. At its core, the goal of this research is to design systems which can facilitate users’ exploration of an extensive item catalogue: be it in the domain of journalism [63], films [32], fashion [21], music [41, 55, 56], or otherwise. Within this larger goal of recommendation, each domain comes with its own specifics that differentiate it from other settings [10, 26, 49]. Particular to the music streaming domain, an extensive body of work has explored the importance of discovery, exploration, and novelty in the larger goal of performing music recommendation [18, 24, 29, 42, 47, 52]. Broadly, discovery can be considered the ability of a curatorial system to expose users to relevant content that they would not have manually discovered themselves [29, 35, 52]. And, most significantly, a collection of works have shown that music discovery to be the second most important factor for customer loyalty respective to a particular streaming platform due to the gratifying nature of constructing playlists and interacting with an algorithmic curatorial system [42, 47, 52].
Crucially, recent work in this domain has begun to uncover an inverse relationship between novelty, one of the keys to discov- ery, and the notion of popularity bias [39, 63]. Within the broader recommendation community, popularity bias has long been an im- portant topic of research. This phenomenon manifest itself when algorithmic reliance on pre-existing data causes new, or less well known items, to be disregarded in favor of previously popular items [4, 13, 17, 38, 50, 60]. And, particularly in the context of discovery, where purpose of a user’s engagement with algorithmic curation hinges on exposure to musical items which they would not have already been familiar with, the presence of popularity bias can clearly hinder a system’s ability to serve this need. In this work, we explore the intrinsic interplay between discovery and popu- larity bias through the lens of graph neural network (GNN) based recommender systems [28, 65]. In the graph space, popularity of individual items is deeply interlaced with the degree centrality of a node, or the number of outgoing edges that leave this node and connect it to others in the graph. This is because the innate process of representation learning is affected by the number of neighbors a node has [40]. And, thus, a node’s centrality can dictate the quality of its learned representation. This suggests that duplicating the fea- ture information of an extremely popular song, creating a new song using these duplicate features, and randomly placing it once at the edge of a graph, will significantly impact its learned representation, even if everything about the song remains exactly the same. As we show in our experimentation, one solution to this disparity lies in


Conference’17, July 2017, Washington, DC, USA
a debiasing method that is aware of similarities between musical items and is, thus, grounded in the musical domain.
However, current approaches for mitigating popularity bias in recommender systems approach this task in a domain agnostic approach [3, 45, 54, 62, 68]. Such abstraction can be extremely rele- vant to domains in which item and user level features are scarce, sparse, or non existent. However, in an environment like music streaming where there is a plethora of valuable feature informa- tion, we believe that grounding fairness notions in domain specific attributes can prove incredibly valuable. In addition, a majority of these methods focus on using either group [54, 68] or counterfac- tual fairness [70, 72], often relying on a binary sensitive attribute to encode popularity. This can cause intrinsic limitations because popularity between items is not necessarily a binary state and such attributes may not be readily available.
In this work, we propose a domain aware, individual fairness based approach for facilitating engaging music discovery. In order to facilitate the domain awareness of our approach we generate nuanced multi-modal track features, extensively augmenting two publicly available datasets. Using these novel feature sets, we show the importance of integrating musical similarity into a debiasing technique and show the effects of our method at learning expressive representations of items that are robust to the effects of popularity bias in the graph setting. Grounding our approach in the musical domain empowers us to leverage a ranking-based individual fair- ness definition and extend it to the bipartite graph setting. In doing so, we design a method that uses music features to fine-tune item representations such that they are reflective of information that is, in essence, a ground truth to the listening experience: two songs that sound similar should, at least somewhat, reflect this similarity in their learned representations. Finally, we compare our individ- ual fairness-based method with three other methods which are grounded in other canonical fairness notions and are not domain- aware. Through a series of empirical results, we show that our fairness framework enables us to outperform a series of accepted fairness benchmarks in both performance and recommendation of lesser known content on two important music recommendation tasks. In summary, the contributions of this paper are the following: (1) Problem Setting: we define the task of music discovery through the lens of domain-aware individual fairness, show- ing the intrinsic connections between individual fairness, musical similarity, popularity bias, and music discovery. (2) Dataset Design: we extensively augment two classic mu- sic recommendation datasets to generate a set of nuanced multi-modal track features, laying the foundation for future domain-aware mitigation techniques.
(3) Method: (1) we provide a novel technical formulation of popularity bias (2) design a domain-aware ranking based individual fairness approach to mitigating popularity bias in graph-based recommendation.
(4) Experiments: we show that our method outperforms three state of the art fairness benchmarks in the cold start setting.
Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi
Figure 1: Classic definition of Long Tail popularity settings[2]
2 Related Work
In this section we contextualize our work by presenting relevant literature on the subjects of (1) popularity bias and (2) graph neural network (GNN) based recommender systems.
2.1 Popularity Bias in Recommendation
Most broadly, popularity bias refers to a disparity between the treatment of popular and unpopular items at the hands of a recom- mender system. As such, this term is loosely tied to a collection of complementary terms including exposure bias [20], superstar eco- nomics [6], long tail recommendation [45], the Matthew effect [48], and aggregate diversity [5, 14]. There have been several different approaches to formulating popularity through some quantitative definition. One body of work defines popularity with respect to in- dividual items’ visibility [20, 45, 69]. Another group of approaches attempts to simplify this process by applying some form of binning to the raw appearance values. Most notably, the long tail model [13, 23, 31, 50, 66] has risen to prominence as a popularity definition. As shown in Figure 1, we can see that the first 20% of items, called short head, take up a vast majority of the user interactions and the remaining 80%, or long tail and distant tail, have, even in aggregate, significantly fewer interactions. Often, splitting items into the short head and long tail (either including or removing distant tail) to define disparity in popularity can overcome the issues of range while still representing concrete disparities in item level visibility. There has been a lot of work done analyzing and codifying the nature of popularity bias in recommender systems [11, 17, 37, 38]. Approaches to mitigating popularity bias are often grounded in one of three methods: pre-processing [8], in-processing [54, 62, 71], or post-processing, [3, 45]. These mitigation strategies are often based on the instrumentation of various canonical fairness notions such as group fairness [3, 8, 54, 58, 68], counterfactual fairness [62, 69, 71], or individual fairness [15, 61].
We contrast our work with previous individual fairness ap- proaches in our use of the music feature space as a form of domain expertise in definition of item-item similarity. We argue that with- out this “anchoring” an individual fairness method that uses the output of a recommender model, whether it be in learned represen- tation [61] or the relevance score [15], is already influenced by an item’s popularity. Finally, in addition to the classical formulation of popularity bias, a group of works have explored the connection between popularity bias and novelty [44, 70, 72] where various metrics are designed to evaluate the novelty of a recommended list. We see our work as complimentary to the exploration in this area however, we differentiate our problem formulation because while novelty is an important aspect of discovery, without domain


Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
awareness novelty alone does not account for musical similarity - a critical aspect of the discovery setting.
2.2 GNNs in Recommendation
In recent years various graph neural network (GNN) architectures have been proposed for the recommendation domain [64]. For brevity, we will focus only on the two methods that are used as the backbone recommenders to the fairness mitigation techniques discussed later in this paper, however we refer to the following surveys [28, 65] for recent innovations in this domain.
In particular, PinSage [67] is an industry solution to graph-based recommendation. Unlike many competing methods, which train on the entire neighborhood set of a node, PinSage trains on a randomly sampled subset of the graph. In order to construct neighborhoods, PinSage uses 𝑘 random walks to select the top 𝑚 most relevant neighbors to use as the neighbor set. However, it is important to note that PinSage learns representations of items but not users. Meanwhile, LightGCN [34], is a method that learns both user and item embeddings simultaneously. Since its proposal in 2020, it is still considered state of the art.
3 Methodology
In this section we detail the dataset augmentation procedure and architecture of our domain-aware, individually fair music recom- mendation system. First, we introduce our datasets in Section 3.1. Then, following the problem setting in Section 3.2, we reformulate popularity bias in Section 3.3 and introduce our domain-aware, individually fair music recommender system in Section 3.4.
3.1 Dataset Augmentation Procedure
One of the limitations with working on music recommendation is the scarcity of up-to-date, publicly available feature-based datasets. This is because the datasets which are available are often purely interaction-based, meaning that they lack the necessary track-level features to implement domain-aware fairness measures. Thus, one of the preliminary steps of our work was the extensive augmenta- tion of two publicly available datasets: LastFM [46] and the Million Playlist Dataset [16]. The augmentation and release of these two complementary datasets is an important contribution because it paves the way for further work in domain-aware music recommen- dation and alleviates the reproducibility issues often posed by the use of music datasets. Although we are limited by the number of publicly available music datasets which are compatible with our feature augmentation procedure, we believe that in selecting these two datasets, we highlight the benefits of our methodology on a broad range of settings related to music recommendation. First, these datasets encompass two important levels of recommendation: playlist (MPD) and user (LFM) based. Second, they showcase two different methods of user feedback data: implicit and explicit. MPD consists of user generated playlists meaning that its interactions consist of songs which are explicitly pronounced as relevant due to the explicit nature of a user selecting the song for their playlist. Meanwhile, LFM contains user/song interactions that are gathered by aggregating all the songs that a user clicked on (even if they did not necessarily finish or enjoy the content). Thus, these implicit interactions have no guarantee of relevance, making the dataset more prone to noise. And, particularly in the cold start setup (see
Conference’17, July 2017, Washington, DC, USA
Section 4.1), this can significantly increase the difficulty of making predictions because implicit interactions are less indicative of a user’s latent taste and less homogeneous in nature than that of a unified playlist.
We augment both of our datasets to include a rich set of features scraped from Spotify API [1]. To achieve this, we draw on a large body of work from the music information retrieval community (MIR) [27, 30]. We will publicly release the constructed datasets, the construction code, along with the code for using various feature sets, in our repository upon the publication of this paper. The details of the augmented features are as follows.
(1) Sonic features. Spotify has a series of 9 proprietary features which are used to define the audio elements associated with a track. They are danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, and tempo. Each of these features is a continuous scalar value. In order to normalize the scales, we apply 10 leveled binning to the values.
(2) Genre features. We identify the primary artist associated with each collect all the genre tags associated with them. For the emebddings, we select the top 20 and convert them to a one-hot encoding.
(3) Track Name features. For each song in the dataset, we extract the song title and pass it through a pre-trained lan- guage transformer model, BERT [19], into an embedding of dimension 512.
(4) Image features. For each song in the dataset we extract the associated album artwork. We pass this image through a pre-trained convolutional neural network, ResNet50 [33], to generate an embedding of dimension 1024.
3.2 Problem Setting
The task of performing recommendation can be seen as link pre- diction an undirected bipartite graph. We denote such undirected bipartite graph as 𝐺 = (𝑉 , 𝐸). The note set 𝑉 = 𝑇 ∪ 𝑃 consists of a set containing song (or track) nodes, 𝑇 , and playlist (or user) nodes, 𝑃 (or 𝑈 ). The edge set 𝐸 are defined between a playlist 𝑝𝑘 (or user 𝑢𝑘 ) and a song 𝑡𝑖 if 𝑡𝑖 is contained in 𝑝𝑘 (or listened to by 𝑢𝑘 ). Following this setting, our goal (link prediction) is to predict whether any two song nodes 𝑡𝑖, 𝑡 𝑗 ∈ 𝑇 share a common parent playlist 𝑝.
3.3 Reformulating Popularity Bias
3.3.1 Defining Popularity As mentioned in Section 2.1, there is no true consensus within the community on how to define popularity. Here, we present a methodology which we believe allows for both the granularity and expressiveness necessary to highlight differ- ences among various mitigation methods. Broadly, our method con- sists of important steps (1) logarithmic smoothing and (2) binning. In doing so, we combine the best of each methodology. Applying a logarithmic transformation to the raw values, solves the scaling issues that are caused by the extremes of the long-tail distribution. Meanwhile, binning allows us to provide aggregate statistics that highlight large scale patterns in the recommendations. And, while there are many methods which apply binning [3, 20, 54], with- out the logarithmic smoothing, due to the nature of our datasets’


Conference’17, July 2017, Washington, DC, USA
Figure 2: Binning procedure for popularity definition. We show the breakdown of the bin locations for both dataset using our method as compared with the classic long tail model [45]. We can see that our logarithmic smoothing and increased bin count allow for a more granular visualization of popularity between various item groups.
distributions the amount of items in the bins would be unevenly distributed, leaving some bins empty. Finally, we select 10 bins based on the distribution of the datasets and the formulation of our BOOST methodology (see Section 3.4).
We use the following steps to define our popularity setting. First, we count the number of times each song track, 𝑡𝑖 appears within the playlist (or user) training interactions such that for each 𝑡𝑖 , 𝑎𝑡𝑖 = ∣ {𝑝𝑖 ∶ 𝑡𝑖 ∈ 𝑝𝑖 } ∣. Then, we apply the base 10 logarithmic smoothing = log10 (𝑎𝑡𝑖 ). Finally, to these values such that for each 𝑡𝑖 , pop𝑡𝑖 we apply binning onto these values to split them into 10 groups such that for each 𝑡𝑖 , pop_bin (𝑡𝑖 ) ∈ {0, . . . , 9} where bin 9 has a higher popularity value than bin 0. The visualization of this binning procedure and its comparison with the long tail method can be seen in Figure 2. As demonstrated by our visualizations, transforming the raw values into the logarithmic space shows that the bins are filled in relatively even intervals, where, as the popularity increases, so does the number of songs included in a bin.
We showcase the gains that our method has over the canonical long tail model in Figure 2 where we compare the positioning of our binning methodology with the classic long tail model. Furthermore, as we later show Figures 5 and 6 our formulation of popularity is able to elucidate crucial differences among both the datasets and baseline model performances on these datasets.
3.3.2 Popularity Bias and Music Discovery In addition we formalize the inverse relationship between music discovery and popularity bias. For each song track, 𝑡𝑖 ∈ 𝑇𝑂𝐺 , we generate a counterfactual example song, 𝑡 ∗ 𝑖 ∈ 𝑇𝐶𝐹 , where everything about the features is exactly the same and the only difference is that 𝑡𝑖 has a high degree while 𝑡 ∗ 𝑖 has a degree of one. We calculate the distance between an original song node, 𝑡𝑖 and its counterfactual duplicate, 𝑡 ∗ 𝑖 . A system with high potential for musical discovery will have a low distance between the songs, showing a low popularity bias and an understanding of musical similarity. We will return to this formulation in Section 5.1, showing that a node’s placement and degree in the graph can exacerbate the presence of popularity bias, reflecting itself in the node’s learned representation.
3.4 Mitigating Popularity Bias Through
Individual Fairness
Ranking-based individual fairness. REDRESS is an individual fairness framework proposed by Dong et al. for learning fair repre- sentations in single node graphs. We extend this framework to the
Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi
bipartite recommendation setting and integrate it into our popu- larity bias mitigation approach. In the REDRESS setting, individual fairness requires that nodes which were similar in their initial fea- ture space should remain similar in their learned representation embeddings [25]. More concretely, for each song node, 𝑡𝑖 , and node pair 𝑡𝑢, 𝑡𝑣 in a graph 𝐺, similarity is defined on the basis of the cosine 𝑑 similarity metric, 𝑠(⋅, ⋅), as applied to either a feature 𝑋 [𝑣] ∈ R , 𝑚 or learned embedding set, 𝑍 [𝑣] ∈ R . Applying this procedure in a pairwise fashion produces two similarity matrices. The first, or apriori similarity, 𝑆𝐺 , in which similarity is calculated on input fea- tures and the second, or learned similarity, 𝑆𝑍 , in which similarity is calculated between learned embeddings generated by some GNN model, 𝑀. The purpose of REDRESS is to formulate this as a ranking on the basis of similarity and differentiate on the disparity between differences in rankings of the two representational spaces. Thus, drawing on principles from learn to rank [9], each entry in these similarity matrices is re-cast as the probability that node 𝑡𝑖 is more similar to node 𝑡𝑢 than 𝑡𝑣 and transformed into an apriori proba- bility tensor, 𝑃𝐺 ∈ R∣𝑇 ∣×∣𝑇 ∣×∣𝑇 ∣, and a learned probability tensor, 𝑃𝑍 ∈ R∣𝑇 ∣×∣𝑇 ∣×∣𝑇 ∣. Having defined these two probability tensors, for each individual node the fairness loss, 𝐿𝑡𝑢,𝑡𝑣 (𝑡𝑖 ), can be treated the cross entropy loss applied to these probability distributions such that for an individual node, 𝑡𝑖 :
𝐿𝑡𝑢,𝑡𝑣 (𝑡𝑖 ) = −𝑃𝐺 [𝑢, 𝑣, 𝑖] log 𝑃𝑍 [𝑢, 𝑣, 𝑖]
− (1 − 𝑃𝐺 [𝑢, 𝑣, 𝑖]) log(1 − 𝑃𝑍 [𝑢, 𝑣, 𝑖])
and aggregated over all nodes 𝑡𝑖 ∈ 𝑉 as:
𝐿fairness =
∣𝑇 ∣ ∑ 𝑖
∣𝑇 ∣ ∑ 𝑢
∣𝑇 ∣ ∑ 𝑣
𝐿𝑡𝑢,𝑡𝑣 (𝑡𝑖 )
Individually fair music discovery. The original formulation of individual fairness requires some form of domain expertise [25] to determine how (dis-)similar two items are. For the music dis- covery domain, we use music features (see Section 3.1 for exact details) as the basis for calculating cosine similarity. Thus, our apriori similarity, 𝑆𝐺 , is defined as the cosine similarity between the musical features, 𝑋 [𝑣] ∈ R∣𝑇 ∣×9, associated with song nodes. Meanwhile, our learned similarity contains the song-level embed- dings, 𝑆𝑍 ∈ R∣𝑇 ∣×𝑚 , learned by PinSage. In this way, REDRESS acts as a regularizer that ensures that rank-based similarity between songs is preserved between the input and embedding space. Thus, our similarity notion is domain-aware and grounded in the essence of musical experiences: acoustics.
To learn the embedding of songs, we follow the learning par- adigm of PinSage [67] and make a few deviations. Unlike Ying et al., we use uniform random sampling to avoid the computational burden of calculating negative samples on our large graph and compensate for the potential loss of information by using focal loss [43], rather than the margin loss, to train the network. It is important to note that since the potential benefits or drawbacks of PinSage as a general recommender system are out of the scope of this paper, we do not focus on the performance gains that such a change might provide and leave the addition of various negative sampling techniques to future work.
(1)
(2)


Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
Bringing popularity into individual fairness. The REDRESS framework does not explicitly encode any attributes of popularity in its training regimen. Thus increased visibility given of niche items comes only from innate similarities in the musical features, not explicit promotion of niche content. To extend this technique for explicitly counteracting popularity bias, we define the BOOST technique which is used to further increase the penalty on misrepre- sentation of items that come from diverse popularity categories. As mentioned in Section 3.3, we define 10 popularity bins by applying a logarithmic transformation and binning the degrees of a node 𝑖 (i.e., deg𝑖 ) such that pop_bin(𝑖) = bin (log10 (deg𝑖 )). This popularity bin can then be integrated with the REDRESS calculations. More formally, given the learned representation matrix, 𝑆𝑍 ∈ R∣𝑇 ∣×∣𝑇 ∣, we define another matrix 𝐵 in which
𝐵𝑖 𝑗 = ∣pop_bin(i) − pop_bin(j)∣ Then, in the BOOST loss formulation, in place of 𝑆𝑍 we use 𝑆′ 𝑆𝑍 + 𝐵. Objective function. The representation learning objective used during training is:
𝐿total = 𝐿utility + 𝛾𝐿fairness (4) where 𝛾 is a scalable hyperparameter which controls the focus given to fairness and can be used to select a balance between utility (𝐿utility) and fairness (𝐿fairness) during the second training phase. For 𝐿utility, we apply the aforementioned focal loss [43]. And 𝐿fairness is Equation 2 defined above. Generating recommendations. Crucially, the PinSage architec- ture is only designed to learn embeddings for songs, not for playlists (or users). Thus we design our own procedure for generating playlist (or user) embeddings using the learned song embeddings. For each playlist (or user) node, 𝑝𝑖 , we have a set of songs, 𝑇 (𝑝𝑖 ) = {𝑡𝑖 ∈ 𝑇 , 𝑒𝑝𝑖,𝑡𝑖 ∈ 𝐸}, which are contained in a playlist. For a test playlist, 𝑝𝑡 , we split the associated track set into two groups: 𝑇 (𝑡𝑝 ) = {𝑡𝑖 ∶ 𝑡𝑖 ∈ 𝑢𝑖 } = 𝑡𝑝𝑒𝑒𝑘 ∪ 𝑡ℎ𝑜𝑙𝑑𝑜𝑢𝑡 such that 𝑡𝑝𝑒𝑒𝑘 is a set of k songs that are used to generate the playlist representation and 𝑡ℎ𝑜𝑙𝑑𝑜𝑢𝑡 are masked for evaluation. Thus, in order to generate a playlist (or user) embedding we define:
∶ 𝑡 𝑗 ∈ 𝑡𝑝𝑒𝑒𝑘 }) where the 𝑧𝑘 ∈ R1×𝑑 are the learned representations of dimension 𝑑. Having learned these playlist representations, we apply cosine similarity between an individual playlist, 𝑧𝑝𝑡 , and the set of songs in ∶ 𝑡 𝑗 ∈ 𝑇 }, selecting the top-k items by their the database, 𝑍𝑇 = {𝑧𝑡 𝑗 score. We leave further experimentation on designing user-based embeddings via the PinSage paradigm for future work.
𝑧𝑝𝑡 = 𝑀𝐸𝐴𝑁 ({𝑧𝑡 𝑗
4 Experimental Settings
In this section we introduce the experimental settings, defining the recommendation scenario (Section 4.1), datasets (Section 4.2), evaluation metrics (Section 4.3), baselines (Section 4.4) , and repro- ducibility settings (Section 4.5) encompassed in our experimenta- tion.
4.1 Recommendation Scenario
As user consumption habits have shifted away from albums and towards playlists, streaming companies have invested significant energy into the task of Automatic Playlist Continuation and Weekly
(3) 𝑍 =
Conference’17, July 2017, Washington, DC, USA
Table 1: Dataset statistics
Dataset MPD LFM
Recommendation Setting Automatic Playlist Continuation Weekly Discovery
Feedback Type Explicit Implicit
#Users/Playlists 11,100 10,267
#Songs 183,408 890,568
#Artists 37,509 100,638
Discovery [57, 59]. In the first task, Automatic Playlist Continuation requires the recommender system to perform next 𝑘 recommenda- tion on a user generated playlist. Meanwhile, in Weekly Discovery, rather than augmenting a specific playlist, an algorithm is tasked with the creation of a new playlist based on a user’s aggregated lis- tening habits. Given our similar treatment of the playlists and users in this recommendation setting, we will use them interchangeably in our formal definition of the task.
Definition 4.1. Automatic Playlist Continuation/Weekly Discov- ery: Given a set of users 𝑈 , or playlists, 𝑃 = 𝑃𝑡𝑟𝑎𝑖𝑛 ∪ 𝑃𝑣𝑎𝑙𝑖𝑑 ∪ 𝑃𝑡𝑒𝑠𝑡 , and a set of songs (or tracks) 𝑇 , our goal is to generate a set of 𝑘 recommendations, 𝑅(𝑃𝑡𝑒𝑠𝑡 ).
Following the paradigm of the cold start setting [57], we extract train/validation/testing splits on the playlist level by randomly sampling without replacement such that each split trains on a distinct subset of the playlist pool. In this way, we simulate the real world situation in which new users are joining the platform and require relevant, unbiased recommendations without providing a large body of their previous interaction data. It is exactly at this junction, before a user’s musical preference solidifies, that the need to mitigate popularity bias is most acute because once a majoritarian pattern has been installed in a user’s embedding, it will continue to influence all further music discovery.
4.2 Datasets
As introduced in Section 3.1, we extensively augment two pub- licly available datasets, LastFM (LFM) [46] and the Million Playlist Dataset (MPD) [16], with rich multi-modal track-level feature sets. Table 1 presents the graph statistics of both datasets.
4.3 Evaluation Metrics
In addition to canonical utility metrics, we design a series of metrics to analyze the effectiveness of our debiasing methods from both a musical and fairness perspective (see Table 2 for the details of the formulations).
4.3.1 Music Performance Metrics The purpose of these metrics is to broaden the scope of evaluation to include hidden positive hits. We use Artist Recall to evaluate a system’s ability to identify correct artists in a recommendation pool, an auxiliary task in music recommendation [41]. In addition, we design Sound Homogeneity to capture the musical cohesiveness of the recommended songs in a playlist [7].
Fairness Metrics To assess the debiasing techniques used to 4.3.2 promote of long tail songs we draw on a series of metrics which have been previously used to evaluate the fairness performance of a model [3, 13, 51]. Percentage metrics capture the ratio of niche to popular content that is being recommended on a playlist (or user) level. Meanwhile, coverage looks at the aggregate sets of niche songs and artists over all recommendations. If a recommender has a high percentage but low coverage, the same niche items are being selected many times. Meanwhile, if an item has high coverage but a low percentage, the algorithm is selecting a diverse set of niche


Conference’17, July 2017, Washington, DC, USA Table 2: Music and fairness performance metrics. We de- fine a ground truth set, 𝐺, and a recommended set, 𝑅, we define the set of unique artists in a playlist as 𝐴(.) and the 𝑑-dimensional musical feature matrix associated with the tracks of a playlist as 𝐹 (.) ∈ R∣.∣×𝑑
.
Metric
Category
Formulation
Artist Recall@100
Music
1 ∣𝑃𝑡𝑒𝑠𝑡 ∣
∑𝑝∈𝑃𝑡𝑒𝑠𝑡
1
∣𝐴(𝐺𝑝 )∣ ∣𝐴(𝐺𝑝 ) ∩ 𝐴(𝑅𝑝 )∣
Sound Homogeneity@100
Music
1 ∣𝑃𝑡𝑒𝑠𝑡 ∣
∑𝑝∈𝑃𝑡𝑒𝑠𝑡 cos(𝐹 (𝑡𝑖 ), 𝐹 (𝑡𝑖 )) ∀(𝑡𝑖, 𝑡 𝑗 ) ∈ 𝑅𝑝
Artist Diversity (per playlist)
Fairness
1 ∣𝑃𝑡𝑒𝑠𝑡 ∣
∑𝑝∈𝑃𝑡𝑒𝑠𝑡
∣𝐴(𝑃 )∣ ∣{𝐴(𝑅𝑝 )}∣
1
Percentage of Long Tail Items
Fairness
1 ∣𝑃𝑡𝑒𝑠𝑡 ∣
∑𝑝∈𝑃𝑡𝑒𝑠𝑡
∣𝑝∣ ∣{𝑡𝑖 ∶ 𝑡𝑖 ∈ 𝑅𝑝 ∩ 𝑡𝑖 ∈ 𝐿𝑇 }∣
1
Coverage over Long Tail Items
Fairness
∣𝐿𝑇 ∣ ∣{𝑡𝑖 ∶ 𝑡𝑖 ∈ 𝑅 ⋂ 𝑡𝑖 ∈ ∣𝐿𝑇 ∣}
1
Coverage over Artists
Fairness
∣𝐴∣ ∣{𝑎𝑟𝑖𝑑(𝑡𝑖 ) ∶ 𝑡𝑖 ∈ 𝑅∣}
1
content but recommending it very rarely. The gold standard is a high value on both metrics.
4.4 Baselines
First, we use two naive baselines: (1) Features: Instead of the learned representations, we use the raw feature vectors and (2) MostPop: we calculate most popular tracks in each dataset and recommend them each time. Then, we select three state of the art fairness mitigation techniques: (1) ZeroSum[54]: an in-processing group fairness that defines a regularization term which forces scores within negative and positive item groups to remain close. Following their original implementation, we select LGCN [34] as the back- bone recommender. (2) MACR[62]: an inprocessing method which uses counterfactual estimation to denoise for the effects of popu- larity bias in user and item embeddings. Here too, Following their original implementation, we select LGCN [34] as the backbone rec- ommender, and (3) Smooth xQuAD[3]: a post-processing method that reranks recommendations to improved diversity.
4.5 Parameter Settings & Reproducibility Each of the baseline methods was tested with learning rates ∼ (0.01, 0.0001), embedding sizes of [10, 24, 64, 128] and batch sizes of [256, 512, 1024]. For the values in the tables below, each stochas- tic method was run 5 times and averaged. All details and further hyperparameter settings can be found on our GitHub repository 1.
5 Results
In this section we present the results of our experimentation. First, we show the connections between individual fairness, popularity bias, and music discovery in the graph domain. Then, we evaluate our method, comparing with a series of the debiasing benchmarks.
5.1 RQ1: How does incorporating individual fairness improve the mitigation of popularity bias and facilitate music discovery?
To showcase the performance of our algorithm in the discovery set- ting and motivate the need for individual fairness in the mitigation
1preliminary 9B7F/README.md
version:
https://anonymous.4open.science/r/RecSys23-
Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi
Figure 3: Simulating Popularity Bias: We select 100 of the most popular songs in MPD [57], duplicate their node level features, add them with new song track ids, and connecting them to one randomly selected playlist. Then, we analyze the distances between the embedding group centroids. We find that REDRESS and BOOST have the lowest distance between the original, popular and duplicated, unpopular track groups, showing the least amount of popularity bias.
of popularity bias, we draw on the definition of music discovery presented in Section 3.3.2 by evaluating the effects of popularity bias on learned representations of popular and unpopular songs.
To simulate a situation of maximal popularity bias, we consider the hypothetical example in which extremely popular songs are reversed to become unpopular and measure the effects of degree on their learned representations. From a discovery perspective, the purpose of this simulation is to imagine the most popular song by a listener’s favorite artist before it became popular. Our simulation aims to approximate how likely it is that they have discovered the song in relation to its musical attributes, with and without debiasing for the effects of popularity. More formally, for each song track, 𝑡𝑖 ∈ 𝑇𝑂𝐺 , we generate a counterfactual example song, 𝑡 ∗ 𝑖 ∈ 𝑇𝐶𝐹 , where everything about the features is exactly the same and the only difference is that 𝑡𝑖 appears in many playlists while 𝑡 ∗ 𝑖 appears only once. We augment the original dataset to include these counterfactual songs, 𝑇 = 𝑇𝑂𝐺 ⋃𝑇𝐶𝐹 . Then, we use five methods to learn the item level representations: one baseline recommender, PinSage, and four bias mitigation methods, ZeroSum [54], MACR [62], REDRESS, and BOOST. We apply 2-dimensional PCA to each embedding set and analyze the Euclidean distance between the centroids of original track embeddings, ¯𝑇𝑂𝐺 , and counterfactual track embeddings, ¯𝑇𝐶𝐹 . Due to the size of our dataset, we run this metric using the 100 most popular tracks in the MPD dataset and leave further exploration of this phenomenon for future work.


Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
As shown in Figure 3, we find that all fairness interventions de- crease the distance between the two centroids. Furthermore, as the granularity of fairness increases, the distance between the centroids of learned representations decreases. For example, PinSage, which has no mitigation of popularity bias, has the largest distance of 0.172. ZeroSum [54], which considers group fairness, decreases the distance to 0.143, MACR [62], which uses counterfactual estimation, shrinks to 0.055. Finally, our methods, REDRESS and BOOST are able to achieve both the lowest distance and the correct orientation between the two embedding spaces.
In these results, we see that the domain-awareness of our method- ology, which enables it to understand musical similarity between items, allows it to be robust to the effects of popularity bias on a learned song embedding. Thus, in the setting of musical discovery, it is able to uncover proximity between items which are musically coherent even if they are not necessarily of similar popularity sta- tus. And, in doing so, we build representations that are complex, expressive, and effective for music recommendation.
5.2 RQ2: How does our individual fairness
approach compare to existing methods for mitigating popularity bias?
In Table 3 we show a side by side comparison of the various recom- mendation and debiasing methods. We apply the Wilcoxon signed- rank test [53] to the BOOST, PinSage results to assess the statistical significance of our method’s performance. In Table 3, we select only the best hyperparameter results for each method. However, for each fairness baseline, there is a hyperparameter which tunes the balance between fairness intervention and performance, 𝛾. Thus, we present Figure 4 to show the ranges of these hyperparameter values. Analyzing Debiasing Performance: First, we look at the com- parison between the backbone recommender systems and their debiasing counterparts. Within the greater fairness community it is typical to see a trade-off between recommendation utility and the effectiveness of a debiasing technique [35]. And, indeed, in our experiments we witness such a trade-off. For example, eval- uating the columns of Recall and NDCG on Table 3 we can see that both recommender systems outperform their debiasing coun- terparts. This can be largely attributed to the formulations of the utility metrics, the interaction with the debiasing objectives, and the underlying distribution of the datasets. Since the premise of the canonical recommendation utility metrics is to reward a system that can accurately recover the exact tracks a user liked, any at- tempts to promote long tail content that wasn’t originally listened to is penalized, even if it isn’t truly indicative of a user’s underlying taste. We note that the trade-off is significantly more severe in the LFM dataset as opposed to the MPD dataset. As shown in Figure 5, this can be attributed to the underlying distribution of a datasets. Where MPD has a training set which contains a significant portion of interactions on the lower end of the popularity spectrum, LFM skews towards higher popularity. Thus, if evaluating using utility metrics that penalize mistakes on the track-level prediction, even if a system is selecting musically coherent and relevant content, this trade-off becomes inevitable. Indeed, within the music recommen- dation community, there have been several works suggesting that
Conference’17, July 2017, Washington, DC, USA
Figure 4: Plots showing the effects of fairness intervention method on performance/fairness metrics: we show the ef- fects of tuning a hyperparameter to balance fairness and performance in each of our fairness methods, we explore the entire range of the values and report the trade-off that increasing this fairness intervention can have. Note, that REDRESS has the best robustness with respect to balancing recall and %LT.
this trade-off, though present in offline testing, doesn’t necessarily carry over into online testing [12, 35]. Thus, to provide a deeper analysis of our debiasing methodology, we present two musically relevant metrics, Artist Recall and Flow. As explained in Section 4.3 and detailed in Table 2, the Artist Recall metric evaluates the recommender systems ability to identify correct artist-level rec- ommendations and the Flow evaluates the overall homogeneity of the selected music. In particular, Flow plays an important role in the music discovery task because studies have indicated that users are drawn to homogeneous listening suggestions when engaging with algorithmic curation [7, 35]. As we can see in both datasets, REDRESS and BOOST consistently achieve the highest Flow. This is because, by harnessing musical features and in our debiasing tech- nique, our method generates representations that are indicative of musical similarity, which affects the downstream musical similarity of the recommendations it generates. Meanwhile, looking at the Artist Recall columns, we can see a much less significant drop (or, in the case of MPD an increase) in the performance between backbone recommender models and their debiasing counterparts. Crucially, if we consider the implications of such a debiasing technique on a user who’s taste skews towards popular music, high performance on these metrics means that our debiasing methods’ awareness of mu- sical similarity will enable it to maintain the stylistic elements that a user is drawn to while simultaneously promoting niche content. Next, we compare the performance among the various fairness promotion methods. Looking at the columns of recall and ndcg on Table 3, we can see that, as expected, xQuAD [3] which is a re- ranking method is able to preserve the highest utility. However, we note that among the remaining methods, REDRESS is able to achieve the second highest utility. Meanwhile, if we look at the fairness metrics, we can see that REDRESS and BOOST are the highest performing methods. In particular, looking at the columns for %LT and LT Cvg, we can see that REDRESS is noticeably better than the other methods and BOOST is able to improve on its performance. Crucially, our method is able to have high values in both coverage and percentage of long tail items meaning that REDRESS/BOOST is not just prioritizing niche items but also choosing a diverse selection from this category. We attribute the relative gains of REDRESS and BOOST to their ability to integrate musical features into their fairness mitigation because they are able to select not just niche


Conference’17, July 2017, Washington, DC, USA Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi Table 3: Comparison between all methods.Note: We use bold highlights to represent the best performance within a column for each of the datasets. The 𝑝 values of this table are calculated by applying the Wilcoxon signed-rank test [53] to results between PinSage and BOOST. As you can see, the BOOST method achieves the best performance along all Fairness metrics when compared with the other debiasing benchmarks.
Classic
Music
Fairness
Data
MPD
LFM
Model Features MostPop LightGCN 0.106 ± 0.004 0.068 ± 0.002 PinSage 0.044 ± 0.002 ZeroSum 0.064 ± 0.005 xQuAD 0.028 ± 0.014 MACR 0.045 ± 0.002 REDRESS 0.020 ± 0.004 BOOST p values 4.408083e-16 Features 0.033 MostPop 0.015 0.026 ± 0.001 LightGCN 0.064 ± 0.001 PinSage 0.001 ± 0.003 ZeroSum 0.055 ± 0.001 xQuAD 0.014 ± 0.001 MACR 0.038 ± 0.002 REDRESS 0.005 ± 0.001 BOOST p values 5.696989e-08
Recall@100 0.041 0.044
NDCG@100 0.073 0.048 0.119 ± 0.004 0.144 ± 0.003 0.043 ± 0.002 0.104 ± 0.006 0.030 ± 0.015 0.100 ± 0.003 0.047 ± 0.003 1.768725e-19 0.037 0.011 0.023 ± 0.001 0.095 ± 0.002 0.001 ± 0.001 0.064 ± 0.001 0.014 ± 0.001 0.053 ± 0.004 0.007 ± 0.001 1.179627e-15
Artist Recall@100 0.073 0.141 0.272 ± 0.011 0.139 ± 0.003 0.220 ± 0.008 0.135 ± 0.013 0.149 ± 0.022 0.162 ± 0.004 0.137 ± 0.002 0.727897 0.041 0.046 0.068 ± 0.001 0.077 ± 0.002 0.045 ± 0.004 0.068 ± 0.002 0.049 ± 0.007 0.057 ± 0.001 0.029 ± 0.002 1.914129e-07
Flow 0.900 0.908 0.905 ± 0.000 0.931 ± 0.001 0.904 ± 0.001 0.927 ± 0.004 0.902 ± 0.002 0.969 ± 0.032 0.979 ± 0.000 3.751961e-61 0.996 0.926 0.998± 0.000 0.969 ± 0.000 0.996 ± 0.008 0.998 ± 0.000 0.996 ± 0.003 0.998 ± 0.002 0.999 ± 0.000 0.001408
Diversity 0.841 0.680 0.672 ± 0.025 0.707 ± 0.003 0.765 ± 0.013 0.703 ± 0.059 0.831 ± 0.034 0.829 ± 0.001 0.899 ± 0.002 1.168816e-29 0.919 0.600 0.505 ± 0.012 0.775 ± 0.003 0.866 ± 0.000 0.801 ± 0.008 0.777 ± 0.050 0.862 ± 0.004 0.941 ± 0.003 1.112495e-34
%LT 0.588 0.0 0.002 ± 0.000 0.476 ± 0.002 0.000 ± 0.003 0.226 ± 0.001 0.019 ± 0.006 0.504 ± 0.003 0.522 ± 0.001 0.000596 0.486 0.000 0.000 ± 0.000 0.437 ± 0.001 0.007 ± 0.000 0.212 ± 0.000 0.002 ± 0.004 0.451 ± 0.000 0.498 ± 0.006 2.477700e-11
LT Cvg 0.022 0.0 0.000 ± 0.000 0.032 ± 0.000 0.000 ± 0.000 0.017 ± 0.000 0.000 ± 0.001 0.036 ± 0.004 0.037 ±0.003 - 0.005 0.000 0.000 ± 0.000 0.008 ± 0.000 0.000 ± 0.000 0.004 ± 0.000 0.000 ± 0.000 0.008 ± 0.002 0.010 ± 0.000 -
Artist Cvg 0.073 0.001 0.025 ± 0.001 0.105 ± 0.000 0.048 ± 0.002 0.098 ± 0.004 0.011 ± 0.003 0.117 ± 0.000 0.125 ±0.000 - 0.034 0.001 0.003 ± 0.001 0.053 ± 0.001 0.032 ± 0.001 0.053 ± 0.001 0.001 ± 0.000 0.056 ± 0.000 0.068 ± 0.001 -
Figure 5: Dataset Breakdown by Long Tail Definition: We show visualizations of the user preferences indicated in the training set for each of the datasets used in evalua- tion. Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fair- ness/performance tradeoffs.
Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity.
items, but also musically relevant ones for recommendation. Finally, our method’s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method’s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method es- pecially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular
definition for popularity bins can synthetically inflate the perfor- mance of %𝐿𝑇 and 𝐿𝑇𝐶𝑣𝑔. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines’ fairness.
6 Conclusion
In this work, we address the problem of mitigating popularity bias in music recommendation. Starting from the perspective of discovery and how it relates to algorithmic curation, we consider the effects of popularity bias on users’ ability to discover novel and relevant music. On the basis of this motivation, we highlight the intrinsic ties between popularity bias and individual fairness on both song and artist levels. We ground our individual fairness notion in the music domain, presenting a method to mitigate popularity bias through fine tuning of representation learning via musical similarities. We perform extensive evaluation on two music datasets showing the improvements of our domain aware method in comparison with three state of the art popularity bias mitigation techniques. We


Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
hope that these promising findings showcase the importance of developing domain aware methods of mitigating popularity bias in addition to domain agnostic options.
Conference’17, July 2017, Washington, DC, USA
References
[1] 2014. Spotipy: Spotify API in Python. //spotipy.readthedocs.io/en/2.19.0/
Retrieved Oct 31, 2021 from https:
[2] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling Popularity Bias in Learning-to-Rank Recommendation. In RecSys ’17. ACM. [3] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing Popularity Bias in Recommender Systems with Personalized Re-ranking.
[4] Himan Abdollahpouri, Masoud Mansoury, Robin Burke, and Bamshad Mobasher. 2020. The Connection Between Popularity Bias, Calibration, and Fairness in Recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys ’20). Association for Computing Machinery, New York, NY, USA, 726–731. https://doi.org/10.1145/3383313.3418487
[5] Gediminas Adomavicius and YoungOk Kwon. 2012. Improving Aggregate Rec- ommendation Diversity Using Ranking-Based Techniques. IEEE Transactions on Knowledge and Data Engineering (2012).
[6] Christine Bauer, Marta Kholodylo, and Christine Strauss. 2017. Music Recom- mender Systems Challenges and Opportunities for Non-Superstar Artists. In Bled eConference.
[7] Théo Bontempelli, Benjamin Chapus, François Rigaud, Mathieu Morlon, Marin Lorant, and Guillaume Salha-Galvan. 2022. Flow Moods: Recommending Music by Moods on Deezer. In RecSys ’22.
[8] Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2021.
Interplay between upsampling and regularization for provider fairness in recommender systems. User Modeling and User-Adapted Interaction (2021).
[9] Christopher Burges. 2010. From ranknet to lambdarank to lambdamart: An
overview. Learning 11 (01 2010).
[10] Robin Burke and Maryam Ramezani. 2011. Matching Recommendation Technolo- gies and Domains. 367–386. https://doi.org/10.1007/978-0-387-85820-3_11 [11] Rocío Cañamares and Pablo Castells. 2018. Should I Follow the Crowd? A Proba- bilistic Analysis of the Effectiveness of Popularity in Recommender Systems. In The 41st International ACM SIGIR Conference on Research & Development in Infor- mation Retrieval (Ann Arbor, MI, USA) (SIGIR ’18). Association for Computing Ma- chinery, New York, NY, USA, 415–424. https://doi.org/10.1145/3209978.3210014 recommender AI Maga- https://doi.org/10.1002/aaai.12051
[12] Pablo Castells and Alistair Moffat. 2022.
Offline
system evaluation: Challenges zine 225–238. arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12051
and new directions.
43,
(2022),
2
[13] Òscar Celma and Pedro Cano. 2008. From Hits to Niches? Or How Popular Artists Can Bias Music Recommendation and Discovery. In Proceedings of the 2nd KDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition (Las Vegas, Nevada) (NETFLIX ’08). Association for Computing Machinery, New York, NY, USA, Article 5, 8 pages. https://doi.org/10.1145/1722149.1722154 [14] Oscar Celma and Perfecto Herrera. 2008. A new approach to evaluating novel
recommendations.
[15] Abhijnan Chakraborty, Anikó Hannák, Asia J. Biega, and Krishna P. Gummadi.
2017. Fair Sharing for Sharing Economy Platforms.
[16] Ching-Wei Chen, Paul Lamere, Markus Schedl, and Hamed Zamani. 2018. Recsys
Challenge 2018: Automatic Music Playlist Continuation. In RecSys ’18.
[17] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and Debias in Recommender System: A Survey and Future Directions. https://doi.org/10.48550/ARXIV.2010.03240
[18] Sally Jo Cunningham, David Bainbridge, and Dana Mckay. 2007. Finding New Music: A Diary Study of Everyday Encounters with Novel Songs. In International Society for Music Information Retrieval Conference.
[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL]
[20] Fernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben Carterette. 2020. Evaluating Stochastic Rankings with Expected Exposure. In CIKM ’20.
[21] Yujuan Ding, P.Y. Mok, Yunshan Ma, and Yi Bin. 2023. Personalized fashion outfit generation with user coordination preference learning. Information Processing & Management 60, 5 (2023), 103434. https://doi.org/10.1016/j.ipm.2023.103434 [22] Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021. Individual Fairness for Graph Neural Networks: A Ranking Based Approach. In KDD ’21. ACM.
[23] Allen B Downey. 2001. Evidence for Long-Tailed Distributions in the Internet. In
IMW ’01. ACM.
[24] Eric Drott. 2018. Why the Next Song Matters: Streaming, Recommendation,
Scarcity. Twentieth-Century Music (2018).
[25] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel.
2011. Fairness Through Awareness.
[26] Michael D. Ekstrand, F. Maxwell Harper, Martijn C. Willemsen, and Joseph A. Konstan. 2014. User Perception of Differences in Recommender Algorithms. In Proceedings of the 8th ACM Conference on Recommender Systems (Foster City, Sili- con Valley, California, USA) (RecSys ’14). Association for Computing Machinery, New York, NY, USA, 161–168. https://doi.org/10.1145/2645710.2645737


Conference’17, July 2017, Washington, DC, USA
[27] Jonathan Foote. 1997. Content-based retrieval of music and audio. [28] Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, and Yong Li. 2023. A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions. ACM Trans. Recomm. Syst. 1, 1 (March 2023). https://doi.org/10. 1145/3568022 Place: New York, NY, USA Publisher: Association for Computing Machinery.
[29] Jean Garcia-Gathright, Brian St. Thomas, Christine Hosey, Zahra Nazari, and Fernando Diaz. 2018. Understanding and Evaluating User Satisfaction with Music Discovery. In The 41st International ACM SIGIR Conference on Research & Devel- opment in Information Retrieval (Ann Arbor, MI, USA) (SIGIR ’18). Association for Computing Machinery, New York, NY, USA, 55–64. https://doi.org/10.1145/ 3209978.3210049
[30] Gijs Geleijnse, Markus Schedl, and Peter Knees. 2007. The Quest for Ground
Truth in Musical Artist Tagging in the Social Web Era.
[31] Sharad Goel, Andrei Broder, Evgeniy Gabrilovich, and Bo Pang. 2019. Anatomy of the Long Tail: Ordinary People with Extraordinary Tastes. In WSDM ’19. [32] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages. https://doi.org/10.1145/2827872
[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In CVPR ’16.
[34] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In SIGIR ’20.
[35] Jonathan L. Herlocker, Joseph A. Konstan, Loren G. Terveen, and John T. Riedl. 2004. Evaluating Collaborative Filtering Recommender Systems. ACM Trans. Inf. Syst. 22, 1 (jan 2004), 5–53. https://doi.org/10.1145/963770.963772
[36] Imran Hossain, Md Aminul Haque Palash, Anika Tabassum Sejuty, Noor A Tanjim, MD Abdullah AL Nasim, Sarwar Saif, Abu Bokor Suraj, Md Mahim Anjum Haque, and Nazmul Karim. 2023. A Survey of Recommender System Techniques and the Ecommerce Domain. arXiv:2208.07399 [cs.IR]
[37] Amir Hossein Jadidinejad, Craig Macdonald, and Iadh Ounis. 2019. How Sensitive
is Recommendation Systems’ Offline Evaluation to Popularity?
[38] Dietmar Jannach, Lukas Lerche, Iman Kamehkhosh, and Michael Jugovac. 2015. What Recommenders Recommend: An Analysis of Recommendation Biases and Possible Countermeasures. User Modeling and User-Adapted Interaction 25, 5 (Dec. 2015), 427–491. https://doi.org/10.1007/s11257-015-9165-3 Place: USA Publisher: Kluwer Academic Publishers.
[39] Iman Kamehkhosh and Dietmar Jannach. 2017. User Perception of Next-Track Music Recommendations. In Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization (Bratislava, Slovakia) (UMAP ’17). Association for Computing Machinery, New York, NY, USA, 113–121. https://doi.org/10.1145/ 3079628.3079668
[40] Jian Kang, Yan Zhu, Yinglong Xia, Jiebo Luo, and Hanghang Tong. 2022. Rawls- GCN: Towards Rawlsian Difference Principle on Graph Convolutional Network. In WWW ’22.
[41] Filip Korzeniowski, Sergio Oramas, and Fabien Gouyon. 2021. Artist Similarity
with Graph Neural Networks. In ISMIR ’21.
[42] Charilaos Lavranos, Petros Kostagiolas, and Konstantina Martzoukou. 2016. The- oretical and Applied Issues on the Impact of Information on Musical Creativity: An Information Seeking Behavior Perspective. 1 – 16. https://doi.org/10.4018/978-1- 5225-0270-8.ch001
[43] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2020. Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
[44] Kachun Lo and Tsukasa Ishigaki. 2019. Matching Novelty While Training: Novel Recommendation Based on Personalized Pairwise Loss Weighting. 2019 IEEE International Conference on Data Mining (ICDM) (2019), 468–477.
[45] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2021. A Graph-Based Approach for Mitigating Multi-Sided Exposure Bias in Recommender Systems. ACM Transactions on Information Systems 40, 2 (nov 2021), 1–31. https://doi.org/10.1145/3470948 [46] Alessandro Melchiorre, Navid Rekabsaz, Emilia Parada-Cabaleiro, Stefan Brandl, Oleg Lesota, and Markus Schedl. 2021. Investigating gender fairness of recommen- dation algorithms in the music domain. Information Processing & Management (2021).
[47] Matti Mäntymäki and Najmul Islam. 2015. Gratifications from using freemium music streaming services: Differences between basic and premium users. In International Confererence on Information Systems.
[48] Judith Möller, Damian Trilling, Natali Helberger, and Bram van Es. 2018. Do not blame it on the algorithm: an empirical assessment of multiple recommender systems and their impact on content diversity. Information, Communication & Society (2018).
[49] Taehyung Noh, Haein Yeo, Myungjin Kim, and Kyungsik Han. 2023. A Study on User Perception and Experience Differences in Recommendation Results by Domain Expertise: The Case of Fashion Domains. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany)
Rebecca Salganik, Fernando Diaz, and Golnoosh Farnadi
(CHI EA ’23). Association for Computing Machinery, New York, NY, USA, Article 14, 7 pages. https://doi.org/10.1145/3544549.3585641
[50] Yoon-Joo Park and Alexander Tuzhilin. 2008. The Long Tail of Recommender
Systems and How to Leverage It. In RecSys ’08. ACM.
[51] Gourab K Patro, Arpita Biswas, Niloy Ganguly, Krishna P. Gummadi, and Abhi- jnan Chakraborty. 2020. FairRec: Two-Sided Fairness for Personalized Recom- mendations in Two-Sided Platforms. In Proceedings of The Web Conference 2020. ACM. https://doi.org/10.1145/3366423.3380196
[52] Andreas Raff, Andreas Mladenow, and Christine Strauss. 2021. Music Discovery as Differentiation Strategy for Streaming Providers. In Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications & Services (Chiang Mai, Thailand) (iiWAS ’20). Association for Computing Ma- chinery, New York, NY, USA, 476–480. https://doi.org/10.1145/3428757.3429151
[53] Denise Rey and Markus Neuhäuser. 2011. Wilcoxon-Signed-Rank Test. [54] Wondo Rhee, Sung Min Cho, and Bongwon Suh. 2022. Countering Popularity
Bias by Regularizing Score Differences. In RecSys ’22.
[55] Guillaume Salha-Galvan, Romain Hennequin, Benjamin Chapus, Viet-Anh Tran, and Michalis Vazirgiannis. 2021. Cold Start Similar Artists Ranking with Gravity- Inspired Graph Autoencoders. https://doi.org/10.48550/ARXIV.2108.01053 [56] Antonia Saravanou, Federico Tomasi, Rishabh Mehrotra, and Mounia Lalmas. 2021. Multi-Task Learning of Graph-based Inductive Representations of Music Content. In Proceedings of the 22nd International Society for Music Information Retrieval Conference. ISMIR, Online, 602–609. https://doi.org/10.5281/zenodo. 5624379
[57] Markus Schedl, Hamed Zamani, Ching-Wei Chen, Yashar Deldjoo, and Mehdi Elahi. 2018. Current challenges and visions in music recommender systems research. International Journal of Multimedia Information Retrieval 7, 2 (apr 2018), 95–116. https://doi.org/10.1007/s13735-018-0154-2
[58] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing Learning and Evaluation. arXiv:1602.05352 [cs.LG]
[59] Darko Stanisljevic. 2020. The impact of Spotify features on music discovery in
the streaming platform age.
[60] Harald Steck. 2011. Item Popularity and Recommendation Accuracy. In RecSys
’11. ACM.
[61] Xiuling Wang and Wendy Hui Wang. 2022. Providing Item-Side Individual
Fairness for Deep Recommender Systems. In FAccT ’22.
[62] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He. 2021. Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. In KDD ’21. ACM.
[63] Chuhan Wu, Fangzhao Wu, Yongfeng Huang, and Xing Xie. 2022. Personalized News Recommendation: Methods and Challenges. arXiv:2106.08934 [cs.IR] [64] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2020. Graph Neural
Networks in Recommender Systems: A Survey.
[65] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph Neural Networks in Recommender Systems: A Survey. ACM Comput. Surv. 55, 5 (Dec. 2022). https://doi.org/10.1145/3535101 Place: New York, NY, USA Publisher: Association for Computing Machinery.
[66] Christopher C. Yang, Hsinchun Chen, and Kay Hong. 2003. Visualization of large category map for Internet browsing. Decision Support Systems 50, 1 (2003), 89–102.
[67] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In KDD ’18. ACM.
[68] An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. 2022. Incorporating Bias-aware Margins into Contrastive Loss for Collaborative Filtering. In NeurIPS ’22.
[69] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In SIGIR ’21. ACM.
[70] Minghao Zhao, Le Wu, Yile Liang, Lei Chen, Jian Zhang, Qilin Deng, Kai Wang, Xudong Shen, Tangjie Lv, and Runze Wu. 2022. Investigating Accuracy-Novelty Performance for Graph-based Collaborative Filtering. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM. https://doi.org/10.1145/3477495.3532005
[71] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding. In Proceedings of the Web Conference 2021. 2980–2991.
[72] Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee. 2021. Popularity-Opportunity Bias in Collaborative Filtering. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM ’21). Association for Computing Machinery, New York, NY, USA, 85–93. https://doi.org/10.1145/3437963.3441820


Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
7 Ethical Considerations
In this work we attempt to promote ethical recommendation by ad- dressing the problem of popularity bias in music recommendation. However, there are a few important caveats to the work presented above. Crucially, our work is influenced by larger movements within the music streaming and information retrieval communities that come with important ethical considerations. First, due to our re- liance on publicly available data, we are limited in our exploration of multi-cultural, and specifically non-Western musical content. The under representation of such content has long been an issue within the music communities and, concretely in our setting, it manifests itself in cutting off the flow of financial capital towards
Conference’17, July 2017, Washington, DC, USA
under-represented artists. Second, due to the lack of transparency regarding the calculation of Spotify’s metadata, we are unable to assess whether the musical features distributed by the platform (and used in our work) are disenfranchising content from non-Western musical traditions. Finally, the construction of user bases can also have very significant effects on how connections between artists are formed. Thus, it is equally important to note that the ground truth data that we are using to perform recommendation is most likely very dominated by North American and European listeners. We hope that in the future, greater collaboration between industry and academia can foster the necessary transparency to address these issues more concretely.