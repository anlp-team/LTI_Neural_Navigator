3 2 0 2
r a
M 4 1
] L C . s c [
1 v 4 2 6 7 0 . 3 0 3 2 : v i X r a
I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION
Yifan Peng1, Jaesong Lee2, Shinji Watanabe1
1Carnegie Mellon University
2NAVER Corporation
ABSTRACT
Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difÔ¨Åcult to deploy these models in some real-world applica- tions. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a Ô¨Åxed architec- ture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efÔ¨Åciency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.
Index Terms‚Äî Dynamic depth, transformer, speech recogni-
tion
1. INTRODUCTION
still determined on a frame-by-frame basis, which requires a spe- cial design for the Ô¨Åned-grained key and query operations in self- attention. For nonstreaming (or chunk-based streaming) ASR, the frame-level prediction may be expensive and suboptimal, as it only captures frame-level local features.
We propose a Transformer encoder with Input-Dependent Dynamic Depth (I3D) for end-to-end ASR. Instead of using carefully designed Ô¨Åne-grained operations within submodules like attention, I3D predicts whether to skip an entire self-attention block or an entire feed-forward network through a series of local gate predictors or a single global gate predictor. The prediction is made at the ut- terance level (or chunk level if extended to streaming cases), which is easier to implement and reduces additional cost. It also captures global statistics of the input. As analyzed in Sec. 3.4, the length of an utterance affects the inference architecture. Some blocks may be useful for longer inputs. Results show that I3D models consistently outperform Transformers trained from scratch and the static pruned models via iterative layer pruning [29], when using a similar num- ber of layers for inference. We also perform interesting analysis on predicted gate probabilities and input-dependency, which helps us better understand the behavior of deep encoders.
Recently, end-to-end automatic speech recognition (ASR) has gained popularity. Typical frameworks include Connectionist Tem- poral ClassiÔ¨Åcation (CTC) [1], Attention-based Encoder-Decoder (AED) [2‚Äì4], and Recurrent Neural Network Transducer (RNN- T) [5]. Many types of networks can be used as encoders in these frameworks, such as Convolutional Neural Networks (CNNs), RNNs, Transformers [6] and their combinations [7‚Äì9]. Trans- formers have achieved great success in various benchmarks [10]. However, they usually contain many cascaded blocks and thus have high computation, which hinders deployment in some real-world ap- plications with limited resource. To reduce computation and speed up inference, researchers have investigated different approaches.
2. METHOD
2.1. Transformer encoder
A Transformer [6] encoder layer contains a multi-head self-attention (MHA) module and a feed-forward network (FFN), which are com- bined sequentially. The function of the l-th layer is as follows:
Y(l) = X(l‚àí1) + MHA(l)(X(l‚àí1)), X(l) = Y(l) + FFN(l)(Y(l)),
A popular method is to compress a large pre-trained model using distillation [11‚Äì13], pruning [14‚Äì16], and quantization [15]. How- ever, the compressed model has a Ô¨Åxed architecture for all types of inputs, which might be suboptimal. For example, this Ô¨Åxed model may be too expensive for very easy utterances but insufÔ¨Åcient for dif- Ô¨Åcult ones. To better trade off performance and computation, prior studies have explored dynamic models [17], which can adapt their architectures to different inputs. Dynamic models have shown to be effective in computer vision [18‚Äì23], which are mainly based on CNNs. For speech processing, [24] trains two RNN encoders of dif- ferent sizes and dynamically switches between them guided by key- word spotting. [25] proposes a dynamic encoder transducer based on layer dropout and collaborative learning. [26] adopts two RNN en- coders to tackle close-talk and far-talk speech. [27] also designs two RNN encoders that are compressed to different degrees and switches between them on a frame-by-frame basis. [28] extends this idea to Transformer-transducers and considers more Ô¨Çexible subnetworks, but it continues to focus on streaming ASR and the architecture is
where X(l) is the output of the l-th Transformer layer and X(l‚àí1) is thus the input to the l-th layer. Y(l) is the output of the MHA at the l-th layer, which is also the input of the FFN at the l-th layer. These sequences all have a length of T and a feature size of d.
2.2. Overall architecture of I3D encoders
Fig. 1a shows the overall architecture of I3D encoders. A waveform is Ô¨Årst converted to a feature sequence by a frontend, and further processed and downsampled by a CNN, after which positional em- beddings are added. Later, the sequence is processed by a stack of N I3D encoder layers to produce high-level features. This overall de- sign follows that of Transformer. However, Transformer always uses a Ô¨Åxed architecture regardless of the input. Our I3D selects different combinations of MHA and FFN depending on the input utterance. To determine whether a module should be executed or skipped, a bi- nary gate is introduced for each MHA or FFN module. The function
(1)
(2)


Frontend
I3D Encoder Layer
CNN
√óùëÅ
PositionalEmbedding
Encoder Output SequenceEncoder Input Sequence
Audio Waveform
‚Ä¶
Mean
Multi-HeadSelf-Attention
Local Gate Predictor
Feed-ForwardNetwork
I3D Encoder Layer
Mean
√óùëÅ‚Ä¶Encoder Input SequenceEncoder Output Sequence
Multi-HeadSelf-Attention
Feed-ForwardNetwork
Layer 1Layer ùëÅ
Global Gate Predictor
‚Ä¶
(a) Overall encoder architecture.
(b) I3D encoder layer with a local gate predictor.
(c) I3D encoder with a global gate predictor.
Fig. 1: Architectures of our proposed I3D encoders.
of the l-th layer (see Eqs. (1) and (2) for the vanilla Transformer) now becomes:
Y(l) = X(l‚àí1) + g(l) X(l) = Y(l) + g(l)
MHA ¬∑ MHA(l)(X(l‚àí1)),
FFN ¬∑ FFN(l)(Y(l)),
where g(l) FFN ‚àà {0, 1} are input-dependent gates. If a gate is predicted to be 0, then the corresponding module will be skipped, which effectively reduces computation. The total training loss is:
MHA, g(l)
Ltotal = LASR + Œª ¬∑ Lutility,
(3)
(4)
(5)
process, but it is differentiable w.r.t. Œ± and thus suitable for gradient- based optimization. As œÑ ‚Üí 0, the approximation becomes closer to the discrete version. We use œÑ = 1 in our experiments.
MHA ‚àà R2 over two possible gate values (0 and 1) is predicted, where 0 means skipping this module and 1 means executing it. Then, a soft sample is drawn from this discrete distribution using Eq. (8), which is used as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a distribution p(l) FFN ‚àà R2 is predicted, from which a soft gate is drawn and used in Eq. (4). The gate distributions are generated by a gate predictor based on the input features, as deÔ¨Åned in Sec. 2.3.
For the l-th MHA, a discrete probability distribution p(l)
Lutility =
1 2N
N (cid:88)
l=1
(cid:16)
MHA + g(l) g(l)
FFN
(cid:17)
,
(6)
2.3. Local and global gate predictors
where LASR is the standard ASR loss and Lutility is a regularization loss measuring the utility rate of all MHA and FFN modules. Œª > 0 is a hyper-parameter to trade off the recognition accuracy and com- putational cost. 1 Note that the utility loss in Eq. (6) is deÔ¨Åned for an individual utterance so the utterance index is omitted. In practice, a mini-batch is used and the loss is averaged over utterances.
A major issue with this training objective is that binary gates are not differentiable. To solve this problem, we apply the Gumbel- Softmax [30, 31] trick, which allows drawing hard (or soft) samples from a discrete distribution. Consider a discrete random variable Z with probabilities P (Z = k) ‚àù Œ±k for any k = 1, . . . , K. To draw a sample from this distribution, we can Ô¨Årst draw K i.i.d. samples {gk}K k=1 from the standard Gumbel distribution and then select the index with the largest perturbed log probability:
We propose two types of gate predictors, namely the local gate pre- dictor and global gate predictor. We employ a multi-layer percep- tron (MLP) with a single hidden layer of size 32 in all experiments, which has little computational overhead.
The local gate predictor (LocalGP or LGP) is associated with a speciÔ¨Åc I3D encoder layer, as illustrated in Fig. 1b. Every layer has its own gate predictor whose parameters are independent. Consider the l-th encoder layer with an input sequence X(l‚àí1) ‚àà RT √ód. This sequence is Ô¨Årst converted to a d-dimensional vector x(l‚àí1) ‚àà Rd through average pooling along the time dimension. Then, this pooled vector is transformed to two 2-dimensional probability vectors for the MHA gate and the FFN gate, respectively:
MHA, p(l) p(l)
FFN = LGP(l)(x(l‚àí1)),
z = arg max k‚àà{1,...,K}
log Œ±k + gk.
The argmax is not differentiable, which can be relaxed to the soft- max. It is known that any sample from a discrete distribution can be denoted as a one-hot vector, where the index of the only non-zero entry is the desired sample. With this vector-based notation, we can draw a soft sample as follows:
(7)
where p(l) FFN ‚àà R2 are introduced in Sec. 2.2, and LGP(l) is the local gate predictor at the l-th layer. With this formulation, the deci- sion of executing or skipping any MHA or FFN module depends on the input to the current layer, which further depends on the decision made at the previous layer. Hence, the decisions are made sequen- tially from lower to upper layers. During inference, a Ô¨Åxed threshold Œ≤ ‚àà [0, 1] is utilized to produce a binary gate for every module:
MHA, p(l)
z = softmax ((log Œ± + g)/œÑ ) ,
(8)
where Œ± = (Œ±1, . . . , Œ±K ), g = (g1, . . . , gK ), and œÑ is a tempera- ture constant. Eq. (8) is an approximation of the original sampling
MHA = 1 if (p(l) g(l) FFN = 1 if (p(l) g(l)
MHA)1 > Œ≤ else 0, FFN)1 > Œ≤ else 0,
1Our method can be extended to consider different costs of MHA and FFN. In Eq. (6), we can use a weighted average of the two types of gates, where the weights depend on their computational costs. Then, the training will minimize the overall computation instead of simply the number of layers.
where (p(l) FFN)1 is the probability of executing the FFN. We use Œ≤ = 0.5 by default, but it is also possible to adjust the inference cost by changing Œ≤.
MHA)1 is the probability of executing the MHA and (p(l)
(9)
(10)
(11)


Transformer I3D-GlobalGP
LayerDrop I3D-GlobalGP (Œ≤ varies)
I3D-LocalGP
13.5
) ‚Üì (
13.0
R E W %
12.5
12.0
11.5
18
20
22
24
26
28
30
32
34
36
Average number of layers
(a) LibriSpeech test clean
30.0
) ‚Üì (
29.0
R E W %
28.0
27.0
26.0
18
20
22
24
26
28
30
32
34
36
Average number of layers
(b) LibriSpeech test other Fig. 2: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on LibriSpeech test sets. Œ≤ is the threshold for generating binary gates as deÔ¨Åned in Eqs. (10) (11).
) ‚Üì (
R E W %
12.4 12.2 12.0 11.8 11.6 11.4
Transformer LayerDrop I3D-GlobalGP
18
20
22
24
26
28
30
32
34
36
Average number of layers
(a) LibriSpeech test clean
) ‚Üì (
R E W %
27.5 27.0 26.5 26.0
Transformer LayerDrop I3D-GlobalGP
18
20
22
24
26
28
30
32
34
36
Average number of layers
(b) LibriSpeech test other Fig. 3: Word error rates (%) of InterCTC-based models vs. average number of layers used for inference on LibriSpeech test sets.
The global gate predictor (GlobalGP or GGP), on the other hand, is deÔ¨Åned for an entire I3D encoder, as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder‚Äôs input, which is also the input to the Ô¨Årst layer: X = X(0) ‚àà RT √ód. In par- ticular, the sequence is transformed to a single vector x = x(0) ‚àà Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively:
{p(l)
MHA}N
l=1, {p(l)
FFN}N
l=1 = GGP(x),
(12)
where p(l) FFN ‚àà R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder‚Äôs input, which has lower computational overhead than LocalGP and allows for more Ô¨Çexible control over the inference architecture. During inference, we can still use a Ô¨Åxed threshold Œ≤ ‚àà [0, 1] to generate binary gates as in Eqs. (10) and (11).
MHA, p(l)
3. EXPERIMENTS
3.1. Experimental setup
We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech
) ‚Üì (
R E W %
13 12 11 10
Transformer
I3D-LocalGP
I3D-GlobalGP
18
20
22
24 30 Average number of layers
26
28
32
34
36
Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set.
Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h.
Model
dev clean
test clean
Ave #layers WER (‚Üì) Ave #layers WER (‚Üì)
Transformer
36 27
7.8 8.2
36 27
8.0 8.5
I3D-LGP-36 I3D-GGP-36
27.3 27.2
7.9 7.8
27.1 27.1
8.3 8.2
100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and Ô¨Åne-tuned with a reduced learning rate (1e ‚àí 3) and various Œª (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The Ô¨Åne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and Ô¨Åxed architectures. This baseline is de- noted as ‚ÄúLayerDrop‚Äù in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models.
3.2. Main results
Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different Œª in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more efÔ¨Åcient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer‚Äôs input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing Œ≤ (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different Œ≤. As Œ≤ decreases, more blocks are used, and the WER is usually improved.
Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model.
3.3. Analysis of gate distributions
Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib-


y t i l i b a b o r P
1
0.5
0
1
6
11
16 21 Layer index
26
31
36
(a) MHA gate probabilities.
y t i l i b a b o r P
1
0.5
0
1
6
11
16 21 Layer index
26
31
36
(b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed.
y t i l i b a b o r P
1
0.5
0
1
6
11
16 21 Layer index
26
31
36
(a) MHA gate probabilities (trained with InterCTC).
y t i l i b a b o r P
1
0.5
0
1
6
11
21 16 Layer index
26
31
36
(b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed.
riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28].
We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the Ô¨Ånal performance.
We believe this gate analysis can provide a way to interpret the
layer-wise behavior of deep networks.
3.4. Analysis of input-dependency
It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process.
We also considered two other factors that may affect the infer-
y c n e u q e r F
0.4
0.2
0
18 blocks 20 blocks
19 blocks 21 blocks
0
5
10
15
20
25
Input speech length in seconds
(a) MHA
y c n e u q e r F
0.1
0
19 blocks 21 blocks
20 blocks 22 blocks
0
5
10
15
20
25
Input speech length in seconds
(b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer.
ence architecture, namely the difÔ¨Åculty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn‚Äôt observe a clear relationship between these metrics and the number of layers used for inference.
3.5. Generalizability
We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor.
4. CONCLUSION
In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and efÔ¨Åciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only Ô¨Åne-tuning gate predictors to signiÔ¨Åcantly reduce training cost.
5. ACKNOWLEDGEMENTS
This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.
2We have also evaluated I3D on LibriSpeech 960h. Observations are con-
sistent with LibriSpeech 100h and Tedlium2.


6. REFERENCES
[1] A. Graves, S. Fern¬¥andez, et al., ‚ÄúConnectionist temporal clas- siÔ¨Åcation: labelling unsegmented sequence data with recurrent neural networks,‚Äù in Proc. ICML, 2006.
[2] K. Cho, B. Merrienboer, et al., ‚ÄúLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,‚Äù in Proc. EMNLP, 2014.
[3] D. Bahdanau, K. Cho, et al., ‚ÄúNeural machine translation by jointly learning to align and translate,‚Äù in Proc. ICLR, 2015.
[4] W. Chan, N. Jaitly, et al., ‚ÄúListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,‚Äù in Proc. ICASSP, 2016.
[5] A. Graves, ‚ÄúSequence transduction with recurrent neural net-
works,‚Äù arXiv:1211.3711, 2012.
[6] A. Vaswani, N. Shazeer, N. Parmar, et al., ‚ÄúAttention is all you
need,‚Äù in Proc. NeurIPS, 2017.
[7] A. Gulati, J. Qin, C.-C. Chiu, et al., ‚ÄúConformer: Convolution- augmented Transformer for Speech Recognition,‚Äù in Proc. In- terspeech, 2020.
[8] Y. Peng, S. Dalmia, et al.,
‚ÄúBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,‚Äù in Proc. ICML, 2022.
[9] K. Kim, F. Wu, Y. Peng, et al., ‚ÄúE-branchformer: Branch- former with enhanced merging for speech recognition,‚Äù arXiv:2210.00077, 2022.
[10] S. Karita, N. Chen, T. Hayashi, et al., ‚ÄúA comparative study on transformer vs rnn in speech applications,‚Äù in Proc. ASRU, 2019.
[11] G. Hinton, O. Vinyals, J. Dean, et al., ‚ÄúDistilling the knowl-
edge in a neural network,‚Äù arXiv:1503.02531, 2015.
[12] H. Chang, S. Yang, and H. Lee, ‚ÄúDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,‚Äù in Proc. ICASSP, 2022.
[13] R. Wang, Q. Bai, et al., ‚ÄúLightHuBERT: Lightweight and Con- Ô¨Ågurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,‚Äù in Proc. Interspeech, 2022.
[14] P. Dong, S. Wang, et al.,
‚ÄúRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,‚Äù in ACM/IEEE Design Automation Conference (DAC), 2020.
[15] K. Tan and D.L. Wang, ‚ÄúCompressing deep neural networks for efÔ¨Åcient speech enhancement,‚Äù in Proc. ICASSP, 2021.
[16] C. J. Lai, Y. Zhang, et al., ‚ÄúParp: Prune, adjust and re-prune for
self-supervised speech recognition,‚Äù in Proc. NeurIPS, 2021.
[17] Y. Han, G. Huang, S. Song, et al., ‚ÄúDynamic neural networks: A survey,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 11, pp. 7436‚Äì7456, 2022.
[18] E. Bengio, P. Bacon, et al., ‚ÄúConditional computation in neural
networks for faster models,‚Äù arXiv:1511.06297, 2015.
[19] A. Veit and S. Belongie, ‚ÄúConvolutional networks with adap-
tive inference graphs,‚Äù in Proc. ECCV, 2018.
[20] X. Wang, F. Yu, et al., ‚ÄúSkipnet: Learning dynamic routing in
convolutional networks,‚Äù in Proc. ECCV, 2018.
[21] Z. Wu, T. Nagarajan, et al., ‚ÄúBlockdrop: Dynamic inference
paths in residual networks,‚Äù in Proc. CVPR, 2018.
[22] J. Shen, Y. Wang, et al., ‚ÄúFractional skipping: Towards Ô¨Åner-
grained dynamic cnn inference,‚Äù in Proc. AAAI, 2020.
[23] C. Li, G. Wang, et al., ‚ÄúDynamic slimmable network,‚Äù in Proc.
CVPR, 2021.
[24] J. Macoskey, G. P. Strimel, and A. Rastrow, ‚ÄúBifocal neural asr: Exploiting keyword spotting for inference optimization,‚Äù in Proc. ICASSP, 2021.
[25] Y. Shi, V. Nagaraja, C. Wu, et al., ‚ÄúDynamic encoder trans- ducer: a Ô¨Çexible solution for trading off accuracy for latency,‚Äù arXiv:2104.02176, 2021.
[26] F. Weninger, M. Gaudesi, R. Leibold, R. Gemello, and P. Zhan, ‚ÄúDual-encoder architecture with encoder selection for joint close-talk and far-talk speech recognition,‚Äù in Proc. ASRU, 2021.
[27] J. Macoskey, G. P. Strimel, J. Su, and A. Rastrow, ‚ÄúAmor- tized neural networks for low-latency speech recognition,‚Äù arXiv:2108.01553, 2021.
[28] Y. Xie, J. J. Macoskey, et al., ‚ÄúCompute Cost Amortized Trans- former for Streaming ASR,‚Äù in Proc. Interspeech, 2022.
[29] J. Lee, J. Kang, and S. Watanabe, ‚ÄúLayer pruning on demand
with intermediate CTC,‚Äù in Proc. Interspeech, 2021.
[30] E. Jang, S. Gu, and B. Poole, ‚ÄúCategorical reparameterization
with gumbel-softmax,‚Äù in Proc. ICLR, 2017.
[31] C. J. Maddison, A. Mnih, and Y. Teh, ‚ÄúThe concrete distribu- tion: A continuous relaxation of discrete random variables,‚Äù in Proc. ICLR, 2017.
[32] A. Paszke, S. Gross, F. Massa, et al., ‚ÄúPytorch: An impera- tive style, high-performance deep learning library,‚Äù in Proc. NeurIPS, 2019.
[33] S. Watanabe, T. Hori, S. Karita, et al., ‚ÄúESPnet: End-to-End Speech Processing Toolkit,‚Äù in Proc. Interspeech, 2018.
[34] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib- rispeech: An ASR corpus based on public domain audio books,‚Äù in Proc. ICASSP, 2015.
[35] A. Rousseau, P. Del¬¥eglise, Y. Esteve, et al., ‚ÄúEnhancing the ted-lium corpus with selected data for language modeling and more ted talks.,‚Äù in Proc. LREC, 2014.
[36] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger, ‚ÄúDeep networks with stochastic depth,‚Äù in Proc. ECCV, 2016.
[37] A. Fan, E. Grave, and A. Joulin, ‚ÄúReducing transformer depth on demand with structured dropout,‚Äù in Proc. ICLR, 2020.
[38] J. Lee and S. Watanabe, ‚ÄúIntermediate loss regularization for ctc-based speech recognition,‚Äù in Proc. ICASSP, 2021.
[39] C. K. Reddy, V. Gopal, and R. Cutler, ‚ÄúDnsmos p.835: A non- intrusive perceptual objective speech quality metric to evaluate noise suppressors,‚Äù in Proc. ICASSP, 2022.