1UsageStatisticsandMarketShareofContentLanguagesforWebsites,February2023—w3techs.com.*Theseauthorscontributedequallytothiswork.data,suchasVietnameseandFrench.Thedevelop-mentofmultilingualdialoguesystemsposestwosignificantchallenges:(i)understandingqueriesinanylanguageandretrievingrelevantpassagesfromacollectionofdocumentsinmultiplelanguages(ii)generatingappropriateresponsesinthesamelanguage.Priorworks(Clarketal.,2020;Asaietal.,2021a)inopen-domainmultilingualquestion-answeringmodelshaveaddressedthesechallengesusingaretriever-readerapproach.Specifically,themultilingualDPR(mDPR)model,anextensionofDPR(Karpukhinetal.,2020),isusedtore-trievedocumentsfromacorpus.AmultilingualreaderbasedonmultilingualT5(Xueetal.,2021a),generatessuitableresponsesinthetargetlanguagebasedontheretrievedmultilingualpassages.Incontrasttoconventionalretrievaltasks,passagere-trievalinconversationalquestionanswering(QA)presentsnewchallengesaseachquestionmustbeinterpretedwithinthecontextoftheongoingdi-alogue.Previousstudies(Wuetal.,2022)haveshownthatrewritingthequestionusingthedia-loguecontextintoastandalonequestioncanen-hancetheretrievalprocess,surpassingtheperfor-manceofcurrentstate-of-the-artretrievers.ThemDPRmodelemploysabi-encoderarchi-tecture,utilizingapre-trainedmultilingualmodeltoencodethequestionsandpassagesindepen-dently.Theencodedrepresentationsarethencomparedusingamaximuminnerproductsearchtoidentifyrelevantpassagesforagivenques-tion.Inthisstudy,weevaluatetwoparadigmsformultilingualpre-trainedtransformermodelsasmDPRbi-encoders,namely,alanguage-agnosticparadigmandalanguage-awareparadigm.Specifi-cally,weconsidertwomodelsformultilingualsen-tenceembedding:Language-AgnosticBERTSen-tenceEmbedding(LaBSE)(Fengetal.,2022)andXLM-RoBERTa(XLM-R)(Conneauetal.,2020).LaBSEcombinesmaskedlanguagemodelingwithtranslationlanguagemodelingtoproducelanguage-
Language-AgnosticTransformersandAssessingChatGPT-BasedQueryRewritingforMultilingualDocument-GroundedQASrinivasGowriraj∗,SohamDineshTiwari∗,MitaliPotnis∗,SrijanBansal,TerukoMitamura,andEricNyberg{sgowrira,sohamdit,mpotnis,srijanb,teruko,en09}@andrew.cmu.eduLanguageTechnologiesInstitute,CarnegieMellonUniversityAbstractTheDialDoc2023sharedtaskhasexpandedthedocument-groundeddialoguetasktoen-compassmultiplelanguages,despitehavinglimitedannotateddata.Thispaperassessestheeffectivenessofbothlanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddensepassageretriever(DPR),conclud-ingthatthelanguage-agnosticapproachissu-perior.Additionally,thestudyinvestigatestheimpactofqueryrewritingtechniquesus-inglargelanguagemodels,suchasChatGPT,onmultilingual,document-groundedquestion-answeringsystems.Theexperimentscon-ducteddemonstratethat,fortheexamplesex-amined,queryrewritingdoesnotenhanceper-formancecomparedtotheoriginalqueries.Thisfailureisduetotopicswitchinginfinaldialogueturnsandirrelevanttopicsbeingcon-sideredforqueryrewriting.1IntroductionEnglishdominatesasthemostwidelyusedlan-guageontheinternet,andforcommunicatingwithvirtualassistants1.However,theprevalenceofEnglish-centriccontentcreatesalanguagebar-rierfornon-Englishspeakerswhowishtoac-cessinformationandservicesonline.Tobridgethisgap,thereisagrowingneedformultilingualknowledge-groundedquestion-answeringdialoguesystemsthatcanenableindividualstoaccesstheinternetandutilizevirtualassistantsintheirna-tivelanguage.WhilethedevelopmentofEnglishdocument-groundeddialoguesystems(Fengetal.,2021)hasbeenextensivelyexplored,theexplo-rationofotherlanguagesremainslimited.Inresponsetothis,DialDoc2023sharedtaskextendsthetaskofdocument-groundeddialoguetoincludemultiplelanguageswithlimitedannotated
101 Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 101–108 July 13, 2023 ©2023 Association for Computational Linguistics


agnosticsentenceembeddings,whileXLM-Risacross-lingualversionofRoBERTa(Liuetal.,2019)pre-trainedonalargecorpusoftextinover100languagesusingaself-supervisedapproach.Al-thoughbothmodelsarebeneficialformultilingualsentenceembeddings,basedonourexperiments,ithasbeenobservedthatLaBSEoutperformsXLM-R.Additionally,weexaminetheimpactofqueryrewritingtechniquesusinglargelanguagemodels(LLMs)suchasChatGPTtosummarizethecon-versationalhistorymoreconciselyandusetransferlearningtogeneralizetoFrenchandVietnameserewrittenqueries.Therefore,inthisstudy,weinvestigatetheper-formancedifferencebetweenthelanguage-awareandlanguage-agnosticparadigms,wherewefoundthatthelanguage-agnosticLaBSEretrieveroutper-formsthelanguage-awareXLM-Rretriever.Addi-tionally,weexploretheimpactofqueryrewritingontheperformanceofsuchsystems.Whilequeryrewritinghasbeenproposedasapotentialsolutionforimprovingperformance,ourresultsindicatethatrewritingqueriesdidnotsignificantlyimproveperformancefortheconsideredsub-samples.OurcodeisavailableonGitHub2.2RelatedWork2.1Language-agnosticMultilingualModelLanguage-agnosticBERTSentenceEmbedding(LaBSE)modelisessentiallytheBERT(Devlinetal.,2019)modeltrainedwithacross-lingualtrainingtechniquetocreatelanguageagnosticsen-tenceembeddingsformanylanguages.Bytrainingonparalleldataconsistingofpairsofsentencesex-pressingthesamemeaningindifferentlanguages,LaBSEisabletolearnhowtomapsentencesfromdifferentlanguagesontoasharedhigh-dimensionalspace,wheresimilarsentencesarelocatedclosetoeachotheranddissimilaronesarefarapart.LaBSEoutperformspreviousstate-of-the-artmodelsinarangeofcross-lingualandmultilingualnaturallan-guageprocessingtasks,includingcross-lingualsen-tenceretrieval,cross-lingualdocumentclassifica-tion,andmultilingualquestionanswering,owingtoitscross-lingualtrainingapproach.LanguageagnosticismenablesLaBSEtotransferknowledgeacrossdifferentlanguagesandgeneratesuperior-qualitysentenceembeddingsfortextsinnumerouslanguages,therebymakingitavaluableinstrument
2https://github.com/srinivas-gowriraj/Multilingual_QA/forresearchersandpractitionersdealingwithmul-tilingualtextdata.WehaveemployedLaBSEinourworkduetoitssharedembeddingspaceanditsabilitytocapturecontextualinformationacrossmultiplelanguagesenablingstrongcross-lingualperformanceandknowledgetransferacrossmulti-plelanguages.2.2MultilingualQueryRewritingGPTmodels,includingChatGPT,possessaremark-ablecapabilityforcomprehendingandinterpret-ingnaturallanguage(Haleemetal.,2023;Walid,2023).ChatGPThasproventobehighlycapableofhigh-qualityresponsestonaturallanguagequeries.Additionally,itisalsoeffectiveatrewritinglongcontextualinformationintocompactqueries(Wangetal.,2023).Promptingmethods(Whiteetal.,2023;ZucconandKoopman,2023)areusedtosteertheLLM’sbehaviourfordesiredoutcomeswithoutupdatingmodelweights.Inthisacademicpaper,wehaveusedChatGPTforthepurposeofqueryrewriting.QueryrewritingbypromptingChatGPThaspotentialtoimprovetheeffectivenessofconversationalquestion-answeringsystemsandaidingtheretrievalofinformationfromextensivetextcollections.3DatasetDialDoc2023sharedtaskdatasetconsistsof797Vietnamesedialogueswithanaverageturncountof4and816Frenchdialogueswithanaverageof5turns.Thesedialoguesaregroundedinmultipledocumentsfromninedifferentdomains,namelyTechnology,Health,HealthCareServices,Veter-ansAffairs,Insurance,PublicServices,SocialSe-curity,DepartmentofMotorVehicles,andStudentFinancialAidintheUSA.Eachdialogueturninthedatasetcontainsroleannotationsforthecon-versationbetweenahumanandaconversationalagent,withtheturnsinreversechronologicalor-der,thelatestturnfirstindialoguehistory.Theretrievaldatasetincludesquery,dialoguedata,pos-itivepassages,andnegativepassages.Positivepas-sagescontaintheanswertothegivenqueryandarewithinthedocument,whilenegativepassagesarecloselyrelatedtothedocumentbuttheydonotcontaintheanswertothequeryinfocus.4MethodologyandExperimentsTheprevailingparadigmfordocument-groundedquestion-answeringmodelsinvolvesaretriever-
102


0.89
3https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25.oflargelanguagemodels,weutilizedChatGPTforqueryrewriting.AspecificpromptstructurewasemployedfortheChatGPTmodel,wherethequestionwasrewrittenusingthelastturninthequery,andthecontextencompassedallprecedingturnsconcatenatedinreverseorder.Thetemplateofthepromptthatweemployedisprovidedbelow:Rewritethequestionintoaninformativequeryexplicitlymentioningrelevantdetailsfromtheprovidedcontext.Context:{dialoguehistory}Question:{last-turn}Re-writtenQuestion:Ourstudy’soutcomes,whichcomparedlanguage-agnosticandmultilingualparadigms,demonstratedthatLaBSE-basedretrieversoutperformedothermethodsformultilingualretrievaltasks.Asaresult,weoptedtoutilizetheLaBSE-basedmDPRretrievermoduleforallsubsequentexperiments.Wealsoevaluatedtheimpactofutilizingforward-ordercontext,buttheresultsindicatedthatitaccentuatedirrelevantinformation.5ResultsandDiscussionLanguageagnosticretrieversoutperformlanguage-awareretrievers.InTable1,wepresenttheresultsofourexperiments,wherewefirstpre-trainedtheretrievermodelsLaBSEandXLM-RonChinese(zh)+English(en)data,andthenfine-tunedthemonvariouscombinationsofFrench(fr)andVietnamese(vi)documentgroundeddatasets,asdescribedinSection4.1.ThefindingsdemonstratethattheLaBSE-basedmDPRretrievermodeloutperformedtheXLM-R-basedmDPRretrievermodel,inallmetricsandtrainingdatasetcombinations.AlthoughXLM-R,whichisbasedonRoBERTa(amoreadvancedversionofBERT)andhas125Mmodelparameters,wastrainedonunsupervisedcross-lingualdata,LaBSEstilloutperformedit.TheBERT-basedarchitecturehas110Mtrainableparameters.
Finetuned
0.92
0.80
0.72
0.95
0.90
0.90
fr+vi
fr+vi
fr+vi
fr+vi
fr+vi
fr+vi
fr+vi
fr+vi
0.76
0.87
0.87
0.57
0.65
0.65
0.83
0.82
0.82
Model
0.78
fr
fr
0.84
LaBSE
LaBSE
LaBSE
R@1
R@10
XLMR
XLMR
XLMR
0.55
zh+en
zh+en
zh+en
zh+en
zh+en
zh+en
0.86
R@20
0.45
vi
vi
Evaluated
0.75
0.75
0.67
Pretrained
R@5
Table1:Performancecomparisonoflanguage-agnosticversuslanguage-awaremultilingualdensepassageretrievalapproachespretrainedonChinese(zh)+English(en)andfine-tunedonFrench(fr)+Vietnamese(vi).readerapproachthatcomprisesofadocumentretrievalmodule,arerankermodule,andanan-swergenerationmodule.However,inthisstudy,ourmainfocushasbeenonthemultilingualre-trievercomponent,whilefixingXLM-Rasreranker.However,wedidexperimentwiththeFusion-in-Decoder(FiD)approach(Raffeletal.,2020)tomodifythemT5modelpreviouslybeingusedastheanswergenerator(Xueetal.,2021b).4.1Retrieval:LanguageAgnosticvsLanguage-awareInthispaper,weemploythemultilingualdensepassageretriever(mDPR)(Asaietal.,2021b)toretrievepassagesfrommultilingualdocumentcol-lections.However,thebi-encodersusedinmDPRconsistoftwodifferentmodels:LaBSE,whichisdesignedforthelanguage-agnosticparadigm,andXLM-R,whichissuitableforthelanguage-awaresetting.Topreparethemodels,wefirstpre-trainthemusingtheEnglishandChineseportionsofadocument-groundeddataset.Wethenfine-tunethemodelsonthreedifferentcombinationsoftargetdatasets,namelyFrenchandVietnamese,Frenchonly,andVietnameseonly.Finally,weevaluatetheperformanceofthemodelsonthecorrespondingvalidationsetsofeachdatasetcombination.ThemDPRmodelsaretrainedusingtheEnglishandChinesesplits,employinggoldpassagesalongwith10hardnegativesminedthroughBM25.3.Thedatasetisdividedintoan80-20train-testsplitusingaconsistentseed.Thetrainingprocessforallconfigurationspersistsfor50epochs.4.2Zero-ShotMultilingualQueryRe-writingToimprovetheefficiencyoftheretrievermodule,wepostulatedthatconvertingthequeryanddialoguecontexthistoryintomoreconciseandinformativequestionswouldbeadvantageous.Drawinginspirationfromtheaccomplishments
103


0.89
Table2:Comparisonoftransferlearningapproachesusingdifferentquery-rewritingapproaches.Rawreferstotheoriginaldialoguequeryi.e.currentturn+HistorywhileChatGPTreferstothequeryre-writtenbyChat-GPTusingcurrentturnanddialoguehistory.WeuseLaBSE-basedmDPRforalltheabovesettings.5.1ErrorAnalysisofRewrittenQueriesThisstudyfocusesontheevaluationoftheperfor-manceofrewrittenqueriesgeneratedbyChatGPTincomparisontotheoriginalqueriesconsistingofaquestionandcontext.Moreover,acomprehen-siveerroranalysisisconductedtoidentifythegapsintherewrittenqueries.Figure1presentsnotableobservations.ThefindingsrevealthatthequalityoftherewrittenqueriesgeneratedbyChatGPTisinferiortothatoftheoriginalqueries.Furtherinves-tigationshowsthattopicswitchingoftenoccursinthelastturnoftheconversation,resultinginrewrit-tenqueriesthatincorporatenon-relevantcontext.ThisphenomenonisillustratedinFigure1.Theswitchingoftopicsadverselyaffectstherelevanceandaccuracyoftherewrittenqueries.Additionally,therewritingprocesstendstosummarizebothrele-vantandnon-relevanttopicsfromtheconversation,andhallucinateinformation,asshowninFigure2.Thisapproachlacksspecificityandclarityintherewrittenqueries,furtherimpedingtheirqualityandeffectiveness.Furthermore,thepromptsarecreatedmanuallybyvisuallyinspectingthegener-atedoutputs.Whilethismethodallowsforqualitycontroloftheprompts,itisinherentlysubjectiveandvulnerabletohumanbiases.Thus,itises-sentialtoexploreadvancedpromptingmethodstoenhancetheoverallqualityoftherewrittenqueries,assuggestedin(Liuetal.,2023).6ConclusionThispaperinvestigatestheeffectivenessoflanguage-agnosticandlanguage-awareparadigmsformultilingualpre-trainedtransformermodelsinabi-encoder-baseddenseretriever.Thepaperalsoevaluatestheimpactofqueryrewritingontaskper-formance.Ourfindingsindicatethatthelanguage-agnosticapproachoutperformsthelanguage-awareapproach.However,fortheconsideredsubsam-ples,queryrewritingdidnotimprovetheperfor-manceovertheoriginalqueries.Furthermore,theobservedtopicswitchingintheconversations’lastturns,andChatGPT’stendencytosummarizenon-relevanttopicsandhallucinationleadtolessac-curaterewrittenqueriescomparedtotheoriginalqueries.7Limitations,PotentialRisks,andFutureWorkThelimitationsofthisstudyareprimarilyduetobudgetandcreditconstraints.Consequently,ourqueryrewritingobservationsarebasedonasam-plesizeof2000,leadingtolimitedgeneralizabilityoffindings.Anotherlimitationofthelimitedre-sourceswasthelimitedcontextsizeofChatGPTandtherelativelylongnatureofthequestionsinourdataset.Hence,wecouldnottestpromptingChatGPTwithin-contextexamplesforbetterqueryrewritingperformance.Henceonepotentialfutureworkistestingtheperformanceofqueryrewritingusingin-contextexamples.Finally,ChatGPTar-chitectureisnotopensource,preventingusfromtestingadvancedpromptingmethods.Hencean-otherfutureworkwouldbetotestqueryrewritingusingopensourcemodelsandwithadvancedfine-tuningmethodslikePrefixTuning(LiandLiang,2021)andPromptTuning(Lesteretal.,2021).Thestudyisalsosubjecttotheriskof"halluci-nations"inChatGPT’sresponses,whichmayleadtoimprecisioninqueryrewriting.Thestudysug-gestsfurtherinvestigationintotheseissuestoim-provetheaccuracyandreliabilityoftheresults.Werecommendfurtherinvestigationintotheselimita-tionsandanypotentialsocietalbiasespresentinourdatasettoenhancethereliabilityandperformanceofqueryrewriting.
vi(ChatGPT)
0.65
0.38
0.33
0.26
0.78
0.70
EvalOn
0.49
0.84
0.84
0.58
fr(ChatGPT)
MultilingualQueryRewritingdoesnotleadtobetterperformance.TheresultspresentedinTable2provideevidencethattheincorporationofmultilingualqueryrewritingdoesnotleadtoenhancedperformanceforthetestedexamples.Moreprecisely,theLaBSEmodel,whichwastrainedonunmodifiedsubsetsofEnglish(en)data,demonstratedsuperiorknowledgetransferabilitiescomparedtomodelstrainedonqueriesthatwererewrittenbyChatGPT.Thus,furtherresearchisnecessarytoelucidatethereasonsforthissuboptimalperformance.
Trainedon
fr(Raw)
0.77
R@1
R@10
0.16
0.46
R@20
en(ChatGPT)
en(ChatGPT)
0.45
0.51
en(Raw)
en(Raw)
R@5
vi(Raw)
104


Figure2:Erroneousrewrittenquery(ChatGPT)whichconsiderednon-relevanttopicsandhallucinatedinformation
Figure1:Anerroneousrewrittenquery(ChatGPT)occurredwhereinthesubjectmatterabruptlychangedduringthefinalexchangeofdialogue,ashighlightedinyellow.
105


ReferencesSumitAgarwal,SurajTripathi,TerukoMitamura,andCarolynPensteinRose.2022.Zero-shotcross-lingualopendomainquestionanswering.InProceed-ingsoftheWorkshoponMultilingualInformationAc-cess(MIA),pages91–99,Seattle,USA.AssociationforComputationalLinguistics.AkariAsai,JungoKasai,JonathanClark,KentonLee,EunsolChoi,andHannanehHajishirzi.2021a.XORQA:Cross-lingualopen-retrievalquestionanswering.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pages547–564,Online.AssociationforComputa-tionalLinguistics.AkariAsai,XinyanYu,JungoKasai,andHannanehHajishirzi.2021b.Onequestionansweringmodelformanylanguageswithcross-lingualdensepassageretrieval.JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454–470.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzmán,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenAri-vazhagan,andWeiWang.2022.Language-agnosticBERTsentenceembedding.InProceedingsofthe60thAnnualMeetingoftheAssociationforCompu-tationalLinguistics(Volume1:LongPapers),pages878–891,Dublin,Ireland.AssociationforComputa-tionalLinguistics.SongFeng,SivaSankalpPatel,HuiWan,andSachindraJoshi.2021.MultiDoc2Dial:Modelingdialoguesgroundedinmultipledocuments.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages6162–6176,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.AbidHaleem,MohdJavaid,andRaviSingh.2023.Aneraofchatgptasasignificantfuturisticsupporttool:Astudyonfeatures,abilities,andchallenges.Bench-CouncilTransactionsonBenchmarks,StandardsandEvaluations,2:100089.GautierIzacardandEdouardGrave.2021a.Leveragingpassageretrievalwithgenerativemodelsforopendomainquestionanswering.GautierIzacardandEdouardGrave.2021b.Leveragingpassageretrievalwithgenerativemodelsforopendo-mainquestionanswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssoci-ationforComputationalLinguistics:MainVolume,pages874–880,Online.AssociationforComputa-tionalLinguistics.VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.Densepassageretrievalforopen-domainquestionanswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages6769–6781,Online.AssociationforComputationalLinguistics.BrianLester,RamiAl-Rfou,andNoahConstant.2021.Thepowerofscaleforparameter-efficientprompttuning.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3045–3059,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.XiangLisaLiandPercyLiang.2021.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingoftheAsso-ciationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pages4582–4597,Online.AssociationforComputationalLin-guistics.PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,andpredict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.55(9).YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:ArobustlyoptimizedBERTpretrainingapproach.CoRR,abs/1907.11692.ColinRaffel,NoamShazeer,AdamRoberts,Kather-ineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.JournalofMachineLearningResearch,21(140):1–67.SiamakShakeri,NoahConstant,MihirSanjayKale,andLintingXue.2021.Towardszero-shotmultilingualsyntheticquestionandanswergenerationforcross-lingualreadingcomprehension.HaririWalid.2023.Unlockingthepotentialofchatgpt:Acomprehensiveexplorationofitsapplications,ad-vantages,limitations,andfuturedirectionsinnaturallanguageprocessing.ShuaiWang,HarrisenScells,BevanKoopman,andGuidoZuccon.2023.Canchatgptwriteagoodbooleanqueryforsystematicreviewliteraturesearch?
106


JulesWhite,QuchenFu,SamHays,MichaelSandborn,CarlosOlea,HenryGilbert,AshrafElnashar,JesseSpencer-Smith,andDouglasC.Schmidt.2023.Apromptpatterncatalogtoenhancepromptengineer-ingwithchatgpt.ZeqiuWu,YiLuan,HannahRashkin,DavidReit-ter,HannanehHajishirzi,MariOstendorf,andGau-ravSinghTomar.2022.CONQRR:Conversationalqueryrewritingforretrievalwithreinforcementlearn-ing.InProceedingsofthe2022ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages10000–10014,AbuDhabi,UnitedArabEmi-rates.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021a.mT5:Amassivelymultilingualpre-trainedtext-to-texttransformer.InProceedingsofthe2021ConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages483–498,On-line.AssociationforComputationalLinguistics.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2021b.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.GuidoZucconandBevanKoopman.2023.Drchatgpt,tellmewhatiwanttohear:Howpromptknowledgeimpactshealthanswercorrectness.
107


62.96
64.73
65.34
Table3:ReaderperformancecomparisonofvanillamT5versusmT5incombinationwithFiDFusion-in-Decoder(FiD)improvestheoverallreaderperformance.AsillustratedinTable3weobservedanimprovementofapproximately4.2%,3.51%,and2.80%inF1,BLEU,andRouge-Lscores,respectivelywhenweincludeFusion-in-DecoderalongwithmT5inthecaseofbothViet-namese(vi)andFrench(fr)languagesthusprovingthatFusion-in-Decoderdoesindeedhelpinbolster-ingourReaderperformance.
fr+vi
fr+vi
fr+vi
fr+vi
63.03
42.22
59.89
58.55
40.87
BLEU
Model
41.47
fr
fr
fr
fr
45.93
45.72
39.42
64.83
vi
vi
vi
vi
Evaluated
65.61
mT5+FiD
mT5+FiD
mT5+FiD
mT5
mT5
mT5
60.00
68.22
ROUGE-L
56.76
F1
Pre-trained
AAppendixA.1Reader:mT5andFusion-in-DecoderInrecentyears,theuseofthemultilingualText-to-TextTransferTransformer(mT5)(Xueetal.,2021b),amultilingualvariantofText-to-TextTransferTransformer(T5)(Raffeletal.,2020)hasgainedpopularityinmultilingualquestionanswer-ingtasks(Shakerietal.,2021).mT5hastheabilitytolearntherepresentationsoftextthatcapturethenuancesoflanguageacrossdifferentlanguagesandcontexts,allowingittoexcelinmultilingualset-tings.ToleveragethisgrowingpopularityofthemT5generativereadermodel,ourworkinvolvesemployingaFusion-in-Decoder(FiD)(IzacardandGrave,2021a)asareaderincombinationwithmT5.FiDincombinationwithmT5hasbeenproventoboosttheperformanceofanswerextractionascom-paredtousingmT5alone(Agarwaletal.,2022)byencodingthererankedpassagesindividuallyone-by-oneandconcatenatingthemtogetherwhilepass-ingthemforthedecoderstage.Weconductedfurtheranalysisontheimpactofourlanguage-agnosticmultilingualretrieverse-lectionbyemployingatwo-stepprocess.Firstly,weretrievedrelevantpassagesusingthechosenretriever,andsubsequently,weutilizedthemT5readertogenerateresponsesbasedonthesere-trievedpassages.Twodifferentversionsofthereaderwereemployed:thevanillamT5andthemT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).ThevanillamT5readertakesthequeryconcatenatedwiththeretrievedpassagesasitsinput.Ontheotherhand,theFiD-mT5readerin-dependentlyencodeseachretrievedpassagealongwiththequery.Theencodedrepresentationsarethenconcatenatedandpassedtothedecoder.Asaresult,theevidencefusiontakesplacesolelywithinthedecoder.Toassessthequalityofthegeneratedresponses,weemployedBLEU,Rouge-L,andF1metricscores.Wefurtherevaluatetheimpactofourchoiceoflanguage-agnosticmultilingualretrieverbypass-ingretrievedpassagestomT5readertogenerateresponse.WeusedtwodifferentreaderswhicharemT5(vanilla)andmT5withFusion-in-Decoder(FiD)(IzacardandGrave,2021b).VanillamT5takesqueryconcatentatewithretrievedpassagesasinputwhileFiD-mT5encodeseachretrievedpas-sagealongwithqueryindependentlywhichisthenconcatenatedandpassedtothedecoder.Themodelthusperformsevidencefusioninthedecoderonly.WeevaluatethegeneratedresponseusingBLEU,Rouge-L,andF1metricscores.
62.43
108