Multi-lingual and Multi-cultural Figurative Language Understanding
Anubha Kabra1∗, Emmy Liu1∗, Simran Khanuja1∗, Alham Fikri Aji2, Genta Indra Winata3, Samuel Cahyawijaya4, Anuoluwapo Aremu5, Perez Ogayo1, Graham Neubig1
1Carnegie Mellon University 2MBZUAI
3Bloomberg 4HKUST 5Masakhane
Abstract
Figurative Expression
Inference
1
Figurative language permeates human commu- nication, but at the same time is relatively un- derstudied in NLP. Datasets have been cre- ated in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). How- ever, the use of figurative language is an ex- pression of our cultural and societal experi- ences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, MABL, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for fig- urative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs’ abili- ties to interpret figurative language in zero- shot and few-shot settings. All languages ex- hibit a significant deficiency compared to En- glish, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be ex- posed to a broader range of linguistic and cul- tural variation during training. 1
Introduction
yo
Omah iku kaya istana (The house is like a palace.)
Omah iku apik banget. (The house is very nice.) Omah iku elek banget. (The house is very ugly.)
id
Rambutnya seperti bihun. (Her hair is like vermicelli.)
Rambutnya keriting. (Her hair is curly.) Rambutnya lurus. (Her hair is straight.)
hi जीवन मीठा गुलकन्द है।
(Life is sweet Gulkand. )
जीवन अ(cid:501)ा है। (Life is good.) जीवन बुरा है। (Life is bad.)
kn
ಅದು ದೋಸೆಯಂತೆ ಗರಿಗರಿಯಾಗಿತ್ತು. (It was crispy like a dosa.)
ಅದು ಗರಿಗರಿಯಾಗಿದೆ (It is crisp.) ಅದು ಗರಿಗರಿಯಾಗಿರಲಿಲ್ಲ (It was not crisp.)
sw
Maneno yake ni sumu. (His words are like poison.)
Maneno yake yanaponya. (His words heal.) Maneno yake yanaangamiza. (His words are devastating.)
Table 1: Examples of figurative expressions and respec- tive inferences from the collected data. Correct answers are highlighted in green.
Lakoff and Johnson, 1981), which is laden with im- plicit cultural references and judgements that vary cross-culturally. Differences in figurative expres- sions used in different languages may be due to cul- tural values, history, or any number of other factors that vary across where the languages are spoken.2 Understanding figurative language therefore relies on understanding what concepts or objects are con- sidered culturally significant, as well as their senti- ment in that culture.
When you are feeling happy, do you think that you are “warm” or “cold”? If you are a monolingual English speaker, you will likely answer “warm”, and use expressions like “this really warmed my heart”. However, if you are a native Hindi speaker, you may answer “cold”, and use expressions like (cid:606)दल को ठं डक पढ़ना (“coldness spreads in one’s heart” ) (Sharma, 2017). Linguistic communica- tion often involves figurative (i.e., non-literal) lan- guage (Shutova, 2011; Fussell and Moss, 2008;
Better understanding of figurative language would benefit tasks such as hate speech detec- tion or sentiment classification (ElSherief et al., 2021; van Aken et al., 2018). However, state-of- the-art language models have been shown to fre- quently misinterpret both novel figurative expres- sions and conventionalized idioms, indicating the need for improved methods (Dankers et al., 2022;
∗ These authors contributed equally. 1Data and code is simran-khanuja/Multilingual-Fig-QA
released at https://github.com/
2The Hindi example is most likely attributable to climatic con- ditions, as cold may be seen as comparatively more positive in an area where extreme heat is more common (Sharma, 2017)


Liu et al., 2022). Most empirical results probing language models’ abilities with respect to figura- tive language have been based on data in English, meaning there is a comparative lack of resources and study in other languages (Chakrabarty et al., 2022; Liu et al., 2022; Pedinotti et al., 2021a).
We find English figurative language datasets may not have cultural relevance for other lan- guages (§2). This is a general challenge in NLP, as assumptions of common knowledge and impor- tant topics to talk about vary from culture to cul- In order to bet- ture (Hershcovich et al., 2022). ter train multilingual models to interpret figura- tive language, as well as to understand linguistic variation in figurative expressions, we construct a multilingual dataset, MABL (Metaphors Across Borders and Languages), of 6,366 figurative lan- guage expressions in seven languages (§3). Exam- ples are shown in Table 1.
We use the dataset to conduct a systematic anal- ysis of figurative language patterns across lan- guages and how well they are captured by cur- rent multilingual models (§4). We find that figura- tive language is often very culturally-specific, and makes reference to important entities within a cul- ture, such as food, mythology, famous people, or plants and animals native to specific regions.
We benchmark multilingual model performance (§5) and analyze model failures (§6), finding that zero-shot performance of multilingual models is relatively poor, especially for lower-resource lan- guages. According to (Liu et al., 2021), main fac- tors which poses challenges on the performance in such cases are cross-lingual transfer and concept shift across languages. However, we observe that concept shift seems to play a larger role due to cul- turally specific examples. Adding a few examples in the target language can improve performance of larger models, but this is more beneficial for lower-resource languages. This highlights the im- portance of including culturally relevant training data, particularly data that highlights not just the existence of a concept, but also how people view that concept within that culture.
2 Linguistic and Cultural Biases of
Existing Figurative Language Datasets
To confirm the importance of building a multi- lingual, multi-cultural figurative language dataset, we first performed a pilot study to examine the fea- sibility of instead translating an existing figurative language dataset, Fig-QA (Liu et al., 2022), from
Lang.
fr
hi
ja
Incorrect Culturally irrelevant
13% 40% 21% 17% 20% 17%
Table 2: Correctness and cultural relevance of Google translations of Fig-QA validation set.
English into other languages. While there are well- known problems with using translation to create multilingual datasets for tasks such as QA (Clark et al., 2020), it is still worth examining these is- sues in the context of figurative language in partic- ular. We used the Google Translate Python API to translate the development set into languages that the authors of this paper understood.3 These were French, Japanese, and Hindi. Each annotator anno- tated 100 examples for both correctness (whether or not the translation was accurate), and cultural relevance (whether or not the expression was one that would make sense to a native speaker from the culture where the language is predominant).
the number of incor- rect examples is large, particularly for Hindi and Japanese. This is mainly due to expressions that don’t translate directly (such as a “sharp” conver- sation in English). Culturally irrelevant examples are due to implicitly assumed knowledge. For in- stance, a crowdworker from the US generated the example “it’s as classic as pancakes for breakfast” with the meaning “it’s very classic”. However, most people from Japan would not see pancakes as a traditional breakfast, and the meaning “it’s not classic” would be more appropriate.
As seen in Table 2,
The shift in topics discussed in cultures associ- ated with different languages can be captured by native speakers familiar with that culture, motivat- ing our collection of natural figurative language ex- amples from native speakers.
3 The MABL Dataset
3.1 Language Selection
We choose the following seven languages: Hindi (hi), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Indonesian (id), and Javanese (jv).
The factors we considered while choosing these
languages are as follows :
i) We aimed to include a range of languages representing the different classes in the resource- based taxonomy of languages, proposed by Joshi et al. (2020), subject to annotator availability.
3https://pypi.org/project/googletrans/


Language id sw su jv hi kn yo
#Samples 1140 1090 600 600 1000 1190 730
Table 3: Number of collected samples per language.
ii) We chose languages with a sizeable speaker
population as shown in Table 5.
iii) Our languages come from 5 typologically di- verse language families spoken in 4 different coun- tries, which allows us to include a wide range of linguistic and cultural diversity in our data.
Details about the characteristics of each lan- guage in terms of available training data and num- ber of speakers can be found in Table 5. Additional information on linguistic properties of these lan- guages can be found in Appendix A.
3.2 Dataset Collection
To create culturally relevant examples, we crowd- sourced sample collection to two or more native speakers in the seven languages. The workers were asked to generate paired metaphors that began with the same words, but had different meanings, as well as the literal interpretations of both phrases.
Workers were not discouraged from generating novel metaphors, but with the caveat that any examples should be easily understood by native speakers of that language, e.g., “it’s as classic as pancakes for breakfast” would not be valid if pan- cakes are not a breakfast food in the country in which that language is spoken.
Instructions given to annotators can be found in Appendix B. After collection, each sample was validated by a separate set of workers who were fluent in that language. Any examples that were incoherent, offensive, or did not follow the format were rejected. The number of samples collected per language can be seen in Table 3. Examples of collected data can be seen in Table 1. We note that because of the limited number of samples in each language, we view the samples collected as a test set for each language, meaning there is no explicit training set included with this release.
4 Dataset Analysis
4.1 Concepts expressed In the structure mapping theory of metaphor, figu- rative language involves a source and target con- cept, and a comparison is made linking some fea- tures of the two (Gentner, 1983). Following Liu et al. (2022), we refer to the source as the “subject” and target as “object” .4
We expect objects referenced to be quite differ- ently cross-culturally. We confirm this by translat- ing sentences from our dataset into English, then parsing to find objects. The number of unique con- cepts per language, including examples, is listed in Appendix C. This may overestimate the number of unique concepts, as some concepts may be closely related (e.g., “seasonal rain” vs. “rainy season”). Despite this, we are able to identify many cultur- ally specific concepts in these sentences, such as specific foods (hi: samosa, hi: sweet gulkand, id: durian, id: rambutan), religious figures (kn: buddha’s smile, sw: king soloman), or references to popular culture (id: shinchan, yo: aníkúlápó movie, en: washington post reporter).
We observe that, excluding pronouns, only 6 ob- jects are present in all languages. These are {“sky”, “ant”, “ocean”, “fire”, “sun”, “day”}. Of course, variations of all these concepts and other generic concepts may exist, since we only deduplicated ob- jects up to lemmatization, but this small set may in- dicate that languages tend to vary widely in figura- tive expressions. Appendix D indicates the Jaccard similarity between objects in each language, which is an intuitive measure of set similarity. The equa- tion is also given below for sets of objects from language A (𝐿𝐴) and langugage B (𝐿𝐵).
𝐽 (𝐿𝐴, 𝐿𝐵) =
|𝐿𝐴 ∩ 𝐿𝐵| |𝐿𝐴 ∪ 𝐿𝐵|
The most similar language based on concepts present is highlighted in Table 4. Languages from the same region tend to group together. The set of concepts in English is actually most similar to Swahili.5 Upon inspection, there were many gen- eral terms related to nature, as well as many ref- erences to Christianity in the Swahili data, which may explain the similarity to English.6
4This terminology may be confusable with subject and object in linguistics, but was used because the source and target tend to appear in these linguistic positions in a sentence. 5There are no particularly closely related languages to English in our dataset 6Authors of this paper examined unique concepts expressed in English, Swahili, and Kannada. Swahili sentences had
(1)


Lang. Most similar
hi kn
id jv
jv sw
kn hi
su jv
sw yo hi
en sw sw
Table 4: Most similar concepts sets for each language, based on Jaccard similarity of objects in each lan- guage’s sentences. Note that as in Appendix A, {hi, kn}, {id, jv, su} and {sw, yo} respectively occur in similar geographic regions.
Lang.
Speakers Training Data (in GB)
Class
(M)
XLM-R
mBERT
en
400
300.8
15.7
5
hi id jv kn su sw yo
322 198 84 44 34 20 50
20.2 148.3 0.2 3.3 0.1 1.6 -
0.14 0.52 0.04 0.07 0.02 0.03 0.012
4 3 1 1 1 2 2
Table 5: Per-language statistics (including en for refer- ence); the speaker population of each language, its rep- resentation in pre-trained multilingual models, and the Joshi et al. (2020) class each language belongs to. First- language speaker population information is obtained from Wikipedia and Aji et al. (2022). We obtain data size estimates for multilingual BERT from Wikipedia 2019 dump statistics.7
4.2 Commonsense Categories
We follow the commonsense categories defined in Liu et al. (2022) to categorize knowledge needed to understand each sentence: physical object knowl- edge (obj), knowledge about visual scenes (vis), social knowledge about how humans generally be- have (soc), or more specific cultural knowledge (cul). The same sentence can require multiple types of knowledge. Table 6 shows the prevalence of each type of commonsense knowledge as doc- umented by annotators. Social and object knowl- edge are the most dominant types required, with Yoruba having an especially high prevalence of so- cial examples. Not many examples were marked as cultural. This may be due to differences in what annotators viewed as cultural knowledge: some knowledge may be considered to fall under the object or social category by annotators, but these same examples may seem culturally specific to people residing in the United States because the objects referenced are not necessarily relevant to English speakers in the US.
18/481 Christianity related concepts, while English had 13/954. Kannada did not have any Christianity related con- cepts but rather concepts related to Hinduism.
Lang. Object Visual
Social
Cultural
hi id jv kn su sw yo
52.4 45.8 34.0 63.3 34.3 48.0 37.3
16.4 5.7 15.0 17.1 8.6 20.2 6.1
42.0 45.6 43.3 20.3 33.3 32.2 81.0
9.2 7.5 10.0 15.2 24.0 5.6 10.7
Table 6: Proportion of common-sense categories.
4.3 Cross-lingual concept distribution
To better understand the linguistic and cultural dis- tribution of examples, we extract sentence-level representations from two models: i) XLM-Rlarge (Conneau et al., 2019), our best performing base- line model; and ii) LaBSE (Feng et al., 2020), a language-agnostic sentence embedding model, op- timized for cross-lingual retrieval. We observed that XLM-R clusters by language, whereas LaBSE clusters sentences from multiple languages to- gether, based on conceptual similarity (as shown in Figure 2). Since LaBSE is optimized for cross- lingual sentence similarity, we chose the latter to conduct further analysis.
First, we probe different edges of the cluster and observe concepts along each edge, as visualized in Figure 1. For each concept, we observe sen- tences from various languages clustering together. Further, these sentences portray cultural traits per- taining to each language. For example, rice is commonly mentioned in languages from Indone- sia, given that it is a staple food product there.8 Other examples include sentences in Hindi such as This house is as old as a diamond (diamonds have a significant historical background in India) or Your house is worth lakhs (lakh is an Indian En- glish term).9
To qualitatively study cultural references, we further analyse metaphors belonging to universal concepts such as food, weather/season, and friend- ship, searching for sentences containing these key- words.10 We obtain 230 sentences containing food, 111 sentences containing weather/season and 307 sentences containing friend. A few examples are as shown in Table 7. We observe multiple regional and cultural references, which may not be under-
8https://www.indonesia-investments.com/business/ commodities/rice/item183 9https://en.wikipedia.org/wiki/Indian_numbering_ system 10We do a regex search over the word and its translation in each language to obtain sentences from all languages in the concept, using https://projector.tensorflow.org/


तरह
हो
House
गया
घर
Life
की
Voice
यह
She eats like a bird. UMAP: n_neighbours : 15, min_dist = 0.1
है।
हीरे
पुराना
This house is as old as a diamond. है।
Figure 1: UMAP visualization of the collected data. Sentence embeddings are obtained using LaBSE (Feng et al., 2020), a multilingual dual encoder model, optimized for cross-lingual retrieval. Refer to Section 4 for more details.
sw en
LaBSE
XLM-R Large
5 Evaluation and Results
5.1 Zero-shot
5.1.1 Zero-shot evaluation Here, we simply fine-tune the Multilingual Pre- trained Language Models (MPLMs) on the English labelled data and evaluate on all target languages. This was performed in the standard format of in- putting each example as [CLS] [sentence] [SEP] [meaning1] [SEP] [meaning2] and using a linear layer on the [CLS] token to classify the answer.
Figure 2: We visualize sentence embeddings for two languages, Swahili (sw) and English (en), using our best-performing model, XLM-R Large (left) and LaBSE (right). Given that en shares the highest number of concepts with sw, we’d expect a tight integration of embedding spaces, which is better displayed by LaBSE.
standable by non-native speakers. For example, annotators make references to the weather/season with Peacock and frying fish on asphalt which are innate comparisons in su. With reference to food, Indian food commonly uses Neem and Tamarind as referenced by metaphors in kn and hi. Neem is a bitter medicinal herb and Tamarind is used to add sourness to food. Finally, we see references to mythological and fictional characters across friend- ship metaphors, where annotators draw from their attributes to describe friendships.
5.1.2 Zero-shot transfer results We present zero-shot evaluation results in Table 8, noting that there can be two contributors to the gap in performance in these seven languages as com- pared to English. First, since our fine-tuning lan- guage is English, there can be a drop in perfor- mance simply due to cross-lingual transfer. Sec- ond, there is a concept shift in these metaphors, as evidenced by our analysis in Section 4. To discern the contribution of both, we machine-translate the target test sets to en (we refer to this as translate- test). The difference between translate-test and zero-shot, can be thought of as the cross-lingual transfer gap, while the rest of the difference be- tween translate-test and en test performance can be attributed to the concept shift. Due to possible MT errors, the results here represent upper bounds for concept shift and cross-lingual shift, which is


References to weather/season
References to food
References to friendship
su
The Indian Ocean is sparkling like a Peacock this Christmas season.
kn
That food is as sweet as Neem
jv
My friend’s father is like a raden werkudara.
kn
The weather is also warm like the rainy season.
hi
Hotel food was like tamarind.
hi
He guided his friend like Krishna.
su
The weather looks like you can fry fish on the asphalt.
sw
His waist is the width of a baobab.
sw
His friend is abunuwasi.
hi
Tina and Ravi’s love is like monsoon season.
jv
The taste of this food is like boiled tempeh.
id
He asks the help of his friends just like the king of Tanah Djawo Kingdom.
Table 7: Translated examples with cultural references specific to regions where these languages are spoken.
further discussed in Section 6.1.
The concept shift gap is generally greater than the cross-lingual gap. As reported in Ta- ble 8, the concept shift gap is greater than the cross-lingual transfer gap for all languages except Swahili, across all models. This result for sw cor- roborates our findings in Section 4, where we ob- serve that en shares the greatest proportion of ob- ject concepts with sw. Given Swahili’s extremely low-representation in MPLMs (Table 5), and its high concept overlap with English, we cover most of the gap by simply translating sw to en. For Indonesian (id), we observe that zero-shot perfor- mance itself is close to en performance (83.6%) for XLM-R, since id is well-represented in this model (Table 5). Hence, translating to en does not help, and the model needs to be competent in better un- derstanding the cultural references specific to id. In mBERT however, id is poorly represented, and translating to en does help improve performance.
Performance increases as model and train- ing data size increase, but moreso for higher resource languages. The smallest model exam- ined, mBERT, has relatively poor performance for all languages, as all languages have < 60% ac- curacy. Hindi and Indonesian, the two highest- resource languages in our dataset, show a high gain in performance when using a larger model, increas- ing to 67.58% and 78.09% accuracy respectively. This is especially true for Indonesian, which has a relatively high amount of training data as shown in Table 5. However, lower resource languages tend to show a more modest gain in performance.
5.2 Few-shot
5.2.1 Few-shot evaluation While it is common to fine-tune MPLMs on En- glish, given its widespread use and availability, several past works have shown how this is sub- optimal (Lin et al., 2019; Debnath et al., 2021) and choosing optimal transfer languages is an important research question in itself (Dhamecha et al., 2021). While the design of an ideal al- location of annotation resources is still unknown, Lauscher et al. (2020) demonstrate the effective- ness of investing in few-shot (5-10) in-language task-specific examples, which provides vast im- provements over the zero-shot setup.
We include between 2-50 labelled pairs of sen- tences from each target language, in addition to the English labelled data, for fine-tuning the model. Training details for all models can be found in Ap- pendix E.
5.2.2 Few-shot results Figure 3 presents the effects of few-shot transfer for each language. Generally, the performance gain is modest. This aligns with results from Lauscher et al. (2020), who found that perfor- mance gains were quite small on XNLI. As our task is also an NLI task, we may expect simi- lar improvements. However, we find collecting some cultural examples could disproportionately help low-resource languages.
Augmenting with few examples usually does not help much We observed that with a few excep- tions, the increase in accuracy on the test set gained was small (< 1%). This is likely because of the di- versity of facts needed in order to improve perfor- mance. As noted in Section 4.1 and Table 1, this dataset contains many unique cultural references that do not repeat, limiting the utility of seeing a few examples.
languages benefit more greatly from augmentation However, there are a few exceptions to this trend. In particular, adding 50 paired Kannada examples to XLM-Rlarge improved performance by 3.83%. Swahili also improves by 1.10% with 50 additional examples for XLM-Rbase, and Sundanese improves by 2.33% with 50 examples for mBERTbase.
Lower-resource
5.3 Evaluation of Large Language Models
In addition to the three MPLMs we examine in de- tail, we also examine the zero-shot performance of large pretrained language models. We choose to


Model
Language
Zero-shot Performance
Translate-test Cross-Lingual Concept Shift Transfer Gap
(to EN)
Gap
XLM-Rlarge
en𝑑𝑒𝑣 hi id jv kn su sw yo
81.50 ±2.41 67.58 ±1.38 78.09 ±1.14 60.93 ±1.95 58.08 ±2.10 60.40 ±1.98 58.16 ±0.73 -
81.50 ±2.41 67.82 ±1.52 77.51 ±0.91 68.13 ±1.66 63.67 ±0.98 70.07 ±0.92 75.29 ±2.05 -
0.00 0.24 -0.58 7.20 5.59 9.67 17.13 -
0.00 13.68 3.99 13.37 17.83 11.43 6.21 -
XLM-Rbase
en𝑑𝑒𝑣 hi id jv kn su sw yo
75.26 ±0.95 62.48 ±0.31 68.88 ±0.71 53.67 ±0.54 54.67 ±1.31 52.41 ±1.79 52.73 ±1.38 -
75.26 ±0.95 63.29 ±0.84 66.54 ±1.22 58.17 ±0.82 57.86 ±1.10 61.33 ±0.68 65.77 ±1.82 -
0.00 0.81 -2.34 4.50 3.20 8.93 13.04 -
0.00 11.97 9.26 17.09 17.40 13.93 7.31 -
mBERTbase
en𝑑𝑒𝑣 hi id jv kn su sw yo
70.88 ±2.46 51.32 ±0.94 56.56 ±1.66 55.06 ±1.70 52.63 ±1.15 52.87 ±1.67 52.12 ±1.09 50.52 ±1.04
70.88 ±2.46 59.45 ±1.77 63.30 ±1.12 60.76 ±2.31 56.70 ±0.77 59.37 ±2.37 63.57 ±0.78 50.60 ±1.28
0.00 8.13 6.74 5.70 4.07 6.51 11.45 0.08
0.00 11.43 7.58 10.12 14.18 11.51 7.31 20.28
text-davinci-003
en𝑑𝑒𝑣 hi id jv kn su sw yo
74.86 50.60 64.21 51.00 50.08 49.67 54.83 50.27
74.86 59.62 66.93 62.17 57.85 58.33 65.33 48.77
0.00 9.02 2.72 11.17 7.76 8.67 10.51 -1.51
0.00 15.24 7.93 12.70 17.02 16.53 9.53 26.10
Table 8: Averaged zero-shot evaluation ± standard deviation of MPLMs (and GPT-3) across five seeds on all seven languages: Hindi (hi), Indonesian (id), Yoruba (yo), Kannada (kn), Sundanese (su), Swahili (sw), Javanese (jv). Additionally, we translate each of these test sets to EN (translate-test). This helps discern the gap in performance due to i) cross-lingual transfer and ii) concept shift in metaphors.. These gaps are calculated using the EN validation set’s performance as a gold reference. Refer to Section 5.1 for more details. The gap that is higher (which indicates a more significant challenge) is highlighted for each model and language. Note that results for Yoruba are not reported for XLM-R, as it was not trained on any Yoruba data.
examine GPT-3 (text-davinci-003) and BLOOM- 176B. As these models are autoregressive rather than masked models, we follow the standard pro- cedure of prediction via choosing the answer with a higher predicted probability (Jiang et al., 2021). The performance of GPT-3 is not very good on most languages when tested zero-shot, but we note that it has a reasonable zero-shot performance on the English development set (74.86%), higher than the reported results of text-davinci-002. (Liu et al., 2022). There is a high concept shift gap as with the other models but also a comparatively higher cross-lingual gap as this model is much stronger in English.
fer: language shift and concept shift. We try to ap- proximate these effects by translating the test set in each language to English. However, this is done with machine translation, so there may be errors. Despite this, translation can still benefit the model if the original language was low-resource. We can divide the model performance into four cases as shown in Table 9.
Translate-EN
. Correct g i r Incorrect O
Correct 53.06% 19.09%
Incorrect 15.52% 12.33%
6 Error Analysis
Table 9: Confusion matrix of examples that were an- swered correctly by XLM-Rlarge before and after trans- lation to English, across all languages combined.
6.1 Effect of English MT
As noted in Section 5.1, there are two major fac- tors that can cause difficulty in cross-lingual trans-
First, there are easy examples (53%) which are answered correctly in both the original language and translated versions. Next there are linguisti-


XLM-R Base
su
Accuracy Change (%)
kn
4
0
sw
24681020304050Number of examples
24681020304050Number of examples
24681020304050Number of examples
hi
jv
id
XLM-R LargeLanguage
2
2
yo
mBERT
Figure 3: Effect of adding up to 50 examples in the target language to the English training data. This strategy is most beneficial for XLM-Rlarge with more than 10 examples in the target language. Exact results can be found in Appendix F.
cally challenging examples (19%) which are orig- inally answered incorrectly, but switch to being answered correctly after being translated to En- glish.11 There are difficult-to-translate or incor- rectly translated examples (15%). It’s likely that these errors can be completely eliminated with a careful enough translation. Lastly, there are hard examples (12%) which are answered incorrectly before and after being translated. These contain many inherently difficult examples, and examples with specific cultural terms. Examples of each type can be found in Appendix G.
6.2 Cultural Examples We examine the accuracy of XLM-Rlarge on the commonsense categories in Section 4.2. Overall, there is a small difference in accuracy between cul- tural examples and the overall accuracy, with over- all accuracy at 63.99% and accuracy on cultural examples at 61.68%. Accuracy for all languages can be found in Appendix H. This is a prelimi- nary analysis, but may indicate that references to explicit named entities may not be the only issue for the model with regard to culture.
7 Related Work
7.1 Figurative Language
English-centric: Most previous inference tasks on figurative language have been in English (Chakrabarty et al., 2022; Liu et al., 2022; Pedinotti et al., 2021a). Further, research on figu- rative language in English centers around training models to detect the presence of metaphors in text (Leong et al., 2020; Stowe and Palmer, 2018;
11Linguistically challenging here means that the language is more challenging for an LM to perform well in, not that the linguistic structure is very difficult.
Tsvetkov et al., 2014). This is done using datasets primarily consisting of idioms and conventional- ized metaphors. However, recognizing common metaphorical phrases may not truly test a model’s ability to interpret figurative language. There is limited research on understanding metaphors, which mostly looks at linking metaphorical phrases to their literal meanings through para- phrase detection (Bizzoni and Lappin, 2018) or generation (Shutova, 2010; Mao et al., 2018). Some studies investigate LMs’ ability to under- stand metaphors, but they do not consider the fact that metaphors have different meanings based on context (Pedinotti et al., 2021b; Aghazadeh et al., 2022). Most recently, Liu et al. (2022) released a dataset which requires a model to infer the correct meaning of metaphor, rather than simply identifying or paraphrasing it, hence calling to test deeper semantic understanding.
Extension to Multilingual: Research in corpus linguistics (Díaz-Vera and Caballero, 2013; Kövec- ses, 2004; Charteris-Black and Ennis, 2001) sug- gests that there significant variation in metaphor- ical language between cultures. There has been some work in detecting metaphors in multilingual text (Tsvetkov et al., 2013; Shutova et al., 2017). These works have focused on three relatively high- resource languages: English, Russian and Span- ish. Both focused on cross-lingual techniques to identify metaphors from newspapers and dictionar- ies. Hence, there hasn’t been any large-scale multi- lingual dataset of figurative language constructed, which would allow one to study cultural variations across metaphors. We fill this gap with the release of our dataset.


8 Conclusion
Despite being relatively widespread, figurative lan- guage is relatively under-studied in NLP. This is es- pecially true for non-English languages. To enable progress on figurative language processing, we cre- ate MABL, a figurative inference dataset across seven languages. We find considerable variation in figurative language use across languages, par- ticularly in the unique objects that people invoke in their comparisons, spanning differences in food, mythology and religion, and famous figures or events. This variation is likely due to differences in cultural common-ground between the countries in which these languages are spoken. We find that multilingual models have considerable room for improvement on this task, and cross-cultural shift may play a significant role in the performance degradation from English. We encourage the NLP community to further examine the role that cul- ture plays in language, and note that figurative lan- guage can be used as a testbed to examine cross- linguistic and cross-cultural variations.
9 Limitations
First, despite our pursuit of attempting to under- stand figurative language use across cultures, we have barely scratched the surface in terms of di- verse representation. Due to limited scope, bud- get, and resources, we collect data from 2-3 anno- tators per language, for seven languages. Further, culture can vary greatly within a language (Hersh- covich et al., 2022). Therefore, until we can repre- sent all of the worlds’ people and their languages, there will always be room for improvement.
We also acknowledge that the syntax captured in the dataset may not be the most diverse, as many examples follow the template “<X> is like <Y>”. However, we create these simpler examples as a first step, since extension to more complex and nat- uralistic language can be included in future work. Second, to analyse concept shift, we machine translate test sets into English. However, these translations can be erroneous to varying degrees, which may have resulted in an over-estimation of error attribution to concept shift. This could not be avoided however, due to limited resources of obtaining human translations.
Third, English may not be the best language to transfer from in zero-shot evaluation of multilin- gual models. While we were constrained by train- ing data availability, past works have shown that
machine-translating train sets can help, an avenue we haven’t explored here. Even though we exper- iment with few-shot evaluation, there may exist an optimal combination of source languages which best transfer to our target languages.
Fourth, the English authors recognized culture- specific terms that were not marked as cultural by annotators in the commonsense categorization across all languages. This may be because anno- tators, being mostly familiar with their own cul- tures, attributed culturally specific facts and terms as being common sense. Likewise, the English- speaking participants may have viewed a separate set of facts as common sense which would not be agreed upon by people from a different culture. It is thus difficult to disentangle common sense and culture in many cases.
References
Ehsan Aghazadeh, Mohsen Fayyaz, and Yadollah Yaghoobzadeh. 2022. Metaphors in pre-trained lan- guage models: Probing and generalization across datasets and languages. In Proceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 2037–2050, Dublin, Ireland. Association for Compu- tational Linguistics.
Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Ra- dityo Eko Prasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022. One country, 700+ lan- guages: NLP challenges for underrepresented lan- guages and dialects in Indonesia. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7226–7249, Dublin, Ireland. Association for Computational Linguistics.
Yuri Bizzoni and Shalom Lappin. 2018. Predicting hu- man metaphor paraphrase judgments with deep neu- ral networks. In Proceedings of the Workshop on Figurative Language Processing, pages 45–55, New Orleans, Louisiana. Association for Computational Linguistics.
Tuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh, and Smaranda Muresan. 2022. Flute: Figurative lan- guage understanding through textual explanations.
Jonathan Charteris-Black and Timothy Ennis. 2001. A comparative study of metaphor in spanish and en- glish financial reporting. English for specific pur- poses, 20(3):249–266.
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark


for information-seeking question answering in typo- logically diverse languages. Transactions of the As- sociation for Computational Linguistics, 8:454–470.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.
Verna Dankers, Elia Bruni, and Dieuwke Hupkes. 2022. The paradox of the compositionality of natural lan- guage: A neural machine translation case study. In Proceedings of the 60th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 4154–4175, Dublin, Ireland. Association for Computational Linguistics.
Arnab Debnath, Navid Rajabi, Fardina Fathmiul Alam, and Antonios Anastasopoulos. 2021. Towards more equitable question answering systems: How much more data do you need? arXiv preprint arXiv:2105.14115.
Tejas Indulal Dhamecha, Rudra Murthy V, Samarth Bharadwaj, Karthik Sankaranarayanan, and Pushpak Bhattacharyya. 2021. Role of language relatedness in multilingual fine-tuning of language models: A case study in indo-aryan languages. arXiv preprint arXiv:2109.10534.
Javier E Díaz-Vera and Rosario Caballero. 2013. Ex- ploring the feeling-emotions continuum across cul- tures: Jealousy in english and spanish. Intercultural Pragmatics, 10(2):265–294.
Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online. Max Planck Institute for Evo- lutionary Anthropology, Leipzig.
Mai ElSherief, Caleb Ziems, David Muchlinski, Vaish- navi Anupindi, Jordyn Seybolt, Munmun De Choud- hury, and Diyi Yang. 2021. Latent hatred: A bench- mark for understanding implicit hate speech. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, pages 345– 363, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Language- Arivazhagan, and Wei Wang. 2020. agnostic bert sentence embedding. arXiv preprint arXiv:2007.01852.
Susan Fussell and Mallie Moss. 2008. Figurative lan-
guage in emotional communication.
Dedre Gentner. 1983. Structure-mapping: A theoret- ical framework for analogy*. Cognitive Science, 7(2):155–170.
Harald Hammarström, Robert Forkel, and Martin Haspelmath. 2022. Glottolog 4.7. Max Planck In- stitute for the Science of Human History.
Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Pi- queras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and strategies in cross- cultural NLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 6997–7013, Dublin, Ireland. Association for Computational Lin- guistics.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Associ- ation for Computational Linguistics, 9:962–977.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the nlp world. arXiv preprint arXiv:2004.09095.
Zoltán Kövecses. 2004.
Introduction: Cultural varia- tion in metaphor. European Journal of English Stud- ies, 8(3):263–274.
G. Lakoff and M. Johnson. 1981. Metaphors we Live
By. University of Chicago Press.
Anne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran Glavaš. 2020. From zero to hero: On the limitations of zero-shot cross-lingual transfer with multilingual arXiv preprint transformers. arXiv:2005.00633.
Chee Wee Leong, Beata Beigman Klebanov, Chris Hamill, Egon Stemle, Rutuja Ubale, and Xianyang Chen. 2020. A report on the 2020 vua and toefl metaphor detection shared task. In Proceedings of the second workshop on figurative language process- ing, pages 18–29.
Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junx- ian He, Zhisong Zhang, Xuezhe Ma, et al. 2019. Choosing transfer languages for cross-lingual learn- ing. arXiv preprint arXiv:1905.12688.
Emmy Liu, Chenxuan Cui, Kenneth Zheng, and Gra- ham Neubig. 2022. Testing the ability of language models to interpret figurative language. In Proceed- ings of the 2022 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4437–4452, Seattle, United States. Association for Computational Linguistics.
Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Visually grounded reasoning Elliott. 2021. across languages and cultures. arXiv preprint arXiv:2109.13238.


Rui Mao, Chenghua Lin, and Frank Guerin. 2018. Word embedding and WordNet based metaphor iden- tification and interpretation. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1222–1231, Melbourne, Australia. Association for Computational Linguistics.
Paolo Pedinotti, Eliana Di Palma, Ludovica Cerini, and Alessandro Lenci. 2021a. A howling success or a working sea? testing what BERT knows about metaphors. In Proceedings of the Fourth Black- boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 192–204, Punta Cana, Dominican Republic. Association for Compu- tational Linguistics.
Paolo Pedinotti, Giulia Rambelli, Emmanuele Cher- soni, Enrico Santus, Alessandro Lenci, and Philippe Blache. 2021b. Did the cat drink the coffee? chal- lenging transformers with generalized event knowl- edge. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Seman- tics, pages 1–11, Online. Association for Computa- tional Linguistics.
Sunil Sharma. 2017. Happiness and metaphors: a perspective from hindi phraseology. Yearbook of Phraseology, 8(1):171–190.
Ekaterina Shutova. 2010. Automatic metaphor interpre- tation as a paraphrasing task. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Com- putational Linguistics, pages 1029–1037, Los Ange- les, California. Association for Computational Lin- guistics.
Ekaterina Shutova. 2011. Computational approaches to
figurative language.
Ekaterina Shutova, Lin Sun, Elkin Darío Gutiérrez, Pa- tricia Lichtenstein, and Srini Narayanan. 2017. Mul- tilingual metaphor processing: Experiments with semi-supervised and unsupervised learning. Compu- tational Linguistics, 43(1):71–123.
Kevin Stowe and Martha Palmer. 2018. Leveraging syntactic constructions for metaphor identification. In Proceedings of the workshop on figurative lan- guage processing, pages 17–26.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, and Chris Dyer. 2014. Metaphor detec- tion with cross-lingual model transfer. In Proceed- ings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 248–258.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh- man. 2013. Cross-lingual metaphor detection using common semantic features. In Proceedings of the First Workshop on Metaphor in NLP, pages 45–51.
Betty van Aken, Julian Risch, Ralf Krestel, and Alexan- der Löser. 2018. Challenges for toxic comment clas- sification: An in-depth error analysis.
A Selected Languages
Table 10 contains additional information on lan- guages included in the dataset. Information on languages was collected from the World Atlas of Language Structures (WALS) and Glottolog 4.7 (Hammarström et al., 2022; Dryer and Haspelmath, 2013).
B Instructions for Annotators
In Liu et al. (2022), workers are prompted with random words taken from English metaphorical frames in Lakoff and Johnson (1981). However, as these metaphorical frames are not readily avail- able in other languages, and we did not want to bias workers toward concepts that are only rel- evant in English, we chose to omit this prompt and have workers generate sentences freely, while encouraging them to emphasize aspects of their culture. Annotators were paid according to their proposed hourly range ($25/hour on average, all above $15/hr). Validators were paid $15/hr. This study was approved by our IRB. No identifying in- formation was collected.
Note that this is the English version of the in- structions, as instructions were machine-translated to each target language and corrected by native speakers.
Your task is to generate pairs of sentences with op- posite or very different meanings, both of which contain metaphors. You can feel free to incorpo- rate creativity into the metaphors, but also make sure that they’re something that could be under- stood by the speakers of the language that you are generating metaphors for, e.g., “this is as classic as pancakes for breakfast” to mean “this is clas- sic” wouldn’t make sense for a culture in which pancakes aren’t traditionally eaten for breakfast.
You can do this by thinking of a metaphor that conveys a certain meaning, and replacing the metaphorical phrase with another metaphorical phrase of the same type (for instance, noun phrases, verb phrases or adjective phrases) that conveys the opposite meaning.
Here are some examples of metaphors to give you an idea of what we’re looking for: Please write both the metaphor and its meaning for each sen- tence.
1. The surgeon is (a lumberjack/a ballet dancer).
2. The movie has the depth of (a wading pool/the grand canyon)
3. Her commitment to the cause was as sturdy as (plywood/oak)
If you’re stuck, a general template you can use is <SUBJECT> is <metaphor 1>/<metaphor 2>.


Language
Branch
Countries
Word Order
Hindi Indonesian Javanese Kannada Sundanese Swahili Yoruba English
Indo-European Austronesian Austronesia Dravidian Austronesian Niger-Congo Niger-Congo Indo-European
India Indonesia Indonesia India Indonesia Tanzania Nigeria, Benin Various
SOV SVO SVO SOV SVO SVO SVO SVO
Table 10: Linguistic characteristics of selected languages.
C Unique Concepts in Different
E Training Details
Languages
Table 11 displays the number of unique concepts and some examples in each language after basic deduplication (lemmatization and casing).
Lang. Unique Concepts
Examples
hi
494
samosa seasonal rain sweet gulkand
id
742
smell of durian young rambutan shinchan
A hyperparameter grid search was conducted over values: epochs ∈ {10, 20, 30}, lr ∈ {2 × 10−4, 5 × 10−4, 2 × 10−5, 5 × 10−5, 2 × 10−6, 5 × 10−6}, and batch size ∈ {32, 64}.
XLM-Rlarge was trained for 20 epochs with a learning rate of 5 × 10−6 and a batch size of 32. XLM-Rlarge was trained for 30 epochs with a learning rate of 2 × 10−5 and a batch size of 64. mBERTbase was trained for 30 epochs with a learn- ing rate of 5 × 10−5 and a batch size of 64. An A6000 GPU was used for each model. Each train- ing run takes on the order of a few minutes.
jv
kn
303
444
elephant riding rickshaw sugar cane tripe skin
dosa ayurveda buddha’s smile
Most seeds lead to a near-random performance on the English dev set, while a small minority of seeds lead to non-random performance. We took the top 5 seeds from 1-100 found in terms of En- glish dev set performance in order to avoid includ- ing results from degenerate seeds.
su
sw
365
481
sticky rice papaya tree lotus flower in water
baobab king solomon clove ointment
We did not experiment with trying to optimize the hyperparameters for the experiments in Sec- tion 5.2.2 but rather used the same ones found pre- viously. This may account for some settings lead- ing to lower performance.
yo
333
president buhari rock of olumu aníkúlápó movie
F Few-shot Full Results
en
954
thanksgiving buffet washington post reporter renaissance artist
Table 13 outlines the effect of adding 𝑘 ∈ {2, ..., 50} examples in each target language.
G Four-Quadrant Examples
Table 11: Number and examples of unique object con- cepts expressed in each language (translated to EN). Unique concepts here are those not shared by any other language in the dataset.
G.0.1 Easy
नद(cid:653) का पानी (cid:607)क्रस्टल क(cid:655) तरह साफ है।/the wa- ter of the river is as clear as crystal
Ia berjalan layaknya siput/he walks like a snail
D Jaccard Similarity between Concepts
Inú yàrá ìdánwò nàá palọ́ lọ́ bí i
Table 12 contains Jaccard similarities for sets of concepts found in each language. Language pairs with the highest similarity (row-wise) are bolded.
Vijana ndio taifa la kesho/youth is the na- tion of tomorrow


hi
id
jv
kn
su
sw
yo
en
hi - id 0.0477 jv 0.0541 kn 0.0945 0.0534 su sw 0.0904 0.0509 yo 0.0631 en
0.0477 - 0.0588 0.0431 0.0405 0.0544 0.0352 0.0425
0.0541 0.0588 - 0.0619 0.067 0.0724 0.0449 0.0377
0.0945 0.0431 0.0619 - 0.0464 0.0842 0.0594 0.0586
0.0534 0.0405 0.067 0.0464 - 0.0563 0.0444 0.0312
0.0904 0.0544 0.0724 0.0842 0.0563 - 0.0671 0.0693
0.0509 0.0352 0.0449 0.0594 0.0444 0.0671 - 0.0311
0.0631 0.0425 0.0377 0.0586 0.0312 0.0693 0.0311 -
Table 12: Jaccard similarities between object sets for each language. The language that is most similar is bolded for each row.
𝑘 = 2
𝑘 = 10
𝑘 = 20
𝑘 = 30
𝑘 = 40
𝑘 = 50
Lang.
Score
Δ
Score
Δ
Score
Δ
Score
Δ
Score
Δ
Score
Δ
e g r a l
R M L X

hi id jv kn su sw yo
67.47 78.01 60.77 58.09 60.47 58.23 -
0.11 -0.08 -0.16 0.01 0.07 0.07 -
67.47 78.04 61.14 58.17 60.55 58.16 -
0.11 -0.05 0.2 0.09 0.15 0 -
67.29 78.22 60.36 59.34 61.36 58.49 -
0.29 0.13 -0.58 1.26 0.96 0.33 -
67.72 77.91 60.78 59.38 60.22 58.88 -
0.14 -0.18 -0.16 1.3 -0.18 0.72 -
67.67 78.04 61.08 60.39 60.35 58.92 -
0.09 -0.05 0.14 2.31 -0.05 0.76 -
67.58 78.56 60.76 61.91 61.28 59.00 -
0 0.47 -0.17 3.83 0.88 0.84 -
e s a b
R M L X

hi id jv kn su sw yo
62.47 69.23 54.09 54.62 51.95 52.78 -
0.01 0.35 0.43 -0.04 -0.46 0.05 -
62.51 69.07 54.31 54.55 51.90 52.76 -
0.03 0.19 0.64 -0.12 -0.51 0.03 -
62.27 69.16 54.04 54.56 51.72 53.00 -
0.21 0.28 0.37 -0.11 -0.69 0.27 -
62.45 69.20 54.53 54.53 51.37 53.04 -
0.03 0.32 0.86 -0.14 -1.03 0.31 -
62.06 68.66 53.92 55.05 51.27 53.50 -
0.42 -0.22 0.25 0.38 -1.14 0.76 -
61.89 69.14 54.60 54.44 50.48 53.83 -
0.59 0.26 0.93 -0.22 -1.93 1.10 -
𝑘 = 2
𝑘 = 4
𝑘 = 6
𝑘 = 8
𝑘 = 10
𝑘 = 50
e s a b
T R E B m
hi id jv kn su sw yo
51.43 56.59 55.13 52.70 52.83 52.12 50.52
0.11 0.02 0.07 0.07 -0.04 0 -0.02
51.41 56.57 55.03 52.67 52.91 52.13 50.50
0.09 0.01 -0.03 0.04 0.04 0.01 -0.10
53.42 56.58 54.93 52.70 52.79 52.14 50.42
2.10 0.01 -0.13 0.07 -0.07 0.02 -0.19
51.50 56.62 55.00 52.66 52.54 52.20 50.31
0.18 0.05 -0.06 0.03 -0.32 0.08 -0.21
51.47 56.59 54.86 52.67 52.68 52.15 50.37
0.15 0.03 0.20 0.04 -0.19 0.03 -0.15
50.93 56.50 54.64 52.42 55.20 51.76 50.35
0.39 -0.07 -0.42 -0.20 2.33 -0.36 -0.17
Table 13: Effect of adding additional examples in the target language to English training data. The highest im- provement is bolded for each language.
Dia menjalani hidup bak singa di kebun bi- natang/he lives life like a lion in the zoo
ಅವರು ನೀಡಿದ್ದ ನೀರು ಸಮುದ್ರದ water ನೀರಿನಂತೆ ಉಪ್ಪಾಗಿತ್ತು/the they gave was as salty as sea water
G.0.2 Challenge - linguistic
Àgbẹ̀ náà pa gbogbo ọmọ tí igi nàá bí lá- nàá/the farmer killed all the children that the tree gave birth to yesterday
G.0.3 Challenge - translation
hirup teh kudu boga kaditu kadieu/life must have here and there
Penzi
lao ni kama moto wa kibatari kwenye upepo/their love is like fire in the wind
लड़क(cid:655) का (cid:557)(cid:604)क्तत्व गुलाब जामुन क(cid:655) तरह मीठा था/the girl’s personality was as sweet as Gu- lab Jamun
Kadang
ipis kulit bengeut/sometimes people can have thin skin
jelema
teh bisa
Ìṣọ̀lá má ń tún ilé rẹ̀
ṣe ní gbogbo nìgbà/honor does not repair his house all the time
Si eta kuliah siga nu teu kantos bobo/that college guy looks like he never sleeps
Nek gawe wedang kopi Painem kaya disoki suruh/if you make a Painem coffee drink, it’s like being told


Bapak tirine sifate kaya Gatot Kaca/his stepfather is like Gatot Kaca
G.0.4 Hard
का(cid:609)लदास भारत के शे(cid:604)ख्चली हैं।/Kalidas is Shekhchili of India
उसके मन का मैल (cid:608)मट(cid:653) क(cid:655) तरह छलनी से (cid:607)नकल गया।/The filth of his mind was removed from the sieve like soil
Wajahku dan adikku ibarat pinang di be- lah dua/My face and my sister are like areca nuts split in half.
Hari ini cuacanya seperti berada di di pun- cak gunung Bromo/Today the weather is like being at the top of Mount Bromo
Doni karo Yanti pancen kaya Rahwana Sinta ing pewayangan/Doni and Yanti are re- ally like Ravana Sinta in a puppet show
H Accuracy on Annotated
Commonsense Categories
Table 14 shows the accuracy on commonsense cat- egories across all languages for XLM-Rlarge. Note that Yoruba is not included due to XLM-Rlarge not being trained on this language.
Language Category
Acc.
hi
obj vis soc cul
67.50 67.48 67.86 70.65
id
obj vis soc cul
76.60 76.56 82.71 77.11
jv
obj vis soc cul
65.02 58.89 64.48 50.82
kn*
obj vis soc cul
57.14 36.36 55.56 77.78
su
obj vis soc cul
57.07 56.86 67.50 61.11
sw
obj vis soc cul
58.06 61.99 56.50 52.46
yo
obj vis soc cul
48.15 52.38 49.58 47.37
Table 14: Performance of XLM-Rlarge on commonsense categories indicated by annotators.12