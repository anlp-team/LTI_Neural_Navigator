3 2 0 2
p e S 7 2
] L C . s c [
1 v 0 0 8 5 1 . 9 0 3 2 : v i X r a
EXPLORING SPEECH RECOGNITION, TRANSLATION, AND UNDERSTANDING WITH DISCRETE SPEECH UNITS: A COMPARATIVE STUDY
Xuankai Chang1, Brian Yan1, Kwanghee Choi1, Jeeweon Jung1, Yichen Lu1, Soumi Maiti1, Roshan Sharma1, Jiatong Shi1, Jinchuan Tian1, Shinji Watanabe1†, Yuya Fujita2, Takashi Maekaku2, Pengcheng Guo3, Yao-Fei Cheng4, Pavel Denisov5, Kohei Saijo6, Hsiu-Hsuan Wang7∗
1Carnegie Mellon University, 2Yahoo Japan Corporation, 3Northwestern Polytechnical University, 4University of Washington,5University of Stuttgart, 6Waseda University,7National Taiwan University
ABSTRACT
Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of dis- crete speech units derived from self-supervised learning representa- tions, which significantly compresses the size of speech data. Apply- ing various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech pro- cessing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.
Fig. 1: Illustration of the E2E speech processing model with discrete speech units. The speech discretization process is shown on the left. On the right side is a Seq2Seq model that takes discrete units to output target text.
Index Terms— Discrete units, end-to-end, speech recognition,
speech translation, spoken language understanding
1. INTRODUCTION
Significant progress has been made in the field of automatic speech recognition (ASR) over the past few decades, largely attributed to the evolution of deep neural networks [1, 2]. Since the emergence of end-to-end (E2E) ASR models [3–5], there have been many exciting outcomes. Among these achievements, a slew of potent architec- tures [6–9] has boosted the performance of speech tasks, including ASR, speech translation (ST), and spoken language understanding (SLU). Also, novel training paradigms have demonstrated improved performance and generalization, including self-supervised learning (SSL) models [10–14] and Whisper [15]. In the majority of prior endeavors, high-dimensional features are derived from raw wave- forms as the input. Conventionally, spectral speech features are ex- tracted from a fixed-length temporal window, such as Mel Frequency Cepstral Coefficients (MFCC) or log Mel filter banks (FBANK). Re- cently, data-driven methods become popular to learn feature extrac- tion using neural networks [12, 13, 16]. However, in most cases, the data storage and transmission efficiency are similar among raw waveforms and speech features [17]. It is not trivial to improve the efficiency of computation without performance degradation.
Recently, a few studies have proposed the use of discrete speech units to represent speech signals, where the information of speech signals in a short window is represented by a single token or a few levels from restricted vocabulary [11, 17, 18], as opposed to employing high-dimensional continuous speech features within the ASR task. For instance, in [17], researchers propose to utilize clus- tering indices derived from features of SSL models as input. This approach condenses the information considerably, reducing the orig- inal 1024-sized float vector to a mere 12-bit binary number: over 3000 times less. Such a compression process substantially reduces data storage and transmission size while maintaining predictive per- formance comparable to conventional high-dimensional features. Moreover, input sequence lengths can be drastically reduced via de-duplication and subword modeling on discrete units, leading to more than 2X faster training and inference. Notably, discrete unit representations can be regarded as a spoken language similar to text data in NLP tasks, which is more straightforward to unify the tasks or models. Being an alternative to traditional speech representation, this paradigm has already been applied in other domains, such as speech translation [19, 20] and audio generation [21–25].
To provide an extensive guide for discrete speech units, we con- ducted a comprehensive exploration into the effectiveness of discrete speech units across various speech processing tasks. We summarize our experiments and findings as follows:
A comparative analysis is conducted under fair conditions, eval- uating the performance and training time reduction using discrete speech units as opposed to traditional speech features.
∗Authors are ordered by the organizations. †Corresponding author.
A diverse range of benchmarks, including 12 ASR (Section 3.2),


3 ST (Section 3.3), and 1 SLU (Section 3.4) corpora, are mostly evaluated for the first time.
To demonstrate wide applicability of the discrete units, we adopted noisy speech, spontaneous speech, telephony speech, and several multi-lingual speech corpora, which would be the first work to explore these aspects (Section 3.2.1).
We show the versatility of discrete units in various E2E frame- works, including connectionist temporal classification (CTC) [3], attention-based encoder-decoder (AED) [5], and RNN-Transducer [4] (Table 3).
We share various tips based on our investigations to get better performance, including SSL feature choice and discretization. Selecting SSL features based on canonical correlation analysis (CCA) [26] improves performance significantly compared to prior work [17] (Section 3.1).
We also explore other possible choices of discrete units, includ- ing clustering SSL [14] or supervised representations, or vector quantization of neural codec models [27] (Section 3.2.5).
We will release fully reproducible recipes and trained models on ESPnet [28], which can significantly benefit the community.
2. SPEECH PROCESSING WITH DISCRETE TOKENS
This section elaborates on the details of our speech processing mod- els, which take discrete units as input. Figure 1 summarizes the whole pipeline. Leveraging the sequence-to-sequence (Seq2Seq) paradigm as our backbone framework enables broad applicability across a spectrum of tasks involving the transformation of speech signals into diverse target outputs.
2.1. Speech Discretization
The discretization process is the pivotal step, which transforms the speech signals into discrete representations. Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10–12], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds:
1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models.
2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes.
3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models.
2.2. Data Manipulations
The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete
units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]:
1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy.
2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training.
2.3. Seq2Seq Modeling with Convolutional Subsampling
We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted.
3. EXPERIMENTS
3.1. General Setup
Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works.
Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance.
The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the
choice of features.


Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework.
Dataset
Language Metric
Evaluation Sets
Discrete Tokenization K-means clusters BPE size
FBank
Results Discrete Units
SSL (top line)
AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3
CH EN FR EN EN EN EN 143 EN EN EN EN
CER WER WER WER WER WER WER CER WER WER WER WER
dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test
2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000
6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000
4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8
4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9
3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8
Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication → subword modeling → convolutional subsampling.
Dataset
Disc unit
+De-dup
Manipulations
+BPE
length reduction ratio
Conv-Sub
Type
length
BPE-ASR Trans
AISHELL CHiME4 CommonVoice (FR) Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) MuST-C SPGIspeech SWBD TEDLIUM3
227 383 236 217 297 639 619 336 321 458 269 306
172 260 177 150 207 455 435 224 229 307 182 208
98 178 153 90 127 257 247 183 122 159 144 143
57% 54% 65% 59% 57% 60% 60% 46% 62% 65% 46% 53%
Conv1d2 Conv1d1 Linear Conv1d2 Conv1d2 Conv1d2 Conv1d2 Conv1d2 Linear Conv1d2 Conv1d2 Conv1d2
49 178 153 45 63 128 123 92 122 79 72 72
14 106 42 16 25 43 41 66 25 30 20 36
WavLM large model was adopted, guided by the learned weights from a pre-trained ASR model. In this paper, we draw inspiration from a recent study [26] using canonical correlation analysis (CCA), assessing the similarity between the layer representation and the word labels. For example, both HuBERT and WavLM share similar patterns in their analyses due to their similar training objectives. For monolingual experiments, we choose WavLM large for all English corpora 1, Chinese HuBERT large for Chinese corpus2, and XLSR- 533 for French corpus. Consequently, we choose layer 21 from large models, layer 9 from the base model and layer 11 from the XLSR-53 model, as they exhibit the highest CCA similarities with word labels. For multilingual experiments on ML-SUPERB (143 languages), we use XLS-R-1b4 and its layer 35 from empirical resynthesis analysis discussed in [31]. The number of K-Means clusters and the subword modeling vocabulary size are empirically selected considering the data variations and the length reduction balance.
speech (Gigaspeech [35], TEDLIUM3 [36], How2 [37], SPGIs- peech [38], MuST-C [39]), and non-English speech (AISHELL [40], CommonVoice [41], ML-SUPERB [42]).
3.2.2. Results
Table 1 provides a summary of the ASR results obtained using joint CTC/AED models. We compare the peroformance of the ASR model using discrete units against conventionals ones using the FBank and SSL features. Note that the same SSL representations are used to yield the discrete units. We downsample both the FBank and SSL representations so that the frameshift is equivalent to 40ms. Results reveal that the discrete units show commendable per- formance. In most instances, the performance falls between FBank and SSL input, but it tends to align more closely with SSL input. Comparing with the previous study [17], using the 21st layer of the WavLM large model results in better performance on LibriSpeech, which further shows the importance of the discretization process.
3.2. Automatic Speech Recognition
3.2.1. Datasets
3.2.3. Efficiency of Discrete Units
To assess the efficacy of discrete speech units, we employed a diverse set of speech corpora, encompassing a wide range of acoustic char- acteristics: read English speech (LibriSpeech [32]), noisy speech telephony speech (SWBD [34]), spontaneous (CHiME4 [33]),
1https://huggingface.co/microsoft/wavlm-large 2https://huggingface.co/TencentGameMate/
chinese-hubert-large
3https://dl.fbaipublicfiles.com/fairseq/wav2vec/
xlsr_53_56k.pt
4https://huggingface.co/facebook/
We summarize the sequence length information in the Table 2. One benefit of using discrete units is that it can improve the training effi- ciency via the sequence length reduction. We can see that the length of the discrete unit after subword modeling is less than half of the SSL feature sequence. Thus, the training efficiency can be dramat- ically improved because of larger batch size and less IO overhead. In most of the discrete unit experiments, convolutional subsampling reduces input sequence length by half. However in CHiME4, halv- ing the input sequence would violate the CTC assumption: the input sequence must not be shorter than the output sequence. For the Com- monVoice and MuST-C, a linear layer leads to better convergence.
wav2vec2-xls-r-1b


Fig. 2: Illustration of training time per epoch using FBank / online SSL represetation / discrete units. We normalize the training time of discrete units to unit 1 for convenience.
Fig. 2 shows the training time for one epoch on several corpora, It can be seen using FBank / online SSL feature / discrete units. that the training time using discrete units is less than 50% of that of using FBank. It’s worth noting that online SSL feature extraction is computationally intensive but friendly to storage. Additionally, more aggressive subsampling (e.g. Conv1d3) can further improve the training speed without performance degradation in some cases5.
Table 3: CER or WER (%) of CTC and RNN-Transducer models.
Dataset
FBank
Discrete Units
SSL (top line)
CTC
AISHELL LibriSpeech TEDLIUM3
5.8 / 6.2 3.9 / 9.8 / 4.0 / 9.7 10.2 / 10.3
5.3 / 5.5 2.7 / 5.4 / 2.9 / 5.5 8.8 / 9.6
3.9 / 4.2 2.3 / 4.5 / 2.3 / 4.6 8.4 / 10.0
RNN-Transducer
AISHELL LibriSpeech TEDLIUM3
5.6 / 6.0 2.5 / 6.2 / 2.7 / 6.2 7.7 / 7.6
5.7 / 6.0 2.2 / 4.6 / 2.4 / 4.5 6.7 / 6.9
4.0 / 4.3 2.0 / 4.1 / 2.1 / 4.3 6.4 / 7.1
3.2.4. ASR with Different Seq2Seq Models
We further verified the efficacy on other alternative Seq2Seq models, namely, CTC and RNN-Transducer. Similar trends are found: using discrete units achieves the performance in between the FBank and SSL features, as shown in Table 3.
Table 4: LibriSpeech WER (%) of different discrete units choices.
Dataset
SSL
OWSM
EnCodec
LibriSpeech
2.2 / 4.5 / 2.4 / 4.6
3.9 / 10.1 / 4.2 / 10.2
3.2 / 8.5 / 3.0 / 8.5
3.2.5. ASR with Different Types of Discrete Units
So far, SSL representations are used to derive discrete units, as they were known to be very robust in different tasks [12, 14]. However, other large models and discrete unit approaches are recently emerg- ing, such as Whisper [15] and neural codec models [27, 30]. We adopt a Whisper-like model trained by ESPnet 6 and the publicly available EnCodec [27] model. For EnCodec, we use all 8-level discrete tokens at each frame, summing up embeddings of 8-levels per frame. The embedding layer is initialized from the pre-trained codebook and frozen, followed by a layer normalization for stabil- ity. Subword modeling cannot be applied in this case. Table 4 shows our preliminary results. However, we observe that the SSL-based
5https://github.com/espnet/espnet/tree/master/
egs2/librispeech/asr2
6https://github.com/espnet/espnet/pull/5120
Table 5: BLEU scores of speech translation tasks.
Dataset
FBank Discrete tokens
SSL (top line)
MuST-C En-De MuST-C En-Es MuST-C En-Fr
26.7 31.2 37.1
28.6 33.0 38.7
29.7 33.7 40.2
Table 6: Intent classification accuracy (%) of SLURP dataset.
Dataset
FBank
Discrete tokens
SSL
SLURP
86.9 / 86.3
81.8 / 80.8
84.2 / 83.3
method still has the upper hand over the others. Nevertheless, we aim to continue exploring various settings in our future work.
3.3. Speech Translation Results
Table 5 shows the ST results; these models use CTC / attention fol- lowing [43, 44]. Similar to the ASR experiments in Section 3.2, the performance of ST using discrete units is slightly worse than that using the SSL features but better than FBank.
3.4. Spoken Language Understanding Results
We conducted experiments on an SLU task using the SLURP [45] dataset. The intent classification accuracy is presented in Table 6. Unlike the other experiments, in this preliminary result, we did not observe an improvement in both SSL continuous features and dis- crete tokens. We hypothesize that the 21st layer of the WavLM large selected following the ASR experiments may not be optimal for SLU purposes due to the dependency of the optimal layer on the down- stream task, as reported in [12, 26].
4. CONCLUSION
This paper explores the efficacy of incorporating discrete speech units as inputs across a spectrum of speech processing tasks, en- compassing ASR, ST, and SLU. To evaluate the versatility of discrete units in diverse scenarios, we conducted experiments on datasets with varying characteristics. Drawing inspiration from canonical correlation analysis (CCA), we improved our choice of self-supervised learning (SSL) features, resulting in a noticeable performance enhancement. Consequently, the utilization of discrete units not only outperforms FBank features but also substantially en- hances efficiency. These findings underscore the promise of discrete unit input in speech processing. Future research avenues could delve into investigating alternative discretization techniques.
5. ACKNOWLEDGEMENTS
Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Sci- ence Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research.


6. REFERENCES
[1] G. Hinton et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal processing magazine, vol. 29, no. 6, pp. 82–97, 2012.
[2] Y. Qian et al., “Very deep convolutional neural networks for noise ro- bust speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263–2276, 2016.
[3] A. Graves et al., “Connectionist temporal classification: Labelling un- segmented sequence data with recurrent neural networks,” in Pro- ceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.
[4] A. Graves, “Sequence transduction with recurrent neural networks,”
arXiv preprint arXiv:1211.3711, 2012.
[5]
J. Chorowski et al., “Attention-based models for speech recogni- tion,” Advances in Neural Information Processing Systems, vol. 2015, pp. 577–585, 2015.
[6] A. Vaswani et al., “Attention is all you need,” in Proc. NeurIPS, 2017,
pp. 5998–6008.
[7] A. Gulati et al., “Conformer: Convolution-augmented Transformer
for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040.
[8]
P. Guo et al., “Recent developments on espnet toolkit boosted by con- former,” in Proc. ICASSP, 2021, pp. 5874–5878.
[9] K. Kim et al., “E-branchformer: Branchformer with enhanced merg-
ing for speech recognition,” in Proc. SLT, 2023, pp. 84–91.
[10]
S. Schneider et al., “Wav2vec: Unsupervised pre-training for speech recognition,” Proc. Interspeech 2019, pp. 3465–3469, 2019.
[11] A. Baevski, S. Schneider, and M. Auli, “Vq-wav2vec: Self-supervised
learning of discrete speech representations,” in Proc. ICLR, 2019.
[12] A. Baevski et al., “Wav2vec 2.0: A framework for self-supervised learning of speech representations,” Advances in neural information processing systems, vol. 33, pp. 12 449–12 460, 2020.
[13] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transac- tions on Audio, Speech, and Language Processing, vol. 29, pp. 3451– 3460, 2021.
[14]
S. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Sig- nal Processing, vol. 16, no. 6, pp. 1505–1518, 2022.
[15] A. Radford et al., “Robust speech recognition via large-scale weak
supervision,” in Proc. ICML, 2023, pp. 28 492–28 518.
[16]
T. N. Sainath et al., “Learning the speech front-end with raw wave- form CLDNNs,” Learning, 2015.
[17] X. Chang et al., “Exploration of efficient end-to-end asr using discretized input from self-supervised learning,” arXiv preprint arXiv:2305.18108, 2023.
[18] A. Baevski and A. Mohamed, “Effectiveness of self-supervised pre-
training for asr,” in Proc. ICASSP, 2020, pp. 7694–7698.
[19] D. Zhang et al., “Dub: Discrete unit back-translation for speech trans-
lation,” arXiv preprint arXiv:2305.11411, 2023.
[20] M. Kim et al., “Many-to-many spoken language translation via uni- fied speech and text representation learning with unit-to-unit transla- tion,” arXiv preprint arXiv:2308.01831, 2023.
[21]
T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine translation problem,” arXiv preprint arXiv:2005.05525, 2020.
[22]
J. Shi et al., “Discretization and re-synthesis: An alternative method to solve the cocktail party problem,” arXiv preprint arXiv:2112.09382, 2021.
[23]
Z. Borsos et al., “Audiolm: A language modeling approach to au- dio generation,” IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2023.
[24] C. Wang et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023.
[25]
P. K. Rubenstein et al., “Audiopalm: A large language model that can speak and listen,” arXiv preprint arXiv:2306.12925, 2023.
[26] A. Pasad, B. Shi, and K. Livescu, “Comparative layer-wise analysis
of self-supervised speech models,” in Proc. ICASSP, 2023, pp. 1–5.
[27] A. D´efossez et al., “High fidelity neural audio compression,” arXiv
preprint arXiv:2210.13438, 2022.
[28]
S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211.
[29] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representa- tion learning,” Advances in neural information processing systems, vol. 30, 2017.
[30] N. Zeghidour et al., “Soundstream: An end-to-end neural audio codec,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495–507, 2021.
[31]
L. Barrault et al., “Seamlessm4t-massively multilingual & multi- modal machine translation,” arXiv preprint arXiv:2308.11596, 2023.
[32] V. Panayotov et al., “Librispeech: An asr corpus based on public do- main audio books,” in Proc. ICASSP, 2015, pp. 5206–5210.
[33]
E. Vincent et al., “The 4th chime speech separation and recognition challenge,” URL: http://spandh. dcs. shef. ac. uk/chime challenge Last Accessed on 1 August, 2018, 2016.
[34]
J. J. Godfrey, E. C. Holliman, and J. McDaniel, “Switchboard: Tele- phone speech corpus for research and development,” in Acoustics, speech, and signal processing, ieee international conference on, vol. 1, 1992, pp. 517–520.
[35] G. Chen et al., “Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,” in 22nd Annual Conference of the International Speech Communication Association, INTER- SPEECH 2021, 2021, pp. 4376–4380.
[36]
F. Hernandez et al., “Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,” in Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18–22, 2018, Proceedings 20, 2018, pp. 198– 208.
[37] R. Sanabria et al., “How2: A large-scale dataset for multimodal lan-
guage understanding,” in NeurIPS, 2018.
[38]
P. K. O’Neill et al., “Spgispeech: 5,000 hours of transcribed finan- cial audio for fully formatted end-to-end speech recognition,” in 22nd Annual Conference of the International Speech Communication Asso- ciation, INTERSPEECH 2021, 2021, pp. 1081–1085.
[39] M. A. Di Gangi et al., “Must-c: A multilingual speech translation
corpus,” in Proc. NAACL, 2019, pp. 2012–2017.
[40] H. Bu et al., “Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,” in 2017 20th conference of the ori- ental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), 2017, pp. 1–5.
[41] R. Ardila et al., “Common voice: A massively-multilingual speech corpus,” English, in Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020, pp. 4218–4222.
[42]
J. Shi et al., “ML-SUPERB: Multilingual Speech Universal PERfor- mance Benchmark,” in Proc. INTERSPEECH 2023, 2023, pp. 884– 888.
[43] B. Yan et al., “Ctc alignments improve autoregressive translation,” in
Proc. EACL, 2023.
[44] B. Yan et al., “ESPnet-ST-v2: Multipurpose spoken language transla-
tion toolkit,” in Proc. ACL, 2023.
[45]
E. Bastianelli et al., “Slurp: A spoken language understanding re- source package,” in Proc. EMNLP, 2020, pp. 7252–7262.