Daphne Ippolito25646044
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', '2023', ['International Conference on Natural Language Generation', 'Int Conf Nat Lang Gener', 'INLG'], 'Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).', ['Daphne Ippolito', 'Nicholas Carlini', 'Katherine Lee', 'Milad Nasr', 'Yun William Yu']]
["A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity", '2023', ['arXiv.org', 'ArXiv'], 'Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.', ['S. Longpre', 'Gregory Yauney', 'Emily Reif', 'Katherine Lee', 'Adam Roberts', 'Barret Zoph', 'Denny Zhou', 'Jason Wei', 'Kevin Robinson', 'David M. Mimno', 'Daphne Ippolito']]
['Extracting Training Data from Diffusion Models', '2023', ['USENIX Security Symposium', 'USENIX Secur Symp'], 'Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.', ['Nicholas Carlini', 'Jamie Hayes', 'Milad Nasr', 'Matthew Jagielski', 'Vikash Sehwag', 'Florian Tramèr', 'B. Balle', 'Daphne Ippolito', 'Eric Wallace']]
['Are aligned neural networks adversarially aligned?', '2023', ['arXiv.org', 'ArXiv'], 'Large language models are now tuned to align with the goals of their creators, namely to be"helpful and harmless."These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.', ['Nicholas Carlini', 'Milad Nasr', 'Christopher A. Choquette-Choo', 'Matthew Jagielski', 'Irena Gao', 'Anas Awadalla', 'Pang Wei Koh', 'Daphne Ippolito', 'Katherine Lee', 'Florian Tramèr', 'Ludwig Schmidt']]
