3 2 0 2
v o N 9
] L C . s c [
2 v 0 0 9 6 1 . 6 0 3 2 : v i X r a
Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Ji-Ung Lee1,2, Haritz Puerto1,2, Betty van Aken3, Yuki Arase4, Jessica Zosa Forde5, Leon Derczynski6,7, Andreas R√ºckl√©10,‚Ä°, Iryna Gurevych1,2, Roy Schwartz8, Emma Strubell9,11, Jesse Dodge11 1Technical University of Darmstadt, 2Hessian AI, 3Berliner Hochschule f√ºr Technik, 4Osaka University, 5Brown University, 6University of Washington, 7IT University of Copenhagen, 8The Hebrew University of Jerusalem, 9Carnegie Mellon University, 10Amazon, 11Allen Institute for AI
Abstract
Many recent improvements in NLP stem from the development and use of large pre- trained language models (PLMs) with bil- lions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such mod- els; and has raised severe concerns about the sustainability, reproducibility, and inclu- siveness for researching PLMs. These con- cerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate In this work, we provide a first at- them. tempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture exist- ing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer re- viewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which al- ready successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.
100
Actual distribution Equal distribution
U P G %
50
0
0
50 % Participants
Figure 1: Distribution of available GPUs across our participants (in %). As can be seen, 87.8% of our survey participants have access to less than 10% of the total number of GPUs.
et al., 2020; Arase et al., 2021) and the AI commu- nity (Patterson et al., 2022; Wu et al., 2022): (1) Understanding and mitigating the environmental cost of NLP research and use, in terms of green- house gas (GHG) emissions, and (2) equity of ac- cess: the extent to which increasing computational requirements restricts who has access to develop and use modern NLP.
100
1
Introduction
Recent advances in hardware and algorithms have transformed the field of NLP. Whereas NLP practi- tioners and researchers used to be able to develop and use cutting-edge NLP technology on relatively affordable hardware such as a laptop or a commod- ity server, modern state-of-the-art approaches have evolved to require more substantial computational power, typically achieved by specialized tensor pro- cessing hardware such as a GPU or TPU. This shift has raised at least two concerns among members of the NLP community (Strubell et al., 2019; Schwartz
In response to these concerns, we formed a work- ing group within ACL with the goal of better under- standing the challenges surrounding efficient NLP and establishing policies to address them. In order to quantify views and impacts in the NLP com- munity related to these concerns, we conducted a survey of the ACL community in July 2021, the results of which we report here. Besides concerns about the (1) environmental impact and (2) equity, we further solicit answers about their (3) impact on the whole peer reviewing process, as this is an important matter for inclusiveness. Overall, we elicited 312 responses from a distributed range of junior and senior researchers hailing from industry and academia. Some of our key findings include:
‚Ä°This work does not relate to AR‚Äôs position at Amazon.
More than 50% of the survey participants are moderately or very concerned about the envi-


ronmental footprint of NLP research; mostly with respect to training and model selection.
Overall, ‚àº62% of our respondents have access to less than eight GPUs and moreover, over 90% have access to less than 10% of the total GPU power (Fig. 1). As a frame of reference, recent work (Izsak et al., 2021) showed that a clever set of techniques can be used to train BERT in 24 hours on 8 GPUs, and it takes about 7 minutes to fine-tune a RoBERTa LARGE model on the MNLI natural language inference dataset (about 400k training sentences) on one GPU (GTX 2080 Ti) to an accuracy of 85% (Zhou et al., 2021).
A majority (76%) our respondents believe that it would be beneficial to have smaller versions of pre-trained models released together with larger ones. In fact, 33% of our free-text re- spondents emphasised the importance of shar- ing artifacts (such as code, models, training logs, etc.).
The group that suffers most from lack of re- sources are students, who struggle to repro- duce previous results when compared to re- searchers from large industry.
While we find disparities between different groups‚Äîespecially regarding the job sector‚Äî our analysis shows that most of them are not statistically significant. Instead, we find out- liers across all groups showing that there exist disparities within. We find no evidence in our survey responses that ‚Äúindustry‚Äù has ac- cess to significantly more compute power than ‚Äúacademia‚Äù. Instead, this mostly seems to be the case for very few extreme outliers (6%).
With this survey, we hope to provide a more solid foundation to back-up the ongoing discussion in the community and for devising concrete actions to make research more inclusive.
2 Survey Description
The survey was open over a period of 17 days, from Monday, July 12, 2021 to Thursday, July 29, 2021. It was conducted via Microsoft Forms and distributed across the *CL community by mass mailing to ACL membership, and shared on Twitter. During that time, we collected 312 responses. The creation of the survey indicated that, ‚Äúinput will
remain anonymous and the responses will also be summarized in aggregate form‚Äù.1 Therefore the data will be made available on request with a state- ment of intended purpose, due to privacy and ethi- cal restrictions.
2.1 Questionnaire The questions were divided into four categories. First, we collected some general information about our participants, like their current position and se- niority (¬ß2.2). Second, we asked our participants about their concerns regarding the environmental impact of NLP experiments (¬ß3) and their access to computational resources (¬ß4). Finally, we asked about the impact of compute-intensive experiments on the reviewing process as well as about specific measurements to alleviate them (¬ß5).
To keep a low-effort for our participants, we crafted most of the (ùëÑ)uestions as simple yes/no/unsure questions. For subjects that require a more fine-grained analysis (e.g., environmental concerns) we used five point scales (either numeric or text-based). Overall, we asked a total of 19 ques- tions from which 15 were multiple-choice questions (13 with a single answer possibility and two with multiple possible answers). ùëÑ4 (available compute resources) and ùëÑ11 (number of times reviewers asked for expensive experiments) required a nu- meric answer. Finally, ùëÑ9, ùëÑ18, and ùëÑ19 allowed free text answers. Participants were asked to pro- vide answers to 13 questions, while six questions (ùëÑ4, ùëÑ9, ùëÑ11, ùëÑ12, ùëÑ14, ùëÑ19) were optional. All questions are provided in Table 1.
2.2 Demographic Overview In our first three questions, we asked the partic- ipants about their seniority, job sector, and geo- graphic location (Fig. 2).
Seniority. We asked our participants about the number of active years in the *CL community as an author, reviewer, or in a related role (ùëÑ1). Overall, a little over half (53.5%) indicated they were junior members of the community, while the remainder were fairly evenly split across mid- and late-career.
Job Sector. We further asked our participants about the current position they are holding (ùëÑ2). Possible responses were student, academic post-doc (Aca. PD), researcher from small (s) and large (l)
1https://www.aclweb.org/portal/
content/efficient-nlp-survey


Demographics Q1. Years active. How many years have you been active in the ACL community (as an author/reviewer/area chair/etc.)? Answer: [1-5], [6-10], [11-15], [16+]. Q2. Current Role. Answer: Student, Academic Postdoc, Academic PI, Researcher in large industry, Research in small industry, other. Q3. Geographic Location. Answer: Americas, Europe/Middle East, Africa, Asia/Oceania.
Equity Q4. Available compute resources. Please provide a rough estimate of the average number of GPUs or equivalent accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs / managers). If you cannot quantify the amount of compute resources, leave this field empty. Answer: Numeric response (optional). Q5. Unable to run experiments. In the last year, have you been unable to run experiments important for one of your projects due to lack of computational resources? Answer: Yes, No, Unsure. Q6. More resources would make your work more valuable. How often do you feel like your work would have been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more computational resources? Answer: Five point scale.
Environmental Concern Q7. Concern about environmental footprint. How concerned are you by the environmental footprint of the field of NLP? Answer: Five point scale. Q8. Most pressing factor. Which of the following do you feel is the most pressing factor with respect to the environmental impact of NLP? Answer: Choose all that apply: Training, Inference, Model selection, None, Other). Q9. Why? Optionally explain the reasons for your choices above. Answer: Free text (optional).
Reviewing Process Q10. Did reviewers ask for too expensive experiments? In the past 3 years, have you received feedback from reviewers who requested experiments that were too expensive for your budget for a particular paper? Answer: Yes, No, Not sure. Q11. If yes, how many times? Q12. Was the critique justified? If yes, do you feel the critique was justified? I.e., that the main scientific claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by the original, smaller-budget experiments? Answer: Yes, No, Not sure. Q13. Lack of resources prevents reproduction of previous results. How often do you find yourself unsuccessful in reproducing a previous result due to lack of computational resources? Answer: Never, Rarely, Sometimes, Often, Always. Q14. Efficiency Track. If you have work on efficient methods and/or enhanced reporting, would you consider submitting it to a dedicated track? Answer: Yes, No, N/A. Q15. Justify allocation of budget for experiments. As a reader, would you prefer authors to be requested to justify the way they allocate their budget to run experiments which adequately support their scientific claims? Answer: Yes, No, Not sure. Q16. Reviewers should justify the petition for additional experiments. As an author, would you prefer it if reviewers took up space in their review to justify their suggestions for additional experiments in terms of the evidence that those additional experiments would provide? I.e., what is currently missing in terms of lack of evidence to support the main claims of the paper, and how the additional experiments would provide evidence for the paper‚Äôs research questions? Answer: Yes, No, Not sure. Q17. Releasing small versions of pretrained models. Would your work benefit from smaller versions of pretrained models released alongside larger ones? Answer: Yes, No, Not sure. Q18. How to encourage the release of models. Which of these solutions would you endorse for encouraging the release of trained models? Answer: Choose all that apply: Best artifact award, Instruct reviewers to reward papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the above, Other)
Q19. Any other thoughts or suggestions? Answer: Free text (optional).
Table 1: List of questions in the survey. Summaries of the questions in bold. Only full questions were shown to the participants.


industries (Ind.), and academic PI (Aca. PI). The largest group of participants were students (38.5%), followed by academic postdocs and PIs (34.3%), and industry researchers (24.7%). Eight partici- pants (2.5%) responded with ‚Äúother‚Äù from which seven were affiliated with academia (e.g., lecturers) and one with industry (consultant). For the fine- grained analysis, we merge each response of ‚Äúother‚Äù into the most fitting group in the survey (one stu- dent, five academic PIs, one academic post-doc, and one small industry researcher). For our analysis, we do not merge the academic and industry sub- groups, as this may obfuscate existing disparities; e.g., between small and large industry.
Geographic location. We further asked respon- dents to share their geographic location (ùëÑ3). Over- all, 45.8% of responses came from the Americas (AM), 40.4% from Europe (EU) and the Middle East (ME), and 13.8% from Asia (AS) and Australia (Aus). We received no responses from researchers in Africa (AF). The heavily skewed responses in terms of geographic location limits the expressive- ness of this factor and thus, will not be considered for our analysis.
2.3 Methodology
In the following sections, we analyse and discuss the participants‚Äô responses with respect to the re- maining three categories (environmental concerns, equity, and impact on the reviewing process). For each section, we first provide an overview of the distribution in the responses and then provide a fine- grained analysis with respect to the seniority and job sector. The goal of the fine-grained analysis is to investigate if we can observe any statistically significant differences across different groups.
Statistical tests. Due to the explorative nature of our survey, the collected data violates the necessary conditions on homoscedasticity (Levene, 1960) and normality (Shaphiro and Wilk, 1965) that are re- quired to conduct an analysis of variances (ANOVA, Fisher 1921). Instead, we perform a Kruskal-Wallis test (Kruskal and Wallis, 1952) as an indicator for any statistically significant differences2 and if so, perform pairwise Welch‚Äôs t-tests (Welch, 1951) against a Bonferroni corrected ùõº = 0.05 where ùëö ùëö is the number of pairwise comparisons; i.e., for ùëõ
2This is the case when ùêª > ùêª ùëõ 0
for ùëõ 0 = 9.488 (job sector) and
with ùêª ùëõ
0 ‚àº ùúí 2
ùëõ‚àí1
groups. For ùõº = 0.05, we get ùêª 5 ùêª 4
0 = 7.815 (seniority) (Abramowitz, 1974).
groups, ùëö = ùëõ‚ãÖ(ùëõ‚àí1) (Bonferroni, 1936). This re- sults in corrected ùõº = 0.0083 for seniority with ùëõ = 4 and ùõº = 0.005 for the job sector with ùëõ = 5 (not merging academia and industry sectors). For the numerical questions (ùëÑ4 and ùëÑ11), we further analyze if there exist disparities within each group, using interquartile ranges with ùëò = 1.5 to detect outliers (Tukey, 1977).
2
3 Environmental Footprint
We quantified existing concerns about the environ- mental impact of NLP experiments using a five point Likert scale (ùëÑ7) and asked our participants to select the most pressing issue in the typical life cycle of an NLP model (ùëÑ8) between (Train)ing, model (Select)tion, and (Infer)ence. Participants were allowed to select all applicable answers and could select (None) or provide (Other) pressing is- sues. They could also provide a textual justification of their answer(s) (ùëÑ9).
3.1 Analysis Figure 3a shows that more than 50% of our partici- pants were moderately (28.2%) or very (27.9%) con- cerned about the environmental footprint of NLP, while around 33% of them were slightly (14.7%) or somewhat (18.6%) concerned. 10.6% of par- ticipants were not concerned at all. Our partic- ipants further agreed that training (75.3%) and model selection (59.9%) are the most pressing is- sues (Fig. 3b).3 Inference took third place with 20.2%, while 6.1% of our participants selected none. The smallest number responses was given for other with hyperparameter tuning and travelling (6 men- tions each) being the most frequent ones. Also men- tioned were storage consumption, hardware, expec- tations about large data experiments, and scale. In- terestingly, many respondents considered inference less pressing than training and model selection.
Job Sector. Although we do not find significant differences by seniority, we see larger (although not statistically significant) differences when look- ing at the responses grouped by researchers from different job sectors (Fig. 4a). We find that respon- dents from the large industry sector were mostly somewhat concerned, while the median for all other groups lies at often concerned. Similarly, we also see larger differences in the most pressing issues
3Note that 39.7% of our participants selected exactly these
two as the only pressing factors.


(a) ùëÑ1: Years at ACL
(b) ùëÑ2: Job sector
(c) ùëÑ3: Located in
s t n a p i c i t r a P %
60 50 40 30 20 10 0
1‚Äì5
6‚Äì10
11‚Äì15
16+
s t n a p i c i t r a P %
40
30
20
10
0
Student
PD Aca.
PI (s) Aca. Ind.
(l) Ind.
Other
s t n a p i c i t r a P %
50 40 30 20 10 0
M A
E
U/M
E
AS/Aus
Figure 2: Demographic statistics: (a) describes the seniority, (b) the job sector, and (c) the geographic location of our participants (in %).
between different groups. For instance, small and large industries were substantially more concerned with respect to inference and much less concerned with respect to model selection than academia.
3.2 Discussion and Recommendations
An analysis of the 81 (26%) free-text responses (ùëÑ9) reveals diverse opinions about the environmental impact of NLP and the reasons behind the most pressing factors. For instance, among respondents that stated to be not at all concerned about NLP‚Äôs environmental footprint, a majority considered the impact of NLP research on climate change to be negligible compared to other factors. Factors men- tioned as being more relevant to climate change include air travel (also mentioned twice in the gen- eral responses ùëÑ19), cars, and more cost intensive computations from other areas (of science). An- other argument brought up multiple times in this group of respondents is that the ACL is not the right institution to tackle challenges of climate change. Some responses alternatively suggested to push for regulatory changes, since big tech companies might not be affected by decisions made by the ACL.
Regarding the most pressing factors, one of the main arguments provided for inference was that in- dustry spends most time on inference, hence it is the most expensive one. However, participants also argued that there exist various methods for efficient inference (see, e.g., Treviso et al., 2023). Prominent arguments with respect to training and model selec- tion were that the pressure to achieve state-of-the- art performance leads to extensive hyperparameter tuning and that a large variety of models are being trained during research and development (even if just for debugging) without being ever deployed.
4 Equity
The (in)equity of the available compute resources across groups (e.g., academia and industry) is an increasingly brought up topic in discussions. While the general gist seems to be that many researchers feel excluded by not having access to substantially large compute power (e.g., thousands of GPUs), it often remains unclear whether this is really the case. One of the main objectives of this survey was therefore to quantify such potential disparities.
4.1 Analysis
For ùëÑ4, 229 participants responded (73.4%) with the number of GPUs they have access to. Fig. 1 shows the distribution of the total number of GPUs across our participants (in %). Overall, we observe a high disparity across our participants in terms of access to GPUs. For instance, 62% of the partici- pants had access to less than eight GPUs (Fig. 6a), the number used for training academic BERT (Izsak et al., 2021), and 87.8% of the participants had ac- cess to only 9.7% of the total number of GPUs. 15 participants (6.6%) had access to more than 100 GPUs, up to 3000 GPUs, representing 85.6% of the total GPU count (‚àº11.2k). An outlier analysis shows that 13.1% of the respondents had access to a substantially higher number of GPUs (more than 22 GPUs) than the rest. We further find that 57.4% of our participants were unable to run exper- iments due to the lack of computational resources, and 36.2% had no lack of resources (ùëÑ5). Finally, Fig. 5 shows that 31.4% of our respondents never or rarely thought that more resources could make their work valuable, while 34.3% of respondents answered sometimes, and 34.3% answered often or always (ùëÑ6).
AF


(a) ùëÑ7: How concerning is the env. footprint?
(b) ùëÑ8: What are the most pressing issues?
80
40
s t n a p i c i t r a P %
30
20
10
s t n a p i c i t r a P %
60
40
20
0
all at Not
Slightly
Somewhat
Moderately
Very
0
Train
Select
Infer
Other
None
Figure 3: Environmental concerns and pressing issues (in % of participant answers).
(a) ùëÑ7: Concerns by job sector
(b) ùëÑ8: Pressing issues by job sector
Very
5
4
3
2
s t n a p i c i t r a P %
80
60
40
20
Student Aca. PD Aca. PI Ind. (s) Ind. (l)
all 1 at Not
Student
PD Aca.
PI Aca.
(s) Ind.
(l) Ind.
0
Train
Select
Infer
Other
None
Figure 4: Concerns and pressing issues, grouped by positions.
Q6: Would more resources make work more valuable?
40
s t n a p i c i t r a P %
30
20
10
0
Never
Rarely
Sometimes
Often
Always
Figure 5: Lack of resources for more valuable work.
Job Sector. As in ¬ß3, our analysis shows no sig- nificant differences w.r.t. the seniority, and we find larger disparities by job sector. As we can observe in Fig. 6c, respondents in industry (large) had access to a higher number of GPUs than industry (small) and academia. This is one of the few cases where we have to resort to pairwise testing, as the Kruskal-
Wallis test indicates that the Null hypothesis cannot be rejected with ùêª 5 = 16.976 > ùêª 5 0 = 9.488. While we do not find significant differences in our pairwise comparisons, there are still substantial differences between Ind. (l) and Aca. PI (p-value = 0.0827), as well as Ind. (s) and Ind. (l) (p-value = 0.0850). Even though students reported the low- est number of available GPUs, the differences seem less substantial compared to researchers at small industry (p-value = 0.110). Additionally, we find that large industry has the highest percentage of outliers and the largest in-group disparity. Interest- ingly, researchers from small industry seem to have the least issues when running experiments; a stark contrast considering they are among those who re- ported the fewest GPU resources (ùëÑ5). Regard- ing ùëÑ6, researchers from large industry responded slightly less often than other groups that their re- search could be more valuable if they had access to more compute power.


(a) ùëÑ4: #GPUs per participant
(b) ùëÑ4: GPUs by seniority
(c) ùëÑ4: GPUs by job sector
104
104
s t n a p i c i t r a P %
50 40 30 20 10 0
0‚Äì1
2‚Äì7
8‚Äì31
32‚Äì999
1000+
s U P G #
103
102
101 100 0
1‚Äì5
6‚Äì10
11‚Äì15
16+
s U P G #
103
102
101 100 0
Student
PD Aca.
PI Aca.
(s) Ind.
(l) Ind.
Figure 6: Distribution of GPUs among participants: (a) overall, (b) by seniority, (c) by job sector.
4.2 Discussion and Recommendations While our survey highlights existing disparities, particularly between small industry or academic researchers and large industry, we also find that there exist substantial disparities within each group. Most surprising might be the general disparity we find across the field, as 87.8% of our partici- pants had access to less than 10% of the total num- ber of GPUs, and 62% had access to less then 8 GPUs. Only a very small faction of researchers (2.2% of our respondents) had access to GPU com- pute (1000 or more) to train models with several hundreds of billion parameters for several days or weeks. Many researchers, hence, could only fine- tune models‚Äîwhich requires far fewer resources than pre-training (Zhou et al., 2021)‚Äîwhich is only possible when pre-trained model weights are avail- able. Unfortunately, many recent models are being kept private, which has intensified the discussion about equity in the field (Togelius and Yannakakis, 2023).
5
Impact on Reviewing
Finally, we quantified how the concerns about the environmental impact and differences in terms of available compute resources affect peer reviewing (ùëÑ10‚ÄìùëÑ13). We further asked our participants four questions (ùëÑ14‚ÄìùëÑ17) which relate to concrete ideas that would change the reviewing process and en- courage model release (ùëÑ18).
5.1 Analysis Figure 7a shows that 30.1% of the participants ex- perienced being asked (during peer-review) to con- duct additional experiments that were too expensive for them (ùëÑ10) with 77 respondents having experi-
enced this more than once (ùëÑ11) and 19.2% having a substantially higher number (five or more times) according to our outlier analysis. Most participants (65.9%) further thought that this criticism was un- justified (ùëÑ12). Figure 7b (ùëÑ13) shows that 34.3% of the respondents often or always lacked resources to reproduce previous experiments and 41.4% some- times. Only 7.7% never or 16.7% rarely faced a lack of resources to reproduce experiments.
With respect to the concrete reviewing actions, Fig. 9a shows that a large majority (89.8%) of our participants would consider submitting their work to a dedicated track on efficient methods (ùëÑ14). Following up on the results from the survey, such an efficiency track was implemented at EMNLP 2022. 35.9% of our participants were unsure about requesting authors to justify the allocation of bud- get for experiments (ùëÑ15), with 41% voting for yes. Also, even though 52.6% of the participants had not been asked for experiments that were too expensive for them, a clear majority of the partic- ipants (83.7%) would like to require reviewers to justify their petitions for more experiments (ùëÑ16). Lastly, we also see a large majority (75.6%) that be- lieved that their work could benefit from the release of small versions of pretrained models alongside large ones (ùëÑ17). To promote this, a majority of our respondents thought that venues should have a visible branding of papers to release a model (59.3%) and that reviewers should be instructed to reward model release (50.6%). 42.6% of respon- dents thought that the venues should grant a best artifact award. 11.5% of respondents supported none of the options. A first step towards increasing the reproducibility and ensuring the submission of experimental code was implemented at NAACL


(a) Reviewer critique
(b) Reproducing results
Q10: Did reviewers ask for too expensive experiments?
Q13: Lack of resources to reproduce results?
Q12: Was the critique justified?
50
0
100
40
Q10
Q12
15
30
19
17
53
66
s t n a p i c i t r a P %
30
20
10
% Participants
Possible answers: yes (‚óº), not sure (‚óº), no (‚óº).
0
Never
Rarely
Sometimes
Often
Always
Figure 7: Analysis on how of a lack of resources can affect research. In (a), we show what percentage of participants had been asked by reviewers for too expensive experiments (ùëÑ10) and if so, if they felt the critique was justified (ùëÑ12). In (b), we show how often our participants could not reproduce previous results due to a lack of computational resources (ùëÑ13).
2022 by introducing a badge system at the repro- ducibility track.4 Upon acceptance, the authors could follow specific procedures to earn three types of badges: 1) open-source code, 2) trained model, and 3) reproducible results.
Seniority. We find no significant differences w.r.t. the seniority of our participants regarding ùëÑ10‚ÄìùëÑ18. However, junior researchers (1‚Äì5 years) showed a substantially higher tendency towards requesting authors to justify their compute bud- get (ùëÑ15) against all other age groups (p-values < 0.035). We also observe in Figure 9c diverging preferences between junior and senior groups in terms of ideas to improve the reviewing process (ùëÑ18). Junior researchers (1‚Äì5 years) seemed to be more inclined towards a visual branding as well as instructing reviewers than senior researchers (11‚Äì 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085).
reproduce experiments (ùëÑ13, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large in- dustry sector with a p-value of 0.002 < 0.005 = ùõº (Bonferroni-corrected). We further find substantial differences between students and academic PIs (p- value = 0.026) and between academic post-docs and large industry labs (p-value = 0.088).
We find no substantial differences when it comes to actionable items for the *CL community (ùëÑ14‚Äì ùëÑ17), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encourag- ing the release of models (ùëÑ18). For instance, Fig- ure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry.
In terms of the job sector, we again Job Sector. find no significant differences with respect to re- viewers asking for too expensive experiments (ùëÑ10) or critique being justified (ùëÑ12). Interestingly, re- spondents from small industry received fewer such requests (ùëÑ11) compared to post-docs (p-value = 0.024), PIs (p-value = 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to
5.2 Discussion and Recommendations Our analysis shows that the two most pressing is- sues among our respondents are the lack of re- sources to reproduce results and reviewers request- ing for too expensive experiments without proper justification. This is reflected in the large support for both respective counter measures; namely, ask- ing reviewers to provide justification and the release of smaller models that would allow researchers to at least reproduce some experiments. Considering the
4https://2022.naacl.org/blog/
reproducibility-track/
5Kruskal-Wallis test: ùêª 5 = 12.486 > ùêª 5
0 = 9.488.


Always
5
4
3
2
Never 1
Student
PD Aca.
PI Aca.
(s) Ind.
(l) Ind.
Figure 8: ùëÑ13: Lack of resources by job sector.
success of badges at NAACL 2022 with 175 code, 98 model and 20 reproducibility badges, introduc- ing an explicit badge for small model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7
Among the 22 additional suggestions for ùëÑ18, we find a high emphasis (68.2%) towards the re- lease of artifacts‚Äîboth because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (ùëÑ19) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind
6https://naacl2022-reproducibility-track.
github.io/results/
7https://aclrollingreview.org/
reviewertutorial
reviewing. Finally, 12 of the free-text responses of ùëÑ18 and ùëÑ19 suggested that artifact release should be mandatory for acceptance.
6 Further Considerations
Finally, we discuss suggestions (ùëÑ19) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents).
Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers.
10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work.
Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020).
7 Conclusion
We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational


(a) Potential changes to reviewing
Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models?
(b) Model release
Q18: How to encourage model release?
s t n a p i c i t r a P %
100
80
60
40
20
0 1
0 9
3 2
6 3
1 4
4
3 1
4 8
8
7 1
5 7
s t n a p i c i t r a P %
60
50
40
30
20
10
0
Q14
Q15
Q16
Q17
Possible answers: yes (‚óº), not sure (‚óº), no (‚óº).
0
Branding proceedings
in
artifact ward A best for
Reviewers release reward
Other
None
(c) Model release (by seniority)
(d) Model release (by job sector)
Q18: How to encourage model release?
Q18: How to encourage model release?
s t n a p i c i t r a P %
80
60
40
20
1‚Äì5 6‚Äì10 11‚Äì15 16+
s t n a p i c i t r a P %
80
60
40
20
Student Aca. PD Aca. PI Ind. (s) Ind. (l)
0
Branding proceedings
in
ward artifact A best for
Reviewers
release
reward
Other
None
0
Branding proceedings
in
ward artifact A best for
Reviewers
release
reward
Other
None
Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants‚Äô responses for ùëÑ14‚ÄìùëÑ17 (in %). A majority of our participants would submit to an efficiency track (ùëÑ14) and would prefer reviewers to justify a request for more experiments (ùëÑ16). They further would benefit from a release of smaller models (ùëÑ16). In contrast, the responses are more mixed about the authors justifying the compute budget (ùëÑ15). In (b‚Äìd), we show our participants‚Äô responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for ùëÑ18.
demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models.
Considering the continuous increase of param-
eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further. However, we find that much can be done to combat this, even on an individual level. As a re- searcher, by making our model weights, code, and data publicly available; and as a reviewer, by being considerate towards the available compute budget.
Limitations
To receive a large number of responses, this survey was advertised throughout various channels. Hence, this is by no means a representative study within the whole *CL community. This is partially reflected in


the evaluation of the geographic locations, e.g., they were too coarse to capture a more precise picture about existing geographic inequalities. Nonethe- less, the fact that we did not receive any responses from bodies located in Africa indicates that there may exist high disparities in terms of geographic lo- cation. For the same reason, the disparities found in this survey are more indicative than representative. Consequently, any action that is being implemented should not be solely derived from the survey data and carefully considered beforehand.
Acknowledgements
This work was initiated at and benefited substan- tially from the Dagstuhl Seminar 22232: Efficient and Equitable Natural Language Processing in the Age of Deep Learning. We further thank Niran- jan Balasubramanian, Jonathan Frankle, Michael Hassid, Kenneth Heafield, Sara Hooker, Alexan- der Koller, Alexandra Sasha Luccioni, Alexander L√∂ser, Andr√© F. T. Martins, Colin Raffel, Nils Reimers, Leonardo Riberio, Anna Rogers, Ed- win Simpson, Noam Slonim, Noah A. Smith, and Thomas Wolf for a fruitful discussion and helpful feedback at the seminar. We further thank Leshem Choshen for helpful feedback on this work.
References
Milton Abramowitz. 1974. Handbook of Mathe- matical Functions, With Formulas, Graphs, and Mathematical Tables,. Dover Publications, Inc., USA.
Yuki Arase, Phil Blunsom, Mona Diab, Jesse Dodge, Iryna Gurevych, Percy Liang, Colin Raf- fel, Andreas R√ºckl√©, Roy Schwartz, Noah A. Smith, Emma Strubell, and Yue Zhang. 2021. Efficient NLP policy document.
Carlo Bonferroni. 1936. Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3‚Äì62.
Roland A. Fisher. 1921. On the" probable error" of a coefficient of correlation deduced from a small sample. Metron, 1:3‚Äì32.
Kenneth Heafield, Biao Zhang, Graeme Nail, Jelmer Van Der Linde, and Nikolay Bogoychev. 2022. Findings of the WMT 2022 shared task on
efficient translation. In Proceedings of the Sev- enth Conference on Machine Translation (WMT), pages 100‚Äì108, Abu Dhabi, United Arab Emi- rates (Hybrid). Association for Computational Linguistics.
Peter Izsak, Moshe Berchansky, and Omer Levy. 2021. How to train BERT with an academic bud- get. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing, pages 10644‚Äì10652, Online and Punta Cana, Dominican Republic. Association for Computa- tional Linguistics.
William H. Kruskal and W. Allen Wallis. 1952. Use of Ranks in One-Criterion Variance Analysis. Journal of the American Statistical Association, 47(260):583‚Äì621.
Howard Levene. 1960. Robust tests for equality of variances. Contributions to probability and statistics, pages 278‚Äì292.
Nafise Sadat Moosavi, Angela Fan, Vered Shwartz, Goran Glava≈°, Shafiq Joty, Alex Wang, and Thomas Wolf, editors. 2020. Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing. Association for Computational Linguistics, Online.
David Patterson, Joseph Gonzalez, Urs H√∂lzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R So, Maud Texier, and Jeff Dean. 2022. The carbon footprint of ma- chine learning training will plateau, then shrink. Computer, 55(7):18‚Äì28.
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2020. Green ai. Communications of the ACM, 63(12):54‚Äì63.
S Shaphiro and MBJB Wilk. 1965. An analy- sis of variance test for normality. Biometrika, 52(3):591‚Äì611.
Emma Strubell, Ananya Ganesh, and Andrew Mc- Callum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics, pages 3645‚Äì3650, Flo- rence, Italy. Association for Computational Lin- guistics.


Julian Togelius and Georgios N Yannakakis. 2023. Choose your weapon: Survival strate- gies for depressed ai academics. arXiv preprint arXiv:2304.06035.
Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, Andr√© F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simp- son, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, and Roy Schwartz. 2023. Effi- cient Methods for Natural Language Processing: A Survey. Transactions of the Association for Computational Linguistics, 11:826‚Äì860.
John W. Tukey. 1977. Exploratory Data Analysis.
Addison-Wesley.
Bernard Lewis Welch. 1951. On the Comparison of Several Mean Values: An Alternative Approach. Biometrika, 38(3/4):330‚Äì336.
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmen- tal implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795‚Äì813.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be- ichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.
Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2021. HULK: An energy efficiency benchmark platform for responsible natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 329‚Äì336, Online. Association for Computational Linguistics.