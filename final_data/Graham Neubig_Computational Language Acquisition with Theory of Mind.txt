3 2 0 2
r a
M 2
] L C . s c [
1 v 2 0 5 1 0 . 3 0 3 2 : v i X r a
Published as a conference paper at ICLR 2023
COMPUTATIONAL LANGUAGE ACQUISITION WITH THEORY OF MIND
Andy Liu Harvey Mudd College Claremont, CA, USA {ajliu}@g.hmc.edu
Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA {zhuhao, mengyan3, ybisk, gneubig}@cs.}cmu.edu
ABSTRACT
Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the abil- ity to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difﬁculty, hy- pothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We ﬁnd that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also ﬁnd some evidence that increasing task difﬁculty in the training process results in more ﬂuent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition1.
1
INTRODUCTION
Human languages are fundamentally shaped by social-communicative goals in the grounded world. Modern theories from developmental psychology often attribute humans’ unique ability to quickly acquire and adapt language to their ability to ascribe mental states to other agents (Tomasello, 2005), an ability also known as Theory of Mind (ToM).
Some previous studies have attempted to perform computational modeling of ToM. For instance, ToM-like mechanisms have been demonstrated to allow models to better predict the behavior of a future agent (Rabinowitz et al., 2018), model agents’ beliefs in a negotiation (Cao et al., 2018) or a cooperative game (Bard et al., 2020), or choose good utterances based on the listener’s linguistic abilities (Zhu et al., 2021). However, the effects of ToM have not yet been studied in the higher-level context of computational language acquisition.
In this paper, we study how an internal ToM mechanism and external environmental pressure con- tribute to language learning. We use an image referential game setting consisting of a series of training episodes between a speaker, which represents a language learner (Zhu et al., 2022), and a listener, which represents a ﬂuent teacher. When presented with a set of images, one of which is the target referent, the speaker must learn to generate an English utterance that the listener can use to select the target. The speaker is rewarded for generating utterances that are used to correctly guess the target image. Additionally, the speaker may be given feedback depending on the conﬁdence the listener has in the selection. This setting provides an attractive test-bed for testing the effects of various reward signals or model designs on the speaker’s learned language; previous studies of pragmatics in language acquisition, such as Andreas & Klein (2016), have used similar settings.
1Code and data can be found at https://github.com/neulab/ToM-Language-Acquisition.
1


1. Yellow shirt man throw frisbee 2. Frisbee front of trees 3. Three people play frisbee
Distractor Image
yellow shirt manthrow frisbee
Target Image
1. Three people play frisbee 2. Frisbee front of trees 3. Yellow shirt man throw frisbee
Candidate Utterance SamplingToM Listener RerankingOutput Utterance
Published as a conference paper at ICLR 2023
Figure 1: An example of how ToM is used by our speaker models in our implementation. The speaker ﬁrst generates candidate utterances before reranking them according to the probability given to each target image, utterance pair by the internal ToM listener. It then selects either the highest- scoring or a random utterance from the candidate pool to output.
Within this setting, we seek to better understand how models with and without ToM adapt to their environments. We focus on two speciﬁc research questions in this area:
RQ1. How does the inclusion of ToM in language acquisition speaker models affect their perfor-
mance and learned language? (internal ToM mechanism)
RQ2. How do our models adapt to more difﬁcult referential game environments during the lan-
guage acquisition process? (external environmental pressure)
We study the impact of ToM (RQ1) by modeling an internal listener module within our speakers that aims to predict which utterances are most likely to result in the desired listener behavior. By incor- porating the probabilities given to the target image by the ToM listener into the utterance reranking process, as shown in Fig. 1, we select for more pragmatic utterances sampled from the speaker’s distribution. To study the impact of environmental pressure (RQ2) on our speakers, we create ref- erential games with different difﬁculties by sampling distractors from different distributions. These distributions are based on the similarity between images calculated by various image representation models, including CLIP (Radford et al., 2021), RoBERTa (Liu et al., 2020), and TF-IDF variants.
In experiments, we ﬁnd that (RQ1) speaker models including ToM components generally outper- form those that do not in terms of ﬂuency and ﬁnal accuracy. We also ﬁnd that (RQ2) training with more visually and semantically similar distractor referents causes the speaker model to develop longer, more ﬂuent, and more precise utterances to distinguish between potential referents, although these do not always translate to gains in referential game performance. These results suggest con- tributions to language acquisition from both ToM and environmental pressures in this setting. We still ﬁnd signiﬁcant gaps between the language that our speaker model acquires and human captions. Additionally, we restrict both the vocabulary and the maximum length of our speakers’ utterances. These both suggest that there is still room for improvement in this class of models. However, we hope our results still hint at both better training methods for pragmatic language models and a deeper computational understanding of human language acquisition.
2
IMAGE REFERENTIAL GAME ENVIRONMENT
Following Lazaridou et al. (2016); Lowe et al. (2019); Zhu et al. (2022), we consider image ref- erential games with real world images, where a speaker and a listener collaborate on identifying a target image x ∈ C among distractors randomly sampled from a set of candidates C. The identity of the target is known only to the speaker. The speaker generates an English-language utterance u
2


Published as a conference paper at ICLR 2023
that describes the target image, which is then passed to the listener. The listener will either select an image ˆx, in the case where they understand the utterance with a high enough probability or refuse to act, in the case where they do not. The listener can also determine whether or not to provide feedback (linguistic input in the form of the ground-truth caption of the target image) to the speaker.
Speaker training is motivated by a reward function that seeks to model the communicative goal of directing the listener to the correct referent. The speaker gets a positive reward of 1 if the listener chooses the target image and a penalty of −1 if the listener chooses the wrong target image. Ad- ditionally, a smaller penalty wnoop is applied to the speaker if the listener chooses to not choose an image; this is done to penalize the speaker for generating unclear utterances.
2.1 AGENT FORMULATION
We follow the speaker and listener formulations deﬁned in Zhu et al. (2022). The speaker is a model f : I → Σ∗ that generates utterances (sequences of tokens from its vocabulary) that correspond to a provided image. We deﬁne Σ∗ as the speaker vocabulary and I as the set of candidate images. The listener is deﬁned on Σ∗ and I N , which is a set of N candidate images sampled from I. Given an utterance ∈ Σ∗ and an observation space ∈ I N , the listener will either return the index of its predicted image or noop, if it cannot predict an image. It may also return the ground-truth caption from its vocabulary. We deﬁne it with the model g : Σ∗ × I N → ([0, 1, . . . N − 1] ∪ noop) × Σ∗.
2.2 SPEAKER DESIGN
The speaker that we use is a captioning model composed of a pretrained ResNet (He et al., 2016) model and an LSTM-based utterance generation model. After generating an embedding vector of the target image using the ResNet model, the speaker autoregressively generates an utterance u = {ui}M
i=1 using the LSTM network:
P (ui | u1, u2, . . . , ui−1, x) ∝ exp(wT
uiLSTM(wu1 , wu2 , . . . , wui−1, h0 = ResNet(x))),
where wui ∈ Rdw is the word embedding of ui. The speaker generates a sequence of tokens to either the maximum length, which is set to 20 in our experiments, or an end-of-sequence token. In our experiments, we use a vocabulary size of 200 to limit the size of the action space and to more realistically mimic the vocabulary of a language learner.
2.3 LISTENER DESIGN
Given a set of candidate images Ij (j ∈ 1, 2 . . . n + 1), where n is the number of distractors, and an utterance u, the listener computes embeddings for each image, L(Ij), for the utterance, L(u). It then takes the dot product of each L(Ij) with L(u), and the softmax of the dot products to compute the probability P (Ij|u) ∝ exp(L(Ij)L(u)) of the utterance referring to image Ij for each image.
The listener also has a rule-based component to control when to give linguistic input in the form of the ground-truth caption C(x). We introduce two thresholds, θ1 and θ2. After the listener computes the most likely target image, t = arg max P (Ij|u), and the probability given to this choice, Pmax = max P (Ij|u), it returns its choice ˆx and input flistener according to the following logic:
(ˆx, flistener) =
 

(noop, 0) Pmax < θ1 (t, C(x)) (t, 0)
θ1 < Pmax < θ2 Pmax > θ2
This control strategy mimics a caregiver who only gives feedback when they understand what the language learner is referring to, but wants to direct it to generate higher-quality utterances. When the listener’s conﬁdence is very low, it will neither select a target image nor provide linguistic input, as the speaker utterance is too low-quality. Meanwhile, when the listener’s conﬁdence is very high, it will stop giving linguistic input, as further improvement is deemed unnecessary.
3
(1)
(2)


Published as a conference paper at ICLR 2023
We use an LSTM-based model to learn embeddings of speaker utterances combined with a ResNet model to derive image features. Because we want our listener to at least model a competent user of the language that the speaker is trying to acquire, some pretraining is required before initializing a listener in a new environment. The controller values θ1, θ2 and the ResNet parameters are set after preliminary experimentation on the dataset. The listener’s language network parameters are then trained with mini-batch stochastic gradient descent to optimize the network values
θlistener = arg max
θ
EC∼U (I)N
1 N
N (cid:88)
i=1
log Plistener(i | U ∗
i , C; θ)
3 SPEAKER TRAINING PROCESS
3.1 COMMUNICATIVE GOALS
We model communicative goals and learning from linguistic input as two separate learning objec- tives for our speaker network, which we combine with an adjustable coefﬁcient λ. The communica- tive goal reward OCG is simply the average reward (deﬁned in §2) over each game in the training episode. The space of possible output utterances is discrete and non-differentiable, so our speaker learns its policy π via the reinforcement learning method PPO (Schulman et al., 2017).
3.2 LEARNING FROM LINGUISTIC INPUT
We model the learning from linguistic input objective as the maximum likelihood of the listener input in the speaker models and optimize this continuous function with stochastic gradient descent.
OLI = Ex,C,u∼π(u|x) log π(U ∗
flistener(u,C) | x, C)
We then combine the separate learning objectives for each task using a coefﬁcient λ ∈ [0, 1]:
Ojoint = λOCG + (1 − λ)OLI
Next, we introduce ToM Modeling, and an additional task objective, OT oM . This is a cross-entropy objective that represents how accurate the speaker’s internal listener model is. Similar to the objec- tive above, we combine these into a single learning objective for the entire speaker network.
4
INTRODUCING TOM MODELING
We incorporate into the speaker architecture an LSTM-based ToM listener. This listener model is trained alongside of the rest of the speaker network to learn the “mental state” of the actual listener – the neural network parameters that allow it to best replicate the probabilities that the listener would assign to image-utterance pairs. It uses a similar architecture to the actual listener, combining its own learned sentence embeddings from an LSTM network with pretrained image embeddings using ResNet. In other words, our ToM listener seeks to learn the sentence embedding model θT oM that will maximize the probabilities assigned to the listener’s choice given an utterance.
θT oM = arg max
PT oM (ui|ˆx) ∝ exp(θT
T oM (ui) · ResNet(ˆx))
θ
We introduce ToM into the speaker model by having our speaker sample candidate utterances and rerank them with the help of the ToM listener. Our ToM speaker ﬁrst samples N candidate utterances U = {u(i)}N
i=1 from its distribution. Next, we generate a speaker and a listener score for each:
Pspeaker(u) ∝
(cid:89)
P (ui|u1, u2, . . . ui−1, x)
i
PToM(x|u) ∝ exp(LSTMT (u)Resnet(x))
4
(3)
(4)
(5)
(6)
(7)
(8)


Published as a conference paper at ICLR 2023
Here, the probability P (ui|u1, u2, . . . ui−1, x) is computed according to 1. We then combine these scores using a listener weight hyperparameter wl to get the overall score assigned to each utterance. Finally, we select the best utterance, ub, as the argmax of this score:
ub = arg max
(PToM(x|uj)wl · pspeaker(uj))
U
In our experiments, we train models with three different settings of wl. We train models with wl = 0 to isolate the effects of the ToM listener from the effects of the new model architecture. We train models with wl = 1 which weigh the speaker and ToM listener input equally. Finally, we train models where wl is the arbitrarily high constant 1000. In this case, the listener score dominates, and our speaker essentially seeks to maximize Plistener(x|ui), which it approximates with the ToM listener score. If we replace the learned ToM listener with a copy of the external listener, this utterance sampling process is close to that of the learned Rational Speech Act (RSA) model in Andreas & Klein (2016), which also trains pragmatic speakers on referential game environments. We compare our ToM speakers to these “RSA” speakers to evaluate the impact of using our learned approximation rather than the actual listener.
Finally, we introduce a hyperparameter σ: our speaker outputs a random utterance with probability σ and ub with probability 1 − σ. σ is set to decay linearly over time; this randomization is done to promote early exploration.
By default, we begin with an untrained listener that will be trained to emulate the actual listener’s outputs over time. To train the ToM listener, we introduce a third training objective in addition to OLI and OCG, the ToM objective OT oM , deﬁned as the cross-entropy loss between the distribution of the ToM listener and that of the true listener. Thus, we are training it to give higher probabilities to the listener’s choice ˆx based on the speaker’s utterances. Formally,
OT oM =
(cid:26)− log PT oM (ˆx|u) Pmax > θ1 Pmax < θ1
0
Note that this requires a choice to be made by the listener, so we mask out the case where the actual listener does not choose an image by setting it to zero. This is done using the same parameter θ1 and listener conﬁdence Pmax that were used to control linguistic input in 2. We add this to the joint learning objective previously deﬁned in 5 to compute our combined objective for the ToM speaker system as a whole:
Ojoint = λOCG + (1 − λ)OLI + OT oM
However, an untrained listener can introduce signiﬁcant noise to the utterance generation process. To counteract this, we anneal the inﬂuence of the listener, by linearly increasing wl from 0 to the ﬁnal wl value over a ﬁxed number of steps. This allows our speaker to only begin using its ToM listener when the listener is at least somewhat capable.
5
INCREASING DISTRACTOR DIFFICULTY
Previous studies on language acquisition have found that infants initially look at familiar objects when hearing semantically related labels for different objects, but adapt to more difﬁcult tasks over time by learning more complex semantic representations of objects (Bergelson & Aslin, 2017a). Ad- ditionally, Yuksekgonul et al. (2022) showed that contrastive training on harder distractors improved visiolinguistic models’ performance on tasks involving compositionality. We hypothesize that the usage of more similar distractor images might similarly force our speaker to generate more complex utterances due to the need to further distinguish between images. This motivated us to generate more similar distractor images in the training process in order to achieve such an effect. To do so, we computed a similarity rank between images based on visual and semantic similarity. Then, after selecting a “target” image in the training process, we sampled images with high similarities to the target image to use as distractors during training.
We experiment with three options for calculating image/caption similarity:
5
(9)
(10)
(11)


a little girl holding a kitten next to a blue fence.
two sheep standing next to each other in the snow.
a woman hugs a gray cat to her chest.
Image Similarity
Caption Similarity
Hybrid Similarity
a little dog sitting on a wooden bench.
Original Image
a kitten that is sitting down by a door.
Random Distractor
Published as a conference paper at ICLR 2023
Figure 2: An example image-caption pair, and distractors ranked as similar by various metrics. Caption and Hybrid similarities are computed with TF-IDF - weighted RoBERTa embeddings.
Visual Cosine Similarity. We compute the most visually similar images based on the cosine similarity of image embeddings from a pretrained ResNet model. For each image, we save a similarity ranking of the next 1000 most visually similar images, and then select distractors from this set during training whenever the original image is selected as a target.
Textual Cosine Similarity. We calculate the textual similarity of captions by taking the cosine similarity of vector representations of the text. We experiment with both (1) dense vectors using embeddings calculated using either the pretrained RoBERTa or the pretrained CLIP mean pooled models from the sentence-transformers library (Reimers & Gurevych, 2019), and (2) sparse vec- tors using one-hot vectors of each word. We also experimented with using term frequency – inverse document frequency (TF-IDF) to weight the vector for each token in our captions when computing caption similarity as a way to upweight rarer content words.
Visual+Textual Similarity.
In cases where both image and caption similarity were used, we added a caption weight parameter wc ∈ [0, 1] and used it to compute a weighted average of image and caption similarity. We hoped that using hybrid methods (wc = 0.5) would capture distractors with both visual and semantic similarities.
We also train models on randomly selected, or “easy”, distractors to create a baseline to study the effects of distractor difﬁculty. Examples of a selection of the distractor settings are shown in Fig. 2.
6 EXPERIMENTAL SETUP AND RESULTS
6.1 EXPERIMENTAL SETUP
We focus on two main results: A speaker’s ability to use language to correctly discriminate between the target and distractor images, as well as the quality of the language speakers use to do so.
To evaluate the former, we primarily consider average accuracy, or how often the listener selects the correct referent. We evaluate the quality of our speakers’ learned languages using ﬂuency, average length, F1 score for salient parts of speech, and caption quality scores. We use a ﬂuency score that measures the grammatical quality of output utterances (Kann et al., 2018). This is deﬁned as the average gain in log probability when moving from a unigram model to a pretrained language model.
ﬂuency =
1 |u|
(ln(pM (u)) − ln(pU (u)))
We train a unigram model, pU , and ﬁne-tune GPT-2 large (Radford et al., 2019), pM , on our training set.
In addition to ﬂuency and accuracy, we consider two additional performance metrics. To better evaluate the overall caption quality of the system, we compute BLEU score (Papineni et al., 2002) using the implementation found in Bird et al. (2009). Additionally, to evaluate the accuracy of the learned listener model, we compute ToM Accuracy, deﬁned as how often the candidate image that the ToM listener believes has the highest probability of being chosen is the actual listener’s choice.
We also consider part-of-speech statistics to evaluate utterance quality. Parts of speech for words in speaker utterances and ground-truth captions are tagged with spaCy (Honnibal et al., 2020). We use
6
(12)


Published as a conference paper at ICLR 2023
Base Speaker: A baseball player holding a holding umbrella.Speaker Trained on Hard Distractors: A baseball player holding a bat on a field.ToM Speaker Trained on Hard Distractors: A group of men are playing baseball outside.
Distractor
a group of men on a field playing baseball.
Target
a person on some skis.
Distractor
a white building with a clock on the front and side of it.
Figure 3: Examples of output utterances generated by various speaker models. Here, the “Hard Distractors” referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more difﬁcult distractors and training with ToM leads to more accurate, ﬂuent utterances.
Table 1: Performance and language features of various ToM speakers.
Model
Performance
POS F1
ToM Weight
Distractors Acc BLEU Fluency ToM ADJ ADP NOUN VERB
Baseline (No ToM) Easy Baseline (No ToM) Hard N/A Gold Standard
0.81 0.81 0.92
0.20 0.24 1.00
1.50 N/A 0.16 1.87 N/A 0.24 2.52 N/A 1.00
0.52 0.58 1.00
0.41 0.46 1.00
Zero Normal High High RSA
Hard Hard Hard Hard
0.83 0.85 0.88 0.87
0.26 0.26 0.27 0.28
1.99 2.25 2.23 2.26
0.81 0.88 0.89 0.93
0.22 0.22 0.22 0.23
0.64 0.65 0.66 0.65
0.49 0.52 0.52 0.50
Zero Normal High High RSA
Easy Easy Easy Easy
0.85 0.88 0.88 0.89
0.25 0.26 0.27 0.29
1.73 2.09 2.07 1.91
0.85 0.91 0.91 0.94
0.21 0.21 0.22 0.17
0.57 0.64 0.65 0.65
0.48 0.50 0.51 0.52
this to compute F1 scores between speaker utterances and ground-truth captions over each part of speech. This allows us to evaluate how effectively our speakers identify referents within the target image that a human captioner would consider salient. Higher F1 scores over a part of speech would suggest that a speaker is more capable of identifying concepts that fall under that part of speech.
Images and captions are drawn from the MS COCO dataset introduced in Lin et al. (2014). The train-val-test split given by the MS COCO 2017 dataset is extended to our experimental setup.
6.2 EFFECTS OF TOM
We train models equipped with ToM listeners with normal weight (i.e. equally weighting the speaker and the listener scores when reranking output utterances) and with high weight (where the listener weight is set arbitrarily to 1000 and the speaker essentially optimizes for P (x|ui)). In order to isolate the effects of using a ToM listener from the effects of modifying the utterance selection process, we also train a model equipped with a ToM listener that it does not give any weight to. Additionally, we train a model without any ToM component for comparison. We do this for both easy and hard distractors, where the hard distractors were those that were chosen by the hybrid similarity between visual and semantic (CLIP) features outlined in 5. Finally, we give the listener the ground-truth captions over the test set to compute gold-standard metrics of accuracy and ﬂuency.
We ﬁnd signiﬁcant performance improvements in Table 1 when speaker models are trained to rerank utterances solely by ToM listener score. Such “high-weight ToM” speaker models achieve accuracy gains of 3.0% and 4.6% on easy and hard distractors, respectively. This suggests that the inclusion of a sufﬁciently inﬂuential ToM reranker during the speaker training process improves speaker per- formance, although the relative gains appear to be much higher when training on easy distractors.
7
0.38 0.45 1.00
0.47 0.49 0.50 0.49
0.49 0.52 0.50 0.49


Published as a conference paper at ICLR 2023
Table 2: Performance and language features of speakers trained on various distractors. We only show the most performant variants of Caption and Hybrid similarity, with the others shown in A.1.
Model Distractors
Performance Acc
POS F1
Fluency ADJ ADP NOUN VERB
Average Length
Base Gold Standard
0.81 0.92
1.50 2.52
0.16 1.00
0.52 1.00
0.41 1.00
0.38 1.00
8.97 10.79
Image Caption Hybrid
0.80 0.86 0.85
2.09 2.01 2.19
0.19 0.19 0.20
0.61 0.62 0.62
0.45 0.49 0.49
0.49 0.48 0.47
10.33 10.04 10.18
However, we ﬁnd that speaker models that rerank utterances using a combined speaker-ToM score generally fail to outperform models that do not use their ToM listener in training.
We also ﬁnd that the usage of a highly-weighted ToM listener leads to signiﬁcant ﬂuency gains when training on both easy (15.6% relative increase in ﬂuency score) and hard (11.6%) distractors. We also see longer and more complex utterances when using normally or highly weighted ToM listeners. Additionally, we ﬁnd limited gains in general captioning ability between baseline and high-weight models, as measured by BLEU score. However, these effects are more subtle, and do not always lead to signiﬁcant accuracy gains, suggesting that the main driver of ToM accuracy gains is increased pragmatic ability. We conclude that usage of a highly inﬂuential ToM listener during the training process leads to signiﬁcant performance and ﬂuency gains. We are also able to qualitatively observe the improvement in model performance from ToM. As seen in one representative example in Fig. 3, our ToM Speaker is able to identify two elements that clearly distinguish the target image from the distractors (i.e. that there are multiple men who are playing baseball) in a ﬂuent utterance.
Finally, we ﬁnd that the ToM listener successfully approximates the external listener. Models with learned listeners and RSA models with the pretrained listener perform comparably in accuracy and ﬂuency. Because the RSA models represent the upper bound of how good a speaker’s listener model can be, this suggests that our learned listeners are very beneﬁcial to the speakers. This is also shown through the high ToM accuracies reported, especially in the most performant models, those with high listener weight. These qualitative and quantitative results provide computational evidence that ToM can play an important role in simulated language acquisition, similarly to how it has been hypothesized to play a critical role in human language acquisition.
6.3 EFFECTS OF DISTRACTOR DIFFICULTY
As shown in Table 2, we generally ﬁnd signiﬁcant improvements in language quality in models trained on more difﬁcult distractors. The largest gains are those seen in the ﬂuency score, where difﬁcult distractors achieve gains ranging between 25% to 46%. We also ﬁnd that models trained on more difﬁcult distractors use more similar vocabulary to the ground-truth captions, as measured by F1 score in the ground-truth captions and utterances produced. This is signiﬁcantly higher over adpositions, nouns, and verbs on models trained with more difﬁcult distractors. Finally, all speakers trained on difﬁcult distractors generate more complex utterances compared to the base speaker, with utterances that are at least one word longer on average. This supports our hypothesis that when confronted with increased environmental pressure, the speaker adapts by becoming more precise, ﬂuent, and complex with its language. These can be seen qualitatively in one representative example in Fig. 3, which shows that a speaker trained on hard distractors is able to generate more ﬂuent utterances that more precisely describe the image (in this example, correctly identifying an object in the image as a baseball bat, as opposed to an umbrella).
We ﬁnd smaller differences between the language of models trained with various types of hard dis- tractors. Speaker models trained with visually similar distractors achieve the highest ﬂuency, at 2.094, and form the longest utterances. They also have more precise verb selection, as measured by F1. Speakers trained on distractors that were selected with hybrid or caption similarity, achieved high noun F1 scores of 0.49 compared to 0.41 for the base speaker model, indicating that semanti- cally similar distractors in training may be better for identifying salient nouns. We ﬁnd that training on more difﬁcult distractors does not consistently improve model performance when evaluating on easier distractors. This suggests some disconnect between a language’s ﬂuency and its suitability to
8


Published as a conference paper at ICLR 2023
the image referential game environment. However, speakers that train on more semantically simi- lar distractors still achieve up to 5 percent higher accuracy than the base speaker, indicating some beneﬁts to performance from training on certain harder distractors.
7 RELATED WORK
Parallels in Human Language Acquisition. The concept of learning language through repeated exposure to referents is a popular model within the psychology community. Smith & Yu (2008) found that infants resolve the uncertainty of determining which referent in a scene a word refers to by statistical learning over word-scene pairings. Yu & Ballard (2007) incorporated a social element into models by considering “social-cognitive” capacities, such as attention reading, that act as a form of ToM. Yu & Smith (2012) studied caregivers’ social impact on early language acquisition using head- mounted cameras, ﬁnding that caregiver feedback during moments where a referent was visually dominant directly led to language learning in the infant. Bergelson & Aslin (2017b) found that unrelated images were easier for infants to differentiate between than semantically similar images.
Pragmatic Modelling. Andreas & Klein (2016) used pragmatic models to jointly train neural speaker and listener models to play a referential game that involved describing contrasting scenes. They also use a sample-and-rerank method for selecting utterances in this setup. However, they use the actual listener, rather than a learned listener, to rerank utterances. Additionally, we apply this process to a computational model of language acquisition. Nematzadeh et al. (2018) created a dataset to evaluate question-answering models’ ability to keep track of inconsistent worldviews. They found that state-of-the-art neural models lack this ability, indicating that they cannot solve tasks with ToM. Monroe et al. (2017) studied the effects of visually similar distractors on a prag- matic model of color identiﬁcation, ﬁnding that pragmatic models had the largest gains in the most difﬁcult settings. Vedantam et al. (2017) also consider the effects of including pragmatic components in image captioning models, namely an internal module to better discriminate between images.
Emergent Language. Lazaridou & Baroni (2020) surveyed recent progress in emergent language from multi-agent communication, claiming that further progress can help deep networks become more interactive and interpretable. Lazaridou et al. (2020) place a generally trained language model in a multi-agent environment with task-speciﬁc rewards. Similarly to our work, this results in a task-conditional language model which the authors claim can better communicate with humans over visual tasks. Chaabouni et al. (2022) analyzed the effects of task difﬁculty on emergent communi- cation tasks by varying the number of distractors, leading to negative effects on model performance during evaluation. Mu & Goodman (2021) attempted to improve interpretability of learned lan- guages in referential games by forcing speakers to communicate over sets of objects representing abstract visual concepts, and analyzed the compositionality of the ensuing emergent languages.
8 CONCLUSION AND FUTURE WORK
In this paper, we extend an existing computational framework that models the language acquisition process in an image referential game environment. Most notably, we add a ToM component to our speaker models, allowing our speakers to pragmatically rerank candidate utterances with a learned internal listener. We also experiment with increasing distractor difﬁculty by upweighting more se- mantically and visually similar distractor images. We ﬁnd that incorporating ToM into our speaker models leads to improvements in speaker performance. We also ﬁnd that incorporating harder dis- tractors leads to the development of more complex and ﬂuent languages.
Future work could measure the similarity of the learning process between human learners and our models, and whether the changes implemented in this paper lead to a more humanlike learning process. Further work could also consider the implications of more dynamic difﬁculty adjustment or curriculum design – for instance, studying whether models trained on a variety of distractor difﬁculties are able to adjust their utterances to ﬁt a context. Finally, we can study these effects in more complex environments by varying the listener architecture or by considering more difﬁcult settings, such as object referential games. We encourage the machine learning and psychological modelling communities to consider the further incorporation of ToM into computational models of language acquisition, which could help develop more pragmatically-aware models.
9


Published as a conference paper at ICLR 2023
REFERENCES
Jacob Andreas and Dan Klein. Reasoning about pragmatics with neural listeners and speakers. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 1: 1173–1182, 2016.
Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A new frontier for ai research. Artiﬁcial Intelligence, 280:103216, 2020.
Elika Bergelson and Richard Aslin. Semantic speciﬁcity in one-year-olds’ word comprehen- sion. Language Learning and Development, 13:1–21, 06 2017a. doi: 10.1080/15475441.2017. 1324308.
Elika Bergelson and Richard N. Aslin. Nature and origins of the lexicon in 6-mo-olds. Pro- ceedings of the National Academy of Sciences, 114(49):12916–12921, 2017b. doi: 10. 1073/pnas.1712966114. URL https://www.pnas.org/doi/abs/10.1073/pnas. 1712966114.
Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly, Beijing, 2009. ISBN 978-0-596-51649-9. doi: http://my.safaribooksonline.com/9780596516499. URL http://www.nltk.org/book.
Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, and Stephen Clark. Emergent
communication through negotiation. arXiv preprint arXiv:1804.03980, 2018.
Rahma Chaabouni, Florian Strub, Florent Altch´e, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent com- In International Conference on Learning Representations, 2022. URL munication at scale. https://openreview.net/forum?id=AUGBfDIV9rL.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
Matthew Honnibal, Ines Montani, Soﬁe Van Landeghem, and Adriane Boyd. spacy: Industrial-
strength natural language processing in python. 2020. doi: 10.5281/zenodo.1212303.
Katharina Kann, Sascha Rothe, and Katja Filippova. Sentence-level ﬂuency evaluation: References In Proceedings of the 22nd Conference on Computational Natural
help, but can be spared! Language Learning, pp. 313–323, 2018.
Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning
era. CoRR, abs/2006.02419, 2020. URL https://arxiv.org/abs/2006.02419.
Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the
emergence of (natural) language. arXiv preprint arXiv:1612.07182, 2016.
Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. CoRR, abs/2005.07064, 2020. URL https://arxiv.org/abs/2005.07064.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European
Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740–755. Springer, 2014.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pre- training approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.
Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, and Joelle Pineau. On the interaction between supervision and self-play in emergent communication. In International Conference on Learning Representations, 2019.
10


Published as a conference paper at ICLR 2023
Will Monroe, Robert X.D. Hawkins, Noah D. Goodman, and Christopher Potts. Colors in context: A pragmatic neural model for grounded language understanding. Transactions of the Association for Computational Linguistics, 5:325–338, 2017. doi: 10.1162/tacl a 00064. URL https: //aclanthology.org/Q17-1023.
Jesse Mu and Noah Goodman.
In M. Ran- zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 17994–18007. Curran Asso- ciates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 9597353e41e6957b5e7aa79214fcb256-Paper.pdf.
Emergent communication of generalizations.
Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Tom Grifﬁths. Evaluating In Proceedings of the 2018 Conference on Empiri- theory of mind in question answering. cal Methods in Natural Language Processing, pp. 2392–2400, Brussels, Belgium, October- November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1261. URL https://aclanthology.org/D18-1261.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic In Proceedings of the 40th Annual Meeting on Association evaluation of machine translation. for Computational Linguistics, ACL ’02, pp. 311–318, USA, 2002. Association for Computa- tional Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/ 1073083.1073135.
David Premack and Guy Woodruff. Does the chimpanzee have a theory of mind? Behavioral and
brain sciences, 1(4):515–526, 1978.
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In International conference on machine learning, pp. 4218– 4227. PMLR, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347.
Linda Smith and Chen Yu.
Infants rapidly learn word-referent mappings via cross-situational statistics. Cognition, 106(3):1558–1568, 2008. doi: https://doi.org/ ISSN 0010-0277. 10.1016/j.cognition.2007.06.010. URL https://www.sciencedirect.com/science/ article/pii/S0010027707001795.
Michael Tomasello. Constructing a Language: A Usage-Based Theory of Language Acquisition,
volume 1. Harvard University Press, 2005.
Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, and Gal Chechik. Context- aware captions from context-agnostic supervision. CoRR, abs/1701.02870, 2017. URL http: //arxiv.org/abs/1701.02870.
Chen Yu and Dana H. Ballard. A uniﬁed model of early word learning: Integrating statistical and social cues. Neurocomputing, 70(13):2149–2165, 2007. ISSN 0925-2312. doi: https://doi.org/ 10.1016/j.neucom.2006.01.034. URL https://www.sciencedirect.com/science/ article/pii/S092523120600508X. Selected papers from the 3rd International Confer- ence on Development and Learning (ICDL 2004) Time series prediction competition: the CATS benchmark.
11


Published as a conference paper at ICLR 2023
Embodied attention and word learning by toddlers. Cog- Chen Yu and Linda B. Smith. nition, 125(2):244–262, 2012. doi: https://doi.org/10.1016/j.cognition. ISSN 0010-0277. 2012.06.016. URL https://www.sciencedirect.com/science/article/pii/ S0010027712001369.
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it?, 2022. URL https://arxiv.org/abs/2210.01936.
Hao Zhu, Graham Neubig, and Yonatan Bisk. Few-shot language coordination by modeling theory of mind. In International Conference on Machine Learning, pp. 12901–12911. PMLR, 2021.
Hao Zhu, Yonatan Bisk, and Graham Neubig. Simulated language learning through communicative goals and linguistic input. Proceedings of the Annual Meeting of the Cognitive Science Society, 44:1351–1358, 2022.
A APPENDIX
A.1 FULL EFFECTS OF DISTRACTOR DIFFICULTY
In Table 2, we select the caption and hybrid distractors involved in the training of the most perfor- mant models to be reported. Here, as promised, we report the results of experiments over a fuller range of distractor variants.
Table 3: Performance and language features of speakers trained on all distractor variants.
Model Distractors
Performance Acc
POS F1
Fluency ADJ ADP NOUN VERB
Average Length
Base Gold Standard
0.81 0.92
1.50 2.52
0.16 1.00
0.52 1.00
0.41 1.00
0.38 1.00
8.97 10.79
Image
0.80
2.09
0.19
0.61
0.45
0.49
10.33
Caption CLIP Caption RoBERTa Caption RoBERTa TFIDF Caption TFIDF
0.83 0.83 0.86 0.83
1.87 1.93 2.01 2.05
0.18 0.20 0.19 0.21
0.57 0.60 0.62 0.63
0.46 0.48 0.49 0.46
0.43 0.47 0.48 0.45
9.56 10.06 10.04 10.30
Hybrid CLIP Hybrid RoBERTa Hybrid RoBERTa TFIDF Hybrid TFIDF
0.81 0.83 0.84 0.85
1.87 1.94 2.09 2.19
0.24 0.20 0.20 0.20
0.58 0.61 0.65 0.62
0.46 0.48 0.48 0.49
0.45 0.47 0.46 0.47
9.78 10.09 10.29 10.18
12