3 2 0 2
l u J
2 2
]
G L . s c [
1 v 3 6 0 2 1 . 7 0 3 2 : v i X r a
Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs
Qingyang Zhang1,2, Yiming Yang1, Jingqing Ruan1,2, Xuantang Xiong1,3, Dengpeng Xing1,3,∗, and Bo Xu1,2,3 1Institute of Automation, Chinese Academy of Sciences, Beijing, China 2School of Future Technology, University of Chinese Academy of Sciences, Beijing, China 3School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China {zhangqingyang2019, yangyiming2019, ruanjingqing2019, xiongxuantang2021, dengpeng.xing, xubo}@ia.ac.cn
Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance1. Our code is available at https://github.com/papercode2022/HILL.
Abstract—Goal-Conditioned
Hierarchical
latent
Index Terms—Hierarchical Reinforcement Learning, Explo- ration and Exploitation, Representation Learning, Landmark Graph, Contrastive Learning
I. INTRODUCTION
Balancing exploration and exploitation is a one of the major challenges in reinforcement learning. Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) [1, 2] is a promising paradigm that leverages task decomposition and temporal abstraction to address this challenge. GCHRL typ- ically consists of two hierarchical levels [3], where a high- level controller periodically decomposes the source task into subgoal conditional subtasks, and a low-level controller learns
to complete those subtasks by reaching the subgoals. The sub- goal sequence helps compress the exploration space, thereby reducing the exploration difficulty. It also enables efficient ex- ploitation by selecting subgoals with higher value estimations. A proper subgoal representation function is crucial for effec- tive GCHRL. Early efforts manually identify bottleneck states as subgoals [4, 5], which require task-specific knowledge. Recent approaches automatically learn subgoal representations in an end-to-end manner along with bi-level policies [6, 7, 8], making them more generic. However, they often overlook the temporal coherence in GCHRL. Temporal coherence refers to the hierarchical relationship in time between different levels of control in a system. The high-level controller operates at a slower timescale and focuses on radical changes in environmental states associated with the completion of the source task, while the low-level controller operates at a quicker timescale and focuses on modest changes in environmental states associated with completing subtasks. Approaches [9, 10] that consider the temporal abstraction perspective have shown great promise in improving exploration efficiency in GCHRL. The subgoal selection strategy is another crucial compo- nent of GCHRL. The balance of exploration and exploitation and the final performance are significantly impacted. If the high-level controller always selects subgoals that have ever yielded high rewards, it may miss out on potentially greater rewards. One line of work uses neural networks trained with environmental rewards as subgoal selection strategies [5, 6, 8], which enable efficient exploitation but often suffer from weak exploration. Another line implements high-level policies as planners, achieving efficient exploitation. They build environ- ment graphs or trees as world descriptors based on state tran- sitions, and subgoals are generated by planning on these world descriptors [11, 12]. However, this line typically builds graphs in state space, where the complexity of graph construction grows exponentially with the dimension of the state.
This work is supported in part by National Key R&D Program of China (No.2022ZD0116405), in part by the Program for National Nature Science Foundation of China (62073324), and in part by the Strategic Priority Research Program of the Chinese Academy of Sciences (No.XDA27030300).
Corresponding author. 1Our paper has been accepted by the conference of International Joint
Conference on Neural Networks (IJCNN) 2023.
This paper proposes a novel method to balance exploration and exploitation in HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL). HILL introduces a negative-power contrastive representation learning objective to train a subgoal representation function that con-


siders temporal coherence. The learned representations serve as possible subgoals, and landmarks are identified to maximize coverage of the explored latent space. Latent landmark graphs are built based on these landmarks, and two measures are defined on the graphs: a novelty measure upon nodes to encourage exploring novel landmarks and a utility measure upon edges to estimate the benefits of landmark transitions to task completion. HILL balances exploration and exploitation by simultaneously considering both measures and choosing the most valuable landmark as the subgoal. Empirical results demonstrate that HILL outperforms state-of-the-art (SOTA) baselines in numerous challenging continuous control tasks with sparse rewards based on the MuJoCo simulator [13]. Furthermore, we conduct visualization analyses and ablation studies to verify the significance of the various components of HILL. We highlight the main contributions below:
We introduce a contrastive representation learning objec- tive to train latent subgoal representations that comply with the temporal coherence in GCHRL.
We propose a latent landmark graph structure built on learned subgoal representations. Based on the graphs, we present a subgoal selection strategy that well tackles the exploration-exploitation dilemma in GCHRL.
Empirical
results demonstrate the superiority of our method over SOTA baselines in numerous continuous control tasks with sparse rewards.
II. RELATED WORK
Subgoal Representation Learning. Learning effective sub- goal representations remains a major challenge in GCHRL. Previous works either pre-define bottleneck states as sub- goals [4, 5] or use external rewards as supervision to train the high-level policy to generate subgoals [6, 14, 15]. The former requires task-specific knowledge and lacks general- ization, while the latter results in challenging exploration and low training efficiency. Recent works use variational autoencoders [16, 17] to extract low-dimensional features from observations. However, such features may include redundant information. Instead, it is more effective to focus on the key as- pects of variation that are essential for decision-making. Ghosh et al. [18] learn actionable representations that emphasize state factors inducing significant differences in corresponding ac- tions and de-emphasize irrelevant features. Two recent studies [9, 10] use slow feature analysis methods [19] to extract useful features as subgoals. However, they use L2 norm as a distance estimation when calculating triplet loss, which may lead to degenerate distributions [20]. In contrast, our method uses a negative-power contrastive representation learning objective to learn informative and robust subgoal representations.
Explore and Exploit in GCHRL. Efficient exploration in GCHRL can be achieved through a variety of methods, such as scheduling between intrinsic and extrinsic task policies [21], restricting the high-level action space to reduce exploration space [8], designing transition-based intrinsic rewards [22], or using curiosity-driven intrinsic rewards [23]. However, these methods rarely consider exploitation efficiency after sufficient
exploration. Efficient exploitation in GCHRL is typically achieved through planning [24, 25], where high-level graphical planners have shown great promise [11]. Some works use previously seen states in a replay buffer as graph nodes to generate subgoals through graph search. For example, Shang et al. [26] propose a method that unsupervisedly discovers the world graph and integrates it to accelerate GCHRL, while Jin et al. [27] use an attention mechanism to select subgoals based on the graphs. However, these methods construct graphs in the state space, where the graphs can be challenging to be built in high-dimensional state spaces. While Zhang et al. [28] leverage an auto-encoder with reachability constraints to learn a latent space and generate latent landmarks through clustering in this space, they ultimately decode the cluster centroids as landmarks, suggesting that they still plan in the state space. In contrast, our method builds graphs in the learned latent space, making it more generic and adaptable to state spaces of varying dimensions.
III. PRELIMINARIES A. Goal-Conditioned Hierarchical Reinforcement Learning
A finite-horizon, subgoal-augmented Markov Decision Pro- cess can be described as a tuple ⟨S, G, A, P, R, γ⟩, where S is a state space, G is a subgoal space, A is an action space, P : S ×A×S → [0, 1] is an environmental transition function, R : S × A → R is a reward function, and γ ∈ [0, 1) is a discount factor. We consider a two-level GCHRL framework, which comprises a high-level policy πθh(g|s) over subgoals given states and a low-level policy πθl(a|s, g) over actions given states and subgoals. The policy πθh operates at a slower timescale and samples a subgoal gt ∈ G when t ≡ 0 (mod c), for fixed c. The policy πθh is trained to optimize the expected cumulative discounted environmental rewards Eπθh ,πθl t=0 γtR(st, at)]. The policy πθl selects an action at ∼ πθl (·|st, gt) at every time step and is intrinsically re- warded with rl t(gt, φ(st)) = −D(gt, φ(st)) to reach gt, where D is a distance function. Following previous work [10], we employ L2 distance as D to provide dense non-zero rewards, thus accelerating low-level policy learning. The policy πθl is trained to optimize the expected cumulative intrinsic rewards Eπθl B. Universal Value Function Approximator
[(cid:80)∞
(cid:104)(cid:80)ci+c−1 t=ci
(cid:105)
rl t
, i = 0, 1, 2, . . . .
We use an off-policy algorithm Soft Actor-Critic (SAC) [29] as the base RL optimizer of both levels. The low-level critic is implemented as a Universal Value Function Approximator (UVFA) [30], which extends the concept of value function approximation to include both states and subgoals as inputs. Theoretically, a sufficiently expressive UVFA can identify the underlying structure across states and subgoals, and thus generalize to any subgoal.
We define a pseudo-discount function σ : S → [0, 1], which takes the double role of state-dependent discounting, and of soft termination, in the sense that σ(s) = 0 if and only if s is a terminal state according to the subgoal g. UVFAs can then be formalized as V (s, g) ≈ Vg,π∗ is the θl
(s), where π∗ θl


Fig. 1. An overview of HILL. HILL learns subgoal representations through a negative-power contrastive learning objective and selects subgoals by building latent landmark graphs. HILL identifies landmarks that maximize the explored latent space’s coverage and constructs latent landmark graphs based on them. HILL then measures novelties of nodes to encourage exploring novel landmarks and utilities of edges to estimate the benefits of transitions between landmarks to task completion. Finally, HILL uses a balanced subgoal selection strategy to tackle the exploration-exploitation dilemma by jointly considering both measures and selecting the most valuable landmark as the next subgoal. The representation function and bi-level policies are trained jointly online.
optimal low-level policy and Vg,πθl is a general value function representing the expected cumulative pseudo-discounted future pseudo-return for each g and any policy πθl :
Vg,πθl
(s) = E
(cid:20) ∞ (cid:88)
t=0
rl t(g, φ(st))
t (cid:89)
k=0
(cid:12) (cid:12) σ(sk) (cid:12) (cid:12)
(cid:21) . s0 = s
V (s, g) is usually implemented by a neural network for better generalization and is trained by the Bellman equation in a bootstrapping way.
IV. METHOD We introduce HILL in detail in this section, and its main
(1)
The function φ is trained with the bi-level policies and value functions jointly. To alleviate the non-stationarity caused by dynamically updated representations z, we adopt a regulariza- tion function Lr similar to HESS [10] to restrict the change of representations that already well-fit Equation 2:
Lr(st) = ||φ(st) − φold(st)||2.
We maintain a replay buffer Bp to record the newest losses of the sampled triplets calculated by Equation 2. When updating φ, we sample the top k triplets with the smallest losses and calculate their Lr to regularize its update. The overall subgoal representation learning loss is:
structure is depicted in Figure 1.
A. Contrastive Subgoal Representation Learning
Lφ(s) = Est∼Bl [Lc(st, st+1, st+c)] + Es′
t∼Bp [Lr(s′
t)],
We define a subgoal representation function φ : S → Rd which abstracts states s ∈ S to latent representations z ∈ Z. Assuming the high-level controller selects a subgoal every c time steps. According to the temporal coherence in GCHRL, the representations of high-level adjacent states (e.g., φ(st) and φ(st+c)) are distinguishable, while those of low-level ad- jacent states (e.g., φ(st) and φ(st+1)) are relatively similar. To train φ, we define a negative-power contrastive representation learning objective as follows:
where Bl is a replay buffer containing historical episodes.
B. Building Latent Landmark Graphs
At time steps t ≡ 0 (mod c), we build a latent landmark graph by randomly sampling K states from Bl, abstracting them into representations using φ, and store the state and its representation in pairs into a temporary buffer Bt. We then build a latent landmark graph that balances subgoal selection for effective exploration-exploitation trade-offs. Every time a new graph is built, Bt is reset.
Lc(st, st+1, st+c) =||φ(st) − φ(st+1)||2 2 1
+ β ·
||φ(st) − φ(st+c)||n
2 + ϵ
,
where n ≥ 1, β is a scaling factor, and ϵ > 0 is a small constant to avoid a zero denominator. The function φ abstracts the high-dimensional state space into a low- dimensional information-intensive latent space (i.e., subgoal space), significantly reducing the exploration difficulty.
(2)
1) Novelty-guided Exploration Based on Nodes: Sampling Nodes. We use the Farthest Point Sampling (FPS) [31] method to select a collection of representations from Bt that maximize the coverage of the latent space explored. These representa- tions, called landmarks and denoted as Lt, serve as nodes of the latent landmark graph and optional subgoals. We also add the representation of the task goal that the agent receives at the start of the current episode, as well as the representation
(3)
(4)


of the current state, to Lt. A single landmark is denoted as l ∈ Lt.
Novelty Measure upon Nodes. To identify novel landmarks that can guide the agent to states it has rarely visited before, we utilize the count-based method [32] and define a novelty measure N for any representation (including l ∈ Lt). This measure estimates the expected discounted future occupancies of representations starting from a given state si:
N (zi) =
(cid:88)
⌊(T −i)/c⌋ (cid:88)
γjη(zi+jc),
T ∈Bl
j=0
where zi = φ(si), T is the length of episode T and η is a hash table [33] that records the historical cumulative visit counts of representations. By leveraging the visit history of past episodes in Bl, N realizes a long-term estimate of novelty. A landmark l with smaller N (l) is considered more worth exploring. We update the hash table η at the end of each episode.
2) Utility-guided Exploitation Based on Edges: Connect- ing Nodes into Edges. We create two edges directed in reverse orders between each pair of latent landmarks li, lj ∈ Lt. Specifically, an edge wi,j is defined from li to lj. Utility Measure upon Edges. We define a utility measure U for any wi,j, which estimates the benefits of transitioning from li to lj in completing the source task:
U (wi,j) = Esx∈Bl [I(φ(sx) = li)V (sx, lj)], where I is an indicator function and V is the low-level UVFA. The measure U depends on the accuracy of V . However, learning a UVFA that accurately estimates V poses special challenges. The agent encounters limited combinations of states and subgoals (s, g), which can lead to unreliable estimates for unseen combinations. To address this issue, we consider that, starting from a point (i.e., a representation) in the latent space, representations within the neighborhood of this point are more likely to be visited, and thus, V provides accurate estimates. Therefore, we obtain U for non-adjacent landmarks by applying the Bellman-Ford [34] method:
U (wi,f ) = max
lj
[U (wi,j) + U (wj,f )]
= max lj ,...,lv
[U (wi,j) +
v−1 (cid:88)
x=j
U (wx,x+1) + U (wv,f )],
(7) where wi,f and wj,f are the edges of non-adjacent landmarks (li, lf ) and (lj, lf ), respectively, and wi,j, wx,x+1 and wv,f are the edges of adjacent landmarks (li, lj), (lx, lx+1) and (lv, lf ), respectively. A landmark lj with greater U (wi,j) is more worth exploiting. Moreover, we accelerate the convergence of V using Hindsight Experience Replay (HER) [35]. HER smartly generates more feedback for the agent by replacing unachievable subgoals with achieved ones in the near future, thus accelerating the training process.
C. Balance Exploration and Exploitation
We tackle the exploration-exploitation dilemma by selecting that well-balances N and U as the
a landmark from Lt
(5)
(6)
Algorithm 1 HILL algorithm Initialize: φ, πθh, πθl and p. 1: for i = 1...num episodes do for t = 0...T − 1 do 2:
3: 4: 5: 6: 7: 8:
if t ≡ 0 (mod c) then
Generate a random float value q ∈ [0.0, 1.0]. if 2q − 1 ≤ p then
Build a latent landmark graph. Select gt with the strategy in Equation 9.
else
Sample gt using πθh .
9: 10: 11: 12: 13: 14:
end if Update θh using SAC.
end if rt, st+1 ← execute at ∼ πθl (·|st, gt).
end for if i ≡ 0 (mod 100) then
15: 16: 17: 18: 19: 20: end for 21: return φ, πθh and πθl .
Update φ using Equation 4. Update p to the newest average train success rate.
end if Update θl and φ using SAC.
subgoal. When mapped to the [0, 1] distribution, U (wi,j) can be regarded as an incremental probability of success as it approximates the expected benefits of transitioning from li to lj. The distribution is as follows:
P(U (wi,j)) =
eU (wi,j ) y=1 eU (wi,y)
(cid:80)Y
,
where Y is the number of landmarks in Lt. Then we define a balanced strategy based on N and P and select a landmark with a smaller historical visit count and a larger transitional benefit as the subgoal gt of the next time interval:
gt = arg min
[(1 − P(U (wi′,j))) × N (lj)],
lj ∈Lt
where li′ = φ(st) and st is the state of the current time step. Algorithm 1 shows the pseudo-code of HILL. Note that we use the subgoal selection strategy in Equation 9 as a high- level teacher policy and use πθh modeled by a neural network as a high-level student policy. Both policies map states to subgoals and are employed with a probability of p and 1 − p, respectively. The role of the student policy is mainly to help increase randomness and prevent falling into local optimum. p is initialized to 0.5 and gradually increases to 1 in the training process. θh is trained with SAC using episodes in Bl.
V. EXPERIMENTS
A. Environmental Settings
We evaluate HILL on a suite of challenging continuous control tasks based on the MuJoCo simulator [13]. These tasks require the agent to master a combination of locomotion and object manipulation skills. The reward is sparse: the agent gets
(8)
(9)


(a) Ant Maze
(b) Ant FourRooms
(c) Ant Push
(d) HalfCheetah Hurdle
(e) HalfCheetah Climbing
(f) HalfCheetah Ascending
Fig. 2. The continuous control environments with sparse rewards for evaluating HILL and baselines.
3
1.0Success
5Steps ×1e6
4
0.2
0.6
0.8
0.0
1
0
2
0.4
0.8
2
0.2
0.0
12
0
8
0.4
0.6
14Steps ×1e6
4
10
6
1.0Success
0.6
0.8
3
0.4
0
1
4
0.0
1.0Success
5Steps ×1e6
0.2
2
(a) Ant Maze
(b) Ant FourRooms
(c) Ant Push
0.8
1.0
0.5
1.0Success
2.0
0.6
2.5Steps ×1e6
0.0
0.2
0.0
1.5
0.4
1.5
0.8
0.2
0.0
1.0Success
2.5
0.4
0.0
0.6
0.5
2.0
3.0Steps ×1e6
1.0
2.00Steps ×1e6
0.00
1.50
0.2
1.0Success
0.6
0.8
0.75
0.0
0.25
0.50
1.00
1.75
1.25
0.4
(e) HalfCheetah Climbing Fig. 3. Learning curves of HILL and baselines on MuJoCo tasks. The x-axis shows the time step of training, and the y-axis shows the average test success rate of 10 episodes. Each line is the mean of 5 runs with shaded regions corresponding to the 95% confidence interval.
(d) HalfCheetah Hurdle
(f) HalfCheetah Ascending
0 when reaching the goal and −1 otherwise. During training, the agent is initialized with random positions and is tested under the most challenging setting to reach the other side of the environment. We conduct experiments on six environments in MuJoCo, and their visualizations are presented in Figure 2. The maximum episode length is limited to 500 steps for the Ant Maze, Ant Push, HalfCheetah Hurdle, and HalfCheetah Ascending tasks and 1000 steps for the Ant FourRooms and HalfCheetah Climbing tasks.
B. Comparative Experiments
We compare HILL with the SOTA baselines, which include: (1) HESS [10]: a GCHRL method realizing active exploration based on stable subgoal representation learning. (2) LESSON [9]: a GCHRL method learning subgoal representations with slow dynamics. (3) HIRO [5]: a GCHRL method using off- policy correction for efficient exploitation. (4) SAC [29]: the base RL method we used for training the bi-level policies. Note that we use the original implementations of LESSON and HESS on all tasks.
The experimental results, as presented in Figure 3, demon- strate that HILL outperforms all baselines in terms of both sample efficiency and asymptotic performance. This success can be attributed to the use of efficient subgoal representations with temporal coherence and a subgoal selection strategy that effectively balances exploration and exploitation. In compari- son, HESS underperforms our method due to its focus on ex- ploring novel and reachable subgoals, lacking consideration of the potential benefits that subgoals can provide in completing the source task. LESSON yields an uptrend similar to HILL in Ant Push in the early stage. However, its unstable subgoal representations lead to a subsequent drop in performance. HIRO improves sample efficiency by relabeling subgoals to maximize the likelihood of past low-level action sequences. However, it lacks active exploration and fails to achieve a balance between exploration and exploitation. SAC performs poorly across all tasks, showing hierarchical advantages in solving long-term tasks with sparse rewards.


Fig. 4. Episodes in the state space and representations in the latent space at different learning stages in the Ant Maze task.
3.5
0.2
1.0Success
4.0Steps ×1e6
0.8
3.0
1.5
0.0
0.0
0.5
1.0
2.5
0.6
2.0
0.4
0.2
0.8
5Steps ×1e6
4
1.0Success
2
0.6
0.0
0.4
1
3
0
(a) Ant Maze
(b) Ant Push
(a) State DIST. (0.5M steps)
(b) State DIST. (1.5M steps)
Fig. 6. Ablation studies of critical modules. “B” denotes “Basic GCHRL”, “CSRL” denotes “Contrastive Subgoal Representation Learning”, “G-N” de- notes “building Graphs with the Novelty measure”, and “G-N-U” denotes “building Graphs with the Novelty measure and the Utility measure”.
stabilize and exhibit little difference. The active exploration in the early stage and adequate exploitation in the later stage demonstrate the effectiveness of our proposed balanced strategy.
(c) REP. DIST. (0.5M steps)
(d) REP. DIST. (1.5M steps)
Fig. 5. Visualization of exploration and exploitation in the Ant Maze task at 0.5 and 1.5 million steps, respectively. The x, y-axes are positions in the corresponding space, and the z-axis is the visit counts statistically obtained from the replay buffer with a capacity of 105. “REP.” denotes the word “representation” and “DIST.” denotes the word “distribution”.
C. Visualization Analysis
1) Exploration and Exploitation: We visualize the explo- ration and exploitation statuses by counting the visits of states in the state space and representations in the latent space, respectively, as shown in Figure 5. With the guidance of the novelty measure, HILL actively selects subgoals that are less explored, resulting in an extensive range of representations in the latent space and a dispersed distribution in the state space. As training progresses, the cumulative visits to various latent representations become uniform, and the utility measure enables more precise estimates and converges gradually. Con- sequently, episodes in the state and latent spaces gradually
2) Representation Learning: We run 10 episodes under the most challenging setting of the Ant Maze task and visualize their subgoal representation learning processes. We use a color gradient from purple to yellow to indicate the trend of an episode, as shown in Figure 4, where purple represents the start position, and yellow represents the end position. HILL learns representations with temporal coherence based on environmental transition relationships in an early stage. Adjacency states along an episode are clustered to similar latent representations, and ones multiple steps apart are pushed away in the latent space. These demonstrate the effectiveness and efficiency of the proposed negative-power contrastive representation learning objective. We conduct ablation studies on various components by incrementally adding critical modules to the basic GCHRL framework. The learning curves are shown in Figure 6. Basic GCHRL, which solely relies on SAC as the base RL optimizer of both levels, fails to learn policies efficiently due to the large exploration space and sparse rewards. The non-stationarity


0.2
1.0Success
4.0Steps ×1e6
0.0
0.0
1.5
3.0
2.5
1.0
0.3
0.5
2.0
0.4
0.4
3.5
0.8
0.6
0.5
0.8
0.2
3
1.0
0.0
1.5
0.6
2
2.0
5
3.0Steps ×1e6
0.5
2.5
0.0
0.4
0.8
0.2
1.0Success
(a) Stable Ratio k
(b) Representation Dimension d
1.0Success
0.2
n=1 β=0.1
0.8
0.0
0.0
n=1 β=10
n=2 β=0.1
n=2 β=1
n=1 β=1
1.5
0.6
3.0Steps ×1e6
n=2 β=10
2.5
1.0
0.4
2.0
0.5
0.0
0.2
c=50
c=10
0.0
2.5
3.0Steps ×1e6
0.6
2.0
1.0
c=100
1.5
c=20
1.0Success
0.8
0.5
0.4
(c) Scaling Factor β, Power n
(d) Subgoal Selection Interval c
1.5
3.0
3.5
2.5
0.6
0.0
0.8
0.0
0.4
0.2
1.0Success
4.0Steps ×1e6
FPS
w/o FPS
2.0
0.5
1.0
200
0.5
3.0Steps ×1e6
1.0Success
0.2
0.0
300
1.5
0.0
500
2.0
400
1.0
2.5
0.6
100
0.8
0.4
(e) Landmark Sampling Strategy
(f) Landmark Number
Fig. 7. Ablation studies of hyperparameter selection in the Ant Maze task.
introduced by joint training hinders bi-level policy updates. Our proposed contrastive representation learning objective compresses the exploration space by considering the temporal coherence in GCHRL (B+CSRL). The use of HER allevi- ates non-stationary issues and enables low-level policies to converge faster, resulting in better bi-level cooperation, thus improving exploration efficiency (B+CSRL+HER). Building latent landmark graphs and measuring novelties at nodes help agents actively explore novel subgoals (B+CSRL+HER+G-N). Finally, HILL (B+CSRL+HER+G-N-U) is obtained by consid- ering both the novelty and utility measures on latent landmark graphs. By selecting the most valuable subgoal that bal- ances the two measures, HILL realizes effective exploration- exploitation trade-offs, resulting in the fastest convergence speed and highest asymptotic performance.
E. Ablation Study on HyperParameters
We set up ablation studies on several typical hyperparam- eters of HILL in the Ant Maze task. Figure 7 shows the evaluation results. We conclude that our method is robust over an extensive range of hyperparameters.
1) Stable ratio k: Sampling the top k of triplets with lower representation losses helps to stably constrain changes of well- learned representations. However, a large k may disturb the update of subgoal representation learning, while a small k re- duces the effectiveness of stability regularization, as indicated in Figure 7 (a). We set k = 0.3 for all tasks.
2) Representation Dimension d: d determines the degree of information compression. The smaller d is, the more compact the representation abstracted by ϕ is. The results in Figure 7 (b) indicate that HILL achieves better performance when d = 2. Therefore, we set d to 2 for all tasks.
3) Scaling Factor β, Power n: β and n both affect the performance of subgoal representation learning. A smaller n or a larger β pushes negative instances further away. As shown in Figure 7 (c), HILL maintains robustness when β is small and achieves better performance when n = 1, β = 0.1. Therefore, we set n = 1, β = 0.1 for all tasks.
4) Subgoal Selection Interval c: c affects the subgoal selec- tion interval and the learning difficulty of the low-level policy. A smaller c results in faster convergence of the low-level pol- icy but makes high-level decision-making more challenging. This adversarial relationship is verified in Figure 7 (d). We set c = 50 for all tasks and baselines in our experiments.
5) Landmark Sampling Strategy: Figure 7 (e) shows that HILL converges faster with FPS than with uniform sampling. This is due to FPS’s ability to identify representations that maximize the coverage of the latent space, which facilitates the agent’s exploration of new regions. Therefore, FPS is adopted as the landmark sampling strategy for all tasks.
6) Landmark Number: The number of landmarks has a significant impact on the coverage of the latent space and the transition distance between nearby landmarks. Inadequate landmark numbers lead to poor coverage, while excessive numbers increase the burden of value estimation. The results in Figure 7 (f) indicate that HILL achieves better performance when the number is 400. Therefore, we use it as the landmark number for all tasks. VI. CONCLUSION
This work proposes to address the exploration and exploita- tion dilemma in hierarchical reinforcement learning by dynam- ically constructing latent landmark graphs (HILL). We propose a contrastive representation learning objective to learn subgoal representations that comply with the temporal coherence in GCHRL, as well as a latent landmark graph structure that bal- ances subgoal selection for effective exploration-exploitation trade-offs. Empirical results demonstrate that HILL signifi- cantly outperforms the SOTA GCHRL methods on numerous continuous control tasks with sparse rewards. Visualization analyses and ablation studies further highlight the effectiveness and efficiency of various HILL components.
REFERENCES
[1] P. Dayan and G. E. Hinton, “Feudal reinforcement learning,” Advances in Neural Information Processing Systems, vol. 5, 1992.
[2] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning,” Artificial intelligence, vol. 112, no. 1-2, pp. 181–211, 1999.
[3] A. G. Barto and S. Mahadevan, “Recent advances in hier- archical reinforcement learning,” Discrete event dynamic systems, vol. 13, no. 1, pp. 41–77, 2003.
[4] T. D. Kulkarni, K. Narasimhan, A. Saeedi et al., “Hierar- chical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,” Advances in neural information processing systems, vol. 29, 2016.


[5] O. Nachum, S. S. Gu, H. Lee et al., “Data-efficient hierarchical reinforcement learning,” Advances in Neural Information Processing Systems, vol. 31, 2018.
[6] A. S. Vezhnevets, S. Osindero, T. Schaul et al., “Feudal networks for hierarchical reinforcement learning,” in In- ternational Conference on Machine Learning, 2017, pp. 3540–3549.
[7] N. Dilokthanakul, C. Kaplanis, N. Pawlowski et al., “Feature control as intrinsic motivation for hierarchical reinforcement learning,” IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 11, pp. 3409–3418, 2019.
[8] T. Zhang, S. Guo, T. Tan et al., “Generating adjacency- constrained subgoals in hierarchical reinforcement learn- ing,” Advances in Neural Information Processing Sys- tems, vol. 33, pp. 21 579–21 590, 2020.
[9] S. Li, L. Zheng, J. Wang et al., “Learning subgoal representations with slow dynamics,” in International Conference on Learning Representations, 2020.
[10] S. Li, J. Zhang, J. Wang et al., “Active hierarchical ex- ploration with stable subgoal representation learning,” in International Conference on Learning Representations, 2021.
[11] B. Eysenbach, R. R. Salakhutdinov, and S. Levine, “Search on the replay buffer: Bridging planning and reinforcement learning,” Advances in Neural Information Processing Systems, vol. 32, 2019.
[12] S. Emmons, A. Jain, M. Laskin et al., “Sparse graphical memory for robust planning,” Advances in Neural In- formation Processing Systems, vol. 33, pp. 5251–5262, 2020.
[13] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based control,” in IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems, 2012, pp. 5026–5033.
[14] O. Nachum, H. Tang, X. Lu et al., “Why does hierarchy (sometimes) work so well in reinforcement learning?” arXiv:1909.10618, 2019.
[15] A. Levy, G. Konidaris, R. Platt et al., “Learning multi- level hierarchies with hindsight,” in International Con- ference on Learning Representations, 2019.
[16] A. P´er´e, S. Forestier, O. Sigaud et al., “Unsupervised learning of goal spaces for intrinsically motivated goal exploration,” arXiv preprint arXiv:1803.00781, 2018.
[17] S. Nair and C. Finn, “Hierarchical
foresight: Self- supervised learning of long-horizon tasks via visual sub- goal generation,” arXiv preprint arXiv:1909.05829, 2019. [18] D. Ghosh, A. Gupta, and S. Levine, “Learning actionable representations with goal-conditioned policies,” arXiv preprint arXiv:1811.07819, 2018.
[19] L. Wiskott and T. J. Sejnowski, “Slow feature analysis: Unsupervised learning of invariances,” Neural computa- tion, vol. 14, no. 4, pp. 715–770, 2002.
[20] L. Li, R. Yang, and D. Luo, “Focal: Efficient fully-offline meta-reinforcement learning via distance metric learning and behavior regularization,” in International Conference
on Learning Representations, 2020.
[21] J. Zhang, N. Wetzel, N. Dorka et al., “Scheduled intrin- sic drive: A hierarchical take on intrinsically motivated exploration,” arXiv:1903.07400, 2019.
[22] M. C. Machado, M. G. Bellemare, and M. Bowling, “Count-based exploration with the successor represen- tation,” in Proceedings of the AAAI Conference on Arti- ficial Intelligence, vol. 34, no. 04, 2020, pp. 5125–5133. [23] F. R¨oder, M. Eppe, P. D. Nguyen et al., “Curious hierarchical actor-critic reinforcement learning,” in Inter- national Conference on Artificial Neural Networks, 2020, pp. 408–419.
[24] K. Yamamoto, T. Onishi, and Y. Tsuruoka, “Hierarchical reinforcement learning with abductive planning,” arXiv preprint arXiv:1806.10792, 2018.
[25] J. Li, C. Tang, M. Tomizuka et al., “Hierarchical planning through goal-conditioned offline reinforcement learning,” arXiv:2205.11790, 2022.
[26] W. Shang, A. Trott, S. Zheng, C. Xiong, and R. Socher, “Learning world graphs to accelerate hierarchical rein- forcement learning,” arXiv:1907.00664, 2019.
[27] J. Jin, S. Zhou, W. Zhang, T. He, Y. Yu, and R. Fakoor, “Graph-enhanced exploration for goal-oriented reinforce- ment learning,” 2021.
[28] L. Zhang, G. Yang, and B. C. Stadie, “World model as a graph: Learning latent landmarks for planning,” in International Conference on Machine Learning, 2021, pp. 12 611–12 620.
[29] T. Haarnoja, A. Zhou, P. Abbeel et al., “Soft actor-critic: Off-policy maximum entropy deep reinforcement learn- ing with a stochastic actor,” in International Conference on Machine Learning, 2018, pp. 1861–1870.
[30] T. Schaul, D. Horgan, K. Gregor et al., “Universal value function approximators,” in International Conference on Machine Learning, 2015, pp. 1312–1320.
[31] S. Vassilvitskii and D. Arthur, “k-means++: The ad- the vantages of careful seeding,” in Proceedings of Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 2006, pp. 1027–1035.
[32] H. Tang, R. Houthooft, D. Foote et al., “# exploration: A study of count-based exploration for deep reinforcement learning,” Advances in Neural Information Processing Systems, vol. 30, 2017.
[33] M. S. Charikar, “Similarity estimation techniques from rounding algorithms,” in Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing, 2002, pp. 380–388.
[34] J. M. McQuillan and D. C. Walden, “The arpa network design decisions,” Computer Networks, vol. 1, no. 5, pp. 243–289, 1977.
[35] M. Andrychowicz, F. Wolski, A. Ray et al., “Hindsight experience replay,” Advances in neural information pro- cessing systems, vol. 30, 2017.