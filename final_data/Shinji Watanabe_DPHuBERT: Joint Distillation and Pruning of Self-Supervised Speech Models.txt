3 2 0 2
y a M 8 2
] L C . s c [
1 v 1 5 6 7 1 . 5 0 3 2 : v i X r a
DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Yifan Peng1, Yui Sudo2, Shakeel Muhammad2, Shinji Watanabe1
1Carnegie Mellon University, Pittsburgh, PA, USA 2Honda Research Institute Japan Co., Ltd., Saitama, Japan yifanpen@andrew.cmu.edu, {yui.sudo, shakeel.muhammad}@jp.honda-ri.com, shinjiw@ieee.org
Abstract
Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behav- ior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task- specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in al- most all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available. Index Terms: model compression, knowledge distillation, structured pruning, self-supervised learning
Experiments on SUPERB [6] show that our method outper- forms prior distillation methods in almost all tasks. Our method also performs well for various speech SSL models such as Hu- BERT Base [2], WavLM Base+ [4] and HuBERT Large [2], even with limited training resources. We will submit our re- sults to the SUPERB leaderboard and release the code and mod- els publicly for reproducibility: https://github.com/ pyf98/DPHuBERT.
2. Background and related work
2.1. Architectures of speech SSL
Speech SSL models such as wav2vec 2.0 [1], HuBERT [2] and WavLM [4] share a similar architecture. The model consists of a convolutional feature extractor (CNN) and a Transformer [26] encoder. The CNN has seven temporal convolutions with nor- malizations and activations. The Transformer encoder contains 12 layers with hidden size 768 for base models and 24 layers with hidden size 1024 for large models. Each layer is com- posed of a multi-head self-attention (MHA) and a position-wise feed-forward network (FFN).
1. Introduction
Self-supervised speech representation learning (speech SSL) has achieved remarkable results in various tasks [1–10]. How- ever, speech SSL models are usually large and slow, making them unsuitable for real-world applications with limited re- sources. Compressing speech SSL has become an important topic. A popular method is knowledge distillation [11], which trains a small student model to match the outputs of a large teacher model. Prior studies such as DistilHuBERT [12] and FitHuBERT [13] have achieved promising results with different student models. Another work [14] shows that the student archi- tecture affects its performance substantially, even if the model size is similar. However, in distillation methods, the student ar- chitecture is pre-specified and remains unchanged, which needs special expertise and might lead to suboptimal results. In con- trast, pruning [15, 16] automatically discovers a compact sub- network from a large model, which has been explored in nat- ural language processing (NLP) [17–20] and speech process- ing [21–25]. Previous pruning methods for speech SSL focus on specific downstream tasks such as automatic speech recognition (ASR) [24, 25] and spoken language understanding (SLU) [25]. It is unclear how they will perform in task-agnostic compres- sion, which is more challenging because the model needs to capture various aspects of speech including content, speaker, semantics and paralinguistics [6].
In this work, we propose DPHuBERT, a task-agnostic com- pression method based on joint Distillation and Pruning. It al- lows the student architecture to be learned during distillation.
2.2. Compression methods for speech SSL
Distillation. Distillation methods optimize a small student model to match the targets generated by a large teacher model. Since different layers of speech SSL capture different infor- mation [27], the student model needs to learn both final and intermediate representations of the teacher [12–14]. DistilHu- BERT [12] trains a shallow student model by mapping the last student layer to multiple intermediate teacher layers. FitHu- BERT [13] learns a deep and thin student model through layer- to-layer mapping. Another work [14] compares prediction- layer and layer-to-layer distillation using various student archi- tectures. It shows that the architecture of a student model af- fects its performance, even when the model size is kept similar. It also finds that deeper networks perform better with layer-to- layer distillation, likely because it explicitly aligns intermediate layers. These observations have inspired our work which allows the student architecture to evolve during distillation. Pruning. Pruning methods identify and remove redundant pa- rameters from a pre-trained model. Unstructured pruning re- moves individual parameters (e.g., a connection between neu- rons) by setting them to zero, which requires sparse matrix computation libraries to achieve actual speedup, whereas struc- tured pruning removes groups of parameters (e.g., an attention head or even an entire layer), which directly reduces the model size and computational cost. For speech SSL, PARP [24] is a magnitude-based unstructured pruning method which prunes It improves downstream tasks like the Transformer encoder.


FFN
FFN
CNN
Audio Waveform
Teacher(frozen)Student(prunable)…
Linear
FFN…
MHA
FFN
Linear
MHA
MHA
MHA
Linear
CNN
Linear
MHA
Linear
Linear
FFN
CNN
CNN
FFN
FFN
FFN
MHA
Teacher(frozen)Student(already pruned)……
Audio Waveform
MHA
MHA
(a) Step 1: jointly distill and prune the student model.
(b) Step 2: further distill the already pruned model.
Figure 1: Two training steps of our task-agnostic compression method, DPHuBERT. (a) The student model is initialized from the teacher model and is jointly distilled and pruned to produce a smaller model which meets a pre-specified sparsity ratio. (b) The already pruned student model is further distilled for better performance. To obtain DPHuBERT (24M) from HuBERT Base (95M), the two steps take around 18 and 6 GPU hours, respectively. (Dashed modules are prunable, i.e., their architectures can evolve during training.)
low-resource ASR. HJ-Pruning [25] is a structured pruning method that jointly prunes heterogeneous components (i.e., CNN and Transformer) of speech SSL models. It significantly reduces the total computation while retaining good performance in ASR and SLU. These methods deal with specific downstream tasks, but do not investigate the universal speech representa- tions. Our work focuses on task-agnostic structured pruning of speech SSL. As there is no labeled data for normal supervised training, we employ a distillation objective along with pruning. Once-for-all training. Compression methods typically gen- erate a single model with a pre-determined size. LightHu- BERT [28] deploys once-for-all training [29] to obtain numer- ous weight-sharing subnets, which shows very strong perfor- mance on SUPERB [6]. However, it requires an expensive two- stage training process and an advanced distillation loss inspired by data2vec [30]. According to the authors, compressing Hu- BERT Base already takes 2k GPUs hours (i.e., 62 hours with 32 V100 GPUs and 19 hours with 8 GPUs for two stages, respec- tively), which is prohibitive for academic researchers and small businesses. Unlike LightHuBERT, our work aims to compress an existing speech SSL model to a specific sparsity ratio within a manageable amount of training time, which is consistent with the standard setup of prior distillation methods [12, 13].
{0, 4, 8, 12} for base models and {0, 8, 16, 24} for large mod- els. The 0th layer is the output of CNN, which is also the input to the first Transformer layer. The loss function L measures the difference between two feature sequences, which can be L1, L2 or cosine distances [12–14]. We follow [12, 14] to use a combi- nation of L1 and cosine distances with equal weights.
3.3. Joint distillation and structured pruning
Structured pruning of the student model is formulated as learn- ing a sparse model through L0 regularization [31], which has been explored in NLP [19,20] and speech [25]. The method will be briefly introduced below. For more comprehensive deriva- tions, please refer to prior research [19, 20, 25, 31]. Consider a frozen teacher model f tea(·) and a student model f stu(·; θ) with learnable parameters θ = {θj}n j=1. Each θj is a group of prunable parameters (including convolution channels, atten- tion heads, and FFN intermediate units) and there are n groups in total. We define a binary variable zj (called mask) for each θj. The masks z follow a probability distribution q(z; α) with parameters α. The regularized distillation objective is:
min θ,α
Ez∼q
(cid:34)
1 D
D (cid:88)
Ldis (cid:16)
f tea(xk), f stu(xk; ˜θ)
(cid:17)
+ λ∥ ˜θ∥0
(cid:35)
,
k=1
3. DPHuBERT
(2)
3.1. Training procedure
Figure 1 illustrates our training procedure consisting of two steps. In Step 1, the student model is initialized from the teacher and is jointly distilled and pruned to generate a smaller model with a pre-specified size. In Step 2, the already pruned student model is further distilled to improve performance. In both steps, only unlabeled speech data are utilized and the teacher is frozen.
j=1 and each ˜θj = θjzj. The unlabeled dataset where ˜θ = {˜θj}n with D samples is {xk}D k=1. λ > 0 is the regularization weight. It is intractable to solve Eq. (2) using gradient descent due to the discrete nature of masks z. To make the loss differentiable, Louizos et al. propose a reparameterization trick which samples z from the Hard Concrete distribution [31]:
u ∼ U (0, 1), v(α) = sigmoid
(cid:18)(cid:18)
log
u 1 − u
+ log α
(cid:19)
/β
(cid:19)
3.2. Distillation loss
¯v(α) = (r − l) · v(α) + l, z = min(1, max(0, ¯v(α))),
Unlike DistilHuBERT [12], we use layer-to-layer distillation since the student initially has the same depth as the teacher (see Section 2.2 for discussions). Suppose the teacher has N tea Transformer layers with hidden size dtea and the student has N stu layers with hidden size dstu. Let Xtea (shape T × dtea) i and Xstu (shape T × dstu) be the output sequences of the i-th i Transformer layers from the teacher and student, respectively, where T is the sequence length. The distillation loss is:
where u follows a uniform distribution in [0, 1]. β is a constant. l < 0 and r > 0 are two constants to stretch v to [l, r], and it is further clamped to [0, 1]. Only α = {αj}n j=1 are learnable parameters in this distribution. With this trick, the objective in Eq. (2) is differentiable and the regularization term has a closed- form expression [31]:
(3)
Ldis =
(cid:88)
i∈S
L (cid:0)Xtea
i , Xstu
i Wi
(cid:1) ,
(1)
Ez∼q
(cid:104)
∥ ˜θ∥0
(cid:105)
=
n (cid:88)
j=1
sigmoid
(cid:18)
log αj − β log
−l r
(cid:19)
,
(4)
where S is a set of layers to match between the teacher and student models after a linear projection Wi. We use S =
which represents the (expected) model size as a differentiable function of current parameters α.
,


Table 1: Comparison of our method versus previous distillation methods on SUPERB [6]. Our DPHuBERT and DPWavLM are compressed from publicly available HuBERT Base [2] and WavLM Base+ [4] checkpoints, respectively.
Method
#Params KS
IC
PR
ASR w/o LM ER
QbE
SF
SID ASV
SD
Millions Acc↑ Acc↑ PER↓
WER↓
Acc↑ MTWV↑
F1↑ / CER↓ Acc↑ EER↓ DER↓
FBANK wav2vec 2.0 Base [1] HuBERT Base [2] WavLM Base+ [4]
0 95.04 94.68 94.70
41.38 9.65 96.23 92.35 96.30 98.34 97.37 99.00
82.01 5.74 5.41 3.92
23.18 6.43 6.42 5.59
48.24 63.43 64.92 68.65
0.0058 0.0233 0.0736 0.0988
69.64 / 52.94 20.06 88.30 / 24.77 75.18 88.53 / 25.20 81.42 90.58 / 21.20 89.42
9.56 6.02 5.11 4.07
10.05 6.08 5.88 3.50
Compressed models using LibriSpeech 960h
DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] 12-Layer Half [14] 3-Layer One [14] DPHuBERT (ours) DPWavLM (ours)
23.49 22.49 31.63 26.87 30.58 23.59 23.59
95.98 94.99 16.27 96.27 91.25 13.32 96.04 93.38 12.22 97.24 96.97 10.67 96.69 94.15 13.34 9.67 96.36 97.92 8.22 96.27 98.58
13.37 12.09 11.44 10.96 12.23 10.47 10.19
63.02 59.82 62.35 63.24 63.95 63.16 65.24
0.0511 0.0489 0.0475 0.0604 0.0489 0.0693 0.0874
82.57 / 35.59 73.54 84.06 / 32.46 55.71 86.65 / 29.40 64.71 86.11 / 30.93 69.52 82.89 / 34.65 75.71 86.86 / 28.26 76.83 87.68 / 26.11 82.11
8.55 8.00 6.65 6.13 6.48 5.84 5.98
6.19 6.84 6.44 6.81 6.56 5.92 5.53
Compressed models using LibriSpeech 100h
DistilHuBERT [12] FitHuBERT [13] FitW2V2 [13] DPHuBERT (ours)
23.49 22.49 22.49 23.57
93.17 96.23 94.20 14.05 94.68 90.03 16.50 96.36 97.42 10.02


14.77 12.66 14.77 11.38
61.67 62.87 62.78
0.0579 0.0380 0.0634
69.46 83.41 / 34.00 54.24 81.95 / 34.74 51.65 84.83 / 33.03 73.37

7.88 7.43 6.25
7.19 6.94 6.03
Now Eq. (2) can be solved to learn a sparse subnet, but the final sparsity cannot be precisely controlled [19, 20]. To explicitly control the final model size, prior studies [19, 20, 25] rewrite the optimization problem with an equality constraint:
s l e n n a h C
500
300
1
2
3
4 CNN Layer
5
6
7
min θ,α
Ez∼q
(cid:34)
1 D
D (cid:88)
Ldis (cid:16)
f tea(xk), f stu(xk; ˜θ)
(cid:17)
(cid:35)
k=1
s.t. s(α) = t,
where s(α) is the current sparsity (percentage of pruned param- eters) of the student model and t is a pre-specified target spar- sity. Note that s(α) can be computed based on Eq. (4) because the L0 norm counts the remaining parameters. The optimiza- tion objective in Eq. (5) can be further converted to a minimax problem using augmented Lagrangian [19]:
(5)
s d a e H
6 3 0
1
2
3
4
5
6
7
8
9
10 11 12
.
m r e t n I
1,500 1,000 500 0
MHA Layer
1
2
3
4
5
6
7
8
9
10 11 12
FFN Layer
Figure 2: CNN channels, attention heads and FFN intermedi- ate sizes of DPHuBERT compressed from HuBERT Base using LibriSpeech 960h. Their original sizes are 512, 12 and 3072, respectively. The target sparsity used for pruning is 75%.
max λ1,λ2
min θ,α
Ez∼q
(cid:34)
1 D
D (cid:88)
k=1
Ldis (cid:16)
f tea(xk), f stu(xk; ˜θ)
(cid:17)
(cid:35)
+ λ1 · (s(α) − t) + λ2 · (s(α) − t)2 ,
where λ1, λ2 ∈ R are Lagrange multipliers. This additional term penalizes the distillation loss and forces the student model to meet our target sparsity. Eq. (6) is our training objective for Step 1 (Figure 1a). For Step 2 (Figure 1b), the objective is sim- ply minimizing the distillation loss in Eq. (1) without any con- straint because the student architecture is already fixed.
4. Experiments
(6)
peak learning rates of main parameters θ and auxiliary param- eters α, λ are 2e-4 and 2e-2, respectively. Warmup and total steps are 15k and 50k, respectively. The target sparsity t is lin- early increased to the desired value in 5k steps, which facilitates training [25]. In Step 2, the peak learning rate is 1e-4. Warmup and total steps are 5k and 25k, respectively. In the default setup, the total training time of DPHuBERT is only 6 hours, i.e., 24 GPU hours (due to our usage of 4 GPUs). Evaluation. The SUPERB [6] benchmark consists of 10 tasks: keyword spotting (KS), intent classification (IC), phoneme recognition (PR), ASR, emotion recognition (ER), query by example (QbE), slot filling (SF), speaker identification (SID), automatic speaker verification (ASV) and speaker diarization (SD). We follow their default configurations in all tasks except that SID uses a learning rate of 5e-3.
4.1. Experimental setup
Toolkits. Our method is implemented with PyTorch [32] and TorchAudio [33]. Pre-trained SSL models are downloaded from fairseq [34] or Hugging Face [35]. Data. The unlabeled LibriSpeech 960h [36] is used by default. In Section 4.2 and Table 1, the train-clean 100h subset is also used to investigate the effect of training data size. Model. In the default setup, we compress HuBERT Base [2]. To verify the generalizability, we also compress WavLM Base+ [4] in Section 4.2 and HuBERT Large [2] in Section 4.5. Training. DPHuBERT is trained on 4 NVIDIA A100 (40GB) GPUs with 640 seconds of audio per mini-batch. In Step 1, the
4.2. Main results
Table 1 compares various methods on SUPERB [6]. DPHu- BERT is compressed from HuBERT Base. With LibriSpeech 960h, DPHuBERT outperforms pure distillation methods (in- cluding DistilHuBERT [12], FitHuBERT [13], FitW2V2 [13] and two best-performing models from [14]) in 8 out of 10 tasks. This shows that DPHuBERT better preserves the general speech representations of the teacher model, covering content, speaker and semantics. With only 100h training data, DPHuBERT still performs much better than prior methods in almost all tasks.


Table 2: Ablation studies on DPHuBERT compressed from HuBERT Base using LibriSpeech 960h. SD
#Params KS
IC
PR
QbE
ASR w/o LM ER
SF
SID ASV
Method
Millions Acc↑ Acc↑ PER↓
WER↓
Acc↑ MTWV↑
F1↑ / CER↓ Acc↑ EER↓ DER↓
DPHuBERT
w/o training step 2 w/ pred-layer distill w/o pruning CNN
23.59 23.59 23.65 23.59
96.36 97.92 9.67 94.87 96.76 10.42 95.55 94.54 16.09 9.63 96.30 97.60
10.47 11.55 14.65 11.00
63.16 62.54 59.06 63.16
0.0693 0.0624 0.0519 0.0717
86.86 / 28.26 76.83 86.12 / 30.15 71.42 81.83 / 36.44 59.88 85.77 / 29.06 75.22
5.84 6.36 7.32 6.13
5.92 6.65 6.75 6.10
) ↑ (
c c A %
98
96
94
HuBERT Base DistilHuBERT DPHuBERT (ours)
) ↓ (
R E W %
15
10
HuBERT Base DistilHuBERT DPHuBERT (ours)
) ↑ (
c c A %
80
70
60
HuBERT Base DistilHuBERT DPHuBERT (ours)
5
10
70
30
90
30
50 Params (M)
50 Params (M) (a) IC
70
10
10
90
30
50 Params (M) (c) SID
(b) ASR Figure 3: DPHuBERT trained with different target sparsities. Models are compressed from HuBERT Base using LibriSpeech 960h. Table 3: Compressing HuBERT Large to have a similar size as HuBERT Base using LibriSpeech 960h. ASR w/o LM ER
#Params
SD
IC
PR
KS
SF
QbE
SID
ASV
Method
70
90
Millions Acc↑ Acc↑
PER↓
WER↓
Acc↑ MTWV↑
F1↑ / CER↓ Acc↑ EER↓ DER↓
HuBERT Large HuBERT Base
316.60 94.68
95.29 96.30
98.76 98.34
3.53 5.41
3.62 6.42
67.62 64.92
0.0353 0.0736
89.81 / 21.76 88.53 / 25.20
90.33 81.42
5.98 5.11
5.75 5.88
DPHuBERT
94.59
94.51
98.47
4.46
6.23
65.11
0.0246
88.37 / 24.60
83.17
7.05
5.79
Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models.
Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37–40]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2.
4.3. Ablation studies
This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42].
4.4. Results at various sparsities
We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks.
4.5. Compressing HuBERT Large
Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks.
Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student’s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes).
5. Conclusion
This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance.
6. Acknowledgements
We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.


7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representa- tions,” in Proc. NeurIPS, 2020.
[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, “HuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021.
[3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, “XLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,” in Proc. Interspeech, 2022.
[4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self- supervised pre-training for full stack speech processing,” IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505–1518, 2022.
[5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, “Unsupervised
speech recognition,” in Proc. NeurIPS, 2021.
[6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Inter- speech, 2021.
[7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe, T. N. Sainath, and S. Watanabe, “Self-supervised speech representation learning: A review,” IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1179–1210, 2022.
[8] X. Chang, T. Maekaku, P. Guo, J. Shi, Y.-J. Lu, A. S. Subrama- nian, T. Wang, S.-w. Yang, Y. Tsao, H.-y. Lee et al., “An explo- ration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021.
[9] Z. Huang, S. Watanabe, S.-w. Yang, P. Garc´ıa, and S. Khudanpur, “Investigating Self-Supervised Learning for Speech Enhancement and Separation,” in Proc. ICASSP, 2022.
[10] Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, S. Dalmia, X. Chang, and S. Watanabe, “A Study on the Integra- tion of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding,” in Proc. SLT, 2022.
[11] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in
a neural network,” arXiv:1503.02531, 2015.
[12] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,” in Proc. ICASSP, 2022.
[13] Y. Lee, K. Jang, J. Goo, Y. Jung, and H. R. Kim, “FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models,” in Proc. Interspeech, 2022.
[14] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, “Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,” in Proc. Interspeech, 2022.
[15] R. Reed, “Pruning algorithms-a survey,” IEEE Trans. on Neural
Networks, vol. 4, no. 5, pp. 740–747, 1993.
[16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking
the value of network pruning,” in Proc. ICLR, 2019.
[17] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really bet-
ter than one?” in Proc. NeurIPS, 2019.
[18] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on
demand with structured dropout,” in Proc. ICLR, 2020.
[19] Z. Wang, J. Wohlwend, and T. Lei, “Structured Pruning of Large
Language Models,” in Proc. EMNLP, 2020.
[20] M. Xia, Z. Zhong, and D. Chen, “Structured Pruning Learns Com-
pact and Accurate Models,” in Proc. ACL, 2022.
[21] P. Dong, S. Wang, W. Niu et al., “Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition,” in ACM/IEEE Design Automation Conference (DAC), 2020.
[22] S. Wang, P. Lin, R. Hu et al., “Acceleration of LSTM With Struc-
tured Pruning Method on FPGA,” IEEE Access, 2019.
[23] K. Tan and D. Wang, “Compressing Deep Neural Networks for
Efficient Speech Enhancement,” in Proc. ICASSP, 2021.
[24] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass, “PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recog- nition,” in Proc. NeurIPS, 2021.
[25] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, “Structured Pruning of Self-Supervised Pre-trained Models for Speech Recog- nition and Understanding,” in Proc. ICASSP, 2023.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. NeurIPS, 2017.
[27] A. Pasad, J.-C. Chou, and K. Livescu, “Layer-Wise Analysis of a Self-Supervised Speech Representation Model,” in Proc. ASRU, 2021.
[28] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li, “LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,” in Proc. Interspeech, 2022.
[29] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one network and specialize it for efficient deployment,” in Proc. ICLR, 2020.
[30] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec: A general framework for self-supervised learning in speech, vision and language,” in Proc. ICML, 2022.
[31] C. Louizos, M. Welling, and D. P. Kingma, “Learning Sparse Neu-
ral Networks through L0 Regularization,” in ICLR, 2018.
[32] A. Paszke et al., “Pytorch: An imperative style, high-performance
deep learning library,” Proc. NeurIPS, 2019.
[33] Y.-Y. Yang et al., “Torchaudio: Building blocks for audio and
speech processing,” arXiv:2110.15018, 2021.
[34] M. Ott et al., “fairseq: A fast, extensible toolkit for sequence mod-
eling,” in Proc. NAACL-HLT: Demonstrations, 2019.
[35] T. Wolf et al., “Huggingface’s transformers: State-of-the-art natu- ral language processing,” arXiv preprint arXiv:1910.03771, 2019.
[36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib- rispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015.
[37] S. Zhang, E. Loweimi, P. Bell, and S. Renals, “On the usefulness of self-attention for automatic speech recognition with transform- ers,” in Proc. SLT, 2021.
[38] K. Shim, J. Choi, and W. Sung, “Understanding the role of self
attention for efficient speech recognition,” in Proc. ICLR, 2022.
[39] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, “Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022.
[40] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, “Attention Weight Smoothing Using Prior Distributions for Transformer- Based End-to-End ASR,” in Proc. Interspeech, 2022.
[41] T.-Q. Lin, H.-y. Lee, and H. Tang, “MelHuBERT: A simplified HuBERT on Mel spectrogram,” arXiv:2211.09944, 2022.
[42] F. Wu, K. Kim, J. Pan, K. J. Han, K. Q. Weinberger, and Y. Artzi, “Performance-efficiency trade-offs in unsupervised pre-training for speech recognition,” in Proc. ICASSP, 2022.