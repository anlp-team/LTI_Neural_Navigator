OntheInteractionsofStructuralConstraintsandDataResourcesforStructuredPredictionZhisongZhang,EmmaStrubell,EduardHovyLanguageTechnologiesInstitute,CarnegieMellonUniversityzhisongz@cs.cmu.edu,strubell@cmu.edu,hovy@cmu.eduAbstractInthiswork,weprovideananalysisontheinteractionsoftheeffectivenessofdecodingwithstructuralconstraintsandtheamountofavailabletrainingdataforstructuredpredic-tiontasksinNLP.Ourexplorationadoptsasimpleprotocolthatenforcesconstraintsuponconstraint-agnosticlocalmodelsattestingtime.Withevaluationsonthreetypicalstructuredpredictiontasks(namedentityrecognition,de-pendencyparsing,andeventargumentextrac-tion),wefindthatmodelstrainedwithlessdatapredictoutputswithmorestructuralvi-olationsingreedydecodingmode.Incorporat-ingconstraintsprovidesconsistentperformanceimprovementsandsuchbenefitsarelargerinlowerresourcescenarios.Moreover,therearesimilarpatternswithregardtothemodelsizesandmoreefficientmodelstendtoenjoymorebenefits.Finally,wealsoinvestigatesettingswithgenretransferanddiscoverpatternsthatarerelatedtodomaindiscrepancies.1IntroductionRecently,neuralmodels,especiallythosebasedonpre-trainedcontextualizedrepresentations,havebroughtimpressiveimprovementsforavarietyofstructuredpredictiontasksinNLP(Devlinetal.,2019;Kulmizevetal.,2019;ShiandLin,2019;Lietal.,2020a).Moreinterestingly,theincorporationofpowerfulneuralmodelsseemstodecreasethepo-tentialbenefitsbroughtbymorecomplexstructuredoutputmodeling.Forexample,forsequencelabel-ing,ithasbeenshownthatreasonablygoodperfor-mancecouldbeobtainedevenwithoutanyexplicitmodelingoftheinteractionsoftheoutputtags(Tanetal.,2018;Devlinetal.,2019).Fordependencyparsing,modelsthatignoretreeconstraintsandcasttheproblemasheadselectionintrainingcanstillobtainimpressiveresults(DozatandManning,2017).Mostofthesepreviousresultsareobtainedinfullysupervisedsettings.Whiletheyshowthatwithabundanttrainingsignals,betterinputmodel-ingandrepresentationlearningcouldshadowthebenefitsbroughtbymorecomplexstructuredmod-eling,itremainsunclearforthecaseswheredataresourcesarelimited.Oneofthemostsalientandimportantpropertiesofstructuredpredictionisthattheoutputobjectsshouldfollowspecificstructuralconstraints.Forexample,theoutputofasyntacticparsershouldbeawell-formedtreeandtheoutputlabelsofaninformationextractionsystemneedtofollowcer-taintyperestrictions.Inthiswork,wefocusonthefacetofstructuralconstraintsandexploreitsinfluenceonstructuredpredictionproblemsunderscenarioswithdifferentamountsoftrainingdata.Ontheonehand,sinceweknowthetargetoutputsshouldconformtocertainconstraints,explicitlyenforcingtheseconstraintswilllikelybringben-efitsandsometimesevenbearequirement.Ontheotherhand,asneuralmodelsaredevelopedtobetterrepresentinputcontexts,theymightalreadybeabletoimplicitlycapturetheoutputconstraintsbylearningfromthedata.Inparticular,itwouldbeunsurprisingthatthemodelcoulddirectlyproduceoutputsthatconformtoconstraintswithoutexplicitenforcement,givenenoughtrainingdata,sincethetraininginstancesarepresentedassuch.Regardingtheinteractionsbetweenexplicitin-corporationofconstraintsandtheamountoftrain-ingdata,weaskthefollowingthreeresearchques-tions(RQs),whichweaimtoexploreinthiswork:RQ1:Whatistheinfluenceofconstraintswithdifferentamountsoftrainingdata?Withpowerfulneuralnetworksandabundanttrain-ingdata,themodelcanbetrainedtoimplicitlycapturestructuralconstraintsevenwithoutexplicitenforcement.Nevertheless,itstillremainsunclearforthecaseswithlimiteddata.Weaimtoexplorehowtheincorporationofconstraintsinfluencestheoutputsandhowsuchinfluenceschangewithdif-
147 Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pages 147–157 July 13, 2023 ©2023 Association for Computational Linguistics


Pt′∈Texpscore(t′|wi)Here,thescore(·)functionisrealizedasalinearlayerstackeduponthewordrepresentations1andTdenotestheoutputtagspace.WiththeBIOtaggingscheme,therearehardcon-straintsbetweentagsofconsecutivetokens:TheItagmustfollowaBorItagofthesameentitytype.Forexample,thetaggedsequence“OI-MISCI-MISCOO”iserroneousbecausethetransition“O→I-MISC”isillegal.Onesolutiontomitigatethisproblemistoforbidsuchillegaltransitionsindecoding.ThiscanbeachievedbyincorporatingatransitionmatrixM∈R|T|×|T|,wheretheentriescorrespondingtoillegaltagtransitionsarefilledwith−∞andthelegalonesarefilledwith0.Forthedecodingprocess,wedefinethescoreofatagsequenceas:s(t1,t2,···,tn)=Xilogp(ti|wi)+XiMti,ti+1Inthisway,thehighestscoringtagsequencewillnotcontaintransitionviolations.Thisdecoding
ferentamountsoftrainingdata.RQ2:Whatistheinfluenceofconstraintswhenusingmoreefficientmodels?Althoughneuralmodelscanobtainimpressivere-sults,oneshortcomingisthattheyareusuallycomputationallyexpensive.Recently,therehavebeenmanyworksonimprovingmodelefficiency.Knowledgedistillationisoneofthemostwidely-utilizedmethods,learningasmallerstudentmodelfromalargerteachermodel(KimandRush,2016;Sanhetal.,2019;Jiaoetal.,2020).Aninterest-ingquestiontoexploreishowthesemoreefficientmodelsinteractwiththeexplicitincorporationofstructuralconstraints.RQ3:Whatistheinfluenceofconstraintsforout-of-domaingeneralization?Weusuallyexpectthemodeltobeabletogener-alizetoscenariosthatcanbedifferentfromthoserepresentedbythetrainingdata,forexample,todif-ferentdomainsortextgenres.Itwillbeinterestingtoexplorehowtheconstraintsinfluencepredictionsforthesecasesandespeciallywhethertherearespecificpatternswithregardtothediscrepanciesbetweenthesourceandthetarget.Toanswerthesequestions,weconductextensiveexperimentsonthreetypicalstructuredpredictiontasks,includingnamedentityrecognition(NER),dependencyparsing(DPAR)andaninformationextractiontaskofeventargumentextraction(EAE).Wefindthatmodelstrainedwithlesstrainingdatatendtoproduceoutputsthatcontainmorestructuralviolationswhenusingconstraint-agnosticgreedydecoding.Furtherapplyingconstraineddecodingbringsconsistentperformanceimprovementsandthebenefitsaremoreprominentinlowerdatasce-narios(§3.2).Asimilartrendcanbefoundwithregardtomodelsize:Smallermodelstendtooutputmoreviolationswithgreedydecodingandbenefitmorefromconstraineddecoding(§3.3).Finally,incross-genresettings,wefindaweakpatternwithregardtogenrediscrepancies:Morestructuralvio-lationstendtobemadewithgreedydecodingwhentransferringtomoredistantgenres(§3.4).2TasksandModels2.1NamedEntityRecognitionOurfirsttaskisnamedentityrecognition(NER),whichaimstoextractentitymentionsfromrawtextsandcanbetypicallycastasasequencela-belingproblem.WeadoptasimpleNERmodelNERTransportORGO I-MISC I-MISC O ODPARw1 w2 w3 w1 w2 w3 w4EAEOrigin
1Ifawordissplitintomultipletokens,wesimplytakeitsfirstsub-token.
Figure1:Examplesofstructuralviolations(markedinred).ForNER,thetagtransitionfrom‘O’to‘I-MISC’isillegal.ForDPAR,theleftsubtreecontainsaloopwhiletherightonehascrossingedges.ForEAE,theORIGINrolecannotbeassignedtoanORGentity.thatutilizesapre-trainedBERTmodelastheen-coderandasoftmaxlayertopredicttheoutputtags.WeadoptthetypicalBIOtaggingscheme(RamshawandMarcus,1995),specifyingtagsfortheBeginning,theInsideandtheOutsideofanentityspan.Morespecifically,foraninputsequenceofwords[w1,w2,···,wn],ourmodelaimstoassignase-quenceofBIOtags[t1,t2,···,tn]forthem.Theprobabilityofeachoutputtagislocallynormalizedforeachword:p(ti|wi)=expscore(ti|wi)
148


Pr′∈R∪{ϵ}expscore(r′|mt,me)Here,Rdenotestherolelabelingspaceandwefurtherincludeanoptionofϵtodenotetherearenoargumentrelationsbetweentheeventtriggerandentitymention.Thescorefunctionisrealizedwithabiaffinemodulethatproducesargumentscoresfortheinputmentionpair.Sinceamentionmaycontainmultiplewords,weconcatenatethewordrepresentationsofthestartingandendingwordstoformthemention’sinputvector.Ineventextraction,thereareconstraintsonthemention(eventandentity)typesandargumentrole
Ph′∈{R,w1,w2,···,wn}expscore(h′|wi)HereweaddanartificialtargetRtotheoutputspacetocoverthecaseofrootnodes.Thescore(·)functionisrealizedwithabiaffinemodulethatproduceshead-modifierscoresfortheinputpairofwords.Weconsidertwoconstraintsfortheoutputstruc-tures.First,thereshouldnotbeanycyclesintheoutputgraphs,otherwise,theywillnotbetrees.Moreover,weconsidertheprojectivityconstraint,2whichspecifiesthattherearenoedgesthatcrosseachother.WeadoptEisner’salgorithm(Eisner,1996)fortheconstraineddecoding,whichisady-namicprogrammingalgorithmthatsearchesthehighestscoredtreesintheconstrainedoutputspace.Ifnotconsideringanyoftheseconstraints,wegreedilypredicttheheadwordforeachtokenbasedontheheadclassificationprobabilities.2.3EventArgumentExtractionFinally,weconsidereventargumentextraction(EAE),aninformationextractiontaskthataimstoextractargumentsfortheeventmentionsfromthetexts(Ahn,2006).Forapairofeventtriggerandentitymention,thistaskaimstolinkthemwithanargumentroleindicatingthattheentitycanplaysucharoleintheeventframe.Ifnosuchroleispossible,thennolinksareadded.Weagainadoptapre-trainedBERTencoderforencodingandfurtherstackatask-specificpredictor,whichisabiaffinescorer,similartodependencyparsing.Themaindifferenceisthathereweperformlocalnormaliza-tionforeachevent-entitypairsincetherearenoconstraintsonhowmanyothermentionsthatonementioncanbelinkedtoforeventargumentextrac-tion.Tobetterexplorerealapplicationscenarios,wetrainanextrasequencelabelertoextracteventandentitymentionsratherthanusinggoldmen-tions.ThismentiondetectionmodelisthesameastheonedescribedinourNERexperiments.Morespecifically,ourmodeltakesapairofeventtriggerandentitymention(mtandme)andassignstheprobabilitiesofargumentrolestothem:p(r|mt,me)=expscore(r|mt,me)
problemcanbesolvedefficientlybytheViterbialgorithm(Viterbi,1967).Ifnotenforcingtheseconstraints,thesecondtermofthesequencescorecanbedroppedandthedecodingwillbegreedilyfindingthemaximally-scoredtagforeachtokenindividually.Noticethatthistreatmentresemblesconditionalrandomfield(CRF)basedmodels(Laffertyetal.,2001),whereinthemaindifferenceisthatweuti-lizealocallynormalizedmodelandthetransitionmatrixismanuallyspecifiedtoexcludeillegaltran-sitions.Inourpreliminaryexperiments,wealsotriedCRFmodelsbutdidnotfindobviousbenefitscomparedtolocalmodelswhenadoptingthesameunderlyingpre-trainedmodel.2.2DependencyParsingWefurtherconsiderdependencyparsing(DPAR)(Kübleretal.,2009),whichaimstoparsethein-putsentenceintowell-formedtreestructures.Weadoptthewidelyutilizedfirst-ordergraph-basedparser(McDonaldetal.,2005).SimilartoNER,weadoptthepre-trainedBERTencodertoprovidethecontextualizedrepresentationsfortheinputtokensandstackabiaffinescorer(DozatandManning,2017)toassignscoresforthedependencyedges.Fortraining,weadoptalocalmodelthatviewstheproblemasahead-findingclassificationtaskforeachinputtoken(DozatandManning,2017;Zhangetal.,2017).Attestingtime,wefurtherconsidertreeconstraintswithspecificdecodingalgorithms.Sincewearemainlyinterestedinstruc-turaltreeconstraints,weonlyperformunlabeledparsing.Morespecifically,foraninputsequenceofwords[w1,w2,···,wn],weaimtofindthedependencyheadwords[h1,h2,···,hn]fortheinputwordsequence.Withlocalnormalization,thiscanbeviewedasaheadclassificationproblem:p(hi|wi)=expscore(hi|wi)
2WeonlyperformexperimentsonEnglish,whichisahighlyprojectivelanguage.Extensionstonon-projectivelan-guagesarelefttofuturework.
149


train
train
train
3https://www.clips.uantwerpen.be/conll2003/ner/4https://universaldependencies.org/5https://catalog.ldc.upenn.edu/LDC2013T196https://catalog.ldc.upenn.edu/LDC2006T067http://blender.cs.illinois.edu/software/oneie/
Split
Table1:Datastatisticsofthedatasetsutilizedinourmainexperiments.labels.Forexample,thePERSONroleofaMARRYeventshouldhavetheentitytypeofPER,whiletheDESTINATIONorORIGINrolesofaTRANSPORTshouldhaveentitytypesdenotingplaces(GPE,LOCorFAC).Weadoptasimplemethodtoin-corporatesuchconstraintsindecodingbyignoring(maskingout)therolesthatarenotpossibleac-cordingtotheeventandentitytypes.Theroleconstraintsaremanuallycollectedaccordingtotheeventannotationguideline(LDC,2005).Ifnotconsideringtheseroleconstraints,wesimplyadoptgreedypredictionforeachevent-entitypair.3Experiments3.1SettingsData.OurexperimentsareconductedonwidelyutilizedEnglishdatasets.Inourmainexperiments,weadopttheCoNLL-2003Englishdataset3(TjongKimSangandDeMeulder,2003)forNERandtheEnglishWebTreebank(EWT)fromUniversalDe-pendencies4v2.10(Nivreetal.,2020)forDPAR.InthegenretransferexperimentsforNERandDPAR,weutilizeOntoNotes5.0dataset5(Weischedeletal.,2013)andsplitthedataaccordingtotextgenres.Fortheeventtask,weadopttheACE05dataset6(Walkeretal.,2006),usingthescriptsfromLinetal.(2020)forthepre-processing.7Ta-ble1showsthedatastatistics.Modelandtraining.Unlessotherwisespecified,weadoptthepre-trainedBERTbaseasthecontex-tualizedencoderforourmodels.Theencoderisfined-tunedwiththetask-specificdecodersinalltheexperiments.Thenumberofmodelparame-tersisaround110M.Wefollowcommonpractices
Task
3.5K46.4K-5.6K--
Model
DPAR
2.0K25.1K----
2.5K34.5K0.5K6.0K0.7K0.8K
84.730.188.920.491.380.2
ACE05
NER
Local
Local
84.270.888.910.691.240.3
82.650.388.920.391.650.2
test
test
test
Global
Global
3.3K51.4K-5.9K--
2.1K25.1K----
12.5K204.6K----
4.0K61.5K1.1K10.8K1.7K1.7K
5K20K100K
Data
14.0K203.6K-23.5K--
UD-EWT
14.4K215.2K3.7K38.0K5.7K6.2K
CoNLL03
84.570.189.460.291.950.1
dev
dev
dev
#Sent.#Token#Event#Entity#Argument#Relation
Table2:ComparisonsbetweenlocalandglobalmodelsforNER(F1%)andDPAR(UAS%).Numbersinthesubscriptsdenotestandarddeviation.forthesettingsofotherhyper-parameters.Adam(KingmaandBa,2014)isutilizedastheoptimizer.Thelearningrateisinitiallysetto1e-5forNERand2e-5forDPARandEAE.Itisfurtherlinearlydecayedto10%oftheinitialvaluethroughoutthetrainingprocess.Themodelsaretrainedfor20Kstepswithabatchsizeofaround512tokens.Wepickfinalmodelsbytheperformanceonthedevel-opmentsetofeachtask.Theoriginaldevelopmentsetsarealsodown-sampledaccordinglyasthetrain-ingsetstosimulatescenarioswithdifferentdataamounts.Allthereportedresultsareaveragedoverfiverunswithdifferentrandomseeds.Localnormalization.Inourmainexperiments,wechooselocallynormalizedmodelsinsteadofmorecomplexglobalmodels.Table2providescomparisonsbetweenthelocalandglobalmodelsforNERandDPAR.Fortheglobalmodels,weuseastandardlinear-chainCRF(Laffertyetal.,2001)forNERandtree-CRF(Paskin,2001)forDPAR.Fortheseresults,constraineddecodingisappliedsinceitisfoundtobehelpfulforbothlocalandglobalmodels.Theresultsshowthattherearenoclearbenefitsofusingglobalmodelsoverthesim-plerlocalmodels,probablyduetothestronginputcontextmodelingcapabilitiesoftheunderlyingpre-trainedencoders.Therefore,wesimplyadoptlocalmodelsinourmainexperiments.
150


  
  
 
 
  
  
   8 $ 6 
 G L I I 
 G L I I 
   
   
   
    .
    .
    .
    .
    .
    .
  
 ' 3 $ 5
 ' 3 $ 5
   )  
   
 Z  R  F R Q V 
 Z  R  F R Q V 
 Z  R  F R Q V 
   
  .
  .
  .
  .
  .
  .
    
 1 ( 5
 1 ( 5
   
   
 
 Z   F R Q V 
 Z   F R Q V 
 Z   F R Q V 
   .
   .
   .
   .
   .
   .
 ( U U    Z  R  F R Q V  
 ( U U    Z  R  F R Q V  
 ( U U    Z  R  F R Q V  
    
    
   )  
 ( U U    Z   F R Q V  Figure2:Illustrationsofconstraintviolationsandre-latederrorrates.Here,“Violation%”denotestheper-centagesofpredicteditemsthatviolatesstructuralcon-straintsinthegreedydecodingmode,and“Err%”de-notesthepercentagesofthepredicteditemsthatviolatetheconstraintsandareincorrectatthesametime.Evaluation.Weadoptstandardevaluationmet-ricsforthetasks:LabeledF1scoreforNER,unla-beledattachmentscore(UAS)forDPAR,labeledargumentF1scoreforEAE(Linetal.,2020).3.2RQ1:OnTrainingDataWefirstinvestigatetheeffectivenessofincorpo-ratingconstraintsindecoding,plottingtheratesofstructuralviolationsandrelatederrorsinFig-ure2.Forallthepredicteditems(allnon-‘O’tagsforNER,alldependencyedgesforDPARandallpredictedargumentlinksforEAE),wecalculatethepercentageofitemsthatviolatethestructuralconstraintswhenusinggreedydecoding(“Viola-tion%”).ForNER,weanalyzeatthetagleveland
   3 H U F H Q W D J H
   .
   .
   .
   .
   .
   .
  
  
 ( $ (
 ( $ (
 
 9 L R O D W L R Q 
 9 L R O D W L R Q 
 9 L R O D W L R Q 
  3 H U F H Q W D J H
  
 
 
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
    
  3 H U F H Q W D J H
  
  
 ( U U    Z   F R Q V  
 ( U U    Z   F R Q V  
 
 
 
    
   
  
  
  
 
 
  
  
   .
   .
   .
   .
   .
   .
 G L I I Figure3:Testresultswithorwithoutapplyingcon-straintsagainstdifferenttrainingsizes.Here,x-axisdenotesthetrainingsize(measuredbythenumberofto-kens).Thelefty-axisdenotestheperformance(F1%forNER,UAS%forDPARandF1%forEAE).Therighty-axisdenotestheperformancedifferencesbetweenthemethodswithorwithoutconstraints.counttheillegaltagtransitions.ForDPAR,weincludetheedgesthatareinsidealoop(violatingtheacyclicconstraint)orgoacrossanotheredge(violatingtheprojectiveconstraint).ForEAE,wecounttheargumentlinkswhoseroledoesnotcom-plywiththetypesoftheeventandtheentitythatitconnects.Wefurthercalculate“Err%”,whichde-notesthepercentageoftheitemsthatcontainviola-tionsingreedydecodingandarewronglypredictedatthesametime.Sucherrorratesarecalculatedforbothgreedy(w/ocons.)andconstrained(w/cons.)modes,andthecomparisonsbetweenthesetwocanillustratetheamountoferrorreductionthatconstraineddecodingcanbring.
151


 0 H G L X P
 0 H G L X P
 0 H G L X P
 0 H G L X P
 0 H G L X P
 0 H G L X P
                                                                                                      ' 3 $ 5
    .
    .
    .
    .
    .
    .
                                                                                                     ( $ (Figure5:Performanceimprovementsbroughtbyconstraineddecodingwithdifferentmodelsandamountsoftrainingdata.Here,x-axisdenotestheunderlyingmodelwhiley-axisdenotestrainingsizes.Theoveralltrendsareconsistentonallthetasks.Aswehavemoretrainingdata,therearefewerstructuralviolationswithoutexplicitlyenforcingconstraints,whichindicatesthatthemodelcanim-plicitlylearntheconstraintsifgivenenoughtrain-ingdata.Moreover,althoughconstraineddecodingcaneliminatesuchviolations,theydonotalwaysleadtothecorrectpredictions;onlyasmallportionofincorrectitemscanbecorrectedwithconstraineddecoding,andsuchimprovementsaremorepromi-nentwithlesstrainingdata.WefurthershowthemaintestresultsinFigure3.Thegeneraltrendsareagainsimilarforallthreetasks:Constraintsprovideconsistentbenefitsforthemodelperformance,andsuchbenefitsarelargeraswehavelesstrainingdata.ThiscorrespondswelltotheviolationanalysisinFigure2:withenoughtrainingdata,themodelimplicitlylearnsthestructuralconstraintsfromthedataandfurtherenhancementofconstraineddecodingwillmakelittledifference;however,withlesstrainingdata,explicitlyenforcingconstraintscanhelp.RQ1Takeaways:Withoutincorporatingcon-straints,therearemoreconstraintviolationsfromthepredictionsofthemodelstrainedwithlessdata.Byenforcingconstraintsindecoding,therecanbeconsistentbenefitsformodelperformanceandsuchimprovementsaregreaterwithmodelslearnedwithlesstrainingdata.3.3RQ2:OnEfficientModelsWefurtherexploretheinfluenceofusingmoreeffi-cientmodels.WetakethedistilledversionsoftheBERTmodelsfromTurcetal.(2019)andrepeatourpreviousexperiments.Specifically,wecon-siderfivemodels(L=LayerNumber,H=DimensionSize):Tiny(L=2,H=128),Mini(L=4,H=256),Small(L=4,H=512),Medium(L=8,H=512),andBase(L=12,H=768).Weplot“Violation%”andperformancedifferencesinFigure4andFigure5,respectively.First,iflookingattheaxisofthetrainingdatasize,theoveralltrendsaresimilartopreviousfind-ings:Therearemoreviolationswithlesstrainingdata,andenforcingconstraintshelpsmoreinlower-resourcescenarios.Thistrendisgenerallyconsis-tentacrossalltheunderlyingmodels.Moreover,comparingacrossthemodelaxisbringsmoreinter-estingfindings.Overall,thesmallermodelstendtooutputpredictionswithmoreviolationsifadopt-inggreedydecodingandincorporatingconstraintsgenerallybringmoreperformanceimprovementsforsmallermodels.Thereasonforthistrendmightbethatsmallermodelscontainfewerparameters
  .
  .
  .
  .
  .
  .
 0 L Q L
 0 L Q L
 0 L Q L
 0 L Q L
 0 L Q L
 0 L Q L
                                                                                                     1 ( 5
   .
   .
   .
   .
   .
   .
 7 L Q \
 7 L Q \
 7 L Q \
 7 L Q \
 7 L Q \
 7 L Q \
 % D V H 0 R G H O
 % D V H 0 R G H O
 % D V H 0 R G H O
 % D V H 0 R G H O
 % D V H 0 R G H O
 % D V H 0 R G H O
                                                                                                     ' 3 $ 5
 6 P D O O
 6 P D O O
 6 P D O O
 6 P D O O
 6 P D O O
 6 P D O O
                                                                                                     1 ( 5
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
                                                                                                              ( $ (Figure4:“Violation%”(percentagesofpredicteditemsthatviolatesconstraintswithgreedydecoding)withdifferentmodelsandamountsoftrainingdata.Here,x-axisdenotestheunderlyingmodelwhiley-axisdenotestrainingsizes.
152


 Z E
 Z E
 Z E
 Z E
                                                                                                                                     ( $ (Figure6:“Violation%”(percentagesofpredicteditemsthatviolatesstructuralconstraints)ondifferenttestinggenreswithdifferentamountsofsourcetrainingdata(“nw”asthetrainingsource).Here,x-axisdenotesthetestinggenres(whicharesortedwiththesimilaritiestothesourcegenre)whiley-axisdenotestrainingsizes.
 Z O
 Z O
                                                                                                                                                                                       1 ( 5
 W F 7 H V W L Q J  * H Q U H
 W F 7 H V W L Q J  * H Q U H
 W F 7 H V W L Q J  * H Q U H
 W F 7 H V W L Q J  * H Q U H
0.560.590.640.801.45
    .
    .
    .
    .
 E F
 E F
 E F
 E F
 E F
 E F
 P ]
 P ]
 P ]
 P ]
  .
  .
  .
  .
  .
  .
TinyMiniSmallMediumBase
 E Q
 E Q
 E Q
 E Q
 E Q
 E Q
   .
   .
   .
   .
   .
   .
DPARw/o
                                                                                                                         ( $ (Figure7:Performanceimprovementsbroughtbyconstraineddecodingondifferenttestinggenreswithdifferentamountsofsourcetrainingdata(“nw”asthetrainingsource).Here,x-axisdenotesthetestinggenres(whicharesortedwiththesimilaritiestothesourcegenre)whiley-axisdenotestrainingsizes.
                                                                                                                                                                                     ' 3 $ 5
                                                                                                                                                                                     ' 3 $ 5
0.230.260.330.471.07DPARw/
0.280.310.360.501.10
   .
   .
   .
   .
   .
   .
 X Q
 X Q
Table3:Decodingspeed(mspersentence)without(w/o)orwith(w/)constraints.tolearnallthepatternsinthetrainingdataandsuchunder-parameterizationmaybringdifficultiesinimplicitlycapturingtheconstraints.Anotherinterestingquestionishowdecodingspeedisinfluencedbytheunderlyingmodelandthedecodingalgorithm.Table3presentsthetimere-quiredtodecodeonesentenceforNERandDPAR.Here,wedonotanalyzetheEAEtask,sincetherearenocomplexalgorithmsinvolvedforourcon-straineddecodingforEAEandwedidnotfindob-viousspeeddifferencesbetweendecodingmethodswithorwithoutconstraints.Generally,constraineddecodingrequiresmorecomputationalcostcom-paredwiththeconstraint-agnosticgreedymethods.Thisisnotsurprisingsincetheconstraint-agnosticdecodingmethodsimplypredictsthelocallymax-imallyscoreditemswhileconstraineddecodingneedstoinvokealgorithmswithhighercomplex-ity.Withsmallermodels,constraineddecodingbringsrelativelymorecostbecausetherearelessintensecomputationalrequirementsfortheunder-lyingencoder.ThistrendisespeciallyobviousfortheNERtask,whereconstraineddecodingcostsnearlytwicethetimeasgreedydecodingwhenus-ingtheTinymodel.Whenadoptinglargermodels,theencoderstartstorequiremorecomputationsandthustherelativeextracostbroughtbyconstraineddecodingtakesasmallerproportion.RQ2Takeaways:SmallerandmoreefficientmodelssuchasdistilledversionsofBERTtendtooutputpredictionswithmorestructuralviolationswithgreedydecoding,andconstraineddecodinggenerallybringsmorebenefits.3.4RQ3:OnGenreTransferFinally,weexploreatransfer-learningscenariowheretherearediscrepanciesbetweenthetrain-ingandtestingdatadistributions.Specifically,weconsidertransferringacrossdifferenttextgenres.Fortheseexperiments,weutilizeOntoNotesforNERandDPAR,andACE05forEAE.Wetakethe
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
 ) X O O 7 U D L Q L Q J  6 L ] H
NERw/o
0.290.320.360.531.19NERw/
 Q Z
 Q Z
 Q Z
 Q Z
 Q Z
 Q Z
                                                                                                                                                                                     1 ( 5
 F W V 7 H V W L Q J  * H Q U H
 F W V 7 H V W L Q J  * H Q U H
   .
   .
   .
   .
153


newswire(nw)portionasthesourcefortraininganddirectlytestthesource-trainedmodelonthetestsetsofothergenres(inazero-shotmanner).TheresultsareshowninFigure6and7,wherethenotationsaresimilartothosein§3.3.Intheseresults,similarpatternsalongthedatasizeaxiscanbefound:Incorporatingconstraintsismorehelpfulinthecaseswithlesstrainingdataandsuchtrendsgenerallyholdforout-of-distributiontestingscenarios(targetgenresthatarenot“nw”)aswell.Anotherinterestingdimensionisthepatternalongtheaxisofgenres.Inthefigures,wesortthetestinggenresaccordingtotheirsimilaritiestothesource(nw).Tocalculatethesimilaritiesbetweengenres,weusetheoverlappingrateofvocabulariessincelexicaloverlapscanbeoneimportantfactorfortheeffectivenessoftransfer.Overall,thereisaweaktrendthatwhentransferringtomoredistantgenres,greedydecodingtendstoproduceoutputswithmorestructuralviolations.However,suchapatternisnotconsistentacrossallcases,andonepotentialreasonmightbetheinstabilityofmodeltransfer.Moreover,therecanbemoreappropriatemeasurementsthanoursimplelexicon-basedsimi-laritythatmaybetterreflecthowthepredictionsareinfluencedbyconstraineddecodingacrossgenres.Weleavemoreexplorationstofuturework.RQ3Takeaways:Thepreviouspatternsstillgen-erallyholdfortestingonout-of-domaininstanceswithgenrediscrepancies:Modelstrainedwithlessdatatendtomakemoreviolationswithgreedyde-codingandbenefitmorefromconstraineddecod-ing.Thereisalsoaweakpatternwhentransferringtomoredistantgenres,whereingreedydecodingtendstoproducemoreviolations.4RelatedWorkForstructuredpredictiontasks,oneimportantprop-ertyisthatthepredictionoutputsarecomplexob-jectswithmultipleinterdependentvariables.Howtomodelsuchinter-dependenciesisanimportantquestionfortraditionalNLPresearch.Classicalalgorithmsfordecodingandlearninghavebeendevelopedforvariousstructuredpredictiontasks,includingtheViterbialgorithm(Viterbi,1967)andforward-backwardalgorithm(Baumetal.,1970)forsequencelabeling,maximumspanningtreealgorithm(ChuandLiu,1965;Edmonds,1967),Inside-Outsidealgorithm(Paskin,2001)andMatrix-TreeTheorem(Kooetal.,2007;SmithandSmith,2007;McDonaldandSatta,2007)forde-pendencyparsing,aswellasmorecomplexalgo-rithmsfortasksinvolvingmorecomplicatedgraphstructures(RushandCollins,2012;BurkettandKlein,2013;Martinsetal.,2015;GormleyandEisner,2015).Thoughrecentdevelopmentsinneu-ralmodelsandpre-trainedlanguagemodelshaveboostedtheperformanceofsimplelocalmodels,bettermodelingofthestructuredoutputshavestillbeenshowneffectiveforvariousstructuredpredic-tiontasks(Wangetal.,2019;FonsecaandMartins,2020;Zhangetal.,2020;Weietal.,2021).Fortheoutputmodelingofstructuredpredictiontasks,thehardstructuralconstraintisakeyfactorforthedevelopmentofdecodingandlearningalgo-rithms.Toenhancegeneralexplicitlystatedcon-straints,RothandYih(2004)tacklethedecodingproblemwithIntegerLinearProgramming(ILP)andsuchparadigmhasbeenappliedtoarangeofstructuredNLPtasks(DenisandBaldridge,2007;RothandYih,2007;ClarkeandLapata,2008;Pun-yakanoketal.,2008).Inadditiontoenforcingwell-formedoutputstructuresfordecoding,constraintscanbealsoincorporatedtoenhancemodellearn-ing(Changetal.,2008;Lietal.,2020b;Panetal.,2020;Wangetal.,2020,2021).Whilewemainlyfocusonsimplyapplyingconstraineddecodingwithlocalmodelstrainedwithdifferentamountsofdata,itwouldbeinterestingtoexplorethein-fluenceswhenfurtherincorporatingconstraintsatmodeltrainingtime.5ConclusionInthiswork,weexploretheinteractionsofconstraint-baseddecodingalgorithmsandtheamountsoftrainingdatafortypicalstructuredpre-dictiontasksinNLP.Specifically,wetrainlocalmodelswithdifferentamountsoftrainingdataandanalyzetheinfluenceofwhethertoadoptcon-straineddecodingornot.Theresultsshowthatwhenthemodelistrainedwithlessdata,thepredic-tionscontainmorestructuralviolationswithgreedydecodingandtherearemorebenefitsonmodelper-formancebyfurtherapplyingconstraineddecod-ing.Suchpatternsalsogenerallyholdwithmoreefficientmodelsandwhentransferringacrosstextgenres,wheretherearefurtherinterestingpatternswithregardtomodelsizesandgenredistances.
154


LimitationsThisworkhasseverallimitations.First,weonlyexperimentonEnglishdatasets.Itwouldbeinter-estingtoexplorewhetherthegeneralpatternsholdfornon-Englishlanguageswithdifferentstructuralproperties.Moreover,weonlyexploreincorporat-inghardconstraintsfordecodingwithlocalmodelsattestingtime.Exploringmoreapplicationsofstructuralconstraints,suchaslearningwithcon-straints,orincorporatingothertypesofconstraints,suchassoftones,wouldbepromisingfuturedi-rections.Finally,weonlyexplorethreesimplesentence-levelstructuredpredictiontasks,whileextentionscanbemadetomorecomplextaskswithlargeroutputspace,suchastextgenerationordocument-levelinformationextraction,whereconstraintsmayplaymoreinterestingroles.ReferencesDavidAhn.2006.Thestagesofeventextraction.InProceedingsoftheWorkshoponAnnotatingandRea-soningaboutTimeandEvents,pages1–8,Sydney,Australia.AssociationforComputationalLinguistics.LeonardEBaum,TedPetrie,GeorgeSoules,andNor-manWeiss.1970.Amaximizationtechniqueoccur-ringinthestatisticalanalysisofprobabilisticfunc-tionsofmarkovchains.Theannalsofmathematicalstatistics,41(1):164–171.DavidBurkettandDanKlein.2013.Variationalinfer-enceforstructuredNLPmodels.InProceedingsofthe51stAnnualMeetingoftheAssociationforCom-putationalLinguistics(Tutorials),pages9–10,Sofia,Bulgaria.AssociationforComputationalLinguistics.Ming-WeiChang,Lev-ArieRatinov,NicholasRizzolo,andDanRoth.2008.Learningandinferencewithconstraints.InAAAI,pages1513–1518.Y.J.ChuandT.H.Liu.1965.Ontheshortestarbores-cenceofadirectedgraph.ScientiaSinica,14:1396–1400.JamesClarkeandMirellaLapata.2008.Globalinfer-enceforsentencecompression:Anintegerlinearprogrammingapproach.JournalofArtificialIntelli-genceResearch,31:399–429.PascalDenisandJasonBaldridge.2007.Jointdeter-minationofanaphoricityandcoreferenceresolutionusingintegerprogramming.InHumanLanguageTechnologies2007:TheConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics;ProceedingsoftheMainConfer-ence,pages236–243,Rochester,NewYork.Associa-tionforComputationalLinguistics.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofdeepbidirectionaltransformersforlanguageunder-standing.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,Minnesota.AssociationforComputationalLinguistics.TimothyDozatandChristopherD.Manning.2017.Deepbiaffineattentionforneuraldependencypars-ing.InICLR.JackEdmonds.1967.Optimumbranchings.JournalofResearchoftheNationalBureauofStandards,B,71:233–240.JasonM.Eisner.1996.Threenewprobabilisticmodelsfordependencyparsing:Anexploration.InCOLING1996Volume1:The16thInternationalConferenceonComputationalLinguistics.ErickFonsecaandAndréF.T.Martins.2020.Revisitinghigher-orderdependencyparsers.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages8795–8800,Online.AssociationforComputationalLinguistics.MatthewR.GormleyandJasonEisner.2015.Struc-turedbeliefpropagationforNLP.InProceedingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing:TutorialAbstracts,pages5–6,Beijing,China.Asso-ciationforComputationalLinguistics.XiaoqiJiao,YichunYin,LifengShang,XinJiang,XiaoChen,LinlinLi,FangWang,andQunLiu.2020.TinyBERT:DistillingBERTfornaturallanguageun-derstanding.InFindingsoftheAssociationforCom-putationalLinguistics:EMNLP2020,pages4163–4174,Online.AssociationforComputationalLin-guistics.YoonKimandAlexanderM.Rush.2016.Sequence-levelknowledgedistillation.InProceedingsofthe2016ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages1317–1327,Austin,Texas.AssociationforComputationalLinguistics.DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.TerryKoo,AmirGloberson,XavierCarreras,andMichaelCollins.2007.Structuredpredictionmodelsviathematrix-treetheorem.InProceedingsofthe2007JointConferenceonEmpiricalMethodsinNat-uralLanguageProcessingandComputationalNat-uralLanguageLearning(EMNLP-CoNLL),pages141–150,Prague,CzechRepublic.AssociationforComputationalLinguistics.SandraKübler,RyanMcDonald,andJoakimNivre.2009.Dependencyparsing.Synthesislecturesonhumanlanguagetechnologies,1(1):1–127.
155


ArturKulmizev,MiryamdeLhoneux,JohannesGontrum,ElenaFano,andJoakimNivre.2019.Deepcontextualizedwordembeddingsintransition-basedandgraph-baseddependencyparsing-ataleoftwoparsersrevisited.InProceedingsofthe2019Confer-enceonEmpiricalMethodsinNaturalLanguagePro-cessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2755–2768,HongKong,China.AssociationforComputationalLinguistics.JohnDLafferty,AndrewMcCallum,andFernandoCNPereira.2001.Conditionalrandomfields:Proba-bilisticmodelsforsegmentingandlabelingsequencedata.InProceedingsoftheEighteenthInternationalConferenceonMachineLearning,pages282–289.LDC.2005.ACE(automaticcontentextraction)en-glishannotationguidelinesforeventsversion5.4.3.LinguisticDataConsortium.JingLi,AixinSun,JiangleiHan,andChenliangLi.2020a.Asurveyondeeplearningfornamedentityrecognition.IEEETransactionsonKnowledgeandDataEngineering,34(1):50–70.TaoLi,ParthAnandJawale,MarthaPalmer,andVivekSrikumar.2020b.Structuredtuningforsemanticrolelabeling.InProceedingsofthe58thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages8402–8412,Online.AssociationforComputa-tionalLinguistics.YingLin,HengJi,FeiHuang,andLingfeiWu.2020.Ajointneuralmodelforinformationextractionwithglobalfeatures.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLin-guistics,pages7999–8009,Online.AssociationforComputationalLinguistics.AndréFTMartins,MárioATFigueiredo,PedroMQAguiar,NoahASmith,andEricPXing.2015.Ad3:Alternatingdirectionsdualdecompositionformapin-ferenceingraphicalmodels.TheJournalofMachineLearningResearch,16(1):495–545.RyanMcDonald,KobyCrammer,andFernandoPereira.2005.Onlinelarge-margintrainingofdependencyparsers.InProceedingsofthe43rdAnnualMeet-ingoftheAssociationforComputationalLinguistics(ACL’05),pages91–98,AnnArbor,Michigan.Asso-ciationforComputationalLinguistics.RyanMcDonaldandGiorgioSatta.2007.Onthecomplexityofnon-projectivedata-drivendependencyparsing.InProceedingsoftheTenthInternationalConferenceonParsingTechnologies,pages121–132,Prague,CzechRepublic.AssociationforComputa-tionalLinguistics.JoakimNivre,Marie-CatherinedeMarneffe,FilipGin-ter,JanHajiˇc,ChristopherD.Manning,SampoPyysalo,SebastianSchuster,FrancisTyers,andDanielZeman.2020.UniversalDependenciesv2:Anevergrowingmultilingualtreebankcollection.InProceedingsoftheTwelfthLanguageResourcesandEvaluationConference,pages4034–4043,Marseille,France.EuropeanLanguageResourcesAssociation.XingyuanPan,MaitreyMehta,andVivekSrikumar.2020.Learningconstraintsforstructuredpredictionusingrectifiernetworks.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4843–4858,Online.AssociationforComputationalLinguistics.MarkAPaskin.2001.Cubic-timeparsingandlearningalgorithmsforgrammaticalbigrammodels.VasinPunyakanok,DanRoth,andWen-tauYih.2008.Theimportanceofsyntacticparsingandinferenceinsemanticrolelabeling.ComputationalLinguistics,34(2):257–287.LanceRamshawandMitchMarcus.1995.Textchunk-ingusingtransformation-basedlearning.InThirdWorkshoponVeryLargeCorpora.DanRothandWen-tauYih.2004.Alinearprogram-mingformulationforglobalinferenceinnaturallan-guagetasks.InProceedingsoftheEighthConfer-enceonComputationalNaturalLanguageLearn-ing(CoNLL-2004)atHLT-NAACL2004,pages1–8,Boston,Massachusetts,USA.AssociationforCom-putationalLinguistics.DanRothandWen-tauYih.2007.Globalinferenceforentityandrelationidentificationviaalinearpro-grammingformulation.Introductiontostatisticalrelationallearning,pages553–580.AlexanderMRushandMJCollins.2012.Atutorialondualdecompositionandlagrangianrelaxationforinferenceinnaturallanguageprocessing.JournalofArtificialIntelligenceResearch,45:305–362.VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.2019.Distilbert,adistilledversionofbert:smaller,faster,cheaperandlighter.arXivpreprintarXiv:1910.01108.PengShiandJimmyLin.2019.Simplebertmodelsforrelationextractionandsemanticrolelabeling.arXivpreprintarXiv:1904.05255.DavidA.SmithandNoahA.Smith.2007.Probabilisticmodelsofnonprojectivedependencytrees.InPro-ceedingsofthe2007JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingandCom-putationalNaturalLanguageLearning(EMNLP-CoNLL),pages132–140,Prague,CzechRepublic.AssociationforComputationalLinguistics.ZhixingTan,MingxuanWang,JunXie,YidongChen,andXiaodongShi.2018.Deepsemanticrolelabel-ingwithself-attention.InProceedingsoftheAAAIConferenceonArtificialIntelligence.ErikF.TjongKimSangandFienDeMeulder.2003.IntroductiontotheCoNLL-2003sharedtask:Language-independentnamedentityrecognition.In
156


ProceedingsoftheSeventhConferenceonNaturalLanguageLearningatHLT-NAACL2003,pages142–147.IuliaTurc,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.Well-readstudentslearnbetter:Ontheimportanceofpre-trainingcompactmodels.arXivpreprintarXiv:1908.08962.AndrewViterbi.1967.Errorboundsforconvolutionalcodesandanasymptoticallyoptimumdecodingal-gorithm.IEEEtransactionsonInformationTheory,13(2):260–269.ChristopherWalker,StephanieStrassel,JulieMedero,andKazuakiMaeda.2006.ACE2005multilingualtrainingcorpus.LinguisticDataConsortium,57.HaoyuWang,MuhaoChen,HongmingZhang,andDanRoth.2020.Jointconstrainedlearningforevent-eventrelationextraction.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLan-guageProcessing(EMNLP),pages696–706,Online.AssociationforComputationalLinguistics.HaoyuWang,HongmingZhang,MuhaoChen,andDanRoth.2021.Learningconstraintsanddescriptiveseg-mentationforsubeventdetection.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages5216–5226,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.XinyuWang,JingxianHuang,andKeweiTu.2019.Second-ordersemanticdependencyparsingwithend-to-endneuralnetworks.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages4609–4618,Florence,Italy.Asso-ciationforComputationalLinguistics.TianwenWei,JianweiQi,ShenghuanHe,andSong-taoSun.2021.Maskedconditionalrandomfieldsforsequencelabeling.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages2024–2035,Online.AssociationforComputationalLinguistics.RalphWeischedel,MarthaPalmer,MitchellMarcus,EduardHovy,SameerPradhan,LanceRamshaw,Ni-anwenXue,AnnTaylor,JeffKaufman,MichelleFranchini,etal.2013.Ontonotesrelease5.0.Lin-guisticDataConsortium,Philadelphia,PA,23.XingxingZhang,JianpengCheng,andMirellaLapata.2017.Dependencyparsingasheadselection.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLin-guistics:Volume1,LongPapers,pages665–676,Valencia,Spain.AssociationforComputationalLin-guistics.YuZhang,ZhenghuaLi,andMinZhang.2020.Effi-cientsecond-orderTreeCRFforneuraldependencyparsing.InProceedingsofthe58thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages3295–3305,Online.AssociationforComputa-tionalLinguistics.
157