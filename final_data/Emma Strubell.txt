Emma Strubell17414362
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], "NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.", ['Sireesh Gururaja', 'Amanda Bertsch', 'Clara Na', 'D. Widder', 'Emma Strubell']]
['Understanding the Effect of Model Compression on Social Bias in Large Language Models', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.", ['Gustavo Gon√ßalves', 'Emma Strubell']]
['Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research', '2023', ['arXiv.org', 'ArXiv'], 'Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.', ['Ji-Ung Lee', 'Haritz Puerto', 'Betty van Aken', 'Yuki Arase', 'J. Forde', 'Leon Derczynski', "Andreas Ruckl'e", 'Iryna Gurevych', 'Roy Schwartz', 'Emma Strubell', 'Jesse Dodge']]
['On the Interactions of Structural Constraints and Data Resources for Structured Prediction', '2023', ['', ''], 'In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.', ['Zhisong Zhang', 'Emma Strubell', 'E. Hovy']]
['Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation', '2023', ['arXiv.org', 'ArXiv'], "Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.", ['Hao Peng', 'Qingqing Cao', 'Jesse Dodge', 'Matthew E. Peters', 'Jared Fernandez', 'Tom Sherborne', 'Kyle Lo', 'Sam Skjonsberg', 'Emma Strubell', 'Darrell Plessas', 'Iz Beltagy', 'Pete Walsh', 'Noah A. Smith', 'Hannaneh Hajishirzi']]
['Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models', '2023', ['arXiv.org', 'ArXiv'], 'Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.', ['Harnoor Dhingra', 'Preetiha Jayashanker', 'Sayali S. Moghe', 'Emma Strubell']]
['Making Scalable Meta Learning Practical', '2023', ['arXiv.org', 'ArXiv'], 'Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.', ['Sang Keun Choe', 'Sanket Vaibhav Mehta', 'Hwijeen Ahn', 'W. Neiswanger', 'Pengtao Xie', 'Emma Strubell', 'Eric P. Xing']]
['Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints', '2023', ['arXiv.org', 'ArXiv'], 'Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up to $2$ points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.', ['Rajshekhar Das', 'Jonathan M Francis', 'Sanket Vaibhav Mehta', 'Jean Oh', 'Emma Strubell', 'Jose Moura']]
['The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], 'Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.', ['Jared Fernandez', 'Jacob Kahn', 'Clara Na', 'Yonatan Bisk', 'Emma Strubell']]
['Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.", ['Zhisong Zhang', 'Emma Strubell', 'E. Hovy']]
