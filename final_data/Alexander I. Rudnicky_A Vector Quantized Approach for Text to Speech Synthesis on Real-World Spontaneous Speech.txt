3 2 0 2
b e F 8
] S A . s s e e [
1 v 5 1 2 4 0 . 2 0 3 2 : v i X r a
A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky
Language Technologies Institute, Carnegie Mellon University liweiche@cs.cmu.edu, swatanab@cs.cmu.edu, air@cs.cmu.edu
Abstract
Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human- level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregres- sive models, leading to unintelligible synthesis, and demon- strate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS sys- tem whose architecture is designed for multiple code genera- tion and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ab- lation analyses to identify the efﬁcacy of our methods. We show that MQTTS outperforms existing TTS systems in sev- eral objective and subjective measures.
1
Introduction
A crucial component of Artiﬁcial Intelligence (AI), espe- cially conversational agents, is the ability to synthesize human-level speech. With recent advances in deep learning, neural-based Text-to-Speech (TTS) systems (Li et al. 2019; Kim et al. 2020; Ren et al. 2019) have led to signiﬁcant im- provements in the quality of synthesized speech. However, standard corpora (Ito and Johnson 2017; Yamagishi, Veaux, and MacDonald 2019) used for training TTS systems for the most part include reading or acted speech recorded in a controlled environment. On the other hand, humans spon- taneously produce speech with diverse prosody that conveys paralinguistic information including subtle emotions. This ability comes from exposure to thousands of hours of real- world speech. It suggests that TTS systems trained on real- world speech enable human-level AI.
In this work, we explore the use of real-world speech col- lected from YouTube and podcasts on TTS. While the ulti- mate goal is to use an ASR system to transcribe real-world speech, here we simplify the setting by using an already transcribed corpus and focus on TTS. Systems successfully
trained on real-world speech are able to make use of the unlimited utterances in the wild. Therefore, we believe it should be possible to replicate the success similar to that of large language models (LLMs) such as GPT-3 (Brown et al. 2020). These systems can be ﬁne-tuned to speciﬁc speaker characteristics or recording environments with few resources available. In this work, we address emerging chal- lenges when training TTS systems on real-world speech: higher variation of prosody and background noise compared to reading speech recorded in controlled environments.
With real-world speech, we ﬁrst provide evidence that mel-spectrogram based autoregressive systems failed to gen- erate proper text-audio alignment during inference, resulting in unintelligible speech. We further show that clear align- ments can still be learned in training, and thus the failure of inference alignment can be reasonably attributed to error ac- cumulation in the decoding procedure. We ﬁnd that replac- ing mel-spectrogram with learned discrete codebooks effec- tively addressed this issue. We attribute this to the higher resiliency to input noise of discrete representations. Our re- sults, however, indicate that a single codebook leads to dis- torted reconstruction for real-world speech, even with larger codebook sizes. We conjecture that a single codebook is in- sufﬁcient to cover the diverse prosody patterns presented in spontaneous speech. Therefore, we adopt multiple code- books and design speciﬁc architectures for multi-code sam- pling and monotonic alignment. Finally, we use a clean si- lence audio prompt during inference to encourage the model on generating clean speech despite training on a noisy cor- pus. We designate this system MQTTS (multi-codebook vector quantized TTS) and introduce it in Section 3.
We perform ablation analysis in Section 5, as well as comparing mel-spectrogram based systems to identify the properties needed for real-world speech synthesis. We fur- ther compare MQTTS with non-autoregressive approach. We show that our autoregressive MQTTS performs better in intelligibility and speaker transferability. MQTTS achieves slightly higher naturalness, and with a much higher prosody diversity. On the other hand, non-autoregressive model ex- cels in robustness and computation speed. Additionally, with clean silence prompt, MQTTS can achieve a much lower signal-to-noise ratio (SNR). We make our code public1.
Copyright © 2023, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.
1https://github.com/b04901014/MQTTS


2 Related Work Autoregressive TTS. Typical autoregressive TTS systems use a mel-spectrogram–vocoder pipeline (Shen et al. 2018; Li et al. 2019). An autoregressive model is trained to synthe- size a mel-spectrogram sequentially, then a neural vocoder is separately trained to reconstruct the waveform. One con- cern in these systems is their low tolerance for variance in the training corpus. For instance, the alignment learning of Tacotron 2 (Shen et al. 2018) is sensitive even to leading and trailing silences. Such characteristics make training on highly noisy real-world speech infeasible. We show empir- ically that this is resolved by replacing mel-spectra with quantized discrete representation. Another recurring issue is faulty alignments during inference, leading to repetition and deletion errors. Research has shown that this can be miti- gated by enforcing monotonic alignment (He, Deng, and He 2019; Zeyer, Schl¨uter, and Ney 2021). Here we use a sim- ilar concept in Monotonic Chunk-wise Attention (Chiu and Raffel 2018), which is already widely used for speech recog- nition (Liu et al. 2020). However, we directly use the atten- tion weights as a transition criterion without modiﬁcations to the training process. We introduce our method in detail in Section 3.3. Non-attentive Tacotron (Shen et al. 2020) approaches the alignment issue by replacing the attention mechanism of autoregressive models with an external du- ration predictor. We will show that this technique actually degrades the performance of MQTTS.
Non-autoregressive TTS. The focus of the TTS com- munity gradually shifted toward non-autoregressive mod- els (Kim, Kong, and Son 2021; Kim et al. 2020; Ren et al. 2019) due to their better robustness, inference speed, and lower training difﬁculty. Recently, they have been shown to surpass autoregressive models in terms of naturalness on benchmark corpora. Typically, a duration predictor is trained to predict the text-audio alignment, and a ﬂow-based net- work is trained to synthesize given the alignment, phoneme sequence, and sampling Gaussian noise. VITS (Kim, Kong, and Son 2021), a recent end-to-end model that uses a stochastic duration predictor for better diversity, achieves near-human naturalness on the VCTK (Yamagishi, Veaux, and MacDonald 2019) dataset. While there are many non- autoregressive TTS systems, we choose VITS as the repre- sentative of this line of research and analyze its performance on real-world data compared to autoregressive models.
Quantization-based Generative Models. An emerging line of research (Ramesh et al. 2021; Esser, Rombach, and Ommer 2021) has shown success in image synthesis by adopting a two-stage approach. A quantizer is trained to en- code the image into discrete tokens, then an autoregressive transformer is used to model the sequential distribution of the image tokens. DiscreTalk (Hayashi and Watanabe 2020) adopts a similar framework for TTS, discretizing speech us- ing a VQVAE and a transformer for autoregressive gener- ation. Unit-based language models (Lakhotia et al. 2021) (uLMs) train the transformer on discrete tokens derived from representations of self-supervised (SSL) models, and a unit- to-speech (u2s) vocoder is separately trained to map the to- kens to speech. VQTTS (Du et al. 2022) further leverages the u2s vocoder for TTS by training a non-autoregressive
model mapping text to discrete tokens. MQTTS differs from these works in the use of multi-code, and the architecture de- signed for multi-code generation and monotonic alignment. MQTTS also learns codebooks specialized for speech syn- thesis instead of using SSL representations which may not contain sufﬁcient information for speech reconstruction.
TTS on real-world speech. An approach to using real- world speech for TTS is to incorporate denoising. Recent work (Zhang et al. 2021) trained the denoising module and the TTS system jointly with external noise data. Our work focuses on the TTS system itself instead of explicit noise modeling, and we utilize a simple audio prompt for noise reduction without additional training. There is also re- search (Yan et al. 2021) focusing on modeling characteris- tics of spontaneous speech, such as ﬁlled pauses (i.e., um, uh). We do not design speciﬁc architectures for these, but rather let the models learn implicitly.
3 MQTTS We now introduce the model architecture as well as the train- ing and inference procedures of MQTTS. The training of our model has two stages. In the ﬁrst stage, as shown in Fig- ure 1, a quantizer is trained to map the raw waveform x into discrete codes c = {ct}, t ∈ [1, · · · , T ] by jointly mini- mizing a quantization loss LV Q and the reconstruction loss between the input x and output y. A discriminator guides the reconstruction using an additional adversarial loss. We denote the pair of encoder and decoder in quantizer as QE, QD, the discriminator as D, and the learnable codebook em- beddings as Gi where i ∈ [1, · · · , N ] for N codebooks. In the second stage, we ﬁx the quantizer and train a transformer to perform autoregressive generation on c, conditioned on a speaker embedding s to enable multi-speaker TTS and a phoneme sequence h. Here we use bold math symbols to denote sequential representations across time.
3.1 Quantization of Raw Speech Quantizer Architecture. We adopt HiFi-GAN (Kong, Kim, and Bae 2020) as the backbone architecture for quantizer decoder QD and the discriminator D, as its structure has been shown to produce high-quality speech given the mel- spectrogram. We replace the input mel-spectrogram with the learned embedding of the discrete codes. As for QE, we reverse the architecture of HiFi-GAN generator by sub- stituting deconvolution layers with convolution layers. We observed that naively adopting the HiFi-GAN architecture leads to the explosion of quantization loss LV Q and train- ing divergence. We attribute this to the unnormalized output of residual blocks in HiFi-GAN. Therefore, we apply group normalization (Wu and He 2020) with 16 channels per group on the output of all residual blocks before aggregation. In addition, we broadcast-add the speaker embedding s on the input to the decoder QD.
Multiple Codebook Learning. In Section 5.1, we will show empirically that a single codebook results in poor re- construction of the input signal for real-world speech. We thus leverage the concept in vq-wav2vec (Baevski, Schnei- der, and Auli 2020) to realize quantization with multiple


Figure 1: Overview of MQTTS. We use different colors to represent the 4 codes from distinct codebooks Gi.
codebooks. As shown in Figure 1, each ct is now represented by N = 4 codes, and the ﬁnal embedding is the concatena- tion of the embeddings of the N codes. We denote that the individual code corresponding to codebook Gi as ct,i. To get the discrete code ct,i from zc t , which is the output of QE, we slice the embedding dimension of zc t into N equal parts, denoting each part as zc
t,i:
ct,i = arg min
||zc
t,i − z||2,
(1)
z∈Gi
Then zq through gradient estimator:
t , the input to the decoder is obtained by the straight-
zq t,i = zc
t,i + sg[Gi(ct,i) − zc
t,i], zq
t = concat[zq
t,i]
where sg[·] is the stop gradient operator, Gi(·) returns the embedding of the given code, and the concat[·] operator con- catenates all codes on the embedding dimension. The ﬁnal loss LF is a combination of the HiFi-GAN loss term LGAN and an additional quantization loss LV Q:
(2)
Figure 2: Detailed view of the multi-output transformer dur- ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding.
tion for ESP K. The transformer is trained to maximize log- likelihood:
LF = λLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3)
LV Q(QE, QD, G) =Ex
(cid:20) 1 T
N (cid:88)
i=1
T (cid:88)
t=1
(||sg[zc
t,i] − zq
t,i||2 2
t,i − sg[zq
+ γ||zc
(cid:21) t,i]||2 2)
where γ and λ are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed deﬁnition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose.
3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa-
(4)
LT = Ec
(cid:34)
−
(cid:88)
log p(ct | c<t, s, h)
(cid:35)
t
Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different conﬁguration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that beneﬁts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder.
2https://huggingface.co/pyannote/embedding
sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin
Multi-output Transformer. While
(5)


2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the ﬁxed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This conﬁguration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1.
Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference.
3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform.
Inference
Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, · · · , bk + Nw] instead of the entire encoder state sequence, and bk is deﬁned recursively by:
b0 = 0, bk+1 =
(cid:40)
bk + 1,
bk,
exp{A(bk,k)} i=0 exp{A(bk,k+i)}
if
(cid:80)Nw otherwise
< 1 Nw
(6)
Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the ﬁrst encoder state (at bk) is lower than a given −1). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to overﬁtting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled
during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise.
4 Experimental Setup
4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and YouTube speech and apply a data- cleaning pipeline detailed in Appendix D to remove ill- conditioned speech. The resulting dataset contains 896 hours for training and 2.4 hours for validation with an utterance duration ranging from 5 to 15 seconds. For human evalu- ation, to ensure speaker diversity, we select 40 utterances from different speakers using the test set of VoxCeleb (Na- grani et al. 2019) as the speaker reference audios, which are also noisy spontaneous speech. We randomly select the corresponding text from GigaSpeech and remove those tran- scriptions from training and validation sets.
4.2 Training and Model Conﬁguration We use γ = 0.25 in Equation 4 and λ = 10 in Equation 3 to balance the relative high LGAN . For training the quan- tizer, Adam optimizer (Kingma and Ba 2015) is used with β1 = 0.5, β2 = 0.9. For the transformer, we use Adam op- timizer with β1 = 0.9, β2 = 0.98 with a batch size of 200 input frames per GPU. The learning rate is linearly decayed from 2 × 10−4 to 0. All models are trained using 4 RTX A6000 GPUs in bﬂoat16 precision. We trained the quantizer for 600k steps and the transformer for 800k steps. We leave optimization details in the released code. For inference, we use nucleus sampling with p = 0.8.
Baseline Models. We use 6 layers for both encoder and decoder for Transformer TTS (Li et al. 2019) and add the speaker embedding with the same approach as our MQTTS. We train a HiFi-GAN vocoder on GigaSpeech with the ofﬁcial implementation3 as its vocoder. We also trained Tacotron 2 (Shen et al. 2018) using the implementation from NVIDIA4, and broadcast-added the speaker embed- ding to the encoder state following GST Tacotron (Wang et al. 2018). For VITS, we modiﬁed from the ofﬁcial imple- mentation and changed their ﬁxed speaker embedding into our own speaker embedding module, and trained it for 800k steps. We follow the original paper to use 0.8 for the noise scaling of the stochastic duration predictor and 0.667 for the prior distribution.
Model Variants. To test the efﬁcacy of the components in MQTTS, we evaluate two ablated versions of the model: one without monotonic alignment, and the other with the sub- decoder module replaced by the same number of linear lay- ers. Nucleus sampling is applied to each code independently
3https://github.com/jik876/hiﬁ-gan 4https://github.com/NVIDIA/DeepLearningExamples


for the version without Sub-decoder. We also evaluate the scalability of model parameters with performance. We report three versions of models: 40M, 100M, and 200M, which dif- fer in the size of transformers while using the same quan- tizer. The context window Nw is set to 4 for 40M, 100M, and 3 for 200M version due to the clearer alignment. The detailed architecture of each version is in Appendix A. To better compare with MQTTS, we also trained a single cross- attention version of Transformer TTS with unique align- ment. Finally, we evaluate the approach from Non-attentive Tacotron (Shen et al. 2020) on improving alignment robust- ness. We replace the attention alignment of Transformer TTS (single cross-attention version) and MQTTS with that produced by the duration predictor from VITS.
4.3 Evaluation Metrics ASR Error Rate. Error rate from Automatic Speech Recog- nition (ASR) models is a common metric to assess synthesis intelligibility (Hayashi et al. 2020). First, we adopt relative character error rate (RCER) to evaluate our quantizer model. By relative, we mean using the transcription of the input ut- terance by the ASR model as the ground truth. We believe that this better reﬂects how the reconstruction affects intel- ligibility. We adopt ASR model5 pretrained on GigaSpeech from ESPNet (Watanabe et al. 2018) for this purpose. We also report the typical word error rate (WER) for the whole TTS system as an objective metric of intelligibility. We use the video model of Google Speech-to-text API as the ASR model and evaluate 1472 syntheses.
Human Evaluation. We report two 5-scale MOS scores, MOS-Q and MOS-N for quantizer evaluation, and only MOS-N for comparing TTS systems. MOS-Q asks the hu- man evaluators to score the general audio quality of the speech, while MOS-N evaluates the speech’s naturalness. We used the Amazon Mechanical Turk (MTurk) platform. Details of the human experiments are in Appendix B. We show both the MOS score and the 95% conﬁdence interval. Prosody FID Score (P-FID). For the evaluation of prosody diversity and naturalness, we visit the Fr´echet In- ception Distance (FID) (Heusel et al. 2017), a widely-used objective metric for the evaluation of image generative mod- els. FID calculates the 2-Wasserstein distance between real and synthesized distributions assuming they are both Gaus- sian, reﬂecting both naturalness and diversity. A pretrained classiﬁer is used to extract the dense representation for each sample. In TTS, FID is seldom used due to the insufﬁcient dataset size. However, the 500k utterances in GigaSpeech afford the sampling complexity of FID. We use the dimen- sional emotion classiﬁer released by (Wagner et al. 2022) pretrained on MSP-Podcast (Lotﬁan and Busso 2019). We use the input before the ﬁnal decision layer and scale it by 10 to calculate the FID score. We randomly sample 50k ut- terances from the training set and generate 50k syntheses using the same set of text but shufﬂed speaker references.
Speaker Similarity Score (SSS). In multi-speaker TTS, it is also important to evaluate how well the synthesizer transfers speaker characteristics. Here we use the cosine
5https://zenodo.org/record/4630406#.YoT0Ji-B1QI
Table 1: Comparison of quantizer and vocoder reconstruc- tion quality on VoxCeleb test set. HF-GAN is HiFi-GAN.
Method
Code size (#groups)
RCER (%)
MOS-Q (95% CI)
MOS-N (95% CI)
GT
n/a
n/a
3.66(.06)
3.81(.05)
HF-GAN
n/a
12.8
3.47(.06)
3.62(.06)
MQTTS Quant.
1024 (1) 65536 (1) 160 (4) 160 (8)
56.5 59.9 19.7 14.2
3.38(.07) 3.40(.06) 3.63(.06) 3.56(.06)
3.49(.06) 3.48(.06) 3.67(.06) 3.71(.06)
similarity between the speaker embedding of the reference speech and that of the synthesized speech. We evaluate the same 50k samples used for P-FID.
Mel-cepstral Distortion (MCD). We follow previous work (Hayashi et al. 2021) and use MCD to measure the reconstruction error between ground truth and synthesis, given the same text and speaker reference speech. We extract 23 mel-cepstral coefﬁcients (MCEPs) and use Euclidean distance as the metric. Dynamic Time Wraping (Giorgino 2009) is applied for the alignment. We calculate MCD using the ground truth utterances from the validation split.
5 Results
5.1 Quantizer Analysis Table 1 shows the reconstruction quality of the quantizer. From RCER, it is clear that using a single codebook (N = 1) drastically distorts the input phone sequence. Their distinc- tive low MOS-Q and MOS-N scores also suggest that N = 1 is not sufﬁcient to generate high-quality and natural speech. Even if we span a much larger codebook size, 65536, there is almost no difference compared to the 1024 codes across all metrics. This result indicates the necessity of having multi- ple code groups, as a single codebook fails even to recon- struct the original input, not to mention when applied to the whole pipeline. Interestingly, we ﬁnd that a single code- book is sufﬁcient for LJSpeech (Ito and Johnson 2017), sug- gesting multiple codebooks are required for the complexity of multi-speaker real-world speech. When compared to the mel-spectrogram based HiFi-GAN (HF-GAN in Table 1), our quantizer is slightly higher in terms of RCER but dis- tinctively better in speech quality (MOS-Q). For the remain- der of our experiments, we use N = 4 with a 160 code size, as we observe little perceptible quality and intelligibility dif- ference when we increase from N = 4 to 8.
5.2 Performance The performance of TTS system training on the given real- world speech corpus are presented in Table 2.
Autoregressive Models. Before comparing systems quantitatively, we showcase some qualitative properties of Tacotron 2 and Transformer TTS. Figure 3 compares autore- gressive models on their alignments. It shows that Tacotron 2 completely failed the inference alignment. On the other


Figure 3: Comparison of inference encoder-decoder alignment of different models. For Transformer TTS we picked one of the cross-attentions which learn alignment. See Appendix C for the cross-attentions of all heads and layers.
Table 2: Comparison of TTS models. MOS is with 95% conﬁdence interval. MCD is with one standard deviation.
Method
#Params. WER (%) ↓
SSS ↑
P-FID ↓
MCD ↓
MOS-N ↑
GT
n/a
14.9
n/a
0.11
n/a
4.01±0.06
VITS
≈ 40M ≈ 100M
28.4 24.8
0.718 0.734
15.48 12.04
8.94±1.16 8.73±1.19
3.84±0.06 3.84±0.06
Transformer TTS Transformer TTS (w. single-attention) Transformer TTS (w. duration predictor)
≈ 100M
82.6 90.2 74.3
0.434 0.331 0.451
442.29 491.49 196.09
12.49±2.77 12.79±4.35 11.70±2.12
2.34±0.09 2.23±0.10 2.75±0.09
MQTTS
≈ 40M ≈ 100M ≈ 200M
24.9 22.3 22.2
0.727 0.751 0.762
11.67 8.58 6.21
10.67±1.78 10.22±1.68 10.17±1.70
3.79±0.06 3.87±0.06 3.89±0.06
MQTTS (w.o. monotonic alignment) MQTTS (w.o. sub-decoder) MQTTS (w. duration predictor)
≈ 100M
34.3 27.0 53.2
0.740 0.740 0.725
10.20 12.12 16.65
10.86±2.00 10.47±1.81 10.88±1.62
3.76±0.06 3.75±0.07 3.67±0.06
hand, Transformer TTS can produce clear alignment dur- ing training. For inference, however, only a sub-sequence of alignment is formed. Perceptually, only several words at the beginning are synthesized for Transformer TTS, then it is followed by unrecognizable noise. As for Tacotron 2, only high-frequency noise is generated. We choose not to in- clude Tacotron 2 for quantitative evaluations as its syntheses are not understandable. From Table 2, it is clear that Trans- former TTS is inferior to other systems in all measures. The poor inference alignment contributes largely to the bad per- formance, including the distinctively high P-FID as the syn- theses contain long segments of non-speech. Applying the duration predictor resolves the alignment issue and signiﬁ- cantly increases the performance, but there is still a sizable gap with other systems. We also noticed that using single at- tention on Transformer TTS leads to worse performance, in- dicating having multiple cross-attentions may still increase the robustness despite the high similarity in their alignment patterns. Additionally, the larger parameter size (additional cross-attention layers) may also be another factor. Nonethe- less, this comes at the cost of the inability to use external duration predictors or monotonic alignment, as some of the heads are trained to average over a larger context.
Model Variants. We can observe the efﬁcacy of mono- tonic alignment and Sub-decoder from Table 2. Both ab- lated versions have lower MOS-N scores. Without mono- tonic alignment, the model suffers from noticeably higher WER. On the other hand, replacing Sub-decoder module im-
pact less on intelligibility but leads to a lower P-FID score. This is reasonable as monotonic alignment mainly mitigates repetition, deletion errors and has less inﬂuence on prosody, while applying Sub-decoder leads to better sampling which improves prosody naturalness and diversity. However, these ablated versions of MQTTS are still much better than Trans- former TTS, indicating the advantage of discrete codes over continuous mel-spectrogram on real-world speech. Replac- ing the alignment with the external duration predictor drasti- cally degrades performance. Inspecting the samples reveals that phonations often transition unnaturally. We believe that as the given alignment is much different from the planned alignment of the MQTTS, it forces abrupt transitions in the middle of phonation.
Non-autoregressive Model. First, we notice that both VITS and MQTTS both have decent MOS-N scores. How- ever, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse sam- ples. We conjecture that generating speech signals in paral- lel is a much harder task compared to autoregressive model- ing. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insufﬁcient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to ﬁnd a conﬁguration that makes the training converge. 5 sam-


ples with the same text from both systems in Figure 4 further support our claim of the diversity comparison. This might also explain the lower MCD of VITS, where the synthe- ses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short inter- mittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD.
MQTTS performs better in terms of both intelligibil- ity and speaker transferability. We ﬁnd MQTTS captures utterance-level properties (i.e., emotion, speaking rate) bet- ter compared to VITS. For naturalness, we observe a consis- tently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diver- sity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all met- rics except MCD, which we explained earlier. This sug- gests MQTTS is generally better than VITS, given enough resources. Additionally, we observed overﬁtting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version. This explains the little improvement from 100M to 200M and suggests that a larger training corpus is needed for further improvement.
Error Analysis. Despite the better average performance of MQTTS in Table 2, we ﬁnd that it suffers from lower sam- pling robustness compared to non-autoregressive systems. This is reasonable as higher diversity inevitably comes with a higher risk of sampling poor syntheses. We observe un- naturally prolonged vowels in some samples with speaker reference speech of a slower speaking style, which is sel- dom the case for VITS. In addition, samples that start with a poor recording environment often result in bad synthe- ses. Deletion errors, which we consider more undesirable than substitution, are also more prevalent in MQTTS; it con- tributes for 8.4 out of 22.3% WER in MQTTS-100M, but only 6.8 out of 24.8% for VITS-100M. We conjecture that as intermittent pauses are not annotated explicitly, and thus it encourages MQTTS to produce silence even if attending to a certain phoneme. However, if the phones are success- fully produced, they often sound more natural and intelligi- ble than those in the syntheses of VITS. We observe these errors gradually rectiﬁed as the model capacity increases (from 40M to 200M), suggesting that more data and larger models can eventually resolve these issues.
5.3 Audio Prompt and SNR To better understand the effect of the audio prompts on the synthesis, we made the audio prompts white noise drawn from different standard deviation σ (then encoded by QE). Figure 5 presents the relationship between σ and the signal-to-noise ratio (SNR). We use the WADA-SNR algo- rithm (Kim and Stern 2008) for SNR estimation. From Fig- ure 5, it is clear that the SNR drops as σ increases, conﬁrm- ing that the model is guided by the prompt. Using σ smaller than 10−4 effectively increases the SNR compared to not using any audio prompt. All our other experiments are done using σ = 10−5. We also noticed that VITS has a lower
(a) VITS.
(b) MQTTS.
Figure 4: Pitch contour for the utterance: “How much varia- tion is there?” from two models within the same speaker.
Figure 5: Comparison of SNR with different level of noise as audio prompt. SNR is calculated with 1472 syntheses.
SNR. Perceptually we can hear a small fuzzy noise univer- sally across the syntheses. We conjecture that VITS tries to also model and sample from the environmental noise, which is extremely difﬁcult. The unsuccessful modeling makes it synthesize only a single type of noise.
6 Conclusion On real-world speech, our empirical results indicate mul- tiple discrete codes are preferable to mel-spectrograms for autoregressive synthesizers. And with suitable modeling, MQTTS achieves better performance compared to the non- autoregressive synthesizer. Nonetheless, a sizable gap still exists between our best-performing syntheses and human speech. We believe that bridging this gap is crucial to the de- velopment of human-level communication for AI. Acquiring more data is one straightforward solution. In this regard, we are interested in combining and leveraging ASR models to transcribe real-world speech corpora.On the other hand, bet- ter modeling can also be designed to mitigate the issues we mentioned in the error analysis. For instance, silence detec- tion can be used in the decoding process to prevent phoneme transitions before the phonation, mitigating deletion errors. Additionally, we plan to further compare and incorporate self-supervised discrete speech and prosody representations with our learned codebooks.


Acknowledgments We are grateful to Amazon Alexa for the support of this re- search. We thank Sid Dalmia and Soumi Maiti for the dis- cussions and feedback.
References Baevski, A.; Schneider, S.; and Auli, M. 2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representa- tions. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Mod- els are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu- ral Information Processing Systems, volume 33, 1877–1901. Curran Associates, Inc. Chen, G.; Chai, S.; Wang, G.; Du, J.; Zhang, W.-Q.; Weng, C.; Su, D.; Povey, D.; Trmal, J.; Zhang, J.; Jin, M.; Khu- danpur, S.; Watanabe, S.; Zhao, S.; Zou, W.; Li, X.; Yao, X.; Wang, Y.; Wang, Y.; You, Z.; and Yan, Z. 2021. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. In Proc. Interspeech 2021. Chiu, C.; and Raffel, C. 2018. Monotonic Chunkwise Atten- tion. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Du, C.; Guo, Y.; Chen, X.; and Yu, K. 2022. VQTTS: High- Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature. In Proc. Interspeech 2022, 1596–1600. Esser, P.; Rombach, R.; and Ommer, B. 2021. Taming Trans- formers for High-Resolution Image Synthesis. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 12873–12883. Fan, A.; Lewis, M.; and Dauphin, Y. 2018. Hierarchical Neural Story Generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 889–898. Melbourne, Australia: Association for Computational Linguistics. Giorgino, T. 2009. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package. Journal of Statistical Software, 31(7): 1–24. Hayashi, T.; and Watanabe, S. 2020. DiscreTalk: Text- CoRR, to-Speech as a Machine Translation Problem. abs/2005.05525. Hayashi, T.; Yamamoto, R.; Inoue, K.; Yoshimura, T.; Watanabe, S.; Toda, T.; Takeda, K.; Zhang, Y.; and Tan, X. 2020. Espnet-TTS: Uniﬁed, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4- 8, 2020, 7654–7658. IEEE.
Hayashi, T.; Yamamoto, R.; Yoshimura, T.; Wu, P.; Shi, J.; Saeki, T.; Ju, Y.; Yasuda, Y.; Takamichi, S.; and Watanabe, S. 2021. ESPnet2-TTS: Extending the Edge of TTS Research. CoRR, abs/2110.07840.
He, M.; Deng, Y.; and He, L. 2019. Robust Sequence-to- Sequence Acoustic Modeling with Stepwise Monotonic At- tention for Neural TTS. In Kubin, G.; and Kacic, Z., eds., Interspeech 2019, 20th Annual Conference of the Interna- tional Speech Communication Association, Graz, Austria, 15-19 September 2019, 1293–1297. ISCA.
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fer- gus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.
Holtzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y. 2020. The Curious Case of Neural Text Degeneration. In International Conference on Learning Representations.
Ito, K.; and Johnson, L. 2017. The LJ Speech Dataset.
Kim, C.; and Stern, R. M. 2008. Robust signal-to-noise ratio estimation based on waveform amplitude distribution analy- sis. In Proc. Interspeech 2008, 2598–2601.
Kim, J.; Kim, S.; Kong, J.; and Yoon, S. 2020. Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Align- ment Search. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Informa- tion Processing Systems, volume 33, 8067–8077. Curran As- sociates, Inc.
Kim, J.; Kong, J.; and Son, J. 2021. Conditional Varia- tional Autoencoder with Adversarial Learning for End-to- End Text-to-Speech. In Meila, M.; and Zhang, T., eds., Pro- ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, 5530–5540. PMLR.
Kingma, D. P.; and Ba, J. 2015. Adam: A Method for In 3rd International Conference Stochastic Optimization. on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.
Kong, J.; Kim, J.; and Bae, J. 2020. HiFi-GAN: Genera- tive Adversarial Networks for Efﬁcient and High Fidelity In Proceedings of the 34th Interna- Speech Synthesis. tional Conference on Neural Information Processing Sys- tems, NIPS’20. Red Hook, NY, USA: Curran Associates Inc. ISBN 9781713829546.
Lakhotia, K.; Kharitonov, E.; Hsu, W.-N.; Adi, Y.; Polyak, A.; Bolte, B.; Nguyen, T.-A.; Copet, J.; Baevski, A.; Mo- hamed, A.; and Dupoux, E. 2021. On Generative Spoken Language Modeling from Raw Audio. Transactions of the Association for Computational Linguistics, 9: 1336–1354.
Li, N.; Liu, S.; Liu, Y.; Zhao, S.; and Liu, M. 2019. Neural Speech Synthesis with Transformer Network. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 33(01): 6706–6713.


Liu, B.; Cao, S.; Sun, S.; Zhang, W.; and Ma, L. 2020. Multi- head Monotonic Chunkwise Attention For Online Speech Recognition. CoRR, abs/2005.00205. Lotﬁan, R.; and Busso, C. 2019. Building Naturalistic Emo- tionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings. IEEE Transac- tions on Affective Computing, 10(4): 471–483. Nagrani, A.; Chung, J. S.; Xie, W.; and Zisserman, A. 2019. Voxceleb: Large-scale speaker veriﬁcation in the wild. Com- puter Science and Language. Press, O.; Smith, N.; and Lewis, M. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length In International Conference on Learning Extrapolation. Representations. Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad- ford, A.; Chen, M.; and Sutskever, I. 2021. Zero-Shot Text- to-Image Generation. In Meila, M.; and Zhang, T., eds., Pro- ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, 8821–8831. PMLR. Ren, Y.; Ruan, Y.; Tan, X.; Qin, T.; Zhao, S.; Zhao, Z.; and Liu, T.-Y. 2019. FastSpeech: Fast, Robust and Controllable Text to Speech. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alch´e-Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 32. Cur- ran Associates, Inc. Shen, J.; Jia, Y.; Chrzanowski, M.; Zhang, Y.; Elias, I.; Zen, H.; and Wu, Y. 2020. Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling. CoRR, abs/2010.04301. Shen, J.; Pang, R.; Weiss, R. J.; Schuster, M.; Jaitly, N.; Yang, Z.; Chen, Z.; Zhang, Y.; Wang, Y.; Skerrv-Ryan, R.; Saurous, R. A.; Agiomvrgiannakis, Y.; and Wu, Y. 2018. Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions. In 2018 IEEE Interna- tional Conference on Acoustics, Speech and Signal Process- ing (ICASSP), 4779–4783. Wagner, J.; Triantafyllopoulos, A.; Wierstorf, H.; Schmitt, M.; Burkhardt, F.; Eyben, F.; and Schuller, B. W. 2022. Dawn of the transformer era in speech emotion recognition: closing the valence gap. Wang, Y.; Stanton, D.; Zhang, Y.; Skerry-Ryan, R. J.; Bat- tenberg, E.; Shor, J.; Xiao, Y.; Ren, F.; Jia, Y.; and Saurous, R. A. 2018. Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis. In ICML. Watanabe, S.; Hori, T.; Karita, S.; Hayashi, T.; Nishitoba, J.; Unno, Y.; Enrique Yalta Soplin, N.; Heymann, J.; Wiesner, M.; Chen, N.; Renduchintala, A.; and Ochiai, T. 2018. ESP- net: End-to-End Speech Processing Toolkit. In Proceedings of Interspeech, 2207–2211. Wu, Y.; and He, K. 2020. Group Normalization. Comput. Vis., 128(3): 742–755. Yamagishi, J.; Veaux, C.; and MacDonald, K. 2019. CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92).
Int. J.
Yan, Y.; Tan, X.; Li, B.; Zhang, G.; Qin, T.; Zhao, S.; Shen, Y.; Zhang, W.; and Liu, T. 2021. AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style. CoRR, abs/2107.02530. Zeyer, A.; Schl¨uter, R.; and Ney, H. 2021. A study of latent monotonic attention variants. CoRR, abs/2103.16710. Zhang, C.; Ren, Y.; Tan, X.; Liu, J.; Zhang, K.; Qin, T.; Zhao, S.; and Liu, T. 2021. Denoispeech: Denoising Text to Speech with Frame-Level Noise Modeling. In IEEE Interna- tional Conference on Acoustics, Speech and Signal Process- ing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, 7063–7067. IEEE.