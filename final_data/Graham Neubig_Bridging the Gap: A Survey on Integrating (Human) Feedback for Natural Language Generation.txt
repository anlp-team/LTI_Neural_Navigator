3 2 0 2
n u J
1
] L C . s c [
2 v 5 5 9 0 0 . 5 0 3 2 : v i X r a
Preprint
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
Patrick Fernandes1,2,3 Aman Madaan1 Emmy Liu1 António Farinhas2,3
Pedro Henrique Martins4 Amanda Bertsch1
José G. C. de Souza4
Shuyan Zhou1
Tongshuang Wu1 Graham Neubig1,5 André F. T. Martins2,3,4 1Carnegie Mellon University 2Instituto Superior Técnico (Lisbon ELLIS Unit)
3Instituto de Telecomunicações
4Unbabel
5Inspired Cognition
pfernand@cs.cmu.edu
1
Abstract
Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelp- ful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feed- back is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to im- prove natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this for- malization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decod- ing): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collec- tion. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.
Introduction
This feedback serves as a guiding force, steering LLMs toward the desired outcomes, much like feed- back mechanisms in physical machines (Åström and Murray, 2021).
Typically, state-of-the-art language generation systems are obtained by training probabilistic, au- toregressive LLMs on massive amounts of data using maximum likelihood estimation (MLE). How- ever, the data used to train these models is generally scraped from the Internet, often containing noise, social biases, and errors (Bolukbasi et al., 2016; Dodge et al., 2021). This, when combined with the objective of maximizing the probability of the next token given the previous ones, might result in a misspecification of target behavior (Kenton et al., 2021b), and might lead to models that gener- ate toxic, inaccurate, and unhelpful content (Sheng et al., 2019; Bender et al., 2021).
Exacerbating the problem above is the fact that these models are often evaluated using automatic metrics that compare the generated text with some “reference” text using surface-level features (such as word overlap), which often do not correlate with human-perceived quality of text (Schluter, 2017; Mathur et al., 2020; Gehrmann et al., 2022a), espe- cially when models are optimized for them (Paulus et al., 2017; Amrhein and Sennrich, 2022). This dif- ficulty in evaluation arises partly because, for many tasks, there is not a single correct answer since the same communicative intent can be conveyed in multiple ways.
For generation systems to be widely useful, they must generate text that is not only fluent and high- quality, but also closely aligned with human de- sires and specifications (Vamplew et al., 2018; Hendrycks et al., 2020; Kenton et al., 2021a; Turner et al., 2022; Ngo, 2022). Achieving such ambi- tious goals requires modern large language mod- els (LLMs) to evolve beyond traditional training methods. Recent improvements in this space have centered on incorporating human feedback (Bai et al., 2022b; Ouyang et al., 2022; OpenAI, 2023a).
Leveraging human assessments to evaluate the quality of texts generated by models is then a popular approach. Crucially, considering human- perceived quality can help close the gap between machine and human generated text, and help in ad- dressing the challenges posed by Goodhart’s law: “when a measure becomes a target, it ceases to be a good measure” (Goodhart, 1984). This realization has spurred a growing interest in improving natural language generation systems by leveraging human
1


Preprint
Numerical
Kreutzer et al. (2018); Liu et al. (2018); Fernandes et al. (2022)
Ranking
Stiennon et al. (2020); Ouyang et al. (2022); Bai et al. (2022a)
Format (§3.1)
Natural Language
Li et al. (2017); Madaan et al. (2023); Scheurer et al. (2023)
Others
Lommel et al. (2014a); Pal et al. (2016); Nguyen et al. (2022)
Task Performance
Kreutzer et al. (2018); Stiennon et al. (2020)
Helpfulness
Objective (§3.2)
Instruction-Following
Ouyang et al. (2022); Askell et al. (2021)
Harmlessness
Ouyang et al. (2022); Bai et al. (2022a,b); Glaese et al. (2022)
Feedback-Based Imitation Learning
Li et al. (2017); Glaese et al. (2022); Scheurer et al. (2023)
Training (§4.1,§5.2.1)
Joint Feedback Modelling
Li et al. (2017); Hancock et al. (2019); Korbak et al. (2023); Yuan et al. (2023)
Usage
Reinforcement Learning
Kreutzer et al. (2018); Stiennon et al. (2020); Askell et al. (2021)
Decoding (§4.2,§5.2.2)
Reranking
Feedback-Conditioning
Fernandes et al. (2022); Gao et al. (2022)
Schick et al. (2022); Madaan et al. (2022)
None (§4)
Li et al. (2017); Kreutzer et al. (2018); Madaan et al. (2022)
Modeling
Feedback-Modeling (§5)
Gao et al. (2018); Stiennon et al. (2020); Bai et al. (2022a)
AI Feedback (§7)
Yang et al. (2022); Bai et al. (2022b); Madaan et al. (2023)
Figure 1: Taxonomy of methods that leverage human-feedback, with some example representative works in the literature that fit in each category.
feedback on model-generated outputs, and has led to the emergence of the first widely-used general- purpose language assistants (OpenAI, 2023a). Hu- man feedback not only enhances system perfor- mance, but also serves as a mechanism to steer the system in alignment with desired outcomes or goals (Rosenblueth et al., 1943; Wiener, 1948).
Feedback, as a concept, encompasses a wide range of meanings and interpretations (Wiener, 1948); however, some universal characteristics can be identified, such as its format, its intended re- sults, and the ways it is utilized as a part of the model development process. In this survey, we fo- cus on the role of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (§2). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (§3). We discuss approaches that directly optimize models against human feedback on (their) outputs,
for example, using reinforcement learning with human reward functions (§4). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (§5). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (§6). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (§7).
2 A Taxonomy for Leveraging (Human)
Feedback for Generation
2.1 Background
Consider a model M : X → Y which, given an input of some type x ∈ X , outputs text ˆy ∈ Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y ⊆ Σ⋆ for some alphabet
2


Σ). This general formulation encompasses a wide range of NLG tasks. For example:
Summarization: X is the space of docu- ments, and Y the space of possible summaries.
Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively.
Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses.
Image Captioning: X is the space of images, and Y is the space of possible captions.
These models are generally realized as a parameter- ized, conditional probability distribution Pθ(y|x), where θ are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters θ⋆ that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output ˆy is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) ≈ arg maxy Pθ⋆(y|x)) or by random sampling (M (x) ∼ Pθ⋆(y|x)).
Evaluating the quality of generated text ˆy ∈ Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see §5.1).
More formally, we consider human feedback to be a family of functions H such that each feedback function h ∈ H takes an input1 x ∈ X and one
1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation.
Preprint
or more outputs y1, · · · , yn ∈ Y and returns some feedback f ∈ F:
h : X × Y1 × · · · × Yn (cid:125)
→ F.
(cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X × Y → {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see §3.1).
(cid:124)
We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in §6). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches.
2.2 Taxonomy
Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback:
What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations.
What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior.
When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process.
How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences.
3 Describing Feedback
3.1 Format
An important decision to make when we want to im- prove language generation systems through human
3
(1)


Preprint
Input
Output(s)
Feedback
Type
0.7
Score
A melhor comida do mundo é a portuguesa.
The worst food in the world are Portuguese.
’worst’: major/accuracy ’are’: minor/fluency
MQM
’worst’ → ’best’, ’are’ → ’is’
Post-Edition
Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled.
AI can change industries.
Fluency: 1 Relevance: 0.7
"Misses the ethical concerns."
Multi-Aspect
Natural Language
Explain the moon landing to a 6 year old
A: People went to the ...
B: The moon is a satellite...
A > B
Ranking
Table 1: Example input and output for three tasks (machine translation, summarization, and instruction following) and possible different (example) feedback that can be given.
feedback is in what format to collect this feedback in. The choice of format has implications on the ex- pressivity of the feedback, the ease of its collection, and how we can use it to improve systems. In par- ticular, the complexity of the feedback format is an important factor: simpler formats are often easier to collect and use as part of the training/decoding process, but contain less information than more “complex” formats, and might not be able to cap- ture important information for improving the sys- tem. The choice of format also has implications in the difficulty for humans to give feedback, its consistency/agreement, and the level of rationality of said feedback (Ghosal et al., 2023). Types of feedback are summarized in Table 1 with examples.
Although easy to leverage, numerical feedback suffers from some limitations: depending on the complexity of the generation task, reducing feed- back to a single score might generally be a hard and ill-defined task for humans, leading to a costly collection process and problems of subjectivity and variance (see §6.2.1). Furthermore, such feedback might not be suited to distinguish between outputs of similar quality.
Ranking-based An alternative to asking humans to assign a single score to a given input-output pair is asking them to rank multiple possible alternative outputs
h : X × Y1 × · · · × Yn → Sn
Numerical Numerical feedback, which takes an input and output and returns a single score (X × Y → N ⊆ R), is one of the simplest feed- back formats to collect and use. Kreutzer et al. (2018) studied using categorical feedback, in the form of 5 possible “stars” that can be assigned to a translation, which are then averaged to produce a score (N = [1, 5]) and used to improve the model. Liu et al. (2018) and Shi et al. (2021) used even simpler feedback, by asking humans to choose if a given response is good or not (N = {0, 1}). Nu- merical feedback has also been extensively used for evaluation, albeit not with the explicit goal of improving generation. For example, direct assess- ments (Graham et al., 2013) in machine translation ask humans to rate translations on a continuous scale, and some works have attempted to use this feedback data to train feedback models (Sellam et al., 2020; Rei et al., 2020a) and improve genera- tion (Freitag et al., 2022a; Fernandes et al., 2022).
where Sn represents the set of all permuta- tions/rankings of n elements (optionally allowing ties). This has been used extensively in evaluation (Chaganty et al., 2018). Compared to numerical feedback, this format tends to be easier to collect, and, potentially, for this reason, ranking-based feed- back tends to be collected to improve model behav- ior rather than just for evaluation (since the for- mer tends to require more feedback data). Ziegler et al. (2019) and Stiennon et al. (2020) asked hu- mans to rank alternative summaries of the system they are trying to improve. Similarly, Ouyang et al. (2022) collected rankings of alternative re- sponses to an instruction given to the model. They utilized these rankings to enhance the model’s instruction-following capabilities. Subsequent re- search has also employed ranking-based feedback for the same task (Askell et al., 2021; Bai et al., 2022a,b).
4


Natural Language Both numerical and ranking- based feedback lack the ability to capture detailed information about problems with the output, which can be crucial for improving generation systems. Instead of asking humans to rank or score outputs, we can instead ask for natural language feedback. In such cases, the feedback typically provides more detailed information, either highlighting the short- comings of the current output or suggesting spe- cific actions for improvement. For example, Li et al. (2017) asked humans to give natural lan- guage feedback to a dialogue question answering model, including positive or negative feedback, but also possibly providing the correct answer to the model or hinting about it. Tandon et al. (2022) and Madaan et al. (2022) gather natural language feed- back on errors present in model-generated graphs and the model’s interpretation of a given instruc- tion. Scheurer et al. (2022, 2023) improve summa- rization capabilities of language models by asking humans to provide natural language feedback of summaries of the model. Li et al. (2022) collect natural language feedback (in addition to numerical feedback) for responses from a Question Answer- ing (QA) system.
Others Besides these feedback types, other (po- tentially domain-specific) types of feedback can be used to improve model behavior. Commonly humans are asked to provide multi-aspect feedback (X × Y → Rd or F d more generally), scoring an output or ranking multiple outputs with respect to multiple dimensions (Böhm et al., 2019; Glaese et al., 2022; Madaan et al., 2023; Nguyen et al., 2022). Post-editions ask humans to provide correc- tions to the output in the form of small edits (e.g., replace X by Y), and post-edition data has been used to directly improve models (Denkowski et al., 2014) or train automatic post edition systems that correct model mistakes (Pal et al., 2016; Mehta and Goldwasser, 2019; Madaan et al., 2021; Talmor et al., 2020; Elgohary et al., 2021). There are also other feedback types that haven’t been fully lever- aged to improve generation: e.g., Multidimensional Quality Metrics (MQM) (Lommel et al., 2014b), the standard for evaluating translation quality, asks professional translators to identify errors spans in a translation, alongside severity and type of error.
3.2 Objective
The purpose of collecting feedback is to align the model’s behavior with some (often ill-defined) goal
Preprint
behavior: we might want our summarization model to generate summaries that contain all core infor- mation, even if it means they are a bit longer; in commercial machine translation, extra care is given to ensure that models do not mistranslate business- critical information; and in dialogue agents, we might want the model to be able to produce polite and harmless responses. This alignment objective has been studied extensively in the AI safety and alignment literature (Bostrom, 2014; Amodei et al., 2016; Bommasani et al., 2021). In addition, Ken- ton et al. (2021b) discuss some behavioral issues in language agents (natural language generation mod- els) arising from a misspecified alignment objective (for example, from noisy labels in the training data), and Leike et al. (2018) proposed using feedback models to tackle the difficulty in specifying this objective.
Bai et al. (2022a) explicitly divided the prob- lem of “aligning” a language model into improving its helpfulness and increasing its harmlessness. Most works implicitly consider either the use of feedback that targets performance factors (such as when targeting overall performance in a task or ability to follow instructions) or harmlessness fac- tors (such as not producing toxic text or providing information that could lead to harm).2
Helpfulness Most often, feedback is collected with some helpfulness objective in mind: a nec- essary (but not sufficient) condition for a helpful system is that it performs the task well, and so feedback related to task performance generally falls under this umbrella. For example, most works in machine translation leverage feedback related to the quality of translation (Kreutzer et al., 2018; Fernandes et al., 2022), which is expected to be correlated with its helpfulness in downstream appli- cations. Similarly, in summarization, most works leverage feedback related to aspects such as rel- evance, consistency and accuracy (Ziegler et al., 2019; Stiennon et al., 2020) (in short, the quality of the summary). One particularly well-studied feedback objective is the ability to follow instruc- tions (Ouyang et al., 2022): the task of instruction- following can encompass a wide range of other tasks, and using feedback to improve (instruction following) language assistants has been considered a benchmark for the alignment problem (Askell et al., 2021).
2We mostly ignore the proposed honesty aspect, as none
of these works tackle this directly.
5


Harmlessness Another important alignment ob- jective is harmlessness: we want our models not to produce certain types of output or violate certain norms. Feedback collected in Ouyang et al. (2022) considered aspects such as the toxicity of text (be- sides the overall ability to follow instructions). Bai et al. (2022a) explored the interaction between the helpfulness and harmlessness objectives, showing a trade-off between both. Thoppilan et al. (2022b) collected feedback on whether their model violates a set of safety objectives and used it to finetune the model. Glaese et al. (2022) also ask humans to provide feedback on the harmlessness of their sys- tem, by defining a set of rules and asking humans if the outputs violate these rules. Bai et al. (2022b) showed that feedback produced by LLMs could in- crease harmlessness without reducing helpfulness.
4 Directly Leveraging Human Feedback
In an ideal scenario, we would directly leverage human feedback to improve generation: humans would provide the feedback for training or decod- ing procedures.
4.1 Optimizing for Human Feedback
Once human feedback has been collected, one way to use it is by optimizing the model parameters directly. However, this requires the feedback to be “optimizable”, i.e., possibly formulated as an opti- mization problem based on which we can obtain an improved model. For instance, if the feedback is a numerical score (f ∈ R), we can create the following optimization problem:
θ⋆ = arg max
Ex∼D[h(x, Mθ(x))].
θ
Where D is the distribution of possible inputs. Various techniques have been suggested to op- timize the model parameters, θ, using the col- lected human feedback. These can be divided into three main categories based on the training mech- anisms, which we will call feedback-based im- itation learning, joint-feedback modeling, and reinforcement learning (RL).
The feedback-based imitation learning ap- proach involves using human feedback to optimize the model by performing supervised learning with a dataset composed of positively-labeled genera- tions together with the corresponding inputs, D+.
Preprint
This can be achieved by minimizing the loss:
θ⋆ = arg min
|D+| (cid:88)
L(i)(θ)
θ
L(i)(θ) = − log pθ
i=1 (cid:16)
y(i) | x(i)(cid:17)
An instance of this approach can be found in Li et al. (2017), in which the authors train a dialogue model by maximizing the likelihood of the model’s answers labeled as correct by humans. Similarly, Kreutzer et al. (2018) trained a machine transla- tion model on a set of positively-labeled transla- tions, and Glaese et al. (2022) performed super- vised learning on the preferred dialogues which comply with their pre-defined rules (concerning correctness, harmfulness, and helpfulness), accord- ing to humans. A slightly different approach was proposed by Hancock et al. (2019): deploying a chit-chat dialogue model and using the human ut- terances as targets to fine-tune the model. Scheurer et al. (2022, 2023) leverage the fact that LLMs can follow instructions and start by collecting nat- ural language human feedback about the model generations, which often describes what an im- proved text would look like. Then, they ask the LM to generate multiple refinements based on the input, previous model generation, and the corre- sponding feedback. The highest similarity refine- ments for each generation are then used to fine- tune the LLM. OpenAI’s text-davinci-002 was trained with both human demonstrations and model outputs with the highest possible rating, an approach deemed FeedME (OpenAI, 2023b). A downside of these approaches is that they disre- gard the generations which do not receive positive feedback, which may contain useful information to optimize the model.
(2)
On the other hand, joint-feedback modeling leverages all the information collected by directly using human feedback to optimize the model. Also, as the feedback is modeled directly by the model, this approach allows feedback in formats other than numerical or ranking-based (e.g., natural language). Having D as the dataset of inputs x, generations y, and human feedback f collected, this can be achieved by minimizing the following loss of the form
L(i)(θ) = − log pθ
(cid:16)
y(i), f (i) | x(i)(cid:17)
Over all examples in D. These equation can be fac- (cid:0)f (i) | y(i), x(i)(cid:1) + torized as L(i)(θ) = − log pθ
6
(3)
(4)
(5)


(cid:0)y(i) | x(i)(cid:1). Some works simply train the log pθ model to predict the feedback given to each genera- tion (Weston, 2016, forward prediction), disregard- ing the second term of the factorization. One exam- ple of this approach is the work of Li et al. (2017), in which the authors asked humans to give natu- ral language feedback (e.g., positive/negative feed- back, providing the correct answer to the model, or giving a hint about the correct answer) to a dia- logue question answering model. Then, after hav- ing collected the feedback, the model is trained to predict it. Hancock et al. (2019) proposed having an auxiliary model predicting the satisfaction of the human speaking with the model. Then, if the satis- faction score is lower than a pre-defined threshold, the model will ask the human for feedback. The model then leverages the natural language feedback humans give by learning to predict it. Yuan et al. (2023); Rafailov et al. (2023) showed that having summarization models predict the rankings of dif- ferent summaries helps the model generate better summaries, and might even outperform more com- plicated approaches using feedback models (§5).
Other works train the model to predict the gener- ations and the corresponding human feedback. Xu et al. (2022) proposed using the DIRECTOR model introduced by Arora et al. (2022) to leverage hu- man feedback. As this model has a unified decoder- classifier architecture, Xu et al. (2022) proposed using positively-labeled examples to train its lan- guage modeling head (similarly to feedback-based imitation learning) and using both the positive and negatively-labeled examples to train a classifier head that directs the model away from generating undesirable sequences. Thoppilan et al. (2022a) follow this approach to enforce the model’s quality and safety. First, they collect dialogues between crowd-workers and the proposed language model LaMDA, which are annotated with feedback pro- vided by the crowd-workers. This feedback states each response’s quality (sensible, specific, and in- teresting) or safety. Then, LaMDA is fine-tuned to predict the high-quality responses and the re- wards given to every response regarding its quality attributes and safety. At inference time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold.
Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the
Preprint
following loss:
L(i)(θ) = − log pθ
(cid:0)yi | f i, xi(cid:1)
Liu et al. (2023) proposed prompt-based fine- tuning, where they create prompts containing pre- vious generations rated by humans, in the order of preference. They also suggest inserting language- based feedback (e.g., “... is a worse answer than ...”) to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer.
Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct opti- mization of a model’s parameters based on human feedback, regardless of the feedback’s differentia- bility. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the fol- lowing gradient:
∇θJ(θ) = Ex∼D,y∼pθ [h(x, y)∇θ log pθ(y | x)] (7) Here, D represents the set of inputs x, and pθ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).
4.2 Decoding with Human Feedback
While directly optimizing model parameters pro- vides greater control, modifying them may not al- ways be feasible, particularly in the case of LLMs. Additionally, feedback might be unavailable during model training, limiting the scope for parameter
7
(6)


adjustments. In such cases, leveraging human feed- back during decoding plays a critical role in enhanc- ing LLMs’s performance. This type of feedback, derived from interactions between LLMs and users in practical scenarios, enables models to learn from their errors and offers opportunities for ongoing refinement without altering model parameters. In addition, the feedback functions as a guiding mech- anism, allowing the model to generate more desir- able outputs by leveraging its existing capabilities. There are two broad categories in which hu- man feedback is used in this setup: 1. Feedback Memory: Feedback Memory Utilization involves maintaining a repository of feedback from prior sessions. Then, when processing new inputs, the system uses relevant feedback from similar inputs in its memory to guide the model toward gener- ating more desirable outputs based on past expe- riences and user preferences. While a classical concept (Riesbeck, 1981; Schank, 1983), recent work has shown the promise of such a memory- augmented approach in both finetuning (Weston et al., 2014; Wu et al., 2018; Tandon et al., 2022) and few-shot setups (Madaan et al., 2022).
2. Iterative Output Refinement: This method em- ploys human feedback to refine the model’s out- put iteratively. Users can provide feedback on in- termediate responses, enabling the model to ad- just its output until it meets the user’s satisfaction. This process allows the model to better understand user preferences and produce more suitable out- comes (Reid and Neubig, 2022; Saunders et al., 2022; Schick et al., 2022; Nijkamp et al., 2022). Feedback can also be provided on model attributes such as the decoding strategy (Passali et al., 2021), rather than directly on its outputs. These two techniques are not mutually exclusive and can be combined to achieve even better per- formance, creating a more adaptive and responsive system that caters to user expectations.
5
Improving Generation using Human Feedback Models
Directly using human feedback to improve model behavior is not feasible in the general case: asking humans to provide feedback for every model output is both expensive and time-consuming.
5.1 Learning Models of Human Feedback
An alternative approach to obtaining human feed- back is to develop models that can predict or ap-
Preprint
proximate it. Although these models may not be perfect, they offer the advantage of providing feed- back at a low cost after training, thereby enabling the scaling of feedback-dependent techniques.
More formally, given a feedback function h : X × Y1 × · · · × Yn → F, we want to learn a para- metric (numerical) feedback model ˆhϕ : X × Y → R (with parameters ϕ) that “agrees” with human feedback. This agreement is expressed through a loss function, and the model is trained to minimize this agreement loss:
Ex,y1,··· ,yn∼Df [L(ϕ)] (cid:16)ˆhϕ(x, y1), · · · , h(x, y1:n)
ϕ⋆ = arg min
ϕ
L(ϕ) = loss
(cid:17)
For example, if the feedback function we are try- ing to model is also numerical (h : X × Y → R), then this loss can just be any standard regression loss, such as the squared difference between the human feedback and model feedback L(ϕ) = (cid:16)ˆhϕ(x, y) − h(x, y) Importantly, while the feedback model is (generally) numerical, the hu- man feedback can be in any other format, as long as a suitable loss function can be specified. Stiennon et al. (2020) train preference models 3 ˆhϕ(x, yn) on ranking-based feedback, using a loss of the form
(cid:17)2
.
L(ϕ) = log
(cid:16)
σ
(cid:16)ˆhϕ(x, y+1) − ˆhϕ(x, y−1)
(cid:17)(cid:17)
(10) such that sample y+1 was preferred to y−1 for the same input x: h(x, y−1, y+1) = (y−1 < y+1). Variants of this loss have subsequently been used in other works (Ouyang et al., 2022; Askell et al., 2021; Liu et al., 2022; Qin et al., 2022; Yuan et al., 2023).
The problem of feedback modeling has been studied extensively in the context of metric learn- ing for NLP. Zhang et al. (2019) and Zhou et al. (2023b) utilized pre-trained masked LMs to com- pute similarity scores between the generated text or code snippets and their references. In MT, Sel- lam et al. (2020) and Rei et al. (2020a) trained BLEURT and COMET, respectively, to regress on human quality assessments of translation quality. For summarization, Zopf (2018) leveraged anno- tated pairwise preferences to train a preference model and Peyrard et al. (2017) learned a summary-
3We specify the feedback model with respect to the hu- man feedback format, i.e., reward and preference model for numerical and ranking-based human feedback, respectively.
8
(8)
(9)


level metric from a set of human judgements in- cluded in older summarization datasets (e.g., TAC- 2008). These metrics have been shown to correlate much better with human judgments than widely used lexical-metrics such as BLEU and ROUGE (Freitag et al., 2022b). It is notable that these re- ward models were not trained with the intent of improving generation directly, although some of them were used for that purpose later, as discussed in §5.2.
Recently, there has been a growing interest in developing feedback models directly with the aim of using them to improve generation (Böhm et al., 2019; Ziegler et al., 2019). As a first step, these models are typically initialized with weights from either the target LM that requires improvement or from a model of the same family (e.g., of a smaller size) (Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). One key consideration in the initializa- tion is the size of the pretrained model: while scal- ing up may improve overall performance (Askell et al., 2021; Bai et al., 2022a), Ouyang et al. (2022) find that larger models may be less stable for future finetuning.
Next, the feedback model is finetuned on a dataset of human feedback. This dataset is typ- ically collected by asking annotators to provide feedback on outputs from an earlier version of the model being improved. However, it is also possible to first finetune the feedback model on naturally oc- curring implicit feedback, such as from user inter- actions on websites (e.g., Reddit, StackOverflow). Though less accurate than explicitly-collected feed- back, it allows feedback models to be trained on much more data. Askell et al. (2021) found that naturally occurring feedback data benefits models larger than 1B parameters, but often has diminish- ing returns when the number of explicit-collected feedback increases.
Nguyen et al. (2022) train a preference model based on rankings on three human-designed ob- jectives: whether the summary has an appropri- ate topic, length, and quality, combining these three into a single objective using a distance-based ranking loss. Interestingly, automatic post-editing (APE) systems in MT (e.g., Simard et al. (2007); Correia and Martins (2019)), trained on human post-edits with the intent of automatically correct- ing the output of an MT system, can also be seen as feedback models (albeit non-numerical).
Preprint
5.2 Leveraging Feedback Models to Improve
Generation
After training a feedback model, we can use it to improve generation almost exactly as we would use human feedback: either by leveraging this feedback model during the training of the generation model, or by incorporating the feedback model during the decoding process.
5.2.1 Optimizing for Feedback Models Similarly to optimizing for human feedback, one possible way to use the feedback model is to opti- mize model parameters with respect to the feedback it gives. If the feedback model outputs numerical feedback (ˆhϕ : X × Y → R) we can define an opti- mization problem similar to Equation 2. However, due to the limitations of feedback models as imper- fect proxies, typically a regularization term R is introduced to avoid “overfitting” to the feedback model (Ziegler et al., 2019) (more on this at the end of this section):
θ⋆ = arg max
Ex∼D
(cid:104)ˆhϕ(x, Mθ(x)) − βR(θ)
θ
(11) Due to the similarities between both optimiza- tion problems, approaches to tackle Equation 11 can be divided into two of the three categories in §4.2: joint-feedback modeling and reinforce- ment learning. Recall that while in §4.2 we dis- cuss approaches for directly optimizing for human feedback, while this section is focused on cases where a model of human feedback is used instead. Unlike when using human feedback directly, most works attempt to optimize for feedback mod- els using reinforcement learning. Gao et al. (2018); Böhm et al. (2019) use the (numerical) feed- back collected in other works to train reward and preference models, and use reinforcement learning to optimize against these models, showing that hu- mans preferred their summarization model to other supervised and RL-trained baselines. Ziegler et al. (2019) proposed a similar approach, but trained preference models using feedback collected on the model being improved, and introduced a KL regu- larization term
R(θ) = log [Pθ(y|x)/PθSL(y|x)]
to avoid the optimized model deviating too much from the original (supervised) model with parame- 4. Stiennon et al. (2020) extended this work, ters θSL
4Note that this KL term is different from other algorithm-
9
(cid:105)
(12)


by scaling both the summarization and preference models, showing that their model was highly pre- ferred by humans, and generalized better than su- pervised baselines. Ouyang et al. (2022) also used reinforcement learning with preference models to improve the ability of LLMs to follow instructions, but combined the RL objective with the original pretraining objective to avoid performance regres- sions in public NLP benchmarks. Other works have also used reinforcement learning with preference models in a similar manner (Askell et al., 2021; Bai et al., 2022a; Wu et al., 2021; Nguyen et al., 2022). Underlying all these methods is that generally the model is first trained with imitation-learning on hu- man demonstrations, which improves performance compared to using reinforcement learning directly on the pretrained policy.
Glaese et al. (2022) compared doing feedback- based imitation learning with human feedback (§4.1) with doing reinforcement learning with a feedback model, finding that the latter led to a bet- ter preference rate and lower rule violation rate.
The joint-feedback modeling with feedback models was explored by Korbak et al. (2023), who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for tox- icity. They showed that this leads to models pro- ducing less toxic generations, when compared to pretraining a model with vanilla MLE.
In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring func- tion learned from human judgments as a fitness function for a genetic algorithm to generate sum- maries of input texts.
5.2.2 Decoding with Feedback Models
As mentioned, feedback models have the advantage that they can be queried cheaply for feedback once trained. Perhaps for this reason, most approaches that leverage feedback models by sampling a large number of candidate generations, and reranking them according to the feedback model:
C = {¯y1, · · ·, ¯yS} where ¯yi ∼ Pθ (y|x)
ˆy = arg max
ˆhϕ(x, ¯y)
¯y∈C
where ˆhϕ is a trained (numerical) feedback model and C is a set of S candidate generations given
specific regularization terms, such as the KL terms in PPO (Schulman et al., 2017).
Preprint
by the model (for example, by sampling from its distribution multiple times).
In machine translation, Fernandes et al. (2022) and Freitag et al. (2022a) build upon recent ad- vances in automatic quality estimation and evalu- ation via feedback model training to improve gen- eration. Their framework comprises a candidate generation stage followed by a ranking stage, in which the candidates are scored using quality met- rics trained to regress on human assessments (re- ward models) (Rei et al., 2020a,b) via N -best list reranking or minimum Bayes risk (MBR) decod- ing (Kumar and Byrne, 2002). The highest-scoring candidate is then chosen as the final translation.
Li et al. (2022) collected a dataset of both numer- ical and natural language feedback for responses from a QA system, and finetuned a pretrained model to predict both kinds of feedback, using the predicted scores from this feedback model to re-rank the predictions from the model.
Gao et al. (2022) also used this approach to study the scaling properties of feedback models and the problem of "overoptimization" (see below).
Additionally, there are several works combining MT and APE systems at decoding time, in which the output of an MT system is further improved by an APE system (Bhattacharyya et al., 2022).
Feedback Model Overoptimization One prob- lem that arises when optimizing a system with a feedback model is that this model is only an im- perfect proxy for the ground truth human feedback, therefore, "overoptimizing" for them can lead to systems that receive good feedback from the model, but not humans. This problem is known as the overoptimization problem, and is the main reason for the regularization term in Equation 11
Gao et al. (2022) studies the overoptimization problem in preference models, by both optimiz- ing against it with reinforcement learning (training) and reranking outputs with it (decoding). They found that both using preference models during training or decoding led to similar levels of overop- timization, and that the scale of the generation model helps little with this problem.
6 Collecting and Using Human Feedback
Collecting human feedback can be rather expen- sive and may present i ssues for the inexperienced, making it important to leverage existing resources and consider additional data collection carefully. We present an introduction to existing datasets and
10


Preprint
their collection methods, along with considerations for experimenters creating preference datasets for their own use cases. Additionally, we discuss ethi- cal considerations in the use and collection of hu- man feedback.
In future, richer types of feedback may be col- lected and we may find ways to make use of this signal. For instance, most existing datasets consist of ranking or numerical scores, but humans prefer to provide richer feedback than labelling (Stumpf et al., 2007; Amershi et al., 2014a; Ghai et al., 2021). Furthermore, variability between human annotators has also not been fully explored (Plank, 2022; Gehrmann et al., 2022b).
6.2 Pitfalls and Ethical Considerations of
Human Feedback
Although we have focused on the idealized form of human feedback in §2.1, actual feedback may be low-quality, contradictory, or adversarial. 5 As discussed in §3.2, we must carefully specify an- notation guidelines so that feedback is aligned to- wards the actual goals for the model (Ziegler et al., 2019). Even in the case where human experts are available, different groups of experts may not agree (Kahneman et al., 2021). In this section, we enu- merate possible issues with human feedback, most of which are shared with other annotation tasks. We also touch on possible mitigation strategies.
6.1 Considerations in Data Collection
6.2.1 Subjectivity and variance in judgment
There are multiple facets to consider when collect- ing human feedback data for a generation task; a non-exhaustive list of axes along which data collec- tion can vary is presented below. 1. Annotator expertise: Depending on task and training (Snow et al., 2008; Sheng et al., 2008; Clark et al., 2021; Gillick and Liu, 2010; Freitag et al., 2021), annotators can be domain experts to crowdworkers or even models.
2. Length of engagement: Involves one-time or long-term collaborations with annotators, with preference datasets often involving extended partnerships (Stiennon et al., 2020; Bai et al., 2022a; Freitag et al., 2021).
3. Collection method: Data can be gathered ex- plicitly through experiments or implicitly from online sources/user interactions, with varying noise (Kreutzer et al., 2018; Freitag et al., 2021). 4. Collection platform: Common platforms in- clude Amazon Mechanical Turk, Upwork, and Scale AI.
Considering K annotators with feedback functions K hi i=1, judgments are given on data D = d1, ..., dN . Inter-rater reliability metrics, such as Cohen’s Kappa, Fleiss’ Kappa, or Krippendorff’s alpha, can assess annotator agreement (Hayes and Krip- pendorff, 2007; Fleiss, 1971; Cohen, 1960). Low reliability may result from unclear tasks or evalu- ation criteria (Gehrmann et al., 2022b; Thomson and Reiter, 2021), inherent subjectivity, or multiple plausible interpretations (Plank, 2022; Nie et al., 2020; Gordon et al., 2022).
Mitigation strategies include viewing humans as making noisily-rational choices (Ghosal et al., 2023), learning the reliability level of feedback from multiple humans (Yamagata et al., 2021), and augmenting evaluation metrics like COMET with confidence intervals (Glushkova et al., 2021; Zerva et al., 2022). Clear annotation guidelines and in- cluding rationales with rankings can reduce biases and improve clarity (Ziegler et al., 2019).
5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11
5. Annotator demographics: Different groups may have varying opinions on quality gener- ations; demographics may be collected during data collection. There is generally a trade-off between the ef- fort needed to create the datasets and the reliability of judgments collected. For higher-stakes appli- cations in specific domains, it may be worth the effort to consult expert annotators in an extended partnership. For general alignment with human preferences, it may instead be prudent to recruit a diverse group of annotators to avoid overfitting to the preferences of specific demographics that may be more accessible in recruitment. 11
6.2.2 Bias in judgment
Even if all K annotators agree on a particular judg- ment for a certain data point, they may all be mis- taken. There are well-known biases in human rea- soning which may cause all annotators or a large percentage of annotators to be mistaken, or not take evidence into account. Furthermore, even if anno- tators are technically unbiased in terms of the task they were instructed to evaluate, instructions can be underspecified or lead the annotators to evaluate a slightly different task, leading to the appearance of


Preprint
Task
Dataset & their descriptions
Collection method
Platform
Feedback Type
Language assistant
HH-RLHF (Bai et al., 2022a; Perez et al., 2022a)
Explicit
Upwork, MTurk
Ranking
Language assistant
SHP (Ethayarajh et al., 2023)
Implicit
Scraped from Reddit
Ranking/Score
Summarization
summarize-from-feedback (Stiennon et al., 2020)
Explicit
Upwork
Ranking
Question Answering
FeedbackQA (Li et al., 2022)
Explicit
MTurk
Score, NL
Translation
WMT Metrics Shared Task (Freitag et al., 2022b)
Explicit
Pro translation workflow MQM, DA
Summarization
TAC Shared Tasks (TAC-2008, TAC-2009)
Explicit
N/A
Score
Table 2: Summary of existing human feedback datasets and their collection methods, which vary along several dimensions. Refer to Table 1 for definitions related to feedback types. A separation is drawn between datasets that were explicitly designed to capture human preferences in a general sense, and datasets designed for more specific use cases, such as MQM/DA datasets in MT. N/A means we could not find information.
systematic bias away from the originally intended task (Parmar et al., 2023).
Anchoring/Confirmation bias: When annota- tors are presented with a text in isolation, they may fail to consider better alternatives and erroneously label the text as high-quality (Bansal et al., 2021). When asked to generate text, anchoring bias can cause people to write in a different manner than usual (Jakesch et al., 2023; Lehmann et al., 2022), which may influence what types of suggestions or corrections they give. Mitigation strategies in- clude asking people to rank several diverse outputs and being explicit about the dimensions people are asked to evaluate.
Positivity bias: When giving feedback to learn- ers in traditional RL environments, users tend to give much more positive feedback than negative feedback, which may lead the agent to avoid the goal they are actually trying to reach in these sce- narios (Amershi et al., 2014b; Knox and Stone, 2013; Thomaz and Breazeal, 2008).
6.2.3 Ethical considerations Some subjectivity in annotator judgment can arise from differences across cultural or social groups. Santurkar et al. (2023) measure opinions in lan- guage model generations, demonstrating varying degrees of representation of demographic groups. Several works observe that tuning with human feedback increases the alignment of generated out- puts with US liberal views on controversial topics (Perez et al. (2022b), Hartmann et al. (2023)). An- notators with different demographic or political backgrounds may disagree on what qualifies as toxic content (Sap et al. (2022), Ding et al. (2022)). This is particularly pronounced when annotators are asked to make ethical judgments, which may vary with cultural context and personal sensibilities (Jiang et al. (2022), Talat et al. (2022)).
Steiger et al. (2021) survey moderators of toxic content, identifying harms ranging from slight dis- comfort to lasting psychological harm from the pro- longed performance of content moderation tasks; however, the severity and frequency of toxic con- tent examined in content moderation likely exceeds that in other types of human feedback annotation. Shmueli et al. (2021) identify toxicity classifica- tion and generation from open-ended inputs as two NLP annotation tasks that may trigger harmful re- sponses in annotators. They further argue that this moves beyond the “minimal risk” requirement for Institutional Review Board exemption in the United States and encourage academic researchers using crowdworker annotation to file for this ethical re- view of their work.
Media attention has also focused on fair pay for annotators, with one TIME article6 describing annotators paid $2 USD or less per hour to review toxic content and provide harmfulness annotations for model training. Research on crowdsourcing (Shmueli et al. (2021); Rothschild et al. (2022); Soratana et al. (2022); Toxtli et al. (2021); Hornuf and Vrankar (2022)) cautions that inadequate pay, especially for workers in lower-resourced regions, can be a form of worker exploitation.
7 AI Feedback
Feedback models have been crucial in advancing generation techniques by effectively leveraging feedback. However, they are heavily reliant on human input: for example, Gao et al. (2022) found that across various preference model sizes, utiliz- ing fewer than 1,000 comparisons resulted in only minor improvements, with outcomes approximat- ing chance. Moreover, employing static feedback
6https://time.com/6247678/ openai-chatgpt-kenya-workers/
12


can create consistency and accuracy challenges, as the integration of feedback leads to changes in the model’s output distribution. AI-generated feed- back, an emerging research area, focuses on har- nessing the large language model’s own abilities to evaluate and improve its output, enhancing the model without constant human intervention. Two primary approaches have emerged in this domain:
Self AI Feedback The first approach involves using the same model to provide feedback and im- prove its output. In this scenario, the model en- gages in a continuous self-improvement process, learning from its evaluations and refining its ca- pabilities accordingly. Examples of this approach include prompting models to generate harmful re- sponses and revising them for harmlessness (Bai et al., 2022b), or employing rule-based reward mod- els for RLHF fine-tuning (OpenAI, 2023a). Tech- niques such as iterative output revision through few-shot prompting (Peng et al., 2023; Shinn et al., 2023; Chen et al., 2023; Paul et al., 2023; Madaan et al., 2023; Yang et al., 2022) have been ex- plored using LLMs like GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023a). Notably, these techniques demonstrate potential when applied to LLMs trained to adhere to human instructions and align outputs with human preferences. This sug- gests that incorporating human feedback during training equips AI models to comprehend task re- quirements better, align outputs with directives, and function as dependable feedback mechanisms, thereby minimizing human intervention. Intrigu- ingly, the capacity to offer valuable AI feedback may depend on the model being trained with hu- man feedback.
External AI Feedback: The second approach employs a separate model to provide feedback on the model’s outputs which is being improved. In this setting, the task model is often paired with a separately trained feedback model (Yasunaga and Liang, 2020; Madaan et al., 2021; Welleck et al., 2022; Bai et al., 2022b; Akyürek et al., 2023). An advantage of this approach is that the feedback model does not need to be a large, general-purpose model like GPT-4. Thus, training smaller feedback models becomes an attractive alternative when a large amount of feedback is available.
Preprint
8 Conclusion
Recent developments in large language models have emphasised the need for human feedback to ensure models have desirable behaviour and gener- ate helpful and harmless text. In this survey paper, we provided an overview of a recent line of re- search on leveraging (human) feedback to improve natural language generation.
Despite the relatively infancy of this field, sev- eral important observations emerge when compar- ing all existing works:
1. Most feedback formats (and available datasets for them) are underleveraged: models are mostly optimized using ranking-based or nu- merical feedback, particularly when using feedback models. However, we have evi- dence that most forms of feedback could also provide useful signals for improving models, and natural language feedback seems to be a promising format due to its expressiveness.
2. The “juice” in leveraging (human) feedback seems to be in the feedback itself, rather than on the specific method to leverage it. De- spite the emphasis given to Reinforcement Learning from Human Feedback (RLHF) by recent popular works, our survey reveals nu- merous other approaches to leverage feedback, all of which report improvements over non- feedback-augmented baselines, and recent comparative work even suggests that RLHF might be outperformed by simpler, easier to leverage methods (Gao et al., 2022; Rafailov et al., 2023). However, a more comprehensive, large-scale study comparing more methods is still lacking.
3. It’s still unclear what role (human) feedback plays in improving the model’s behavior, and how much of it is actually needed: the success of AI Feedback seems to hint that we can mas- sively reduce the need for human supervision, and some recent work (Zhou et al., 2023a) raises questions if feedback is needed at all when a small amount of high-quality data with human instructions is available for supervised learning.
Overall, we hope this survey can help re- searchers understand the current state of the art, and identify new and existing sources of feedback and ways of leveraging it.
13


Acknowledgments
This work was supported by EU’s Horizon Europe Research and Innovation Actions (UTTER, con- tract 101070631) and by the projects MAIA and NextGenAI (LISBOA-01-0247-FEDER-045909 and 2022-C05i0102-02).
References
Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wi- jaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs.
Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014a. Power to the people: The role of humans in interactive ma- chine learning. Ai Magazine, 35(4):105–120.
Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014b. Power to the people: The role of humans in interactive ma- chine learning. AI Magazine, 35(4):105–120.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in AI safety. CoRR, abs/1606.06565.
Chantal Amrhein and Rico Sennrich. 2022. Iden- tifying weaknesses in machine translation met- rics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1125–1141, Online only. Associ- ation for Computational Linguistics.
Kushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and Jason Weston. 2022. Director: Generator- Classifiers For Supervised Language Model- ing. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing.
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
Preprint
Jones, Nicholas Joseph, Ben Mann, Nova Das- Sarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.
Karl Johan Åström and Richard M Murray. 2021. Feedback systems: an introduction for scientists and engineers. Princeton university press.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Her- nandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran- Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sell- itto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mc- Candlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feed- back.
Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the whole exceed its parts? the effect of ai explanations on complementary team per- In Proceedings of the 2021 CHI formance. Conference on Human Factors in Computing Systems, pages 1–16.
14


Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 610–623, New York, NY, USA. Associa- tion for Computing Machinery.
Pushpak Bhattacharyya, Rajen Chatterjee, Markus Freitag, Diptesh Kanojia, Matteo Negri, and Marco Turchi. 2022. Findings of the WMT 2022 shared task on automatic post-editing. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 109–117, Abu Dhabi, United Arab Emirates (Hybrid). As- sociation for Computational Linguistics.
Florian Böhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. rewards yield better sum- 2019. maries: Learning to summarise without refer- ences. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3110–3120, Hong Kong, China. Association for Computational Linguistics.
Better
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Ni- ladri S. Chatterji, Annie S. Chen, Kathleen A. Creel, Jared Davis, Dora Demszky, Chris Don- ahue, Moussa Doumbouya, Esin Durmus, Ste- fano Ermon, John Etchemendy, Kawin Etha- yarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tat- sunori Hashimoto, Peter Henderson, John He- witt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam-
Preprint
cheti, Geoff Keeling, Fereshte Khani, O. Khat- tab, Pang Wei Koh, Mark S. Krass, Ranjay Kr- ishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Mun- yikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel J. Orr, Isabel Pa- padimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghu- nathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R’e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Para- suram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Ya- sunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. ArXiv, abs/2108.07258.
Nick Bostrom. 2014. Superintelligence: Paths, Dangers, Strategies, 1st edition. Oxford Uni- versity Press, Inc., USA.
Arun Chaganty, Stephen Mussmann, and Percy Liang. 2018. The price of debiasing auto- matic metrics in natural language evalaution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 643–653, Mel- bourne, Australia. Association for Computa- tional Linguistics.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large lan- arXiv preprint guage models to self-debug. arXiv:2304.05128.
Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that’s ‘human’ is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
15


Papers), pages 7282–7296, Online. Association for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.
Gonçalo M. Correia and André F. T. Martins. 2019. A simple and effective approach to automatic post-editing with transfer learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3050–3056, Florence, Italy. Association for Computational Linguistics.
Michael Denkowski, Chris Dyer, and Alon Lavie. 2014. Learning from post-editing: Online model adaptation for statistical machine trans- lation. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395–404, Gothenburg, Sweden. Association for Computa- tional Linguistics.
Yi Ding, Jacob You, Tonja-Katrin Machulla, Jen- nifer Jacobs, Pradeep Sen, and Tobias Höllerer. Impact of annotator demographics 2022. on sentiment dataset labeling. Proc. ACM Hum.-Comput. Interact., 6(CSCW2).
Jesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groen- eveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled cor- In Proceedings of the 2021 Conference pus. on Empirical Methods in Natural Language Processing, pages 1286–1305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. Nl- edit: Correcting semantic parse errors through natural language interaction. arXiv preprint arXiv:2103.14540.
Kawin Ethayarajh, Heidi Zhang, Yizhong Wang, and Dan Jurafsky. 2023. Stanford human prefer- ences dataset.
Patrick Fernandes, António Farinhas, Ricardo Rei, José De Souza, Perez Ogayo, Graham
Preprint
Neubig, and Andre Martins. 2022. Quality- aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1396–1412, Seattle, United States. Association for Computational Linguis- tics.
J.L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76:378–382.
Markus Freitag, George Foster, David Grang- ier, Viresh Ratnakar, Qijun Tan, and Wolf- gang Macherey. 2021. Experts, errors, and context: A large-scale study of human evalu- ation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460–1474.
Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022a. High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the Association for Computational Linguistics, 10:811–825.
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi- kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Results of André F. T. Martins. 2022b. WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Associa- tion for Computational Linguistics.
Leo Gao, John Schulman, and Jacob Hilton. 2022. Scaling laws for reward model overoptimization.
Yang Gao, Christian M. Meyer, and Iryna Interactively Gurevych. 2018. learning to summarise by combining active preference learning and reinforcement learn- ing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4120–4130, Brussels, Bel- gium. Association for Computational Linguis- tics.
APRIL:
Sebastian Gehrmann, Abhik Bhattacharjee, Abi- naya Mahendiran, Alex Wang, Alexandros Pa-
16


pangelis, Aman Madaan, Angelina McMillan- Major, Anna Shvets, Ashish Upadhyay, Bing- sheng Yao, Bryan Wilie, Chandra Bhagavat- ula, Chaobin You, Craig Thomson, Cristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra Gkatzia, Dragomir R. Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip Ginter, Genta Indra Winata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny Chim, Jiawei Zhou, Jordan Clive, Joshua Maynez, João Se- doc, Juraj Juraska, Kaustubh D. Dhole, Khy- athi Raghavi Chandu, Leonardo F. R. Ribeiro, Lewis Tunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz, Michael White, Mihir Kale, Moussa Kamal Eddine, Nico Daheim, Nis- hant Subramani, Ondrej Dusek, Paul Pu Liang, Pawan Sasanka Ammanamanchi, Qinqin Zhu, Ratish Puduppully, Reno Kriz, Rifat Shahri- yar, Ronald Cardenas, Saad Mahamood, Sa- lomey Osei, Samuel Cahyawijaya, Sanja vSta- jner, Sébastien Montella, Shailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao Shen, Tosin P. Adewumi, Vikas Raunak, Vipul Ra- heja, Vitaly Nikolaev, Vivian Tsai, Yacine Jer- nite, Yi Xu, Yisi Sang, Yixin Liu, and Yu- fang Hou. 2022a. Gemv2: Multilingual nlg benchmarking in a single line of code. In Conference on Empirical Methods in Natural Language Processing.
Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2022b. Repairing the cracked foun- dation: A survey of obstacles in evaluation arXiv preprint practices for generated text. arXiv:2202.06935.
Bhavya Ghai, Q Vera Liao, Yunfeng Zhang, Rachel Bellamy, and Klaus Mueller. 2021. Explainable active learning (xal) toward ai explanations as interfaces for machine teachers. Proceedings of the ACM on Human-Computer Interaction, 4(CSCW3):1–28.
Gaurav R. Ghosal, Matthew Zurek, Daniel S. Brown, and Anca D. Dragan. 2023. The effect of modeling human rationality level on learning rewards from multiple feedback types.
Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language
Preprint
Data with Amazon’s Mechanical Turk, pages 148–151, Los Angeles. Association for Com- putational Linguistics.
Amelia Glaese, Nat McAleese, Maja Tr˛ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari- beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving align- ment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.
Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, and André F. T. Martins. 2021. Uncertainty- aware machine translation evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics.
C. A. E. Goodhart. 1984. Problems of Monetary Management: The UK Experience. Macmillan Education UK, London.
Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S. Bernstein. 2022. Jury learning: Integrating dissenting voices into machine learning models. In CHI Conference on Human Factors in Computing Systems. ACM.
Yvette Graham, Timothy Baldwin, Alistair Mof- fat, and Justin Zobel. 2013. Continuous mea- surement scales in human evaluation of machine translation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33–41, Sofia, Bulgaria. Asso- ciation for Computational Linguistics.
Braden Hancock, Antoine Bordes, Pierre- Emmanuel Mazare, and Jason Weston. 2019. Learning from Dialogue after Deployment: Feed Yourself, Chatbot! In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3667–3684.
Jochen Hartmann, Jasper Schwenzow, and Maxim- ilian Witte. 2023. The political ideology of con- versational ai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation.
A.F Hayes and K Krippendorff. 2007. Answer- ing the call for a standard reliability measure for coding data. Communication Methods and Measures, 1:77–89.
17


Dan Hendrycks, Collin Burns, Steven Basart, An- drew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning ai with shared hu- man values. arXiv preprint arXiv:2008.02275.
Lars Hornuf and Daniel Vrankar. 2022. Hourly wages in crowdworking: A meta-analysis. Business & Information Systems Engineering, 64(5):553–573.
Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman. 2023. Co- writing with opinionated language models af- fects users’ views. ArXiv, abs/2302.00560.
Natasha Jaques, Asma Ghandeharioun, Judy Han- wen Shen, Craig Ferguson, Àgata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind W. Pi- card. 2019. Way off-policy batch deep reinforce- ment learning of implicit human preferences in dialog. CoRR, abs/1907.00456.
Liwei Jiang, Jena D. Hwang, Chandra Bhagavat- ula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Bor- chardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, and Yejin Choi. 2022. Can machines learn morality? the delphi experiment.
Daniel Kahneman, Sibony Olivier, and Cass R. Sunstein. 2021. Little, Brown Spark, New York.
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021a. Alignment of language agents. arXiv preprint arXiv:2103.14659.
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021b. Alignment of language agents. CoRR, abs/2103.14659.
W. Bradley Knox and Peter Stone. 2013. Learn- ing non-myopically from human-generated re- ward. In Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI ’13, page 191–202, New York, NY, USA. Asso- ciation for Computing Machinery.
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason Phang, Samuel R. Bowman, and Ethan Perez. 2023. Pretraining language models with human preferences.
Preprint
Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler. 2018. Can neural machine translation be improved with user feedback? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 92–105, New Orleans - Louisiana. As- sociation for Computational Linguistics.
Shankar Kumar and William Byrne. 2002. Min- imum bayes-risk word alignments of bilin- In Proceedings of the ACL-02 gual texts. Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP ’02, page 140–147, USA. Association for Computa- tional Linguistics.
Florian Lehmann, Niklas Markert, Hai Dang, and Daniel Buschek. 2022. Suggestion lists vs. con- tinuous generation: Interaction design for writ- ing with generative models on mobile devices affect text length, wording and perceived au- thorship. Proceedings of Mensch und Computer 2022.
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. 2018. Scalable agent alignment via reward modeling: a research direction. ArXiv, abs/1811.07871.
Jiwei Li, Alexander H Miller, Sumit Chopra, Marc’Aurelio Ranzato, and Jason Weston. 2017. Dialogue Learning With Human-in-the- Loop. In International Conference on Learning Representations.
Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Cheung, and Siva Reddy. 2022. Using interac- tive feedback to improve the accuracy and ex- plainability of question answering systems post- deployment. In Findings of the Association for Computational Linguistics: ACL 2022. Associ- ation for Computational Linguistics.
Bing Liu, Gokhan Tür, Dilek Hakkani-Tür, Pararth Shah, and Larry Heck. 2018. Dialogue learn- ing with human teaching and feedback in end- to-end trainable task-oriented dialogue systems. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages
18


2060–2069, New Orleans, Louisiana. Associa- tion for Computational Linguistics.
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Languages are Rewards: Hindsight Fine- tuning using Human Feedback. arXiv preprint arXiv:2302.02676.
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. 2022. BRIO: Bringing order to abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2890–2903, Dublin, Ireland. As- sociation for Computational Linguistics.
Arle Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014a. Multidimensional qual- ity metrics (MQM): A framework for declar- ing and describing translation quality metrics. Tradumàtica: tecnologies de la traducció, 0:455– 463.
Arle Lommel, Hans Uszkoreit, and Aljoscha Bur- chardt. 2014b. Multidimensional quality met- rics (MQM): A framework for declaring and describing translation quality metrics. Revista Tradumàtica: tecnologies de la traducció.
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve GPT-3 after deploy- ment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833–2861, Abu Dhabi, United Arab Emirates. Association for Compu- tational Linguistics.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yim- ing Yang, Sean Welleck, Bodhisattwa Prasad Ma- jumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative re- finement with self-feedback.
Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. improving defeasible 2021. Think about it! reasoning by first modeling the question sce- nario. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291–6310, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Preprint
Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevalu- ating the evaluation of automatic machine trans- lation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984–4997, Online. Association for Computational Linguis- tics.
Nikhil Mehta and Dan Goldwasser. 2019. Improv- ing natural language interaction with robots us- ing advice. arXiv preprint arXiv:1905.04655.
Richard Ngo. 2022. The alignment problem from arXiv preprint
a deep learning perspective. arXiv:2209.00626.
Duy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-Sinh Nguyen, Dung Tien Tien Le, Sha- hab Sabahi, Minh-Tien Nguyen, and Hung Le. 2022. Make the most of prior data: A solution for interactive text summarization with prefer- ence feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1919–1930, Seattle, United States. Asso- ciation for Computational Linguistics.
Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What can we learn from collective human opin- ions on natural language inference data? arXiv preprint arXiv:2010.03532.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. A conversational paradigm for program synthesis. arXiv e-prints, pages arXiv–2203.
OpenAI. 2023a. Gpt-4 technical report.
OpenAI. 2023b. Model index for researchers. Ac-
cessed: 2023-05-01.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel- ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feed- back.
Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, and Josef van Genabith. 2016. A neural net- work based approach to automatic post-editing.
19


In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 281–286, Berlin, Germany. Association for Computational Linguistics.
Mihir Parmar, Swaroop Mishra, Mor Geva, and Chitta Baral. 2023. Don’t blame the annotator: Bias already starts in the annotation instructions.
Tatiana Passali, Alexios Gidiotis, Efstathios Chatzikyriakidis, and Grigorios Tsoumakas. Towards human-centered summa- 2021. rization: A case study on financial news. the First Workshop on In Proceedings of Bridging Human–Computer Interaction and Natural Language Processing, pages 21–27, On- line. Association for Computational Linguistics.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904.
Romain Paulus, Caiming Xiong, and Richard A deep reinforced model CoRR, summarization.
Socher. 2017. abstractive for abs/1705.04304.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Lidén, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with exter- nal knowledge and automated feedback. ArXiv, abs/2302.12813.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022a. Red teaming language models with lan- guage models.
Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jack- son Kernion, James Landis, Jamie Kerr, Jared
Preprint
Mueller, Jeeyoon Hyun, Joshua Landau, Ka- mal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2022b. Discovering language model behaviors with model-written evaluations.
Maxime Peyrard, Teresa Botschen, and Iryna Gurevych. 2017. Learning to score system sum- maries for better content selection evaluation. In Proceedings of the Workshop on New Frontiers in Summarization, pages 74–84, Copenhagen, Denmark. Association for Computational Lin- guistics.
Maxime Peyrard and Iryna Gurevych. 2018. Ob- jective function learning to match human judge- ments for optimization-based summarization. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 654–660, New Orleans, Louisiana. Association for Computational Linguistics.
Barbara Plank. 2022. The ’problem’ of human la- bel variation: On ground truth in data, modeling and evaluation.
Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5score: Discriminative fine- tuning of generative evaluation metrics.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimiza- tion: Your language model is secretly a reward model.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020a. COMET: A neural frame- In Proceedings of work for MT evaluation. the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
20


2685–2702, Online. Association for Computa- tional Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020b. Unbabel’s participation in the WMT20 metrics shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 911–920, Online. Association for Compu- tational Linguistics.
Machel Reid and Graham Neubig. 2022. Learn- ing to model editing processes. arXiv preprint arXiv:2205.12374.
Christopher Riesbeck. 1981. Failure-driven re- In IJCAI,
minding for incremental learning. pages 115–120. Citeseer.
Arturo Rosenblueth, Norbert Wiener, and Julian Bigelow. 1943. Behavior, purpose and teleology. Philosophy of science, 10(1):18–24.
Annabel Rothschild, Justin Booker, Christa Davoll, Jessica Hill, Venise Ivey, Carl DiSalvo, Ben Ry- dal Shapiro, and Betsy DiSalvo. 2022. Towards fair and pro-social employment of digital piece- workers for sourcing machine learning training data. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1–9.
Ananya B. Sai, Akash Kumar Mohankumar, and Mitesh M. Khapra. 2022. A survey of evaluation metrics used for nlg systems. ACM Comput. Surv., 55(2).
Shibani Santurkar, Esin Durmus, Faisal Lad- hak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models reflect?
Maarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022. Annotators with attitudes: How annotator beliefs and identities bias toxic language detec- tion.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators.
Roger C Schank. 1983. Dynamic memory: A theory of reminding and learning in computers and people. cambridge university press.
Preprint
Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with language feedback.
Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2023. Train- ing language models with language feedback at scale.
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izac- ard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model. ArXiv, abs/2208.11663.
Natalie Schluter. 2017.
The limits of auto- matic summarisation according to ROUGE. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 41–45, Valencia, Spain. Associa- tion for Computational Linguistics.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Associa- tion for Computational Linguistics.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine trans- lation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683–1692, Berlin, Germany. Association for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Premkumar Natara- jan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in lan- guage generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Joint Conference on Natural International
21


Language Processing (EMNLP-IJCNLP), pages 3407–3412, Hong Kong, China. Association for Computational Linguistics.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? improv- ing data quality and data mining using multi- ple, noisy labelers. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’08, page 614–622, New York, NY, USA. Asso- ciation for Computing Machinery.
Weiyan Shi, Yu Li, Saurav Sahay, and Zhou Yu. 2021. Refine and imitate: Reducing rep- etition and inconsistency in persuasion dia- logues via reinforcement learning and human demonstration. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3478–3492, Punta Cana, Dominican Re- public. Association for Computational Linguis- tics.
Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection.
Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. 2021. Beyond fair pay: Ethical implica- tions of NLP crowdsourcing. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3758–3769, Online. Association for Com- putational Linguistics.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and Roland Kuhn. 2007. Rule-based transla- tion with statistical phrase-based post-editing. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 203–206.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, Hawaii. Association for Computa- tional Linguistics.
Teerachart Soratana, Yili Liu, and X Jessie Yang. 2022. Effects of payment rate and country’s
Preprint
income level on attitude toward acrowdsourc- ing task. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, vol- ume 66, pages 2220–2224. SAGE Publications Sage CA: Los Angeles, CA.
Miriah Steiger, Timir J Bharucha, Sukrit Venkata- giri, Martin J. Riedl, and Matthew Lease. 2021. The psychological well-being of content mod- erators: The emotional labor of commercial moderation and avenues for improving support. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI ’21, New York, NY, USA. Association for Comput- ing Machinery.
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Rad- ford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback.
Simone Stumpf, Vidya Rajaram, Lida Li, Mar- garet Burnett, Thomas Dietterich, Erin Sulli- van, Russell Drummond, and Jonathan Her- locker. 2007. Toward harnessing user feedback for machine learning. In Proceedings of the 12th international conference on Intelligent user interfaces, pages 82–91.
Zeerak Talat, Hagen Blix, Josef Valvoda, Maya In- dira Ganesh, Ryan Cotterell, and Adina Williams. 2022. On the machine learning of In ethical judgments from natural language. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 769–779, Seattle, United States. Association for Computational Linguis- tics.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Teach- ing pre-trained models to systematically rea- son over implicit knowledge. arXiv preprint arXiv:2006.06609, 4(6).
Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repair- ing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339–352, Seattle, United States. Association for Computational Linguis- tics.
22


Andrea L. Thomaz and Cynthia Breazeal. 2008. Teachable robots: Understanding human teach- ing behavior to build more effective robot learn- ers. Artificial Intelligence, 172(6):716–737.
Craig Thomson and Ehud Reiter. 2021. Genera- tion challenges: Results of the accuracy evalu- ation shared task. In Proceedings of the 14th International Conference on Natural Language Generation, pages 240–248, Aberdeen, Scotland, UK. Association for Computational Linguistics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022a. Lamda: Language mod- els for dialog applications.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng- Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenber- gen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera- Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022b. Lamda: Language models for dialog applications. CoRR, abs/2201.08239.
Carlos Toxtli, Siddharth Suri, and Saiph Sav- age. 2021. Quantifying the invisible labor in crowd work. Proceedings of the ACM on human-computer interaction, 5(CSCW2):1–26.
Alexander Matt Turner, Aseem Saxena, and Prasad Tadepalli. 2022. Formalizing the problem of side effect regularization. In NeurIPS ML Safety Workshop.
Peter Vamplew, Richard Dazeley, Cameron Foale, Sally Firmin, and Jane Mummery. 2018. Human- aligned artificial intelligence is a multiobjective
Preprint
problem. Ethics and Information Technology, 20:27–40.
Christopher JCH Watkins and Peter Dayan. 1992.
Q-learning. Machine learning, 8:279–292.
Sean Welleck, Ximing Lu, Peter West, Faeze Brah- man, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053.
Jason Weston, Sumit Chopra, and Antoine Bor- des. 2014. Memory networks. arXiv preprint arXiv:1410.3916.
Jason E Weston. 2016. Dialog-based language Advances in Neural Information
learning. Processing Systems.
Norbert Wiener. 1948. Cybernetics; or control and communication in the animal and the machine.
Ronald J. Williams. 1992.
Simple statistical gradient-following algorithms for connection- learning. Mach. Learn., ist reinforcement 8(3–4):229–256.
Bin Wu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2018. Query suggestion with feed- back memory network. In Proceedings of the 2018 World Wide Web Conference, pages 1563– 1571.
Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris- tiano. 2021. Recursively summarizing books with human feedback.
Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston. 2022. Learning New Skills after Deployment: Improv- ing open-domain internet-driven dialogue with human feedback.
Taku Yamagata, Ryan McConville, and Raul Santos-Rodriguez. 2021. Reinforcement learn- ing with feedback from multiple humans with diverse skills.
Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer sto- ries with recursive reprompting and revision. In Conference on Empirical Methods in Natural Language Processing.
23


Michihiro Yasunaga and Percy Liang. 2020. Graph- based, self-supervised program repair from di- agnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730–10739.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language mod- els with human feedback without tears. arXiv preprint arXiv:2304.05302.
Chrysoula Zerva, Taisiya Glushkova, Ricardo Rei, and André F. T. Martins. 2022. Disentangling uncertainty in machine translation evaluation.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. Lima: Less is more for alignment.
Shuyan Zhou, Uri Alon, Sumit Agarwal, and Gra- ham Neubig. 2023b. Codebertscore: Evaluating code generation with pretrained models of code. arXiv preprint arXiv:2302.05527.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human pref- erences. CoRR, abs/1909.08593.
Markus Zopf. 2018. Estimating summary qual- ity with pairwise preferences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1687–1696, New Orleans, Louisiana. Association for Com- putational Linguistics.
Preprint
24