Daniel Fried26266850
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['Pragmatic Inference with a CLIP Listener for Contrastive Captioning', '2023', ['Annual Meeting of the Association for Computational Linguistics', 'Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations', ['Jiefu Ou', 'Benno Krojer', 'Daniel Fried']]
["SantaCoder: don't reach for the stars!", '2023', ['arXiv.org', 'ArXiv'], 'The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.', ['Loubna Ben Allal', 'Raymond Li', 'Denis Kocetkov', 'Chenghao Mou', 'Christopher Akiki', 'Carlos Muñoz Ferrandis', 'Niklas Muennighoff', 'Mayank Mishra', 'A. Gu', 'Manan Dey', 'Logesh Kumar Umapathi', 'Carolyn Jane Anderson', 'Yangtian Zi', 'J. Poirier', 'Hailey Schoelkopf', 'S. Troshin', 'Dmitry Abulkhanov', 'M. Romero', 'M. Lappert', 'F. Toni', "Bernardo Garc'ia del R'io", 'Qian Liu', 'Shamik Bose', 'Urvashi Bhattacharyya', 'Terry Yue Zhuo', 'I. Yu', 'Paulo Villegas', 'Marco Zocca', 'Sourab Mangrulkar', 'D. Lansky', 'Huu Nguyen', 'Danish Contractor', 'Luisa Villa', 'Jia Li', 'Dzmitry Bahdanau', 'Yacine Jernite', 'S. Hughes', 'Daniel Fried', 'Arjun Guha', 'Harm de Vries', 'Leandro von Werra']]
['Grounding Language Models to Images for Multimodal Generation', '2023', ['arXiv.org', 'ArXiv'], 'We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an ef-fective, general solution for leveraging pretrained language models in visually grounded settings.', ['Jing Yu Koh', 'R. Salakhutdinov', 'Daniel Fried']]
['StarCoder: may the source be with you!', '2023', ['arXiv.org', 'ArXiv'], 'The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.', ['Raymond Li', 'Loubna Ben Allal', 'Yangtian Zi', 'Niklas Muennighoff', 'Denis Kocetkov', 'Chenghao Mou', 'Marc Marone', 'Christopher Akiki', 'Jia Li', 'Jenny Chim', 'Qian Liu', 'Evgenii Zheltonozhskii', 'Terry Yue Zhuo', 'Thomas Wang', 'Olivier Dehaene', 'Mishig Davaadorj', 'J. Lamy-Poirier', 'João Monteiro', 'Oleh Shliazhko', 'Nicolas Gontier', 'Nicholas Meade', 'Armel Zebaze', 'Ming-Ho Yee', 'Logesh Kumar Umapathi', 'Jian Zhu', 'Benjamin Lipkin', 'Muhtasham Oblokulov', 'Zhiruo Wang', 'Rudra Murthy', 'J. Stillerman', 'S. Patel', 'Dmitry Abulkhanov', 'Marco Zocca', 'Manan Dey', 'Zhihan Zhang', 'N. Fahmy', 'Urvashi Bhattacharyya', 'W. Yu', 'Swayam Singh', 'Sasha Luccioni', 'Paulo Villegas', 'M. Kunakov', 'Fedor Zhdanov', 'Manuel Romero', 'Tony Lee', 'Nadav Timor', 'Jennifer Ding', 'Claire Schlesinger', 'Hailey Schoelkopf', 'Jana Ebert', 'Tri Dao', 'Mayank Mishra', 'A. Gu', 'Jennifer Robinson', 'Carolyn Jane Anderson', 'Brendan Dolan-Gavitt', 'Danish Contractor', 'Siva Reddy', 'Daniel Fried', 'Dzmitry Bahdanau', 'Yacine Jernite', 'Carlos Muñoz Ferrandis', 'Sean M. Hughes', 'Thomas Wolf', 'Arjun Guha', 'Leandro von Werra', 'Harm de Vries']]
['Generating Images with Multimodal Language Models', '2023', ['arXiv.org', 'ArXiv'], 'We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.', ['Jing Yu Koh', 'Daniel Fried', 'R. Salakhutdinov']]
['WebArena: A Realistic Web Environment for Building Autonomous Agents', '2023', ['arXiv.org', 'ArXiv'], 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.', ['Shuyan Zhou', 'Frank F. Xu', 'Hao Zhu', 'Xuhui Zhou', 'Robert Lo', 'Abishek Sridhar', 'Xianyi Cheng', 'Yonatan Bisk', 'Daniel Fried', 'Uri Alon', 'Graham Neubig']]
