FINDINGSOFTHEIWSLT2023EVALUATIONCAMPAIGNMilindAgarwal1SwetaAgrawal2AntoniosAnastasopoulos1LuisaBentivogli3OndˇrejBojar4ClaudiaBorg5MarineCarpuat2RoldanoCattoni3MauroCettolo3MingdaChen6WilliamChen7KhalidChoukri8AlexandraChronopoulou9AnnaCurrey10ThierryDeclerck11QianqianDong12KevinDuh13YannickEst`eve14MarcelloFederico10SouhirGahbiche15BarryHaddow16BenjaminHsu10PhuMonHtut10HirofumiInaguma6D´avidJavorsk´y4JohnJudge17YasumasaKano18TomKo12RishuKumar4PengweiLi6XutaiMa6PrashantMathur10EvgenyMatusov19PaulMcNamee13JohnP.McCrae20KentonMurray13MariaNadejde10SatoshiNakamura18MatteoNegri3HaNguyen14JanNiehues21XingNiu10AtulKr.Ojha20JohnE.Ortega22ProyagPal16JuanPino6LonnekevanderPlas23PeterPol´ak4ElijahRippeth2ElizabethSalesky13JiatongShi7MatthiasSperber24SebastianSt¨uker25KatsuhitoSudoh18YunTang6BrianThompson10KevinTran6MarcoTurchi25AlexWaibel7MingxuanWang12ShinjiWatanabe7RodolfoZevallos261GMU2UMD3FBK4CharlesU.5U.Malta6Meta7CMU8ELDA9LMU10AWS11DFKI12ByteDance13JHU14AvignonU.15Airbus16U.Edinburgh18NAIST19AppTek20U.Galway21KIT22NortheasternU.23IDIAP24Apple25Zoom26U.PompeuFabraAbstractThispaperreportsonthesharedtasksorga-nizedbythe20thIWSLTConference.Thesharedtasksaddress9scientificchallengesinspokenlanguagetranslation:simultane-ousandofflinetranslation,automaticsubti-tlinganddubbing,speech-to-speechtransla-tion,multilingual,dialectandlow-resourcespeechtranslation,andformalitycontrol.Thesharedtasksattractedatotalof38submis-sionsby31teams.Thegrowinginterestto-wardsspokenlanguagetranslationisalsowit-nessedbytheconstantlyincreasingnumberofsharedtaskorganizersandcontributorstotheoverviewpaper,almostevenlydistributedacrossindustryandacademia.1IntroductionTheInternationalConferenceonSpokenLan-guageTranslation(IWSLT)isthepremieran-nualscientificconferenceforallaspectsofspokenlanguagetranslation(SLT).IWSLTisorganizedbytheSpecialInterestGrouponSpokenLan-guageTranslation(SIG-SLT),whichissupportedbyACL,ISCAandELRA.Likeinallpreviouseditions(Akibaetal.,2004;EckandHori,2005;Paul,2006;Fordyce,2007;Paul,2008,2009;Pauletal.,2010;Federicoetal.,2011,2012;Cettoloetal.,2013,2014,2015,2016,2017;Niehuesetal.,2018,2019;Ansarietal.,2020;Anasta-sopoulosetal.,2021,2022),thisyear’sconferencewasprecededbyanevaluationcampaignfeatur-ingsharedtasksaddressingscientificchallengesinSLT.Thispaperreportsonthe2023IWSLTEval-uationCampaign,whichofferedthefollowing9sharedtasks:•OfflineSLT,withfocusonspeech-to-texttranslationofrecordedconferencesandinter-viewsfromEnglishtoGerman,JapaneseandChinese.•SimultaneousSLT,focusingonspeech-to-texttranslationofstreamedaudioofconfer-encesandinterviewsfromEnglishtoGerman,JapaneseandChinese.•AutomaticSubtitling,withfocusonspeech-to-subtitletranslationofaudio-visualdocu-mentsfromEnglishtoGermanandSpanish.•MultilingualSLT,withfocusonspeech-to-texttranslationofrecordedscientifictalksfrom
1 Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 1–61 July 13-14, 2023 ©2023 Association for Computational Linguistics


NAVER
I2R
NEURODUB
U.ofScienceandTechnologyofChina(Dengetal.,2023;Zhouetal.,2023)
ON-TRACConsortium,France(Laurentetal.,2023)
XIAOMI
NAIST
AppTek,Germany(Baharetal.,2023)
GMU
NEMO
BIT
BIGAI
USTC
KoreaUniversityXUpstage,SouthKorea(Wuetal.,2023;Leeetal.,2023)
BeijingInstituteofTechnology,China(Wangetal.,2023b)
GeorgeMasonUniversity,USA(MbuyaandAnastasopoulos,2023)
NPU-MSXF
U.ofSci.andTechn.ofChina,TencentAILab,StateKeyLab.ofCognitiveIntelligence(Duetal.,2023)
NeuroDub,Armenia
ON-TRAC
Table1:ListofParticipantsEnglishintoArabic,Chinese,Dutch,French,German,Japanese,Farsi,Portuguese,Russian,andTurkish.•Speech-to-speechtranslation,focusingonnatural-speechtosynthetic-speechtranslationofrecordedutterancesfromEnglishtoChinese.•AutomaticDubbing,focusingondubbingofshortvideoclipsfromGermantoEnglish.•DialectSLT,focusingonspeechtranslationofrecordedutterancesfromTunisianArabictoEnglish.•Low-resourceSLT,focusingonspeechtrans-lationofrecordedutterancesfromIrishtoEn-glish,MarathitoHindi,MaltesetoEnglish,PashtotoFrench,TamasheqtoFrench,andQuechuatoSpanish.•FormalityControlforSLT,focusingonfor-mality/registercontrolforspokenlanguagetranslationfromEnglishtoKorean,Viet-namese,EUPortuguese,andRussian.Thesharedtasksattracted38submissionsby31teams(seeTable1)representingbothacademicandindustrialorganizations.Thefollowingsec-tionsreportoneachsharedtaskindetail,inpar-ticular:thegoalandautomaticmetricsadoptedforthetask,thedatausedfortrainingandtestingdata,thereceivedsubmissionsandthesummaryofre-sults.Detailedresultsforsomeofthesharedtasksarereportedinacorrespondingappendix.2OfflineSLTOfflinespeechtranslationisthetaskoftranslatingaudiospeechinonelanguageintotextinadiffer-enttargetlanguage,withoutanyspecifictimeorstructuralconstraints(as,forinstance,inthesi-multaneous,subtitling,anddubbingtasks).Un-derthisgeneralproblemdefinition,thegoalof
KIT
KUXUPSTAGE
UCSC
UniversitatPolit`ecnicadeCatalunya,Spain(Tsiamasetal.,2023)
BeijingInstituteofGeneralArtificialIntelligence,China(Xie,2023)
Team
NiuTrans,China(Hanetal.,2023)
CUNI-KIT
SRI-B
(Guoetal.,2023;Shangetal.,2023;Raoetal.,2023)
NorthwesternPolytechnicalU.,NanjingU.,MaShangCo.,China(Songetal.,2023)
CarnegieMellonUniversity,USA(Yanetal.,2023)
APPTEK
JohnsHopkinsUniversity,USA(Husseinetal.,2023;Xinyuanetal.,2023)
U.ofCalifornia,SantaCruz,USA(Vakhariaetal.,2023)
HuaweiTranslationServicesCenter,China(Lietal.,2023;Wangetal.,2023a)
BrnoUniversityofTechnology,Czechia(Kesirajuetal.,2023)
InstituteforInfocommResearch,A*STAR,Singapore(Huzaifahetal.,2023)
NIUTRANS
XiaomiAILab,China(Huangetal.,2023)
QUESPA
KU
MINETRANS
JHU
BUT
KarlsruheInstituteofTechnology,Germany(Liuetal.,2023)
KyotoUniversity,Japan(Yangetal.,2023)
CMU
FBK
CharlesUniversity,Czechia,andKIT,Germany(Pol´aketal.,2023)
SamsungR&DInstituteBangalore,India(Radhakrishnanetal.,2023)
FondazioneBrunoKessler,Italy(Papietal.,2023b)
NVIDIANeMo,USA(Hrinchuketal.,2023)
TranslatedSrl,Italy(Perone,2023)
UM-DFKI
AmazonAlexaAI,USA(Vishnuetal.,2023)
U.ofMalta,Malta,andDFKI,Germany(Williamsetal.,2023)
NAVERLabsEurope,France(Gow-Smithetal.,2023)
Organization
UPC
ALEXAAI
MATESUB
NaraInstituteofScienceandTechnology,Japan(Fukudaetal.,2023)
HW-TSC
NortheasternU,USA,U.dePompeuFabra,Spain,CMU,USA(Ortegaetal.,2023)
2


theofflineSTtrack(oneofthespeechtaskswiththelongesttraditionattheIWSLTcampaign)istoconstantlychallengeatechnologyinrapidevolu-tionbygraduallyintroducingnoveltyaspectsthatraisethedifficultybar.2.1ChallengeIncontinuitywithlastyear,participantsweregiventhreesub-taskscorrespond-ingtothreelanguagedirections,namelyEnglish→German/Japanese/Chinese.Partici-pationwasallowedbothwithcascadearchitec-turescombiningautomaticspeechrecognition(ASR)andmachinetranslation(MT)systemsascorecomponents,orbymeansofend-to-endapproachesthatdirectlytranslatetheinputspeechwithoutintermediatesymbolicrepresentations.Alsothisyear,oneofthemainobjectiveswasindeedtomeasuretheperformancedifferencebetweenthetwoparadigms,agapthatrecentresearch(Bentivoglietal.,2021)andIWSLTfind-ings(Ansarietal.,2020;Anastasopoulosetal.,2021,2022)indicateasgraduallydecreasing.TheothermainobjectiveofthisroundwastoassesstheabilityofSLTtechnologytodealwithcomplexscenariosinvolvingdifferenttypesofin-putcharacterizedbyphenomenalikespontaneousspeech,noisyaudioconditionsandoverlappingspeakers.Inlightofthis,themainnoveltyofthe2022offlineSLTtaskliesinarichervarietyofspeechdatatobeprocessed.Tothisaim,inaddi-tiontotheclassicTEDtalkstestset,twonoveltestsetswerereleased:•ACLpresentations,inwhichasinglespeakerispresentingonastage.AlthoughsimilartotheTEDtalksscenario,additionalchallengesposedbythistestsetincludethepresenceofnon-nativespeakers,differentac-cents,variablerecordingquality,terminol-ogy,andcontrolledinteractionswithasecondspeaker.•Pressconferencesandinterviews,inwhichtwopersonsinteractondifferenttopics.Inherentchallenges,therefore,includethepresenceofspontaneousspeech,non-nativespeakers,differentaccents,andcontrolledin-teractionwithasecondspeaker.AllthetestsetswereusedforevaluationintheEnglish-Germansub-task,whileonlyTEDTalksandACLpresentationswereusedtotestthesubmissionstotheEnglish-JapaneseandEnglish-Chinesesub-tasks.2.2DataandMetricsTraininganddevelopmentdata.Participantswereofferedthepossibilitytosubmitsystemsbuiltunderthreetrainingdataconditions:1.Constrained:theallowedtrainingdataislimitedtoamedium-sizedframeworkinordertokeepthetrainingtimeandre-sourcerequirementsmanageable.Thecom-pletelist1ofallowedtrainingresources(speech,speech-to-text-parallel,text-parallel,text-monolingual)doesnotincludeanypre-trainedlanguagemodel.2.Constrainedwithlargelanguagemodels(constrained+LLM):inadditiontoallthecon-strainedresources,arestrictedselection1oflargelanguagemodelsisallowedtogivepar-ticipantsthepossibilitytoleveragelargelan-guagemodelsandmedium-sizedresources.3.Unconstrained:anyresource,pre-trainedlanguagemodelsincluded,canbeusedwiththeexceptionofevaluationsets.Thissetupisproposedtoallowtheparticipationofteamsequippedwithhighcomputationalpowerandeffectivein-housesolutionsbuiltonaddi-tionalresources.Thedevelopmentdataallowedunderthecon-strainedconditionconsistofthedevsetfromIWSLT2010,aswellasthetestsetsusedforthe2010,2013-2015and2018-2020IWSLTcam-paigns.BesidesthisTED-derivedmaterial,ad-ditionaldevelopmentdatawerereleasedtocoverthetwonewscenariosincludedinthisroundofevaluation.FortheACLdomain,5presentationsfromtheACL2022conferencewithtranslationsandtranscriptionswereprovided.Duetoaddi-tionalconstraints,thesereferencesweregener-atedbyhumanpost-editingofautomatictranscrip-tionsandtranslation.Forthepressconferencesandinterviewsdomain,12videos(totalduration:1h:3m)wereselectedfrompubliclyavailablein-terviewsfromtheMultimediaCentreoftheEuro-peanParliament(EPTV)2.
1SeetheIWSLT2023offlinetrackwebpage:https://iwslt.org/2023/offline2https://multimedia.europarl.europa.eu
3


TED
TED
TED
3h:47m:53s
3h:19m:34s
6PerformedwithmwerSegmenter-https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz
1h:1m
59m:22s
59m:22s
59m:22s
ACL
ACL
ACL
10
Duration
Talks/Videos
Testdata.Threenewtestsetswerecreatedforthethreelanguagedirections.Thenewtestsetsincludeheterogeneousmaterialdrawnfromeachscenario.ForthetraditionalTEDscenario,anewsetof42talksnotincludedinthecurrentpublicreleaseofMuST-Cwasselectedtobuildtheen-detestset.3Startingfromthismaterial,thetalksforwhichJapaneseandChinesetranslationsareavail-ablewereselectedtobuildtheen-zhanden-jatestsets(respectively,38and37talks).Similartothe2021and2022editions,weconsidertwodifferenttypesoftarget-languagereferences,namely:•TheoriginalTEDtranslations.Sincethesereferencescomeintheformofsubtitles,theyaresubjecttocompressionandomissionstoadheretotheTEDsubtitlingguidelines.4Thismakesthemlessliteralcomparedtostandard,unconstrainedtranslations;•Unconstrainedtranslations.Thesereferenceswerecreatedfromscratch5byadheringtotheusualtranslationguidelines.Theyarehenceexacttranslations(i.e.literalandwithproperpunctuation).FortheACLpresentationscenario,paperpre-sentationsfromACL2022weretranscribedandtranslatedintothetargetlanguages.Adetailedde-scriptionofthedatasetcanbefoundinSaleskyetal.(2023).Thereare5presentationsineachofthedevandtestsetswithatotalduration1hpersplit.Talkswereselectedtoincludediversepapertopicsandspeakerbackgrounds.ThistestsetissharedwiththeMultilingualtask(§5).Forthepressconferencesandinterviewssce-nario,thetestsetcomprises10EPTVvideosofvariableduration(6monaverage),amountingtoatotalof1h:1m.ThedetailsofthenewtestsetsarereportedinTable2.Metrics.Systemswereevaluatedwithrespecttotheircapabilitytoproducetranslationssimilartothetarget-languagereferences.ThesimilaritywasmeasuredintermsofBLEUandCOMET(Reietal.,2020a)metrics.Thesubmittedrunswere
EPTV
42
37
English-Japanese
3Thissetof42TEDtalksisalsoreferredtoasthe“Common”testset(nottobeconfusedwithMuST-C“tst-COMMON”)becauseitservesinbothOfflineandSimul-taneoushttps://iwslt.org/2023/simultaneoustasks.4http://www.ted.com/participate/translate/subtitling-tips5WewouldliketothankMetaforprovidinguswiththisnewsetofreferences.
3h:2m:22s
38
English-Chinese
Table2:StatisticsoftheofficialtestsetsfortheIWSLT2023offlinespeechtranslationtask.rankedbasedontheBLEUcalculatedonthecon-catenationofthethreetestsetsbyusingautomaticresegmentation6ofthehypothesesbasedonthereferencetranslations.FortheBLEUcomputedontheconcatenationofthethreetestsets,thenewunconstrainedoneshavebeenusedfortheTEDdata.AsobservedonIWSLT2022manualeval-uationofsimultaneousspeech-to-texttranslation(Mach´aˇceketal.,2023),COMETiscorrelatingwithhumanjudgmentsbestandBLEUcorrelationisalsosatisfactory.Moreover,tomeettherequestsoflastyear’sparticipants,ahumanevaluationwasperformedonthebest-performingsubmissionofeachparticipant.2.3SubmissionsThisyear,10teamsparticipatedintheofflinetask,submittingatotalof37runs.Table3providesabreakdownoftheparticipationineachsub-taskshowing,foreachtrainingdatacondition,thenumberofparticipants,thenumberofsubmittedrunsand,foreachtrainingdatacondition(con-strained,constrained+LLM,unconstrained),thenumberofsubmittedrunsobtainedwithcascadeanddirectsystems.•BIGAI(Xie,2023)participatedbothwithcascadeanddirectmodelsforen-de,en-ja,anden-zhtranslations,whichweretrainedundertheconstrained+LLMcondition.ThecascadeistheconcatenationofanASRmodelandanMTsystem.TheASRconsistsofthefirst12Transformerlayers
5
5
5
English-German
4


Direct
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Direct
Participants
Participants
Participants
8
3
3
3
11
Unconstrained
Unconstrained
Unconstrained
12
1
1
1
1
1
1
1
1
1
1
1
7
7
English-Japanese
Runs
Runs
Runs
Cascade
Cascade
Cascade
Cascade
Cascade
Cascade
Cascade
Cascade
Cascade
16
16
English-Chinese
Table3:Breakdownoftheparticipationineachsub-task(English→German,English→Chinese,English→Japanese)oftheIWSLTofflineSTtrack.Foreachlanguagedirection,wereportthenumberofpar-ticipants,thenumberofsubmittedrunsand,foreachtrainingdatacondition(constrained,constrained+LLM,un-constrained),thenumberofsubmittedrunsobtainedwithcascadeanddirectsystems.fromwav2vec2-large-960h-lv60-selfandanadaptermodeltocompressthefeaturevectors.TranscriptsareobtainedthroughaCTCgreedydecodingstep.TheMTisbasedonmbart-large-50-one-to-many-mmt.Thedirectmodelconsistsoftwoseparateencodersforspeechandtext,followedbyashareddecoder.ThespeechandtextencodersarerespectivelybasedonthecascadeASRandMTencoders.Anadaptermodelisintroducedtoconnectthetwoencoders.ThedirectmodelcombinesthecrossentropylossforMTandtheCTClossforASR,togetherwithahyperparametertobalancetheweightsbetweenthetwolosses.Thetrainingprocedureinvolvesdedicatedfine-tuningsteps,datafilteringandaudiore-segmentationintoshortersegments.•I2R(Huzaifahetal.,2023)participatedwithadirectapproachforen-detrans-lation,whichwastrainedundertheconstrained+LLMcondition.Themodelconsistsoftwoseparateencodersforspeechandtext,followedbyasharedencoderandadecoder.ThespeechencoderisinitialisedwithWavLMlarge,whileDeltaLMbaseisusedtoinitialisethetextencoder,thesharedencoderandthedecoder.Toleveragebothtextandspeechsources,thesharedencoderisinducedtolearnajointmultimodalrepre-sentationobtainedthroughforcedalignmentofspeechandtextdata.Theresultingmixedspeech-textrepresentationispassedtothesharedencoderinitiallypre-trainedontextdataonly.ADeltaLM-basedMTmodelincrementallytrainedonin-domainandout-of-domaindataisusedasateacherduringfine-tuningoftheSTsystem.TheSTmodelisbuiltonamixofASR,STandsyntheticdata.Additionaltechniquesappliedincludeon-the-flyaudioaugmentationtoincreaserobustnesstovariableaudioquality,domaintaggingtoconditiontheSToutputtothedifferentoutputstylesofthetestdata,andSTmodelensembling.•HW-TSC(Lietal.,2023)participatedwithcascadesystemsforalllanguagedirectionsandinallthreetrainingdataconditions.TheASRmodelusedfortheconstrainedtrain-ingconditionistheConformer.Fortheconstrained+LLMcondition,theencoderofwav2vec2andthedecoderofmBART50arecombinedtofine-tuneonalldataanASRmodeltrainedonMuST-C.Whisper(Rad-fordetal.,2022),fine-tunedonMuST-C,isinsteadusedfortheunconstrainedtrainingcondition.Allmodelsarebuiltusingau-dioinputsaugmentedwithSpecAugmentandCTC.TheMTcomponentisaTransformer-basedmodeltrainedinaone-to-manymul-tilingualfashion.Itexploitsdatafilter-inganddataaugmentationtechniques,com-binedwithdropoutregularizationanddo-mainadaptationmethods,aswellassolutions
2
2
2
2
2
2
2
Constrained+LLM
Constrained+LLM
Constrained+LLM


6
Constrained
Constrained
Constrained
5
5
English-German
5


toincreaserobustnesstoASRnoise(throughsyntheticnoisegenerationanddataaugmen-tation).•MINETRANS(Duetal.,2023)participatedwithen-zhcascadesystemstrainedunderconstrainedandunconstrainedconditions.ThesubmittedrunsareobtainedwithapipelineofASR,punctuationrecognition,andMTcomponents.TheASRisanRNN-Transducer.Fortheunconstrainedcondi-tion,GigaSpeechisaddedtothetrainingdataallowedintheconstrainedsetting.Inbothconditions,pre-processingandfilter-ingtechniquesareappliedtoimprovedataquality,whileSpecAugmentisusedfordataaugmentation.BeforebeingpassedtotheMTcomponent,theunpunctuatedASRout-putisprocessedbymeansofaBERT-basedpunctuationrecognitionmodel.FortheMTcomponent,twostrategiesareimplemented.ThefirstonereliesondifferentTransformer-basedmodelsforsupervisedtraining.AbaseTransformerandanM2M
7Unofficialparticipant,asnosystempaperisavailable.en-desystemtrainedundertheunconstrainedcondition.Itconsistsofa4-stagedprocessincludingtheASR,thepunctuationmoduleperformingbothsentenceextractionandpunctuationplacement,thespeaker-andgenderdistinctioncomponent,andthetranslationmodel.Everystageistrainedonthecrawleddatafromtheweb.•NEMO(Hrinchuketal.,2023)participatedwithdirectsystemsforalllanguagedi-rectionsintheconstrainedtrainingdatacondition.Pre-trainedmodelsandsynthetictrainingdataareexploitedindifferentwaystocopewiththescarcityofdirectSTdata.AConformer-basedASRmodeltrainedonallallowedspeech-to-textdataisusedtoinitial-izetheSLTencoder.ATransformer-basedNMTmodeltrainedonallallowedparalleldataandfine-tunedonTEDtalksisusedtogeneratesynthetictranslationalternativesforallavailablespeech-to-textandtext-to-textdata.ATTSmodelbasedonFastPitch(Ła´ncucki,2021)andtrainedontheEnglishtranscriptsofallTED-deriveddataisusedtogeneratethesyntheticspeechversionofEnglishtextsintheavailabletextcorpora.ThesubmittedSLTsystemsarebasedonaConformer-basedencoderfollowedbyaTransformerdecodertrainedonthismixof(goldandsynthetic)speech-to-textandtext-to-textdata.•XIAOMI(Huangetal.,2023)participatedwithadirecten-zhsystemtrainedundertheconstrained+LLMcondition.Itconsistsofaspeechencoder,atextencoder,andatextdecoder,withallparametersinitializedusingthepre-trainedHuBERTandmBARTmod-els.Thespeechencoderiscomposedofafeatureextractorbasedonconvolutionalneu-ralnetworksandaTransformerencoder.Inadditiontothecross-entropyloss,ASR,MT,andacontrastiveloss,whichtriestolearnanencoderthatproducessimilarrepresentationsforsimilarinstancesindependentlyfromthemodalities,areadded.Self-trainingisalsousedtoleverageunlabelleddata.Inadditiontothealloweddatasets,alargesetofpseudoreferencesaregeneratedtranslatingthe
100modelareusedfortheconstrainedcondition.Atranslationmodeltrainedonadditionalin-housecorporaisusedfortheunconstrainedcondition.ThesecondstrategyadoptedfortheMTcomponentreliesonalargelanguagemodel(Chat-GPT)forprompt-guidedtrans-lation.•NIUTRANS(Hanetal.,2023)participatedwithadirecten-zhsystemtrainedundertheconstrainedcondition.Itconsistsoftwoseparateencodersforspeechandtextwithanadapterinbetween,followedbyadecoder.Thespeechencoderispre-trainedwithanASRencoder,whilethetextualencoderandthedecoderwithpre-trainedMTcomponents.DifferentarchitectureswithvariablesizeweretestedbothforASR(enhancedwithCTClossandinter-CTClosstospeedupconvergence)andMT(usedtogeneratepseudo-referencessoastoincreasethesizeoftheSLTdata).Thefinalsystemisanensembleaimingatmaximizingthediversitybetweenmodels.•NEURODUB7participatedwithacascade
6


transcriptsoftheASRcorpora.Duringtrain-ing,asecondfine-tuningisperformedonMuST-Casin-domaindata.Thefinalsystemisanensembleofthetwobest-performingmodels.•UPC(Tsiamasetal.,2023)participatedwithadirecten-desystemtrainedundertheconstrained+LLMcondition.Itconsistsofaspeechencoder,atextualencoder,andatextdecoder.Thespeechencoderincludesasemanticencodertoalignspeechandtextencoderrepresentations.ThecouplingmodulesincludetheCTCandOptimalTransport(OT)lossestotheoutputsoftheacousticandsemanticencoders,andtheadditionofasecondauxiliaryOTlossfortheinputsofthesemanticencoder.Thespeechencoderisbasedonwav2vec2.0,whilethetextualencoderusesmBART50.Knowledgedistillationisusedtogenerateadditionaldatatofine-tunepartoftheSLTmodelarchitecture(thefeatureextractor,theacousticencoder,andtheCTCmodulearefrozenduringfine-tuning).USTC(Zhouetal.,2023)participatedwithcascadeanddirecten-zhmodelstrainedun-dertheunconstrainedcondition.FortheASRofthecascade,twoapproachesareimple-mented.Thefirstoneexploitsafusionmod-elstrainedonthealloweddataexpandedwithspeedperturbation,oversampling,concate-nationofadjacentvoicesandsyntheticdatagenerationviaTTS.ThesecondapproachisbasedonWhisperlarge(Radfordetal.,2022)andSHASforaudiosegmentation.TheMTcomponentofthecascadesystemexploitsanensembleofTransformer-basedmodelsen-hancedwithknowledgedistillation,domainadaptationandrobusttrainingstrategies.FordirectSLT,twoapproachesareimplemented.Thefirstoneisanencoder-decoderinitial-izedwiththeASRandMTmodelsofthecascade.ThesecondapproachisaStackedAcoustic-and-TextualEncodingextensionofSATE(Xuetal.,2021).Thefinalsub-missionsalsoincludeensemblesobtainedbycombiningcascadeanddirectsystems.2.4ResultsAlsothisyear,thesubmissionstotheIWSLTOf-flinetranslationtaskwereevaluatedbothwithau-tomaticmetricsandthroughhumanevaluation.Theresultsforeachsub-taskareshownindetailintheAppendix.2.4.1AutomaticEvaluationTheresultsforeachofthelanguagepairsareshowninthetablesinAppendixB.1.WepresentresultsforEnglish-German(Table14),English-Chinese(Table16)andEnglish-Japanese(Table15).TheevaluationwascarriedoutintermsofBLEU(theprimarymetric,incontinuitywithpre-viousyears),andCOMET.Wereportindividualscoresforthethree(ortwo,asinthecaseofen-jaanden-zh)differenttestsetsaswellasmetricscal-culatedontheconcatenationofthedifferenttestsets.Foreachsub-task,systemsarerankedbasedontheBLEUscorecomputedontheconcatenatedtestsets.End-to-EndvsCascadedThisyearthecas-cadedsystemsperformedingeneralbetterthantheend-to-endsystems.ForEnglish-to-German,fornearlyallmetrics,thecascadedsystemsareal-waysrankedbest.ForEnglish-to-Japanese,theresultsshowasimilarsituationtoEnglish-to-German,withthecascadesystemsoutperformingtheend-to-endmodel.Thesupremacyofthecas-cademodelsisconfirmedbyallthemetrics,withacleargapinperformancebetweentheworstcas-cadeandthebestend-to-endmodels.ForEnglish-to-Chinese,thepictureisnotasclear.However,theonlyparticipantwhosubmittedaprimarysys-temusingthecascadedandoneusingtheend-to-endparadigm(USTC),thecascadedperformedbetterinallmetrics.MetricsForEnglish-to-German,ingeneral,theresultsoftheBLEUmetriccorrelatequitewellwiththescoresoftheCOMETmetric.Exceptforrelativelysmallchanges,e.g.theorderisdifferentforthedifferentHW-TSCsystems.Oneexcep-tionisthesubmissionsbyUPCandNeMothatarerankeddifferentlyinthetwometrics.Therefore,acomparisontothehumanevaluationwillbeinter-esting.IntheEnglish-to-Japanesetask,thescoresoftheHW-TSCsystemsareveryclosetoeachotherandsomeswapsarevisiblebetweenBLEUandCOMET.However,thechangesareonlyre-latedtotheHW-TSCsystemsanddonotmod-
7


ifytheoverallevaluationofthesystems.IntheEnglish-to-Chinesetask,therearetwosituationswherethemetricsdiffersignificantly.Therank-ingforUSTCend-to-endcomparedtotheHW-TSCsystemsisdifferentwithrespecttoCOMET,whichrewardstheHW-TSCsubmissions.Asim-ilarsituationisvisibleforNiuTransandXiaomi,whereBLEUfavorstheNiuTranstranslations,whileCOMETassignshigherscores,andranking,totheXiaomisubmissions.DataconditionsForthedifferentdatacondi-tions,thegainsbyusingadditionallargelanguagemodelsoradditionaldataarenotclear.HW-TSCsubmittedthreeprimarysystemsforeachdataconditionandtheyallperformverysimilarly.However,foren-zhtheunconstrainedsystembyUSTCwasclearlythebestandforen-dethebestsystemexceptHW-TSCwasalsoanunconstrainedone.Theadditionalbenefitofthepre-trainedmod-elsisevenlessclear.Thereisnoclearpicturethatthesystemswithorwithoutthistechnologyper-formbetter.DomainsOnenewaspectthisyearistheevalu-ationofthesystemsonthreedifferenttestsetsanddomains.Firstofall,theabsoluteperformanceonthedifferentdomainsisquitedifferent.Thesys-temsperformclearlyworseontheEPTVtestsets.FortherelationshipbetweenACLandTED,thepictureisnotasclear.WhiletheBLEUscoresonACLarehigher,theCOMETscoresarelower.OnlyforEnglish-to-Japanese,bothmetricsarehigherontheACLtestset.OneexplanationcouldbethatthereferencesfortheACLtalksaregen-eratedbypost-editinganMToutput.Thiscouldindicatethatthepost-editedreferencesinflatetheBLEUscore,whiletheCOMETscoreseemstobemorerobusttothisphenomenon.Whencompar-ingthedifferentsystems,thetendencyisforallcasesthesame.However,someperformslightlybetterinonecondition.Forexample,theend-to-endsystemfromUSTCperformsverywellonTEDcomparedtoothersystemsbutlesswellonACL.2.4.2HumanEvaluationAtthetimeofwriting,humanevaluationisstillinprogress.Itsresultswillbereportedattheconfer-enceandtheywillappearintheupdatedversionofthispaperinAppendixA.3SimultaneousSLTSimultaneousspeechtranslationmeansthesystemstartstranslatingbeforethespeakerfinishesthesentence.Thetaskisessentialtoenablepeopletocommunicateseamlesslyacrossdifferentback-grounds,inlow-latencyscenariossuchastransla-tionininternationalconferencesortravel.Thisyear,thetaskincludedtwotracks:speech-to-textandspeech-to-speech,coveringthreelan-guagedirections:EnglishtoGerman,ChineseandJapanese.3.1ChallengeTherearetwomajorupdatescomparedwithpre-viousyears:•Removalofthetext-to-texttrack.Thetaskfocusesonthereal-worldlive-translationset-ting,wherethespeechistheinputmedium.•Additionofaspeech-to-speechtrack.Trans-lationintosyntheticspeechhasgainedin-creasingattentionwithintheresearchcom-munity,givenitspotentialapplicationtoreal-timeconversations.Tosimplifythesharedtask,asinglelatencyconstraintisintroducedforeachtrack:2sec-ondsofAverageLaggingforspeech-to-text,and2.5secondsofstartingoffsetforspeech-to-speech.Theparticipantscansubmitnomorethanonesystempertrack/languagedirection,aslongasthelatencyofthesystemisundertheconstraint.ThelatencyofthesystemisqualifiedontheopenMuST-Ctst-COMMONtestset(DiGangietal.,2019a).Theparticipantsmadesubmissionsinaformatofdockerimages,whichwerelaterrunbyorga-nizersontheblind-testsetinacontrollableen-vironment.AnexampleofimplementationwasprovidedwiththeSimulEvaltoolkit(Maetal.,2020a).3.2DataThetrainingdataconditionofthesimultaneoustaskfollows“constrainedwithlargelanguagemodels”settingintheOfflinetranslationtask,asdescribedinSection2.2Thetestdatahastwoparts:CommonTEDtalks.ThisisthethesameasintheOfflinetask,asdescribedinSection2.2.ForEnglishtoGerman,ChineseandJapanese.
8


Non-NativeseeAppendixA.1.1.ForEnglishtoGerman.3.3EvaluationTwoattributesareevaluatedinthesimultaneoustask:qualityandlatency.Forquality,weconductedbothautomaticandhumanevaluation.BLEUscore(Papinenietal.,2002a)isusedforautomaticqualityevaluation.Forspeechoutput,theBLEUscoreiscomputedonthetranscriptsfromWhisper(Radfordetal.,2022)ASRmodel.Therankingofthesubmis-sionisbasedontheBLEUscoreontheCom-monblindtestset.Furthermore,weconductedBLASER(Chenetal.,2022)evaluationonthespeechoutput.Wealsoconductedhumanevalu-ationonspeech-to-texttranslationquality,includ-inggeneralhumanevaluationforallthreelan-guagepairs,andtaskspecifichumanevaluationonGermanandJapaneseoutputs.Forlatency,weonlyconductedautomaticeval-uation.Wereportthefollowingmetricsforeachspeech-to-textsystems.•AverageLagging(AL;Maetal.,2019,2020b)•LengthAdaptiveAverageLagging(LAAL;Pol´aketal.,2022;Papietal.,2022)•AverageTokenDelay(ATD;Kanoetal.,2023)•AverageProportion(AP;ChoandEsipova,2016)•DifferentiableAverageLagging(DAL;CherryandFoster,2019)Wealsomeasuredthecomputationawareversionofthelatencymetrics,asdescribedbyMaetal.(2020b).However,duetothenewsynchronizedSimulEvalagentpipelinedesign,theactualcom-putationawarelatencycanbesmallerwithcare-fullydesignedparallelism.Forspeech-to-speechsystems,wereportstart-offsetandend-offset.Thelatencymetricswillnotbeusedforranking.3.4SubmissionsThesimultaneoussharedtaskreceivedsubmis-sionsfromsixteams,whereasalltheteamspar-ticipatedinatleastonelanguagedirectioninspeech-to-texttranslation.Amongtheteams,fiveteamsenteredtheEnglish-to-Germantrack;fourteamsenteredtheEnglish-to-Chinesetrack;threeteamsenteredtheEnglish-to-Japanesetrack.Eventhoughthisyearisourfirsttimeintroducingthesimultaneousspeech-to-speechtrack,threeteamsoutofsix,submittedspeech-to-speechsystems.•CMU(Yanetal.,2023)participatedinboththespeech-to-textandspeech-to-speechtracksforEnglish-Germantranslation.Theirspeech-to-textmodelcombinedself-supervisedspeechrepresentations,aConformerencoder,andanmBARTdecoder.Inadditiontothecross-entropyattentionalloss,thetranslationmodelwasalsotrainedwithCTCobjectives.Theyusedmachinetranslationpseudolabelingfordataaug-mentation.Simultaneousdecodingwasachievedbychunkingthespeechsignalsandemployingincrementalbeamsearch.Fortheirspeech-to-speechsystem,theyincorporatedaVITS-basedtext-to-speechmodel,whichwastrainedseparately.•HW-TSC(Guoetal.,2023;Shangetal.,2023)participatedinboththespeech-to-textandspeech-to-speechtracksforallthreelanguagedirections.TheirmodelwasacascadedsystemthatcombinedanU2ASR,aTransformer-basedmachinetrans-lationmodel,andaVITS-basedtext-to-speechmodelforspeech-to-speechtransla-tion.TheMTmodelwasmultilingualandofferedtranslationinallthreedirectionsbyconditioningonlanguageembeddings.Fordataaugmentation,theyadopteddatadi-versificationandforwardtranslationtech-niques.Theirsimultaneousdecodingpolicyemployedchunk-basedincrementaldecod-ingwithstablehypothesesdetection.TheyalsoutilizedadditionalTTSmodelsforthespeech-to-speechtrack.•NAIST(Fukudaetal.,2023)participatedinthespeech-to-texttranslationdirectionforallthreelanguagedirectionsandEnglish-to-Japanesespeech-to-speechtranslation.TheirsystemconsistedofaHuBERTencoderandanmBARTdecoder.Theyemployedthreetechniquestoimprovetranslationquality:inter-connectiontocombinepre-trainedrep-resentations,prefixalignmentfine-tuningforsimultaneousdecoding,andlocalagreement
9


tofindstableprefixhypotheses.TheyalsoutilizedanadditionalTacotron2-basedTTSmodelforspeech-to-speechtranslationwiththewait-kdecodingpolicy.•FBK(Papietal.,2023b)participatedintheEnglish-to-Germanspeech-to-texttranslationtrack,usinganend-to-endConformer-basedspeech-to-textmodel.Consideringcomputa-tionallatency,theirfocuswasonefficientus-ageofofflinemodels.Theyemployedthreesimultaneouspolicies,includinglocalagree-ment,encoder-decoderattention,andEDATTv2,toachievethis.•CUNI-KIT(Pol´aketal.,2023)partici-patedintheEnglish-to-Germanspeech-to-texttranslationtrack.TheirsystemutilizedWavLMandmBARTasthebaseframework.Thekeyhighlightsoftheirsystemwereinthedecodingstrategyandsimultaneouspolicies.TheyappliedempiricalhypothesesfilteringduringdecodingandadoptedCTCtodetectthecompletionofblockinference.•XIAOMI(Huangetal.,2023)participatedinboththespeech-to-textandspeech-to-speechtracksforEnglish-Chinesetransla-tion.Theirend-to-endsystemutilizedHu-BERTandmBARTwithawait-kdecodingstrategyandanInformation-Transport-basedarchitecture.Theyfurtherenhancedtheirsys-tembyapplyingdatafilteringonlongsen-tencesandmisalignedaudio/text,dataaug-mentationwithpseudolabeling,andpunctu-ationnormalization.Theyalsoincorporatedcontrastivelearningobjectives.3.5AutomaticEvaluationWerankthesystemperformancebasedonBLEUscores.ThedetailedresultscanbefoundinAp-pendixB.2.3.5.1Speech-to-TextEnglish-GermanOntheCommontestset,therankingisHW-TSC,CUNI-KIT,FBK,NAIST,CMU,asshowninTable17.Meanwhile,ontheNon-Nativetestset,therankingdiffersconsider-ably.WhileHW-TSCperformsbestonCommontestset,theyendupsecondtolastonNon-Native.ThesituationisreversedforNAISTandCMUwhoendupatthetailofCommonscoringbutreachthebestscoresontheNon-Nativeset.WeattributethistobetterrobustnessofNAISTandCMUtowardsthenoiseinNon-Nativetestset.English-ChineseTherankingisHW-TSC,CUNI-KIT,XIAOMI,NAIST,asshowninTable18.English-JapaneseTherankingisHW-TSC,CUNI-KIT,NAIST,asshowninTable19.3.5.2Speech-to-SpeechDespitethegreatnoveltyanddifficultyofspeech-to-speechtrack,thereare5submissionsintotal:2inGerman,2inChineseand1inJapanese.ThefullresultscanbeseenintableTable20.ForEnglish-to-German,therankingisCMU,HW-TSC.ForEnglish-to-Chinese,HW-TSCistheonlyparticipant.ForEnglish-to-Japanese,therankingisHW-TSC,NAIST.WealsoprovidetheBLASERscores,whichdirectlypredictthequalityoftranslationsbasedonspeechembeddings.Wenotethatsincerefer-enceaudiosarenotavailableinourdatasets,weusetextLASER(Heffernanetal.,2022)toembedreferencetexttocomputethescores.WhiletheBLASERscoresindicatethesamequalityrank-ingforEnglishtoGermanasBLEUscores,ontheJapaneseoutputtheyaresimilar.It’spos-siblethatBLASERisadequatelydevelopedonJapaneseoutputs3.6HumanEvaluationIntheSimultaneoustask,speech-to-texttrack,English-GermanandEnglish-Japanesewereman-uallyevaluated,eachwithadifferentscoringmethod.3.6.1English-GermanForEnglish-to-German,weusedthesamehumanevaluationmethodaslastyear,originallyinspiredbyJavorsk´yetal.(2022).Weevaluated(1)thebestsystemselectedbyBLEUscore,and(2)tran-scriptionofhumaninterpretation,thesameasusedinlastyearevaluation(moredetailscanbefoundinAnastasopoulosetal.(2022),Section2.6.1).Figure1plotsautomaticandmanualevalua-tioninrelationwitheachother.WeconfirmthegenerallygoodcorrelationwithBLEU(Pearson.952acrossthetwotestsetparts),asobservedbyMach´aˇceketal.(2023),althoughindividualsys-temresultsareratherinterestingthisyear.
10


Figure1:ManualandautomaticevaluationofSimulatenousspeech-to-textEnglish-to-GermantranslationontheCommon(TEDtalks)andNon-Nativetestsets.Theerrorbarswereobtainedbybootstrapresampling,seethecaptionofTable22.OntheCommontestset,HWTSCperformedbestintermsofBLEUbutthemanualscor-ingseemstopreferCUNI-KITandFBK.CMUandNAISTareworstinBLEUbutonparwithHWTSCintermsofmanualscores.ThesituationisverydifferentontheNon-Nativetestset:CMUandNAISTscorebestbothinmanualscoresandinBLEUwhileCUNI-KITandesp.FBKgetmuchworsescores,again,bothmanualandautomatic.TheNon-Nativetestsetissubstantiallyharderwithrespecttosoundconditions,andthestrikingdifferencedropobservedforbothCUNI-KITandFBKcanbeanindicationofsomeformofover-fittingtowardsthecleaninputofCommon(TEDtalks).AppendixA.1.1presentsdetailsofthehumanevaluationandresultsareshowninTable22.3.6.2English-JapaneseForEnglish-to-Japanese,wealsofollowedthemethodologyinthelastyear.Wehiredaprofes-sionalinterpreterforhumanevaluationusingJTFTranslationQualityEvaluationGuidelines(JTF,2018)basedonMultidimensionalQualityMetrics(MQM;Lommeletal.,2014).WeappliedtheerrorweightingbyFreitagetal.(2021a).Ap-pendixA.1.2presentsdetailsofthehumaneval-uation.ThehumanevaluationresultsareshowninTa-ble23.TheerrorscorealmostcorrelateswithBLEUagainsttheadditionalreference,butthedif-ferenceintheerrorscoreswasverysmallbetweenHW-TSCandCUNI-KITinspiteofthe0.8BLEUdifference.3.7FinalremarksThisyear,wesimplifiedtheconditionsbyfocus-ingsolelyonlow-latencysystemstoreducetheburdenofsubmissionandevaluation.Wealsointroducedthenovelandchallengingspeech-to-speechtrack,andwerehappytoreceive5submis-sions.Wenotepotentialmodificationsforfutureedi-tions:•Providingfurthersimplifiedsubmissionfor-mat.•Rankingwithbetterdesignedmetricstoad-dresstheoverfittingtowardsBLEUscores.•Aligningmorewithofflinetasksonmoretestdomainsandevaluationmetrics.4AutomaticSubtitlingInrecentyears,thetaskofautomaticallycreatingsubtitlesforaudiovisualcontentinanotherlan-guagehasgainedalotofattention,aswehave
11


AV
9
8
45084037
17
docs
10
Table4:Statisticsofthedevandtestsetsforthesubti-tlingtask.4.2DataandMetricsData.Thistrackproposedtwotrainingcondi-tionstoparticipants:constrained,inwhichonlyapre-definedlistofresourcesisallowed,andun-constrained,withoutanydatarestrictions.TheconstrainedsetupallowedtousethesametrainingdataasintheOfflineSpeechTranslationtask(seeSection2.2forthedetailedlist),withtheinclusionoftheMuST-Cinemacorpus(Karakantaetal.,2020)andtheobviousexclusionoftheparallelre-sourcesnotinvolvingtheEnglish-{German,Span-ish}pairs.Inaddition,twomonolingualGermanandSpanishtextcorporabuiltonOpenSubtitles,enrichedwithsubtitlebreaks,documentmeta-infoongenreandautomaticallypredictedlinebreaks,havebeenreleased.Foreachlanguageanddomain,adevelopmentsetandatestsetwerereleased.Theywereallnewsets,neverreleasedbefore,exceptfor:thedevel-opmentsetoftheEPTVdomain,forwhichtheEu-roparlInterviewstestsetby(Papietal.,2023a)wasreused.Table4providessomeinformationaboutthesesets.Theevaluationwascarriedoutfromthreeper-spectives,subtitlequality,translationqualityandsubtitlecompliance,throughthefollowingauto-maticmeasures:•Subtitlequalityvs.referencesubtitles:–SubER,primarymetric,usedalsoforranking(Wilkenetal.,2022);12–Sigma(Karakantaetal.,2022b).13•Translationqualityvs.referencetranslations:
h:mm
refsubtitles
01:22
12
891874
8https://www.ted.com/9https://multimedia.europarl.europa.eu10https://www.onepeloton.com11https://www.itvstudios.com
01:03
7
7
44894763
12https://github.com/apptek/SubER13https://github.com/fyvo/EvalSubtitle
Pelotondev
14
03:59
13751422
test
test
test
test
seenasurgeintheamountofmovies,seriesanduser-generatedvideoswhicharebeingstreamedanddistributedallovertheworld.Forthefirsttime,thisyearIWSLTproposedaspecifictrackonautomaticsubtitling,wherepar-ticipantswereaskedtogeneratesubtitlesofaudio-visualdocuments,belongingtodifferentdomainswithincreasinglevelsofcomplexity.4.1ChallengeThetaskofautomaticsubtitlingismulti-faceted:startingfromspeech,notonlythetranslationhastobegenerated,butitmustbesegmentedintosubtitlescompliantwithconstraintsthatensurehigh-qualityuserexperience,likeaproperread-ingspeed,synchronywiththevoices,themaxi-mumnumberofsubtitlelinesandcharactersperline,etc.Mostaudio-visualcompaniesdefinetheirownsubtitlingguidelines,whichcandifferslightlyfromeachother.ParticipantswereaskedtogeneratesubtitlesaccordingtosomeofthetipslistedbyTED,inparticular:•themaximumsubtitlereadingspeedis21characters/second;•linescannotexceed42characters,whitespacesincluded;•neverusemorethantwolinespersubtitle.Itwasexpectedthatparticipantsusedonlytheau-diotrackfromtheprovidedvideos(devandtestsets),thevideotrackbeingoflowqualityandpro-videdprimarilyasameanstoverifytimesyn-chronicityandotheraspectsofdisplayingsubtitlesonscreen.ThesubtitlingtrackrequirestoautomaticallysubtitleinGermanand/orSpanishaudio-visualdocumentswherethespokenlanguageisalwaysEnglish,andwhichwerecollectedfromthefol-lowingsources:•TEDtalks;8•pressinterviewsfromtheMultimediaCentreoftheEuropeanParliament(EPTV);9•physicaltrainingvideosofferedbyPeloton;10•TVseriesfromITVStudios.11
960909
ITVdev
04:11
02:43
27002661
TEDdev
domainset
EPTVdev
48074897
05:08
dees
49064964
06:01
hh:m
01:01
12


–BLEU14andCHRF15viasacreBLEU;–BLUERT(Sellametal.,2020).Automaticsubtitlesarerealignedtotheref-erencesubtitlesusingmwerSegmenter(Ma-tusovetal.,2005a)16beforerunningsacre-BLEUandBLEURT.•Subtitlecompliance:17–rateofsubtitleswithreadingspeedhigherthan21char/sec(CPS);–rateoflineslongerthan42char(CPL);–rateofsubtitleswithmorethantwolines(whitespacesincluded)(LPB).4.3SubmissionsThreeteamssubmittedautomaticallygeneratedsubtitlesforthetestsetsofthistask.•APPTEK(Baharetal.,2023)submittedrunsintheconstrainedsetupforbothlanguagepairs.Theprimarysubmissionscamefromacascadearchitecturecomposedofthefollow-ingmodules:neuralencoder-decoderASR,followedbyaneuralMachineTranslationmodeltrainedonthedataallowedinthecon-strainedtrack,withthesource(English)sidelowercasedandnormalizedtoresemblerawASRoutput,aswellasadaptedtotheIWSLTsubtitlingdomains,followedbyasubtitlelinesegmentationmodel(intelligentlinesegmen-tationbyAPPTEK).Acontrastiverunwasgeneratedfortheen→depaironlybyadirectspeechtranslationsystemwithCTC-basedtimestampprediction,followedbytheintel-ligentlinesegmentationmodelofAPPTEK.Thesystemwastrainedontheconstrainedal-loweddataplusforwardtranslatedsyntheticdata(translationsofallowedASRtranscripts)andsyntheticspeechdataforselectedsen-tencesfromtheallowedparalleldata.Fortheen→depair,APPTEKalsosubmittedarunintheunconstrainedsetup,whereacascadear-chitecturewasemployedconsistingof:neu-ralencoder-decoderCTCASR,followedbyaneuralpunctuationpredictionmodeland
14sacreBLEUsignature:nrefs:1|case:mixed||eff:no|tok:13a|smooth:exp|version:2.0.015sacreBLEUsignature:nrefs:1|case:mixed||eff:yes|nc:6|nw:0|space:no|version:2.0.016https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz17https://github.com/hlt-mt/FBK-fairseq/blob/master/examples/speech_to_text/scripts/subtitle_compliance.pyinversetextnormalizationmodel,followedbyanMTmodeladaptedtotheIWSLTdo-mains(sentencessimilarinembeddingsim-ilarityspacetothedevelopmentsetsofthefourdomainsTED,EPTV,ITV,Peloton),fol-lowedbyasubtitlelinesegmentationmodel(intelligentlinesegmentationbyAPPTEK).•FBK(Papietal.,2023b)submittedprimaryrunsforthetwolanguagepairs,generatedbyadirectneuralspeechtranslationmodel,trainedintheconstrainedsetup,thatworksasfollows:i)theaudioisfedtoaSubtitleGeneratorthatproducesthe(un-timed)sub-titleblocks;ii)thecomputedencoderrepre-sentationsarepassedtoaSourceTimestampGeneratortoobtainthecaptionblocksandtheircorrespondingtimestamps;iii)thesub-titletimestampsareestimatedbytheSource-to-TargetTimestampProjectorfromthegen-eratedsubtitles,captions,andsourcetimes-tamps.•MATESUB(Perone,2023)submittedprimaryrunsforthetwolanguagepairs,automaticallygeneratedbytheback-endsubtitlingpipelineofMATESUB,itsweb-basedtoolthatsup-portsprofessionalsinthecreationofhigh-qualitysubtitles(https://matesub.com/).TheMATESUBsubtitlingpipelineisbasedonacascadearchitecture,composedofASR,textsegmenterandMTneuralmodels,whichal-lowscoveringanypairfromabout60lan-guagesandtheirvariants,includingthetwolanguagepairsofthetask.SinceMATESUBisaproductionsoftware,itsneuralmodelsaretrainedonmoreresourcesthanthoseal-lowedfortheconstrainedcondition,there-forethesubmissionsfallintotheuncon-strainedsetup.4.4ResultsScoresofallrunsascomputedbyautomaticmet-ricsareshowninTables24and25intheAp-pendix.Averagedoverthe4domains,APPTEKachievedthelowestSubERscoreswiththeirpri-marysubmissionforen→deintheconstrainedandunconstrainedcondition,withtheoverallbestre-sultsforthelatter.Foren→es,MATESUBobtainedtheoveralllowestSubERwiththeirunconstrainedsystem.Weobservethatintermsofdomaindifficulty,theTVseries(fromITV)posethemostchallenges
13


forautomaticsubtitling.Thishastodowithdi-verseacousticconditionsinwhichspeechisfoundinmoviesandseries-backgroundmusic,noises,shouts,andcross-talk.Allofthismakesthetaskofrecognizingspeechquitechallenging,whichresultsinerroraccumulationinthedownstreamcomponents.UnconstrainedsystemsbyAPPTEKandMATESUBperformsignificantlybetteronthisdomain,whichshowstheimportanceoftrainingonadditionaldatathatismorerepresentativeofreal-lifecontent.Thesecond-hardestdomainarethefitnessvideosfromPeloton.Here,despiteagener-allyclearsingle-speakeraudiowithreducedback-groundnoise,thechallengeistheMT:someofthefitness-andsports-specificterminologyandslangposesignificantchallengesintranslationtotheirGermanandSpanishequivalents.Surprisingly,eventheEPTVinterviewsposesignificantchallengesforsubtitling,despitethefactthatthetopicsdiscussedintheinterviewsarefoundinabundanceintheallowedspeech-to-textandtext-to-textparalleldataforthecon-strainedcondition(Europarl,Europarl-ST).Here,theissuessuchasspontaneousspeechwithmanypauses,aswellasspeakerseparationmayhavebeencauseofsomeoftheerrors.TheTEDtalkswhichhavebeenthemaindomainfortheIWSLTevaluationsinthepastyearsaretheeasiesttobeautomaticallysubti-tled.WhereasthecurrentlevelofsubtitlequalityforTEDtalksmayrequireminimalhumancor-rectionsorcanevenbeshownuneditedonthescreen,fortheotherthreedomainstheautomaticsubtitleswillrequiresignificantpost-editing.ThisshowstheimportanceofrunningevaluationsnotonlyunderverycontrolledconditionsasinthecaseofTEDtalks,butonavarietyofreal-lifecon-tentwheremultipleresearchchallengesinspeechtranslationareyettobeovercome.Thisyear’sdirectspeechtranslationsystemsseemtobetooweaktocompetewiththecascadedapproaches.Inparticular,afullend-to-endap-proach(Papietal.,2023a)liketheonefromFBKthatdirectlygeneratessubtitleboundariesiscur-rentlyinferiorincomparisonwiththesystemsthatadoptaspecificsolutionforsegmentingthetext(intelligentlinesegmentationbyAPPTEKandaneuraltextsegmenterbyMATESUB).Suchspe-cificsolutionsleadtoalmostperfectsubtitlecom-pliance.Butevenintermsofpurespeechtrans-lationqualityasmeasurede.g.withBLEUandBLEURTthecascadedsystemscurrentlyprovidebettertranslationsevenunderconstrainedtrainingdataconditions.Regardingtheautomaticmetricsusedintheevaluation,weobservedthatthemetricSigmapro-videsscoreswhicharenotconsistentwiththeothermeasures:forexample,GermansubtitlesfromMATESUBseemtobetheworstasmeasuredbySigma,butthisisunlikelybasedontheval-uesoftheothermetrics.YetthepureMTqualitymetricsalsoexhibitsomediscrepanciesinhowtheperformanceofthesamesystemonthefourdo-mainsisranked.ThisrankingsometimesdiffersdependingonwhetheryouchooseBLEU,ChrF,orBLEURTasthe“primary”metric.Thetwomoststrikingcasesare:•theen→deAPPTEKunconstrainedprimarysubmission,forwhichtheBLEUscorefortheITVtestdatawas14.43andforPelo-ton10.47,buttheBLEURTscoreswereverysimilar:0.4069and0.4028;•theen→deFBKconstrainedprimarysystem,forwhichtheBLEUscorewas7.73onthePelotonpartofthetestdatavs.8.05ontheITVpart,buttheBLEURTscoresshowedabetterqualityforPelotontranslations:0.3137vs.0.2255.Allofthesediscrepancieshighlighttheimpor-tanceofhumanevaluation,whichwehavenotconductedthistime.Oneofthereasonsforthisisthatinmostpriorresearch(Matusovetal.,2019;Karakantaetal.,2022a)theautomaticsub-titlingqualityisevaluatedinpost-editingscenar-ios,whicharetooexpensivetoberunonsignifi-cantamountsofdataastheyrequireprofessionalsubtitletranslators.Ontheotherhand,asmen-tionedabove,for3outof4domainsthequalityoftheautomaticallygeneratedsubtitletranslationsislow,sothatanevaluationofuserexperiencewhenwatchingsubtitleswouldbealsochallenging,es-peciallyiftheuserswouldhavetoassignevalu-ationscorestoindividualsubtitlesorsentences.Withallofthisinmind,wedecidedtopostponeanyhumanevaluationtothenexteditionofthesubtitlingtrackatIWSLT.Overall,thisfirsteditionofthesubtitlingtrackemphasisedthecrucialroleofthefollowingcom-ponentsrelatedtospeechprocessing:noisere-ductionand/orspeechseparation,speakerdiariza-tion,andsentencesegmentation.Sofarthey
14


18https://www.2022.aclweb.org/dispecialinitiativemainadaptation).Toevaluatesubmissions,weuseevaluationsetscuratedfrompresentationsatACL2022whichwereprofessionallytranscribedandtranslatedwiththesupportofACLandthe60-60initiativeasdescribedinSaleskyetal.(2023).5.2DataandMetricsData.WeusetheACL60-60evaluationsetscre-atedbySaleskyetal.(2023)toevaluatethischal-lengetask.ThedatacomesfromACL2022tech-nicalpresentationsandisoriginallyspokeninEn-glish,andthentranscribedandtranslatedtotentargetlanguagesfromthe60/60initiative:Ara-bic,MandarinChinese,Dutch,French,German,Japanese,Farsi,Portuguese,Russian,andTurk-ish.Theresultingdatasetcontainsparallelspeech,transcripts,andtranslationfortenlanguagepairs,totalingapproximatelyonehourforthedevelop-mentsetandonehourfortheevaluationset.Duringtheevaluationcampaign,theonlyin-domaindataprovidedisthedevelopmentset.Tosimulatetherealisticusecasewhererecordedtechnicalpresentationswouldbeaccompaniedbyaresearchpaper,inadditiontothetalkaudioweprovidethecorrespondingpapertitleandab-stract,whicharelikelytocontainasubsetofrelevantkeywordsandterminologyandcouldbeusedbyparticipantstobiasoradapttheirsystems.ConstrainedtrainingdatafollowstheOfflinetask(seeSec.2.2)withpretrainedmodelsandout-of-domainparallelspeechandtextprovidedforall10languagepairs.Theunconstrainedsettingal-lowedparticipantstopotentiallycrawladditionalin-domaindatatoassistwithadaptation,aswasdonebyoneteam(JHU).Fortheofficialrankings,weusetheofficialevaluationset,whichwasheldblinduntilaftertheevaluationcampaign.Tomimicrealistictestconditionswheretheaudiofortechnicalpresentationswouldbepro-videdasasinglefile,ratherthangold-sentence-segmented,forboththedevelopmentandevalu-ationsetsweprovidedthefullunsegmentedwavfiles,aswellasanautomaticallygeneratedbase-linesegmentationusingSHAS(Tsiamasetal.,2022)togetparticipantsstarted.Twoteamsusedthebaselinesegmentation,whileone(JHU)usedlongersegmentswhichimprovedtheASRqual-ityoftheirparticularpretrainedmodel.Toevalu-atetranslationqualityofsystemoutputusinganyinputsegmentation,weprovidedgoldsentence-segmentedtranscriptsandtranslations,whichsys-
havebeenunderestimatedinspeechtranslationre-search.Currentautomaticsolutionsdonotreachthelevelofqualitythatisnecessaryinsubti-tling.Therefore,weencouragefurtherresearchintotheseareas,forwhichsubtitletranslationisagoodtestcase.5MultilingualSLTTheNLPandspeechcommunitiesarerapidlyex-pandingwithincreasingfocusonbroaderlan-guagecoverageandmultilinguality.However,de-spitethecommunity’seffortsonASRandSLT,re-searchisrarelyfocusedonapplyingtheseeffortstothedatawithinthescientificdomain.Itisclearfromrecentinitiativestocaptiontechnicalpresen-tationsatNLPandspeechconferencesthattran-scriptionandtranslationinthetechnicaldomainisneeded,desired,andremainsadisproportionatechallengeforcurrentASRandSLTmodelscom-paredtostandarddatasetsinthesespaces.Mo-tivatedbytheACL60-60initiative18totranslatetheACLAnthologytoupto60languagesforthe60thanniversaryofACL,whichwillbereportedonatthisyear’sACLconferenceco-locatedwithIWSLT,thisyear’sMultilingualTaskevaluatestheabilityofcurrentmodelstotranslatetechnicalpre-sentationstoasetoftendiversetargetlanguages.5.1ChallengeTranslatingtechnicalpresentationscombinessev-eralchallengingconditions:domain-specificter-minology,recordingconditionsvaryingfromclose-rangemicrophonestolaptopmicrophoneswithlightbackgroundnoiseorfeedback,diversespeakerdemographics,andimportantlyunseg-mentedspeechtypically10-60minutesindura-tion.Thistaskfocusesonone-to-manytranslationfromEnglishtotentargetlanguages.ProvidingEnglishASRwasoptionalthoughencouraged.In-domaindataisscarce,particularlyparalleldata,thoughalllanguagepairsarecoveredbycurrentpubliclyavailablecorpora;furtherchallengingforcurrentdomainadaptationtechniques,monolin-gualdataistypicallyavailableforthesourcelan-guage(English)only.Wepresenttwoconditions:constrained(usingonlytheout-of-domaindataallowedandprovidedforothertasksthisyear)andunconstrained(allowinganyadditionaldata,includedcrawled,whichmayfacilitatee.g.,do-
15


temoutputcouldbescoredwithasdescribedbe-lowin‘Metrics.’Metrics.Translationoutputwasevaluatedus-ingmultiplemetricsforanalysis:translationout-putusingchrF(Popovi´c,2015a),BLEU(Pap-inenietal.,2002b)ascomputedbySACREBLEU(Post,2018),andCOMET(Reietal.,2020b)andASRoutputusingWER.ForBLEUweusetherecommendedlanguage-specifictokenizationinSACREBLEUforChinese,Japanese,Korean,andthemetric-defaultotherwise.Translationmetricswerecalculatedwithcaseandpunctuation.WERwascomputedonlowercasedtextwithpunctua-tionremoved.NFKCnormalizationwasappliedonsubmittedsystemsandreferences.Alloffi-cialscoreswerecalculatedusingautomaticreseg-mentationofthehypothesisbasedontherefer-encetranscripts(ASR)ortranslations(SLT)bymwerSegmenter(Matusovetal.,2005b),usingcharacter-levelsegmentationforresegmentationforthoselanguageswhichdonotmarkwhites-pace.TheofficialtaskrankingisbasedonaveragechrFacrossall10translationlanguagepairs.5.3SubmissionsWereceived11submissionsfrom3teams,asde-scribedbelow:•BIT(Wangetal.,2023b)submittedasingleconstrainedone-to-manymultilingualmodeltocoverall10languagepairs,trainedusingacollectionofmultipleversionsoftheMuST-Cdataset(DiGangietal.,2019b).TheyuseEnglishASRpre-trainingwithdataaugmen-tationfromSpecAugment(Parketal.,2019),andmultilingualtranslationfinetuningforalllanguagepairstogether.Thefinalmodelisanensembleofmultiplecheckpoints.Noadap-tationtothetechnicaldomainisperformed.•JHU(Xinyuanetal.,2023)submittedtwocascadedsystems,oneconstrainedandoneunconstrained,combiningmultiplediffer-entpretrainedspeechandtranslationmod-els,andcomparingdifferentdomainadap-tationtechniques.Theirunconstrainedsys-temusesanadaptedWhisper(Radfordetal.,2022)ASRmodelcombinedwithNLLB(NLLBTeametal.,2022),M2M-100(Fanetal.,2020),ormBART-50(Tangetal.,2020)MTmodelsdependingonthelan-guagepair,whiletheconstrainedsystemuseswav2vec2.0(Baevskietal.,2020a)andmBART-50orM2M-100.Theycompareus-ingtalkabstractstopromptWhispertotrain-ingin-domainlanguagemodelsoneitherthesmallamountofhighly-relevantdatainthetalkabstractorlargerLMstrainedonsignifi-cantlymoredatatheyscrapedfromtheACLAnthologyandreleasewiththeirpaper.TheyseeslightimprovementsovertheprovidedSHAS(Tsiamasetal.,2022)segmentsus-inglongersegmentscloserwhatWhisperob-servedintraining.TheyshowthatpromptingWhisperisnotcompetitivewithin-domainlanguagemodels,andprovideananalysisoftechnicaltermrecallandotherfine-graineddetails.•KIT(Liuetal.,2023)submittedmultipleconstrainedmultilingualmodels,bothend-to-endandcascaded,whichcombineseveraltechniquestoadapttothetechnicaldomaingiventheabsenceofin-domaintrainingdata,usingpretrainedspeechandtranslationmod-elsasinitializations(WavLM:Chenetal.2021,DeltaLM:Maetal.2021,mBART-50:Tangetal.2020).TheseincludekNN-MTtobiasgeneratedoutputtothetechni-caldomain;datadiversificationtoenrichpro-videdparalleldata;adaptersforlightweightfinetuningtothelanguagepairsfortrans-lation(thoughtheynotethatthisdoesnotnecessarilystackwithdatadiversification);andfortheircascadedmodel,adaptationoftheASRmodeltothetargettechnicaldo-mainusingn-gramre-weighting,notingthatitistypicallyeasiertoadaptoraddlexicalconstraintstomodelswithseparateLMs,asopposedtoencoder-decodermodels.Addi-tionaltechniques(ensembling,updatedASRencoder/decodersettings,knowledgedistilla-tion,synthesizedspeech)arealsousedforfurthersmallimprovements.5.4ResultsAlltaskresultsareshowninAppendixB.4.Theofficialtaskrankingwasdeterminedbytheaver-agechrFacrossall10targetlanguagesafterreseg-mentationtothereferencetranslations.Table26.ScoresforallsubmissionsbyindividuallanguagepairsareshowninTable28(chrF),Table29(COMET),andTable30(BLEU).Overall,themajorityofapproachescombined
16


terminologyFigure2:Officialtaskmetricperformance(chrF)vsterminologyrecallforteams’primarysubmissions.gualdata,conventionaltechniquesforadaptationofend-to-endSTmodelsdidnotapply(finetun-ing,backtranslation,...).Thedatadiversifica-tionappliedbyKITviaTTS‘backtranslation’(contrastive5,contrastive7)didnotaffectchrForBLEU,butdidprovidesmall(0.5-0.6)improve-mentsonCOMET.Inadditiontotheoverallevaluationset,welookattherecallofspecificterminologyannotatedfortheACLevaluationsets.Forthethreesubmissions(JHUunconstrained,KITprimary,JHUconstrained)whichprovidedsupplementaryASR,wefirstin-vestigateterminologyrecallandpropagationbe-tweenASRanddownstreamST.RecallthattheoverallWERofthesesystemswas16.9,23.7,and34.1,respectively.Ofthe1107labeledterminol-ogywordsandphrasesfromtheACL60-60eval-uationsetannotations,87.8%/77.3%/71.7%in-dividualinstanceswerecorrectlytranscribedbythesesystems,respectively.Ofthese,12.0%/7.4%/7.9%werethenmaintainedandcorrectlytranslatedtoeachtargetlanguagerespectivelyonaverage.Weplottheofficialtaskmetric(chrF)againstterminologyrecallinFigure2forallpri-marysubmissions.Weseethattherewereconsis-tentdifferencesacrosslanguagesinhowterminol-ogywasmaintained,whichgenerallybutnotfullycorrespondstooverallperformance(ex:Dutch,Turkish).WhilethedomainadaptationtechniquesusedensuredstrongtranscriptionperformancefortheJHUandKITsubmissions,thiswasnotgen-erallymaintainedfortranslationwithasignificant
nl
50
JHU-unconstrained
KIT-primary
Metric
fa
BIT
ja
60
10
0
30
System
fr
JHU-constrained
tr
strongpretrainedspeechandtranslationmod-elstodoverywellontheACL60-60evalua-tiondata.Forthistask,cascadedmodelsper-formedconsistentlybetterthandirect/end-to-endapproaches;allofthetop6submissionswerecas-cades,and4/5ofthelowest-performingsystemsweredirect.OptionalEnglishASRtranscriptsweresubmittedfor3systems(JHUunconstrained,KITprimary,JHUconstrained),allofwhichwerecascades;weseethatWERalignswithspeechtranslationperformanceinthesecases.Theonlyunconstrainedmodel,fromJHU,utilizedlargerpretrainedmodelsandcrawledin-domainlan-guagemodelingdataforASRtogreatsuccess,andwasthetopsystemonallmetrics(Table26).Theremainingsubmissionswereallconstrained(heremeaning,usedthewhite-listedtrainingdataandsmallerpretrainedmodels).TheKITprimarysys-temwasthebestperformingconstrainedmodel.WhileBITtrainedmodelsfromscratchonTEDtoreasonableperformanceonMuST-C,largepre-trainedmodelsanddomainadaptationwerekeyforhighperformanceonthetechnicalin-domaintestset.chrFandBLEUresultinthesamesys-temrankings,whileCOMETfavorstheend-to-endmodelsslightlymore,thoughnotaffectingthetop3systems(JHUunconstrained,KITprimary,KITconstrastive1).Domainadaptationtechniqueshadconsistentpositiveimpactonsystemperformance.TheKITteamsubmittedconstrainedsystemsonlyandthuswerelimitedtothedevbitextandtalkabstractsfordomainadaptation.Despiteitssmallsize(<500sentences)theywereabletogeneratecon-sistentimprovementsofupto∼1chrFand∼1BLEUusingkNN-MT(primary/contrastive1vscontrastive2);withthismethod,extendingthedevdatatoincludetheabstractsfortheevaluationsettalks(primaryvscontrastive1)hadneglibleef-fectonall3metrics.TheJHUsubmissionssawthatdecodingwithinterpolatedin-domainlan-guagemodelsoutperformedknowledgedistilla-tionorpromptingpretrainedmodelswithinforma-tionforeachtalkinthiscase;smalltalk-specificLMsdidprovideslightimprovementsinWER,butsignificantimprovementsof2-3WERweregainedbyextendingthelimitedhighlyrelevantdatafromtalkabstractsandthedevsettothelargerdomain-generaldatacrawledfromthe2021ACLconfer-enceandworkshopproceedings.Withoutin-domaintarget-languagemonolin-
chrF
de
ar
zhLanguage
40
pt
ru
20
70
17


drop,convergingwithBITwhichdidnotperformdomainadaptation.Additionalworkisneededtoensuretargetedlexicaltermsarecorrectlytran-scribedandtranslated,bothingeneralaswellascomparablyacrossdifferentlanguages.WhiletheJHUsubmissionsfinetunedtoeachtargetlanguageindividually,theKITsystemsfine-tunedmultilingually;nocontrastivesystemsweresubmittedwithwhichtoablatethispoint,butbothteams’papersdescribeconsistentlyworseperfor-mancefinetuningmultilinguallyratherthanbilin-gually,whichKITwasabletolargelymitigatewithlanguageadaptersindevelopmentinisola-tionbutintheirfinalsubmissiononevallanguageadapterswereconsistentlyslightlyworse(con-trastive4‘with’vscontrastive3‘without.’).Itre-mainstobeseenthedegreetowhichone-to-manymodelscanbenefitfrommultilingualtraining.TheOfflinetaskadditionallyusedtheACL60-60evaluationsetsaspartoftheirbroaderevalu-ationfor3languagepairs(en→de,ja,zh),en-ablingawidercomparisonacross25totalsys-tems.WeshowtheMultilingualtasksubmissionscomparedtotheOfflineontheselanguagesinTa-ble27.Onthesethreelanguagepairs,perfor-manceisgenerallyhigherthantheremaininglan-guagepairsintheMultilingualtask.Weagainconsistentlyseestrongerperformanceonthistaskfromcascadedmodels,andunconstrainedsub-missionsorthosewithlargerpretrainedLLMs,thoughtherearenotableoutlierssuchastheHW-TSCconstrainedmodel.TheOfflinesubmissionsdidnotperformdomainadaptationspecificallytothetechnicalACLdomain,butappeartobebenefitfrombetterdomain-generalperformanceinsomecases,particularlyforsubmissionstargetingonlyChinese.Wenoteslightdifferencesinsystemrankingsbetweenmetrics(COMETandBLEU)andtargetlanguages,particularlyforJapaneseandChinesetargets,possiblyhighlightingthediffer-enceinmetrictokenizationforthesepairs.6Speech-to-SpeechTranslationSpeech-to-speechtranslation(S2ST)involvestranslatingaudioinonelanguagetoaudioinan-otherlanguage.Intheofflinesetting,thetransla-tionsystemcanassumethattheentireinputaudioisavailablebeforebeginningthetranslationpro-cess.Thisdiffersfromstreamingorsimultaneoussettingswherethesystemonlyhasaccesstopar-tialinput.Theprimaryobjectiveofthistaskistoencouragetheadvancementofautomatedmethodsforofflinespeech-to-speechtranslation.6.1ChallengeTheparticipantsweretaskedwithcreatingspeech-to-speechtranslationsystemsthatcouldtranslatefromEnglishtoChineseusingvariousmethods,suchasacascadesystem(ASR+MT+TTSorend-to-endspeech-to-texttranslation+TTS),oranend-to-end/directsystem.Theywerealsoal-lowedtouseanytechniquestoenhancetheper-formanceofthesystem,apartfromusinguncon-straineddata.6.2DataandMetricsData.ThistaskallowedthesametrainingdatafromtheOfflinetaskonEnglish-Chinesespeech-to-texttranslation.MoredetailsareavailableinSec.2.2.InadditiontotheOfflinetaskdata,thefollowingtrainingdatawasallowedtohelpbuildEnglish-Chinesespeech-to-speechmodelsandChinesetext-to-speechsystems:•GigaS2S,targetsyntheticspeechfortheChi-nesetargettextofGigaST(Yeetal.,2023)thatwasgeneratedwithanin-housesingle-speakerTTSsystem;•aishell3(Shietal.,2020),amulti-speakerChineseTTSdataset.It’snotedthatseveraldatasetsallowedfortheOfflinetasksuchasCommonVoice(Ardilaetal.,2019)actuallycontainmulti-speakerChi-nesespeechandtextdatathatcouldhelpforthistask.Metrics.Allsystemswereevaluatedwithbothautomaticandhumanevaluationmetrics.Automaticmetrics.Toautomaticallyevaluatetranslationquality,thespeechoutputwasauto-maticallytranscribedwithaChineseASRsys-tem19(Yaoetal.,2021),andthenBLEU20(Pa-pinenietal.,2002a),chrF21(Popovi´c,2015b),COMET22(Reietal.,2022)andSEScore223(Xu
19https://github.com/wenet-e2e/wenet/blob/main/docs/pretrained_models.en.md20sacreBLEUsignature:nrefs:1|case:mixed|eff:no|tok:zh|smooth:exp|version:2.3.121sacreBLEUsignature:nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.3.122https://huggingface.co/Unbabel/wmt22-comet-da23https://github.com/xu1998hz/SEScore2
18


24https://github.com/facebookresearch/stopes/tree/main/demo/iwslt_blaser_evalbyiterativelydenoisingonpureGaussiannoise.Basedontheresult,theyconcludethatthediffusionmodeloutperformsnormalTTSmodelsandbringspositivegaintotheentireS2STsystem.•KU(Yangetal.,2023)submittedacascadesystemcomposedofaspeech-to-texttransla-tion(ST)modelandaTTSmodel.TheirSTmodelcomprisesaSTdecoderandanASRdecoder.Thetwodecoderscanexchangein-formationwitheachotherwiththeinteractiveattentionmechanism.FortheTTSpart,theyuseFastSpeech2astheacousticmodelandHiFi-GANasthevocoder.•NPU-MSXF(Songetal.,2023)submittedacascadedsystemofseparateASR,MT,andTTSmodels.ForASR,theyadoptROVER-basedmodelfusionanddataaugmentationstrategiestoimprovetherecognitionaccu-racyandgeneralizationability.Thentheyuseathree-stagefine-tuningprocesstoadaptapre-trainedmBART50modeltotranslatetheoutputofASRmodel.Thethree-stagefine-tuningisbasedonCurriculumLearninganditinvolvesthreesetsofdata:(1)theoriginalMTdata,(2)theMTdatainASRtranscrip-tionformatand(3)theASRoutputs.ForTTS,theyleverageatwo-stageframework,usingnetworkbottleneckfeaturesasaro-bustintermediaterepresentationforspeakertimbreandlinguisticcontentdisentangle-ment.Basedonthetwo-stageframework,pre-trainedspeakerembeddingisleveragedasaconditiontotransferthespeakertimbreinthesourcespeechtothetranslatedspeech.•XIAOMI(Huangetal.,2023)submittedacas-cadesystemcomposedofaspeech-to-texttranslation(ST)modelandaTTSmodel.TheSTmodelisthesameastheonetheysub-mittedtotheOfflineSLTtrack.Itisbasedonanencoder-decoderarchitecturefromthepre-trainedHuBERTandmBARTmodels.FortheTTSmodel,theyusetheTacotron2framework.ItisfirsttrainedwithAISHELL-3datasetandthenfinetunedwithGigaS2Sdataset.Furthermore,theyimplementsev-eralpopulartechniques,suchasdatafiltering,dataaugmentation,speechsegmentation,andmodelensemble,toimprovetheoverallper-formanceofthesystem.
etal.,2022)werecomputedbetweenthegeneratedtranscriptandthehuman-producedtextreference.BLEUandchrFwerecomputedusingSacreBLEU(Post,2018).Furthermore,theoutputspeechcouldbeevaluateddirectlyusingBLASER(Chenetal.,2022).Moreinformationcouldbefoundatstopes24(Andrewsetal.,2022).Humanevaluation.Outputspeechtranslationswereevaluatedwithrespecttotranslationqualityandspeechquality.•Translationquality:Bilingualannotatorswerepresentedwiththesourceaudio,sourcetranscriptandthegeneratedtargetaudio,thengavescoresonthetranslationqualitybe-tween1and5(worst-to-best)).Therewere4annotatorspersampleandweretainedthemedianscore.•Outputspeechquality:Inadditiontotrans-lationquality(capturingmeaning),thequal-ityofthespeechoutputwasalsohuman-evaluated.Theannotatorswererequestedtogiveanoverallscorebyconsideringthreedi-mensions:naturalness(voiceandpronunci-ation),clarityofspeech(understandability),andsoundquality(noiseandotherartifacts).Eachsamplewasassessedby4annotatorsandscoredonascaleof1-5(worst-to-best)),withaminimumscoreintervalof0.5.Thedetailedguidelinesforoutputspeechqual-ityevaluationweresimilartolastyear(Anasta-sopoulosetal.,2022).6.3SubmissionsWereceivedeightsubmissionsfromfiveteams.TheMINETRANSteamsubmittedfoursystemsandeachoftheotherteamssubmittedonesystem.•HW-TSC(Wangetal.,2023a)submittedacascadedsystemcomposedofanensembleofConformerandTransformer-basedASRmodels,amultilingualTransformer-basedMTmodelandadiffusion-basedTTSmodel.Theirprimaryfocusintheirsubmissionistoinvestigatethemodelingabilityofthediffu-sionmodelforTTStasksinhigh-resourcescenarios.ThediffusionTTSmodeltakesrawtextasinputandgenerateswaveform
19


Cascade).Theirend-to-endsystemsadoptthespeech-to-unittranslation(S2UT)framework.Theend-to-endS2UTmodelcomprisesaspeechencoder,alengthadapterandanunitde-coder.TheS2UTmodelistrainedtoconvertthesourcespeechintounitsoftargetspeech.Aunit-basedHiFi-GANvocoderisfinallyappliedtoconverttheunitsintowaveform.Basedontheirresults,theyconcludethatthewidelyusedmulti-tasklearningtechniqueisnotimportantformodelconvergenceoncelarge-scalelabeledtrainingdataisavailable,whichmeansthatthemappingfromsourcespeechtotargetspeechunitscanbelearneddirectlyandeasily.Further-more,theyapplyothertechniques,suchasconsistencytraining,dataaugmentation,speechsegmentation,andmodelensembletoimprovetheoverallperformanceofthesystem.TheircascadesystemconsistsofASR,MTandTTSmodels.TheirASRandMTreplicatesthoseusedfortheOfflineSLTsubmission.TheirTTSmodelisacombinationofFastSpeech2andHiFi-GAN.6.4ResultsResultsasscoredbyautomaticmetricsareshowninTable31andhumanevaluationresultsareshowninTable32intheAppendix.Overallresults.Accordingtotheautomaticmetricsusedintheevaluation,XIAOMIobtainedthehighestscoreinASR-BLEU,ASR-chrF,ASR-COMETandASR-SEScore2.NPU-MSXFob-tainedthesecondhighestscore,followedsub-sequentlybyHW-TSC,MINETRANS
Cascade,andfinallyKU.Thisrankingwasmostlycon-sistentwiththeautomaticranking,showingthatautomaticmetricswereusefulinevaluatingthetranslationqualityofsystems.Forhumanevalu-ationalongthespeechqualityperspective,NPU-MSXFobtainedthehighestscore,followedbyHW-TSC,XIAOMI,MINETRANS
MINETRANS(Duetal.,2023)submittedthreeend-to-endS2STsystems(MINE-TRANS
CascadeandKU.Withaequalweightingoftranslationqualityandspeechquality,NPU-MSXFobtainedthehighestoverallscoreinhu-manevaluation,followedbyXIAOMIandtheoth-ers.S2STapproaches.Thisyear,allsystemsbutMINETRANS
Cascade.TheBLEU,chrF,COMETandSEScore2rankingswereexactlythesame.Thescoresforthetest-expandeddatawerelowerthanthoseforthetest-primarydata,likelyduetoadomainmismatchwiththetrainingdata.Forhumanevaluationalongthetranslationqualityperspective,XIAOMIobtainedthehighestscore,followedbyNPU-MSXF,thenHW-TSCandMINETRANS
E2Ewerecascadedsystems,withthreesystemsadoptinganASR+MT+TTSap-proachandtwosystemsadoptinganend-to-endS2T+TTSapproach.Thisshowedthatcascadeapproachwasstilldominantinthecommunity.Al-thoughMINETRANS
E2E,KUandMINETRANS
E2E,MINE-TRANS
E2E,thenMINETRANS
Cascadeinallevaluationmetrics,wecouldnotdrawconclusionsonthecomparisonbetweencascadeandend-to-endgiventhelimiteddatapoints.Futurechallengescanencouragemoredirectorend-to-endsubmissions.6.5ConclusionThisisthesecondtimethatspeech-to-speechtranslation(S2ST)ispresentedinoneoftheIWSLTtasks.S2STisanimportantbenchmarkforgeneralAIasotherNLPtasks,e.g.dialoguesys-tem,questionansweringandsummarizationcanalsobeimplementedinspeech-to-speechmanner.Comparedtothesettinglastyear,thesizeofthetrainingdatasetavailabletotheparticipantsismuchlarger.TheBLEUscoresobtainedinthischallengeishighingeneral,comparedtoMTandSTofthesamelanguagedirection.Althoughnotrequiredbythetask,NPU-MSXFistheonlyteamthatimplementedspeakertimbretransferintheirsystem.Weplantoincludeevaluationmet-ricsaddressingthisaspectinthenextedition.7DialectSLTTheDialectSpeechTranslationsharedtaskisacontinuationoflastyear’stask.Weusethesametrainingdataas2022andevaluatedsystemsonthe2022evaluationsettomeasureprogress;inaddition,weaddedanew2023evaluationsetasblindtest.Fromtheorganizationalperspective,wemergedthecallforsharedtaskwiththetheLow-Resourcetasks(Section8)inordertoencouragecross-submissionofsystems.
E2E,includingprimary,con-trastive1,andcontrastive2),andacascadeS2STsystem(MINETRANS
E2EperformedbetterthanMINETRANS
20


7.1ChallengeDiglossiccommunitiesarecommonaroundtheworld.Forexample,ModernStandardArabic(MSA)isusedforformalspokenandwrittencom-municationinmostpartsoftheArabic-speakingworld,butlocaldialectssuchasEgyptian,Moroc-can,andTunisianareusedininformalsituations.Diglossiaposesuniquechallengestospeechtrans-lationbecauselocal“low”dialectstendtobelow-resourcewithlittleASRandMTtrainingdata,andmaynotevenhavestandardizedwriting,whilere-sourcesfrom“high”dialectslikeMSAprovidesopportunitiesfortransferlearningandmultilin-gualmodeling.7.2DataandMetricsParticipantswereprovidedwiththefollowingdatasets:•(a)160hoursofTunisianconversationalspeech(8kHz),withmanualtranscripts•(b)200klinesofmanualtranslationsoftheaboveTunisiantranscriptsintoEnglish,mak-ingathree-wayparalleldata(i.e.alignedau-dio,transcript,translation)thatsupportsend-to-endspeechtranslationmodels•(c)1200hoursofModernStandardArabic(MSA)broadcastnewswithtranscriptsforASR,availablefromMGB-2•Approximately42,000klinesofbitextinMSA-EnglishforMTfromOPUS(specifi-cally:Opensubtitles,UN,QED,TED,Glob-alVoices,News-Commentary).In2022,weconstructedthreeconditions:Thebasicconditiontrainson(a)and(b),providedbytheLinguisticDataConsortium(LDC);thedi-alectadaptationconditiontrainson(a),(b),(c),(d);theunconstrainedconditioncanuseanyaddi-tionaldataandpre-trainedmodels.In2023,duetothecoordinatedorganizationwithotherLow-ResourceTasksthisyear,werenamedbasiccon-ditionas“constrainedcondition”,andtheothertwoconditionsaremergedasthe“unconstrainedcondition”.Alltrainandtestsetsaretime-segmentedattheutterancelevel.StatisticsareshowninTable5.TherearethreetestsetsforevaluationwithBLEU25.
25SacreBLEUsignaturefordialectspeechtranslationtask:nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0•test1:Participantsareencouragedtousethisforinternalevaluationsincereferencesareprovided.ThisispartofLDC2022E01re-leasedtoparticipantsfortraininganddevel-opment,obtainedbyapplyingthestandarddatasplitandpreprocessing26.•test2:officialevaluationfor2022,fromLDC2022E02•test3:officialevaluationfor2023,fromLDC2023E097.3SubmissionsWereceivedsubmissionfromfourteams:•GMU(MbuyaandAnastasopoulos,2023)participatedinfivelanguage-pairsintheLow-Resourcetasksaswellasthistask.Theyfocusedoninvestigatinghowdifferentself-supervisedspeechmodels(Wav2vec2.0,XLSR-53,andHuBERT)comparewhenini-tializedtoanend-to-end(E2E)speechtrans-lationarchitecture.•JHU(Husseinetal.,2023)submittedbothcascadedandE2Esystems,usingtransformerandbranchformerarchitectures.Theyinves-tigatedtheincorporationofpretrainedtextMTmodels,specificallymBART50anddis-tilledNLLB-200.Further,theyexploreddif-ferentwaysforsystemcombinationandhan-dlingoforthographicvariationandchannelmismatch.•ON-TRAC(Laurentetal.,2023)partici-patedintwolanguage-pairsintheLow-Resourcetaskaswellasthistask.Forthistask,theyfocusedonusingSAMU-XLS-Rasthemultilingual,multimodalpretrainedspeechencoderandmBARTasthetextde-coder.•USTC(Dengetal.,2023)proposedamethodforsynthesisofpseudoTunisian-MSA-Englishpaireddata.Forthecascadedsystem,theyexploredASRwithdifferentfeatureextraction(VGG,GateCNN)andneu-ralarchitectures(Conformer,Transformer).ForE2E,theyproposedusingSATEandahybridSATEarchitecturetotakeadvantage
26https://github.com/kevinduh/iwslt22-dialect
21


LDC2022E01train
MSA
200k
200k
42M
42M
Text(#lines)
4248
4248
Officialevaluationfor2023
Speech
Dataset
3
3
3
3
1.1M
Participant’sinternalevaluation
UnconstrainedconditionOPUS
ConstrainedconditionLDC2022E01dev
27https://commonvoice.mozilla.org/en/datasets28https://github.com/Idlak/Living-Audio-Dataset
UnconstrainedconditionTable5:DatasetsforDialectSharedTask.ofthepseudoTunisian-MSA-Englishtextdata.Additionally,methodsforadaptingtoASRerrorsandsystemcombinationwereex-amined.7.4ResultsThefullsetofBLEUresultsontheEnglishtrans-lationsareavailableinTables33and34.WealsoevaluatedtheWERresultsfortheASRcomponentofcascadedsystems,inTable35.Ingeneral,thereisanimprovementcomparedto2022.Ontest2,thebestsystemin2022(achievedbytheCMUteam)obtained20.8BLEU;severalsystemsthisyearimproveduponthatresult,forexampleUSTC’sprimarysystemachieved23.6BLEUandJHU’sprimarysystemachieved21.2BLEU.Ontheofficialevaluationontest3,thebestsystemachieved21.1BLEUintheunconstrainedconditionand18.1BLEUintheconstrainedcon-dition.Fromthesystemdescriptions,itappearsthein-gredientsforstrongsystemsinclude:(a)effectiveuseofpretrainedspeechandtextmodels,(b)sys-temcombinationamongbothcascadedandE2Esystems,and(c)syntheticdatagenerationtoin-creasethesizeofdialectaldata.Wedonotplantocontinuethissharedtasknextyear.Instead,theplanistomakethedataavailablefromtheLDC.Weencourageresearcherstocon-tinueexploringdialectalanddiglossicphenomenainthefuture.8Low-resourceSLTTheLow-resourceSpeechTranslationsharedtaskfocusesontheproblemofdevelopingspeechtran-scriptionandtranslationtoolsforlow-resourcedlanguages.8.1ChallengeThisyear,thetaskintroducedspeechtranslationofrecordedutterancesfromIrishtoEnglish,MarathitoHindi,MaltesetoEnglish,PashtotoFrench,TamasheqtoFrench,andQuechuatoSpanish.Thedifferentlanguagepairsvarybytheamountofdataavailable,butingeneral,theyhaveincommonthedearthofhigh-qualityavailablere-sources,atleastincomparisontoothermuchhigher-resourcedsettings.8.2DataandMetricsWedescribethedataavailableforeachlanguagepairbelow.Table6providesanoverviewoftheprovideddatasets.Irish–EnglishIrish(alsoknownasGaeilge)hasaround170,000L1speakersand1.85millionpeo-ple(37%ofthepopulation)acrosstheisland(ofIreland)claimtobeatleastsomewhatproficientwiththelanguage.IntheRepublicofIreland,itisthenationalandfirstofficiallanguage.ItisalsooneoftheofficiallanguagesoftheEuropeanUnion(EU)andarecognizedminoritylanguageinNorthernIrelandwiththeISOgacode.TheprovidedIrishaudiodatawerecompiledfromCommonVoice(Ardilaetal.,2020a),27andLiving-Audio-Dataset.28ThecompileddatawereautomaticallytranslatedintoEnglishandcorrectedbyanIrishlinguist.TheIrish–Englishcorpusconsistsof11.55hoursofIrishspeechdata(seeTable6),translatedintoEnglishtexts.Marathi–HindiMarathiisanIndo-Aryanlan-guagewhichhastheISOcodemr,andisdomi-
Evaluateprogressfrom2022
Unconstrainedcondition
4288
4288
Anyotherdata
3833
3833
160
1100
LDC2022E02test2
(#hours)
English
MGB2
Tunisian
Use
LDC2023E09test3
ConstrainedconditionLDC2022E01test1













4204
4204
22


Table6:Training,developmentandtestdatadetails(inhours)forthelanguagepairsofthelow-resourcesharedtask.nantlyspokeninthestateofMaharashtrainIndia.Itisoneofthe22scheduledlanguagesofIndiaandtheofficiallanguageofMaharashtraandGoa.Asperthe2011CensusofIndia,ithasaround83millionspeakerswhichcovers6.86%ofthecoun-try’stotalpopulation.29MarathiisthethirdmostspokenlanguageinIndia.TheprovidedMarathi–Hindicorpusconsistsof22.33hoursofMarathispeechdata(seeTable6)fromthenewsdomain,extractedfromNewsOnAir30andtranslatedintoHinditexts.31ThedatasetwasmanuallysegmentedandtranslatedbyPanlin-gua.32Additionally,theparticipantsweredirectedthattheymayusemonolingualMarathiaudiodata(withtranscription)fromCommonVoice(Ardilaetal.,2020a),33aswellasthecorpusprovidedbyHeetal.(2020)34andtheIndianLanguageCor-pora(Abrahametal.,2020).35Maltese–EnglishMalteseisaSemiticlan-guage,withabouthalfamillionnativespeakers,spokenintheofficiallanguageofMaltaandtheEU.ItiswritteninLatinscript.Theprovideddatawasdividedintothreeparts.First,around2.5hoursofaudiowithMaltesetran-scriptionandanEnglishtranslationwerereleased,
29https://censusindia.gov.in/nada/index.php/catalog/4256130https://newsonair.gov.in31https://github.com/panlingua/iwslt2023_mr-hi32http://panlingua.co.in/33https://commonvoice.mozilla.org/en/datasets34https://www.openslr.org/64/35https://www.cse.iitb.ac.in/˜pjyothi/indiccorpora/alongwithabout7.5hoursofaudiowithonlyMal-tesetranscriptions.Last,theparticipantsweredi-rectedtoseveralmonolingualMaltesetextualre-sources.TheprovideddatasetsweretakenfromtheMASRIcorpus(HernandezMenaetal.,2020).Pashto–FrenchPashtoisspokenbyapproxi-matelyfortytosixtymillionpeopleintheworld.ItisparticularlyspokenbythePashtunpeopleinthesouth,eastandsouthwestofAfghanistan(itisoneofthetwoofficiallanguages),aswellasinthenorthandnorthwestPakistanbutalsoinIran,TajikistanandIndia(UttarPradeshandCash-mere)andoneofthetwoofficiallanguagesofAfghanistan.ThecorpuswastotallyprovidedbyELDA,andisavailableontheELRAcatalog:TRADPashtoBroadcastNewsSpeechCorpus(ELRAcatalogue,2016b)thatconsistsofaudiofilesandTRADPashto-FrenchParallelcorpusoftran-scribedBroadcastNewsSpeech-Trainingdata(ELRAcatalogue,2016a)whicharetheirtran-scriptions.Thisdatasetisacollectionofabout108hoursofBroadcastNewswithtranscriptionsinPashtoandtranslationsintoFrenchtext.Thedatasetisbuiltfromcollectedrecordingsfrom5sources:AshnaTV,AzadiRadio,DeewaRadio,MashaalRadioandShamshadTV.Originaltrainingdatacontains99hoursofspeechinPashto,whichcorrespondsto29,447utterancestranslatedintoFrench.Train-ingdatacorrespondsto61hoursofspeech(Ta-ble6).Tamasheq–FrenchTamasheqisavarietyofTu-areg,aBerbermacro-languagespokenbynomadic
LanguagePairsTrainSetDevSetTestSetAdditionalData
Irish–Englishga–eng9.461.030.44n/aMarathi–Hindimr–hi15.33.74.4monolingualaudiowithtranscriptions(ASR),monolingualtextMaltese–Englishmlt–eng2.5-1.35monolingualaudiowithtranscriptions(ASR),monolingualtextPashto–Frenchpus–fra612.52n/aTamasheq–Frenchtmh–fra17--untranscribedaudio,datainotherre-gionallanguagesQuechua–Spanishque–spa1.601.031.0360hoursofmonolingualaudiowithtranscriptions(ASR)andMTdata(nottranscribed)
23


tribesacrossNorthAfricainAlgeria,Mali,NigerandBurkinaFaso.Itaccountsforapproximately500,000nativespeakers,beingmostlyspokeninMaliandNiger.Thistaskisabouttranslatingspo-kenTamasheqintowrittenFrench.Almost20hoursofspokenTamasheqwithFrenchtransla-tionarefreelyprovidedbytheorganizers.Ama-jorchallengeisthatnoTamasheqtranscriptionisprovided,asTamasheqisatraditionallyorallan-guage.TheprovidedcorpusisacollectionofradiorecordingsfromStudioKalangou36translatedtoFrench.Itcomprises17hoursofcleanspeechinTamasheq,translatedintotheFrenchlanguage.Theorganizersalsoprovideda19-hourversionofthiscorpus,including2additionalhoursofdatathatwaslabeledbyannotatorsaspotentiallynoisy.Bothversionsofthisdatasetsharethesamevali-dationandtestsets.Boitoetal.(2022a)providesathoroughdescriptionofthisdataset.Inadditiontothe17hoursofTamasheqaudiodataalignedtoFrenchtranslations,andinlightofrecentworkinself-supervisedmodelsforspeechprocessing,wealsoprovideparticipantswithun-labeledrawaudiodataintheTamasheqlanguage,aswellasinother4languagesspokenfromNiger:French(116hours),Fulfulde(114hours),Hausa(105hours),Tamasheq(234hours)andZarma(100hours).Allthisdatacomesfromthera-diobroadcastingsofStudioKalangouandStudioTamani.37Notethatthislanguagepairisacontinuationoflastyear’ssharedtask.Anadditionalseparatetestsetwasprovidedthisyear.Quechua–SpanishQuechuaisanindigenouslanguagespokenbymorethan8millionpeo-pleinSouthAmerica.ItismainlyspokeninPeru,Ecuador,andBoliviawheretheofficialhigh-resourcelanguageisSpanish.Itisahighlyinflec-tivelanguagebasedonitssuffixeswhichaggluti-nateandarefoundtobesimilartootherlanguageslikeFinnish.Theaveragenumberofmorphemesperword(synthesis)isabouttwotimeslargerthaninEnglish.Englishtypicallyhasaround1.5mor-phemesperwordandQuechuahasabout3mor-phemesperword.TherearetwomainregionaldivisionsofQuechuaknownasQuechuaIandQuechuaII.Thisdatasetconsistsoftwomaintypesof
36https://www.studiokalangou.org/37https://www.studiotamani.org/QuechuaspokeninAyacucho,Peru(QuechuaChankaISO:quy)andCusco,Peru(QuechuaCollaoISO:quz)whicharebothpartofQuechuaIIand,thus,considereda“southern”languages.Welabelthedatasetwithque-theISOnormforQuechuaIImixtures.TheconstrainedsettingallowedaQuechua-Spanishspeechtranslationdatasetalongwiththeadditionalparallel(text-only)dataformachinetranslationcompiledfrompreviouswork(Ortegaetal.,2020).Theaudiofilesfortraining,valida-tion,andtestpurposesconsistedofexcerptsoftheSiminchikcorpus(Cardenasetal.,2018)thatweretranslatedbynativeQuechuaspeakers.Fortheun-constrainedsetting,participantsweredirectedtoanotherlargerdatasetfromtheSiminchikcorpuswhichconsistedof60hoursoffullytranscribedQuechuaaudio(monolingual).8.2.1MetricsWeusestandardlowercaseBLEUaswellascharF++toautomaticallyscoreallsubmissions.Additionalanalysesforsomelanguagepairsareprovidedbelow.Duetotheexceptionallyhardsetting,whichcurrentlyleadstogenerallylesscompetenttransla-tionsystems,wedidnotperformthehumaneval-uationoftheoutputs.8.3SubmissionsBelowwediscussallsubmissionsforalllanguagepairs,giventhattherewereseveraloverlaps.Abriefsummaryperlanguageisbelow:•Irish–Englishreceivedfoursubmissionsfromoneteam(GMU);•Marathi–Hindireceivedsubmissionsfromfourteams(ALEXAAI,BUT,GMU,andSRI-B);•Maltese–Englishreceivedfivesubmissionsfromoneteam(UM-DFKI);•Pashto–Frenchreceivedsubmissionsfromtwoteams(GMU,ON-TRAC);•Tamasheq–Frenchreceivedsubmissionsfromfourteams(ALEXAAI,GMU,NAVER,andON-TRAC);•Quechua-Spanishreceivedthreesubmissions(GMU,NAVER,andQUESPA).
24


38https://huggingface.co/LIA-AvignonUniversity/IWSLT2022-tamasheq-onlyESPnet(Inagumaetal.,2021)toolkit.Theprimarysystemwasbuiltwiththeend-to-endandbilingualASRmodelwhilethecon-trastivewasbuiltwithacascadewhichusesvariousbackbonemodelsincludingASR,thebilingualASR,transformer-basedseq2seqMT,LMforre-scoringandXLM.•GMU(MbuyaandAnastasopoulos,2023)focusedonend-to-endspeechtranslationsystems.End-to-end(E2E)transformer-basedencoder-decoderarchitecture(Vaswanietal.,2017)wasusedforprimarycon-strainedsubmission.Forunconstrainedsub-missions,theyexploredself-supervisedpre-trainedspeechmodelsandusedwav2vec2.0(Baevskietal.,2020a)andHuBERT(Hsuetal.,2021)forthelowresourcetask.Theyusedwav2vec2.0-withremovingthelastthreelayers-fortheirprimarysubmission.HuBERTwasusedforthecontrastive1sub-mission-withoutremovinganylayer.Forcontrastive2,End-to-endwithASR(E2E-ASR)architectureusesthesamearchitec-tureastheE2E.Thedifferenceisthatapre-trainedASRmodelwasusedtoinitializeitsencoder.•ON-TRAC(Laurentetal.,2023)partic-ipatedinthePashto–French(oneprimaryandthreecontrastivesystems,bothforcon-strainedandunconstrainedsettings)andTamasheq–French(oneprimaryandfivecon-trastivesystems,allofwhichareuncon-strained(c.f.Table44).ForPashto–French,theprimarycascadedsystemisbasedonaconvolutionalmodel(Gehringetal.,2017)upgraded,whilecontrastive3isbasedonsmallbasictransformers.ForPrimaryandcontrastive1systems,SAMU-XLS-R(Khu-ranaetal.,2022)wasusedwithpre-trainedencoderwith100and53languages.ThetwoconstrainedcontrastiveE2Esystemssharethesameencoder-decoderarchitectureusingtransformers(Vaswanietal.,2017).Thedif-ferenceliesintheuseornotofatransformerlanguagemodeltrainedfromscratchontheprovideddataset.AlloftheirsystemsforTamasheq–Frencharebasedonthesameend-to-endencoder-decoderarchitecture.Inthisarchitec-ture,theencoderisinitializedbyapre-
Belowwediscusseachteam’ssubmissioninde-tail:•ALEXAAI(Vishnuetal.,2023)submittedoneprimaryandthreecontrastivesystems,alloftheseareintheunconstrainedcondition(Table44)forTamasheq-French,andonepri-maryandfivecontrastivesystemsontheun-constrainedconditionforMarathi–Hindi.ForMarathi–Hindi,theirsystemsreliedonanend-to-endspeechtranslationapproach,us-ingthewav2vec2.0basemodelfinetunedon960hoursofEnglishspeech(Baevskietal.,2020b)asencoderbaselineanditwasalsofinetunedon94hoursofMarathiau-diodata.Theteamfocusedonevaluatingthreestrategiesincludingdataaugmentation,anensemblemodelandpost-processingtech-niques.ForTamasheq–French,theyreusethesameend-to-endASTmodelproposedbytheON-TRACConsortiuminthelastyear’sIWSLTedition(Boitoetal.,2022b).Thismodelconsistsofaspeechencoderthatisinitializedbythewav2vec2.0(Baevskietal.,2020a)basemodelpre-trainedon243hoursofTamasheqaudiodatareleasedbytheON-TRACConsortium38.Thedecoderofthismodelisashallowstackof2trans-formerlayerswith4attentionheads.Afeed-forwardlayerisputinbetweentheen-coderandthedecoderformatchingthedi-mensionoftheencoderoutputandthatofthedecoderinput.Inthiswork,theyfo-cusonleveragingdifferentdataaugmenta-tiontechniquesincludingaudiostretching,backtranslation,paraphrasing,andweightedloss.Anotherimportantendeavoroftheirworkisexperimentingwithdifferentpost-processingapproacheswithLLMs,suchasre-ranking,sentencecorrection,andtokenmasking.Besides,theyalsoensembleASTmodelstrainedwithdifferentseedsanddataaugmentationmethods,whichisproventoimprovetheperformanceoftheirsystems.Theirprimarysystemscores9.30BLEUonthe2023testset.•BUT(Kesirajuetal.,2023)submittedoneprimaryandonecontrastivesystemusingthe
25


trainedsemanticspeechrepresentationlearn-ingmodelnamedSAMU-XLS-R(Khuranaetal.,2022),whilethedecoderisinitializedwiththedecoderofthepre-trainedmBARTmodel.TheirworkheavilyreliesondifferentversionsoftheSAMU-XLS-Rmodel,whicharepre-trainedondifferentcombinationsofmultilingualcorporaof53,60,and100lan-guages.Inaddition,theyleveragetrainingdatafromhigherresourcecorpora,suchasCoVoST-2(Wangetal.,2020a)andEuroparl-ST(Iranzo-S´anchezetal.,2020),fortrain-ingtheirend-to-endmodels.Theirprimarysystem,whichscores15.88BLEUontheTamasheq–French2023testset,wastrainedonthecombinationof(CoVoST-2,Europarl-STandtheIWSLT2022’stestset),withtheencoderisinitializedbytheSAMU-XLS-Rmodeltrainedonthedatagatheredfrom100languages.•NAVER(Gow-Smithetal.,2023)submit-tedoneprimaryandtwocontrastivesys-temstotheTamasheq–Frenchtrack,aswellasoneprimaryandtwocontrastivesys-temsfortheunconstrainedconditionintheQuechua–Spanishtrack.IntheirworkfortheTamasheq–Frenchtrack,theyconcentrateonparameter-efficienttrainingmethodsthatcanperformbothSTandMTinamultilin-gualsetting.Inordertodoso,theyinitial-izetheirmodelswithapre-trainedmultilin-gualMTmodel(mBART(Liuetal.,2020)orNLLB(NLLBTeametal.,2022)),whichisthenfine-tunedontheSTtaskbyinputtingfeaturesextractedwithafrozenpre-trainedspeechrepresentationmodel(wav2vec2.0orHuBERT(Hsuetal.,2021)).Theencoderoftheirtranslationmodelisslightlymodifiedwheretheystackseveralmodality-specificlayersatthebottom.Inaddition,adapterlayersarealsoinsertedinbetweenlayersofthepre-trainedMTmodelatboththeen-coderanddecodersides.Whilethesenewcomponentsgetfine-tunedduringthetrain-ingprocess,thepre-trainedcomponentsoftheMTmodelarefrozen.Oneoftheappeal-ingcharacteristicsoftheirapproachisthatitallowsthesamemodeltodobothspeech-to-textandtext-to-texttranslation(ortranscrip-tion).Furthermore,theirmethodmaximizesknowledgetransfertoimprovelow-resourceperformance.Theirprimarysystem,whichisensembledfrom3differentrunsonthecom-binationofbothSTandASRdata,scores23.59BLEUonthe2023testset.FortheQuechua–Spanishtrack,theoverallarchitecturefortheirsystemsconsistsoffirstinitializingaPLMwhichwasthenfine-tunedonthespeechtranslationtaskbyinputtingfeaturesfromafrozenpre-trainedspeechrep-resentation.SimilaradaptationsweredonewithanMTmodeltocontroldomainandlengthmismatchissues.Oneoftheinterest-ingtakeawaysfromtheirapproachesisthattheircontrastive2system(1.3billionpa-rameters(NLLBTeametal.,2022))outper-formedtheircontrastive1system(3.3billionparameters(NLLBTeametal.,2022))de-spiteithavinglessparameters.NAVER’sprimarysubmissionwasanensembleap-proachthatincludedtheuseofPLMsforboththeASR(Baevskietal.,2020a)andMTsystems((NLLBTeametal.,2022))andincludedtrainingonbothTamasheqandQuechuadata.TheirsubmissionstoQUE–SPAdidnotincludetheuseofmBARTorHuBERT(Hsuetal.,2021)aswasdoneforotherlanguagepairsthatNLEsubmitted.•QUESPA(Ortegaetal.,2023)submittedtobothconditions(constrainedanduncon-strained)atotalofsixsystemsincludingaprimary,contrastive1,andcontrastive2foreachcondition.Theyalsoclaimtohavetriedseveralothercombinationsbutdidnotsub-mitthosesystems.Fortheconstrainedcondi-tion,theirprimarysystemscoredsecondbest,slightlylessthanteamGMUwithaBLEUscoreof1.25andchrF2of25.35.Theyalsoscoredthirdbestfortheconstrainedcondi-tionwith0.13BLEUand10.53chrF2us-ingtheircontrastive1system.Itisworth-whiletonotethatchrF2wasusedbytheorganizerswhenBLEUscoreswerebelowfive.Fortheirconstrainedsystems,adi-rectspeechtranslationsystemwassubmit-tedsimilartotheGMUteam’sprimaryap-proachthatusedFairseq(Wangetal.,2020b).QUESPAextractedmel-filterbank(MFB)featuressimilartotheS2Tapproachinprevi-ousworkWangetal.(2020b).Themaindif-ferencebetweenQUESPA’ssubmissionandGMU’ssubmissionswasthattheGMUteam
26


increasedthenumberofdecoderlayersto6whichresultedinaslightlybettersystemforGMU.TheothersystemssubmittedfortheconstrainedsettingwerecascadesystemswhereASRandMTwerecombinedinapipelinesetting.Theircontrastive1and2systemsubmissionsfortheconstrainedtaskrespectivelyusedwav2letter++(Pratapetal.,2019)andaconformerarchitecturesimilartopreviouswork(Gulatietal.,2020)alongwithanOpenNMT(Kleinetal.,2017)trans-lationsystemtrainedontheconstrainedSTandMTdata.Bothofthosesystemsper-formedpoorlyscoringlessthan1BLEU.Fortheunconstrainedcondition,thethreesys-temsthatwerepresentedbyQUESPAcon-sistedofpipelineapproachesofPLMsthatwerefine-tunedontheadditional60hoursofSiminchikaudiodataalongwiththecon-straineddata.Theirprimaryandcontrastive1unconstrainedASRsystemsweretrainedusingthe102-languageFLEURS(Conneauetal.,2023)modelandusedtheMTsys-temthatwasbasedonNLLB(NLLBTeametal.,2022)whichjustsohappenstoin-cludeQuechuaasoneofitslanguages.Theircontrastive2ASRsystemwasbasedonwav2letter++(Pratapetal.,2019)whiletheircontrastive2MTsystemwasidenticaltotheMTsystemsusedfortheirPrimaryandCon-trastive1submissions.•SRI-B(Radhakrishnanetal.,2023)submit-tedfoursystems.ForMarathi–English,theysubmittedoneprimaryandonecontrastivesystemintheconstrainedsettingandoneprimaryandonecontrastivesystemintheunconstrainedsetting.Theyusedend-to-endspeechtranslationnetworkscomprisingaconformerencoderandatransformerdecoderforbothconstrainedandunconstrained.•UM-DFKI(Williamsetal.,2023)submit-tedfivesystems.Itincludedoneprimaryandfourcontrastivesystemsinunconstrainedset-tings.Theyusedapipelineapproachforalloftheirsubmissions.ForASR,theirsystembuildsupon(Williams,2022)onfine-tuningXLS-Rbasedsystem.mBART-50wasusedforfine-tuningtheMTpartofthepipeline.8.4ResultsIrish–EnglishAsdiscussedearlier,onlytheGMUteamparticipatedintheGA–ENGtrans-lationtrackandsubmittedoneprimarysystemtoconstrained,oneprimarysystemtounconstrainedandtherestofthetwosystemstocontrastiveonunconstrainedconditions.Theend-to-endandend-to-endwithASRmodelssubmittedprimaryconstrainedandcontrastive2unconstrainedsys-tems.Boththesystemsachieved15.1BLEUscores.Theydidnotperformwellincomparisontothewav2vec2.0andHuBERTmodels.Thede-tailoftheresultsofthistrackcanbefoundinTa-ble36and37.Marathi–HindiTheresultsofthistranslationtrackcanbefoundinTable38and39.Over-allweseevaryingperformancesamongthesys-temssubmittedtothistrack,withsomeperform-ingmuchbetteronthetestset.Outofthe16submissions,theSRI-Bteam’sprimarysystemachievedthebestresultof31.2and54.8inBLEUandincharF++respectivelyontheconstrainedconditionwhiletheBUTteam’sprimarysystemachievedthebestresultsof39.6inBLEUand63.3incharF++ontheunconstrainedcondition.Inbothconstrainedandunconstrainedconditions,theGMUsystemsachievedthelowestresultsof3.3and5.9inBLEUand16.8and20.3incharF++respectively.Maltese–EnglishTheresultsofthistranslationtrackcanbefoundinTable42.UM-DFKIusedcontrastiveapproachesintrainingtheirASRsys-tem.Fortheircontrastive1system,theirfine-tuningconsistedofusingMaltese,Arabic,FrenchandItaliancorpora.Theircontrastive2,con-trastive3,andcontrastive4approachesrespectivelyuseasubsetfromArabic,FrenchandItalianASRcorpusalongwithMaltesedata.Thebestresultof0.7BLEUwasachievedwiththeircontrastive1system.Pashto–FrenchThedetailedresultscanbefoundinTable41andTable40oftheAppendix.WerankthesystemperformancebasedontestBLEUscores.ThebestscoreBLEUwasachievedbyON-TRACprimarysystem(SAMU-XLS-Rmodeltrainedon100languages).Forthecon-strainedcondition,thecascadedapproachbasedonconvolutionalmodels,givesthebestperfor-mance.
27


Tamasheq-FrenchTheresultsofthistransla-tiontrackcanbefoundinTable43and44.Com-paredtothelastyear’sedition,thisyearhaswit-nessedagrowinginterestinthislow-resourcetranslationtrackintermsofbothquantityandqualityofsubmissions.Almostallsubmissionsachieverelativelybetterresultsthanthelastyear’sbestsystem(5.7BLEUontest2022(Boitoetal.,2022b)).Furthermore,itisnotablethatcascadedsystemsarenotfavorableinthistrackwhilenoneofthesubmittedsystemsisofthiskind.Thisyear,thislanguagepairremainsachal-lenginglow-resourcetranslationtrack.Thereisonlyonesubmissiontotheconstrainedcondi-tionfromGMUwithanend-to-endmodelscor-ing0.48BLEUonthisyear’stestset.Forthisreason,alltheparticipantsareinfavorofexploitingpre-trainedmodels,hencebeingsub-jecttotheunconstrainedcondition.Amongthesepre-trainedmodels,self-supervisedlearn-ing(SSL)fromspeechmodelsremainsapopu-larchoiceforspeechencoderinitializing.Us-ingawav2vec2.0modelpre-trainedonunlabelledTamasheqdataforinitializingtheirspeechen-coder,GMUgains+7.55BLEUscoreincompari-sonwiththeirTransformer-basedencoder-decodermodeltrainingfromscratch(theirprimarycon-strainedsystem).Atthedecoderside,pre-trainedmodelssuchasmBARTorNLLBarecommonlyleveragedforinitializingthedecoderoftheend-to-endSTmodel.Besides,dataaugmentationanden-semblingarealsobeneficialasshownbyALEXAAIwhentheyconsistentlyachieve∼9BLEUinalloftheirsettings.OutstandingBLEUscorescanbefoundintheworkoftheON-TRACteam.Aninterestingpre-trainedmodelnamedSAMU-XLS-Risshowntobringsignificantimprovements.Thisisamultilin-gualmultimodalsemanticspeechrepresentationlearningframework(Khuranaetal.,2022)whichfine-tunesthepre-trainedspeechtransformeren-coderXLS-R(Babuetal.,2021)usingsemanticsupervisionfromthepre-trainedmultilingualse-mantictextencoderLaBSE(Fengetal.,2022).Exploitingthispre-trainedmodelandtrainingend-to-endSTmodelsonthecombinationsofdif-ferentSTcorpora,theyachievemorethan15BLEUinalloftheirsettings.NAVERtopsthistranslationtrackbyamultilin-gualparameter-efficienttrainingsolutionthatal-lowsthemtoleveragestrongpre-trainedspeechandtextmodelstomaximizeperformanceinlow-resourcelanguages.BeingabletobetrainedonbothSTandASRdataduetothemultilingualna-ture,alloftheirsubmissionsheavilyoutperformthesecondteamON-TRACbyconsiderablemar-gins.Theirprimarysystem,whichisensembledfrom3differentruns,usesNLLB1.3Basthepre-trainedMTsystem,andwav2vec2.0Niger-Mali39asthespeechpresentationextractor.Afterbe-ingtrainedonacombinationofbothSTcorpora(Tamasheq-French,mTEDxfr-en,mTEDxes-fr,mTEDxes-en,mTEDxfr-es(Saleskyetal.,2021))andASTcorpora(TED-LIUMv2(Rousseauetal.,2014),mTEDxfr,mTEDxes),thissystemestab-lishesanimpressivestate-of-the-artperformanceoftheTamasheq-Frenchlanguagepair,scoring23.59BLEUonthe2023testset.Quechua–SpanishTheQUE–SPAresultsforallsystemssubmittedtothislow-resourcetrans-lationtrackcanbefoundinTable45and46oftheappendix.Toourknowledge,thisfirstedi-tionoftheQUE–SPAlanguagepairinthelow-resourcetrackofIWSLThaswitnessedthebestBLEUscoresachievedbyanyknownsysteminresearchforQuechua.Thetwobestperformingsystems:1.46BLEU(constrained)and15.70(un-constrained)showthatthereisplentyofroomtoaugmentapproachespresentedhere.Nonetheless,submissionsfromthethreeteams:GMU,NAVER,andQUESPAhaveshownthatitispossibletousePLMstocreatespeech-translationsystemswithaslittleas1.6hoursofparallelspeechdata.Thisisanotablecharacteristicofthistaskandsurpassespreviousworkinthefield.WehavefoundthattheNLLB(NLLBTeametal.,2022)system’sinclusionofQuechuainre-centyearshashadagreaterimpactthanexpectedforease-of-use.Similarly,theuseofFairseq(Wangetal.,2020b)seemstobethepreferredtoolkitforcreatingdirectS2Tsystems,cascadedornot.TheQUE–SPAsubmissionsfortheun-constrainedconditionspreferredtheuseofacas-cadingsysteminapipelineapproachwherepre-trainedmodelswerefine-tunedfirstforASRandthenforMT.Theconstrainedsettingleavesmuchroomforimprovement.Nonetheless,GMUandQUESPA’snearidenticalsubmissionshaveshownthatthein-
39https://huggingface.co/LIA-AvignonUniversity/IWSLT2022-Niger-Mali
28


creaseof3layersduringdecodingcanbepowerfulandshouldbeexploredfurther.Itwouldbeworth-whilefortheorganizersoftheQUE–SPAtracktoobtainmoreparalleldataincludingtranslationsforfutureiterationsofthistask.Theunconstrainedsettingclearlycanbenefitfromanensemblingtechniqueandtrainingwithmultiplelanguages–inthesesubmissions,thetrainingofamodelwithanadditionallanguagelikeTamasheqalongsideQuechuadoesnotseemtohaveanegativeimpactonperformance.Al-though,itishardtoascertainwhethertheslightperformancegainoflessthan1BLEUpointoftheNLEteam’ssubmissioncomparedtoQUESPA’ssubmissionwasduetotheensembling,freezingofthemodels,orthelanguageaddition.Asafinaltakeaway,theNLEteam’ssubmis-sionsscoredquitewellundertheunconstrainedcondition.Itshouldbenotedthatforotherlan-guagepairsNLE’shighsystemperformancewasalsoduetotheensemblingofsystemsthatwereexecutedusingdifferentinitializationparametersonatleastthreeuniqueruns.Asanaside,smallgainswereachievedundertheconstrainedcondi-tionwhencomparingtheGMUsubmissiontotheQUESPAsystemduetotheincreaseindecodinglayers.QUESPA’sinclusionofalanguagemodelontopofastate-of-the-artdataset(Fleurs)allowedthemtoachievescoressimilartoNAVER’swith-outadditionaltuningorensembling.State-of-the-artperformancewasachievedbyallthreeteamsthatsubmittedsystems.GeneralObservationsAsinpreviousyears,thelow-resourcesharedtaskprovedparticularlychal-lengingfortheparticipants,butthereareseveralencouragingsignsthatfurtherreinforcetheneedformoreresearchinthearea.First,moreteamsthaneverparticipatedinthesharedtask,showingacontinuedinterestinthefield.Second,wenotethatforthelanguagepairthatwasrepeatedfromlastyear(Tamasheq–French),almostallsubmissionsoutperformedlastyear’sbestsubmission,withanaccuracyincreaseofmorethan17BLEUpointsintheunconstrainedsetting.Last,wehighlightthebreadthofdifferentapproachesemployedbytheparticipants,rangingfromtheuseoffinetunedpre-trainedmodelstopre-trainingfromscratch,toparameterefficientdine-tuningaswellascascadedpipelinesystems,allofwhichseemtohavebenefitstooffer,toacertainextent,todifferentlanguagepairs.LimitationsAsnotedbysomeparticipants,theIrish–EnglishandMaltese–Englishtransla-tiontrackdatahaslimitations.ForIrish–English,thespeechtranslationsystemscanachieveveryhighBLEUscoresonthetestsetifthebuiltsystemshaveusedwav2vec2.0and/ortheIrishASRmodelwhichistrainedontheCommonVoice(Ardilaetal.,2020b)dataset.Similarly,theGMUteamhasachievedhighBLEUscoresespeciallywhentheyusedwav2vec2.0andHu-BERTmodels.WeplantocontinuethistranslationtracknextyearbyupdatingthetestandtrainingdatatothoroughlyinvestigatethedataqualityaswellasthereasontoobtainthehighBLEUscores.ForMaltese–English,someparticipantsreportedissueswiththedataquality,whichwehopetore-solveinfutureiterationsofthesharedtask.9FormalityControlforSLTDifferentlanguagesencodeformalitydistinctionsindifferentways,includingtheuseofhonorifics,grammaticalregisters,verbagreement,pronouns,andlexicalchoices.Whilemachinetranslation(MT)systemstypicallyproduceasinglegenerictranslationforeachinputsegment,SLTrequiresadaptingthetranslationoutputtobeappropriatetothecontextofcommunicationandtargetaudience.Thissharedtaskthuschallengesmachinetransla-tionsystemstogeneratetranslationsofdifferentformalitylevels.9.1ChallengeTaskGivenasourcetext,XinEnglish,andatargetformalitylevel,l∈{F,IF},thegoalinformality-sensitivemachinetranslation(Niuetal.,2017)istogenerateatranslation,Y,inthetargetlanguagethataccuratelypreservesthemeaningofthesourcetextandconformstothedesiredformal-itylevel,l.Thetwoformalitylevelstypicallycon-sideredare“F”forformaland“IF”forinformal,resultingintwotranslations:YFandYIFrespec-tively.Forexample,theformalandinformaltrans-lationsforthesourcetext“YeahDidyourmomknowyouwerethrowingtheparty?”(originallyinformal)inKoreanareshowninthetablebelow:Thissharedtaskbuildsonlastyear’soffering,whichevaluatedsystems’abilitytocontrolfor-malityonthefollowingtranslationtasks:trans-lationfromEnglish(EN)intoKorean(KO)andVietnamese(VI)inthesupervisedsetting,andfromEnglish(EN)intoPortugalPortuguese(PT)
29


Source:YeahDidyourmomknowyouwerethrowingtheparty?KoreanInformal:그,어머님은[F]네가[/F]그파티연거[F]아셔[/F]?KoreanFormal:그,어머님은[F]님이[/F]그파티연거[F]아세요[/F]?Table7:ContrastiveformalandinformaltranslationsintoKorean.Grammaticalformalitymarkersareanno-tatedwith[F]text[/F].andRussian(RU)inthezero-shotsetting.Re-sultsshowedthatformality-controlischalleng-inginzero-shotsettingsandforlanguageswithmanygrammaticalandlexicalformalitydistinc-tions.Thisyear’seditioninvitedparticipantstoadvanceresearchineffectivemethodsforbridg-ingthegapinformalitycontrolforzero-shotcasesandforlanguageswithrichgrammaticalandlexi-calformalitydistinctions.9.2DataandMetricsParticipantswereprovidedwithtestdata,aswellasMTqualityandformalitycontrolmetrics.Inaddition,weprovidedtrainingdata,consistingofformalandinformaltranslationoftextsforthesu-pervisedlanguagepairs(EN-KO,EN-VI).9.2.1FormalityAnnotatedDatasetWeprovidetargeteddatasetscomprisingsourcesegmentspairedwithtwocontrastivereferencetranslations,oneforeachformalitylevel(informalandformal)fortwoEN-VI,EN-KOinthesuper-visedsettingandEN-RU,EN-PTinthezero-shotsetting(seeExample7)40.Thesizesandproper-tiesofthereleaseddatasetsforallthelanguagepairsarelistedinTable8.FormaltranslationstendtobelongerthaninformaltextsforVietnamesecomparedtootherlanguagepairs.Thenumberofphrasalformalityannotationsrangesfrom2to3.5persegment,withKoreanexhibitingahigherdiversitybetweentheformalandinformaltransla-tionsasindicatedbytheTERscore.9.2.2TrainingConditionsWeallowedsubmissionsundertheconstrainedandunconstraineddatasettingsdescribedbelow:
40https://github.com/amazon-science/contrastive-controlled-mt/tree/main/IWSLT2023Constrained(C)Participantswereallowedtousethefollowingresources:TextualMuST-Cv1.2(DiGangietal.,2019b),CCMatrix(Schwenketal.,2021),OpenSubtitles(LisonandTiede-mann,2016)anddatasetintheconstrainedset-tingfromtheFormalityControltrackatIWSLT22(Anastasopoulosetal.,2022).Unconstrained(U)Participantscoulduseanypubliclyavailabledatasetsandresources:theuseofpre-trainedlanguagemodelswasalsoallowed.Additionally,usingadditionallyautomaticallyan-notatedbitextwithformalitylabelswasalsoal-lowed.9.3FormalityClassifierWereleaseamultilingualclassifier(MC)trainedtopredicttheformalityofatextforallthelan-guagepairs:EN-KO,EN-VI,EN-RU,andEN-PT.Wefinetuneanxlm-roberta-base(Con-neauetal.,2020)modelonhuman-writtenformalandinformaltranslationsfollowingthesetupfromBriakouetal.(2021).Ourclassifierachievesanaccuracyof>98%indetectingtheformalityofhuman-writtentranslationsforthefourtargetlan-guages(Table10).Participantswereallowedtousetheclassifierbothformodeldevelopmentandforevaluationpurposesasdiscussedbelow.9.4AutomaticMetricsWeevaluatethesubmittedsystemoutputsalongthefollowingtwodimensions:1.Overalltranslationquality,evaluatedusingSacreBLEUv2.0.0(Papinenietal.,2002b;Post,2018),andCOMET(Reietal.,2020b)onboththesharedtask-providedtestsetsbasedontopicalchat(Gopalakrishnanetal.,2019)andontheFLORESdevtest(NLLBTeametal.,2022;Goyaletal.,2022).2.Formalitycontrol,evaluatedusing:•Matched-Accuracy(mACC),areference-basedcorpus-levelautomaticmetricthatleveragesphrase-levelformalitymarkersfromthereferencestoclassifyasystem-generatedhypothesisasformal,informal,orneutral(Nadejdeetal.,2022).•Classifier-Accuracy(cACC),areference-freemetricthatusesthemultilingualfor-malityclassifierdiscussedabovetolabelasystem-generatedhypothesisasformalorinformal.
30


SETTINGS
FORMALITYUMD-baselineU✓AllMultilingualExemplarsCOCOA-baselineC✗EN-{VI,KO}BilingualSide-constraintAPPTEKU✗EN-{PT,RU}BilingualSide-constraintHW-TSCU+C✓AllBilingualSide-constraintKUXUPSTAGEU✓AllBilingualN/AUCSCU✗EN-{VI,KO}MultilingualStyle-EmbeddingTable9:FormalityTrackSubmissionsSummary.Mostparticipantstrainbilingualsystemsbutleverageadiversesetofformalityencodingmechanismsforcontrol.
MODELTYPE
LANGUAGE
LANGUAGES
INFORMAL
INFORMAL
TER(F,IF)
SIZE
SOURCE
#PHRASALANNOTATIONS
FORMAL
FORMAL
PARTICIPANT
LENGTH
AccuracyKorean99.9%Vietnamese99.3%Russian99.9%Portuguese98.6%Table10:Themultilingualclassifiercanidentifythetargetformalityforhumanwrittentextacrossalllan-guageswith>98%accuracy.Thefinalcorpus-levelscoreforeachofthetwometricsdescribedaboveisthepercent-ageofsystemoutputsthatmatchesthede-siredformalitylevel.Forexample,thecACCforthetargetformality,Formal(F),isgivenby,cACC(F)=1
TYPE
CLASSIFIERUSE
TargetLanguage
EN-VITrain40020.3528.5225.482.711.4923.70Test60021.8229.5926.772.791.5523.00EN-KOTrain40020.0013.4113.403.353.3524.52Test60021.2213.5613.553.513.5125.32EN-RUTest60021.0218.0318.002.062.0513.59EN-PTTest60021.3620.2220.271.931.9310.46Table8:FormalityTrackSharedTaskDataStatistics.
M∑Mi=11[MC(Y)==F],whereMisthenumberofsystemoutputs.9.5SubmissionsWeprovidemethodologydescriptionsandasum-maryofthetwobaselinesystemsandfoursub-missionsreceivedforthesharedtaskbelowandinTable9.Threeoutofsixsubmissionsmadeuseoftheformalityclassifierreleasedforsystemde-velopment.Wereceivedtwomultilingualandfourbilingualsystems.Wereferthereadertothesys-temdescriptionpapersformoredetails.•COCOA(baseline)usesasupervisedmethodwhereagenericneuralMTmodelisfine-tunedonlabeledcontrastivetranslationpairs(Nadejdeetal.,2022).Fortheconstrained,supervisedsetting,thegenericneuralMTmodelwastrainedonparalleldataallowedfortheconstrainedtaskandfine-tunedonfor-malandinformaldatareleasedforthesharedtask.FollowingNadejdeetal.(2022),con-trastivepairswereupsampledwithafixedup-samplingfactoroffiveforalllanguagepairs.•UMD(baseline)uses16few-shottar-getformality-specificexemplarstopromptXGLM-7.5B(Linetal.,2021)togeneratestyle-controlledtranslations.Forthesu-pervisedsetting,theseexamplesaredrawnfromtheofficialtrainingdata,whereasforthezero-shotsetup,theexamplesfromtheTatoebacorpus(ArtetxeandSchwenk,2019)arefilteredandmarkedwithtargetformalityusingtheprovidedformalityclassifier.•APPTEK(Baharetal.,2023)submittedout-putsusingtheirproductionqualitytranslationsystemsthatsupportformality-controlledtranslationgenerationforEN-PTandEN-
31


RU.TheseareTransformer-BigmodelstrainedonalargepublicdatasetfromtheOPUScollection(Tiedemann,2012),auto-maticallymarkedwithformalityusingase-quenceofregularexpressions.Theformalitylevelisencodedwithapseudo-tokenatthebeginningofeachtrainingsourcesentencewithoneof3values:formal,informal,ornostyle.•HW-TSC(Wangetal.,2023a)describesasystemthatusesamulti-stagepre-trainingstrategyontask-provideddatatotrainstrongbilingualmodels.Usingthesebilingualmod-els,theyemploybeamre-rankingontheout-putsgeneratedusingthetestsource.Thegen-eratedhypothesisarerankedusingthefor-malityclassifierandphrasalannotations,it-erativelyfine-tuningthemodelonthisdatauntiltestperformanceconvergences.Initialformalitycontrolisenabledbyaspecialto-kenandre-affirmedthroughclassifieroutputandannotationsfromtraining.•KUXUPSTAGE(Leeetal.,2023)useslarge-scalebilingualtransformer-basedMTsys-temstrainedonhigh-qualitydatasetsandMBARTforthesupervisedandzero-shotset-tingsrespectively.Theygenerateaformality-controlledtranslationdatasetforsupervisioninthezero-shotsettingusingGPT-4andfil-terthegeneratedsource-translationpairsus-ingtheformalityclassifier.Allbilingualmodelsarethenfinetunedindependentlyforthetwotargetformalitydirectionstogen-erateformality-controlledoutputs,resultingin#(Language-pairs)×2(Formal/Informal)models.•UCSC(Vakhariaetal.,2023)focusedonus-ingasinglemultilingualtranslationmodelforallthelanguagepairsundertheuncon-strainedsetting.Theyfinetunethepre-trainedmodel,mBART-large-50(Tangetal.,2020),usingtheprovidedcontrastivetransla-tions(§9.2.1)withanaddedstyleembeddinginterventionlayer.9.6ResultsTables47and48intheAppendixshowthemainautomaticevaluationresultsforthesharedtask.OverallResultsForthesupervisedlanguagepairsinbothconstrainedandunconstrainedset-tings,mostsubmittedsystemsweresuccessfullyabletocontrolformality.TheaveragemAccscoresrangedfrom78-100.ControllingformalityinKoreanwasfoundtobemorechallengingthantranslatingwithformalitycontrolinVietnameseasreflectedbytherelativelylowermAccscoreswhichwebelievetobeduetothevariationinfor-malityexpressionofKoreanhonorificspeechre-flectedinpretrainingdata.HW-TSCconsistentlyachievesthebestscoresacrosstheboardforalllanguagepairsandbothsettingsduetotheuseoftransductivelearning.Interestingly,theconstrainedsubmissionbyHW-TSCachievesbetterorcompetitiveresultscom-paredtotheirunconstrainedsystemsuggestingthattheuseofapre-trainedlanguagemodeloradditionalresourcesisnotnecessarytogener-atehigh-qualityformality-controlledtranslations.Generally,thesystemsgeneratehigherqualityout-putsintheformalsettingrelativetotheinformalsettingforbothsupervisedlanguagepairsaccord-ingtoBLEUandCOMET,whichmightbeduetothebiasofthedatasetusedduringpre-trainingwhichistypicallynewsandhencemoreformal.Inthezero-shotunconstrainedsetting,thisfor-malitybiasisevenmoreprominent.WeobserveamuchwiderdistributionintheformalityscoresforEnglish-Portuguese(mAcc:F90-100,IF:58-100),possiblyduetothehighambiguityintheinformallanguageandtheconfoundingdialectalinfluenceofBrazilianPortuguesedominantinthepre-trainingcorpora,whichisknowntousefor-malregisterevenintypicallyinformalcontexts(Costa-juss`aetal.,2018).HW-TSCandAPPTEKachievethebesttranslationqualityforEnglish-PortugueseandEnglish-Russianrespectively.Thelowestscoringsubmissioninbothqualityandfor-malitycontrol(UCSC)didnotincludeanyfine-tuningoradaptationofthebaseMBARTmodeltothetwozero-shotlanguagepairs:English-RussianandEnglish-Portuguese.Thissuggeststhatfor-malityinformationisnottransferredfromtheun-relatedlanguagepairs,EN-KOandEN-VI,andthatsomelanguage-specificsupervisionisneededtomarkgrammaticalformalityappropriatelyinRussianandPortuguese.Howwelldosystemsmatchthedesiredtar-getformality?Weshowthedistributionofthescoresgeneratedusingtheformalityclassifierfor
32


Figure3:FormalityClassifierScores’DistributiononthesubmittedsystemoutputsintheUnconstrainedsetting:HW-TSCcanpreciselymatchthetargetformalityasdepictedbythepeakydistribution.allthesystemssubmittedtoalllanguagepairsun-dertheunconstrainedsettinginFigure3.Forsu-pervisedlanguagepairs,formal(blue)andinfor-mal(orange)outputscorespeakat1.0and0.0re-spectively.Inthezero-shotsetting,forbothPor-tuguese(APPTEK,UCSC)andRussian(UCSC)translations,theinformaloutputshaveabimodaldistribution,highlightingthatthesemodelsgener-atemanyformaltranslationsunderinformalcon-trol.Howcontrastivearethegeneratedtransla-tions?WeshowtheTranslationEditRate(TER)betweentheformalandinformaloutputsforallsubmittedsystemsacrossalllanguagepairsinFig-ure4.Whilethereferencesaredesignedtobemin-imallycontrastive,theformalandinformalsystemoutputsexhibitamuchlargereditdistance.HW-TSChasthelowestTERrateforalllanguagepairsexceptEnglish-Korean.DiscussionOverall,thesharedtaskresultsshowthatfinetuningastrongsupervisedgeneral-purposeMTsystemwithaslowas400in-domaincontrastivesamplesseemstobesufficientingeneratinghigh-qualitycontrastiveformality-controlledtranslations.However,severalavenuesforimprovementremainopen.Thelanguagesthat
Figure4:TERbetweentheFormal(F)andInformal(IF)Outputsforallsubmittedsystemsacrossalllan-guagepairs.exhibitanambiguousorricherformalitydistinc-tioneitherduetoclosedialectalvariations(likePortuguese)orduetomultiplelevelsofhonorifics(likeKoreanandJapanese)stillremainchalleng-ing.Unsupervisedtransferofformalityknowl-edgebetweenrelatedlanguagesremainsrelativelyunexplored(Sartietal.,2023).Furthermore,thisyear’staskonlyconsideredtwolevelsofformal-itydistinctionswithminimaledits.Itremainsun-clearwhetherthemodelsarealsocapableofmod-elingmultiplelevelsofformalitypotentiallywithminimaleditsinthegeneratedtranslations.Fi-nally,nosubmissionshaveexploredmonolingualeditingoftranslationsasapotentialsolutionfor
33


43Eachvolunteerprovidedtheirconsenttousethisdataforautomaticdubbingtask.44Medianspeechoverlapisjust0.731inalargecorpusofhumandubs(Brannonetal.,2023)
formality-controlledMT,despitetheedit-focusednatureofthecontrastivetranslations.Werecom-mendthatfutureworkonformality-controlledma-chinetranslationtargetsthesechallenges.10AutomaticDubbing10.1ChallengeThistaskfocusesonautomaticdubbing:translat-ingthespeechinavideointoanewlanguagesuchthatthenewspeechisnaturalwhenoverlayedontheoriginalvideo(seeFigure5).ParticipantsweregivenGermanvideos,alongwiththeirtexttranscripts,andwereaskedtopro-duceddubbedvideoswheretheGermanspeechhasbeentranslatedintoEnglishspeech.Automaticdubbingisaverydifficult/complextask(Brannonetal.,2023),andforthissharedtaskwefocusonthecharacteristicwhichisper-hapsmostcharacteristicofdubbing:isochrony.Isochronyreferstothepropertythatthespeechtranslationistimealignedwiththeoriginalspeaker’svideo.Whenthespeaker’smouthismoving,alistenershouldhearspeech;likewise,whentheirmouthisn’tmoving,alistenershouldnothearspeech.Tomakethistaskaccessibleforsmallacademicteamswithlimitedtrainingresources,wemakesomesimplifications:First,weassumetheinputspeechhasalreadybeenconvertedtotextusinganASRsystemandthedesiredspeech/pausetimeshavebeenextractedfromtheinputspeech.Sec-ond,toalleviatethechallengesoftrainingaTTSmodel,theoutputisdefinedtobephonemesandtheirdurations.Thesephonemesanddurationsareplayedthroughanopen-sourceFastSpeech2(Renetal.,2022)text-to-speechmodeltoproducethefinalspeech.4110.2DataandMetricsOfficialtrainingandtestdatasetswereprovided42bytheorganizers.ThetrainingdatawasderivedfromCoVoST2(Wangetal.,2021)andconsistsof:1.Source(German)text2.Desiredtargetspeechdurations(e.g.2.1sofspeech,followedbyapause,followedby1.3sofspeech)
41https://github.com/mtresearcher/FastSpeech242https://github.com/amazon-science/iwslt-autodub-task/tree/main/data3.Target(English)phonemesanddurationscor-respondingtoatranslationwhichadherestothedesiredtimingThetestdatawasproducedbyvolunteersandconsistsofvideosofnativeGermanspeakersreadingindividualsentencesfromtheGermanCoVoST-2testset.43Thistestsetwasdividedintotwosubsets;Subset1wheretherearenopausesinthespeechandSubset2wherethereisoneormorepauseinthespeech.Moredetailsonthisdataarepresentedin(Chronopoulouetal.,2023).10.3SubmissionsDespitehighinitialinterest,wereceivedonlyonesubmission,whichwasfromtheHuaweiTranslationServicesCenter(HW-TSC)(Raoetal.,2023).However,wehadtwosystems(Chronopoulouetal.,2023;Paletal.,2023)builtforthetaskforwhichwehadnotyetperformedhumanevaluation,sowestillhadenoughsystemsforainterestingcomparison.•Interleaved(Baseline):OurfirstbaselineandthebasisforthissharedtaskisfromChronopoulouetal.(2023).Theyproposetojointlymodeltranslationsandspeechtiming,givingthemodelthefreedomtochangethetranslationtofitthetiming,orandmakescar-ifiesintranslationqualitytomeettimingcon-straintsorrelaxtimingconstraintstoimprovetranslationquality.Thisisachievedbysim-plybinningtargetphonemedurationsandin-terleavingthemwithtargetphonemesduringtrainingandinference.Toavoidteachingthemodelthatspeechdurationsshouldbeprior-itizedovertranslationquality44,noisewithstandarddeviation0.1isaddedtothetargetphrasedurationstosimulatethesourcedura-tionsusedatinference.•Factored(Baseline):Paletal.(2023)buildonthefirstbaselinebyusingtargetfactors(Garc´ıa-Mart´ınezetal.,2016),wherealong-sidepredictingphonemesequencesasthetarget,wealsopredictdurationsforeachphonemeasatargetfactor.Additionally,theyproposeauxiliarycounters,whicharesimi-lartotargetfactorsexceptthemodelisnot
34


45https://en.wikipedia.org/wiki/Mean_opinion_score
Figure6:SystemdiagramforHW-TSCdubbingsys-tem.ImagefromRaoetal.(2023).wereresearchersinautomaticdubbing.Foreachvideointhethetestset,onejudgewasshownthefoursystemoutputsinrandomorderandaskedtoratethemfrom1-6.Thejudgeswerenotgivenadefinedrubricorguidelinestofollowbutwereaskedtobeconsistent.Asametricweoptedformeanopinionscore(MOS)methodologywherethescoresforasystemasjudgedbyhumansareaveragedinonescore.45Feedbackfromthejudgesindicatethatthebase-lineandsubmittedsystemsoftenproducepoortranslations(perhapsduetothesmallamountoftrainingdatausedbyeachsystem),andthevoicequalityfromtheFastSpeech2modelwasfarfromperfect.However,theyfeltthathavingallsystemssharethesamevoicemadeitmucheasiertocom-pareacrossdubbingsystems.Whenwelookedatthedistributionofscoresper
Figure5:Toillustrate,here’sanexampleinwhich“hallo!weigehts?”istranslatedto“hi!howareyou?”suchthattheoutputwillfitinthedesiredtargetspeechdurationsof0.4sand1.3s,withapauseinbetweentrainedtopredictthem.Instead,theypro-vidingadditionalinformationtothedecoderconsistingof(1)thetotalnumberofframesremaining,(2),thenumberofpausesremain-ing,and(3)thenumberofframesremaininginthecurrentphrase.Asinthefirstbase-line,noiseofstandarddeviation0.1isaddedtothetargetphrasedurationsduringtrainingtosimulatesourcedurations.•Text2Phone(Baseline):Asasanitycheck,weaddedathird,non-isochronicbaselinetrainedtotakeinGermantextandproduceEnglishphonemes,withoutanydurationin-formation.Wetrainonthesamedataasthefirsttwobaselines,butexcludedurationin-formationfromtrainingandinsteadpredictphonemedurationsusingthedurationmodelfromtheFastSpeech2model.•HW-TSC:Incontrasttoourthreebaselines,(Raoetal.,2023)tookamoretraditionalapproachtodubbingandfollowedthepriorworksonverbositycontrol(Lakewetal.,2021,2019)tofirstgenerateasetoftransla-tioncandidatesandlaterre-rankthem.Theirsystemconsistsoffourparts:1)voiceac-tivitydetectionfollowedbypausealignment,2)generatingalistoftranslationcandidates,3)phonemedurationprediction,followedby4)re-ranking/scalingthecandidatesbasedonthedurations(seeFigure6).Withthelaststepinthepipeline,thetopscoredcandidateisensuredtohavethebestspeechoverlapwiththesourcespeechamongstallcandidatetranslations.10.4Evaluation&MetricThedubbedEnglishvideoswerejudgedbyamix-tureofnativeandnon-nativespeakers,allofwhich
35


3.33±0.18
Table13:ResultsofLip-SyncErrorDistance(LSE-D)viaSyncnetpre-trainedmodel(ChungandZisserman,2016).Lowerthebetter.theamountofLip-Syncerrorsinthevideo.FromTable13,Subset1consistentlyhasalowerlip-syncerrorthanSubset2inallcasespointingthatitsdifficulttogeneratelip-synceddubsforSub-set2.ThisresultisalsoinlinewiththeMOSscoresweobtainedfortwosubsetswherethean-notatorspreferreddubsforSubset1.Secondly,originalvideosshowsignificantlylowerlip-syncerrordistance(12.xv/s7.x)thandubbedvideosshowingthatautomaticdubbingresearchstillhasalongwaytogotoreachlip-syncqualityinorigi-nalvideos.AcknowledgementsClaudiaBorg,ThierryDeclerck,RishuKumarandJohnJudgeacknowledgeH2020LT-BridgeProject(GA952194).RishuKumarwouldalsoliketothanktheEMLCT46programme.AtulKr.OjhaandJohnP.McCraewouldliketothankScienceFoundationIreland(SFI)underGrantNumberSFI/12/RC/2289
12.35
3.43±0.19
A3
A1
±0.13
12.77
No
3.77±0.19
P2ADPAT.OndˇrejBojarwouldliketoacknowledgethegrant19-26934X
MeanCI
MOS↑
MOS↑
P2Insight
±0.19
A4
Subset2
±0.15
Text2Phone
Text2Phone
3.07
System
System
11.73
Factored
Factored
Subset1
Yes
Yes
Yes
11.71
Original
2,andPanlinguaLanguageProcessingLLPforprovid-ingtheMarathi-Hindispeechtranslationdataandfortheirsupport.JohnJudgewouldalsoliketoacknowledgethesupportofSFIundergrantSFI/13/RC/2106
Table11:MOS(onascaleof1-6)withconfidencein-terval(CI)at95%perannotatorshowingthebiasesto-wardsgeneralpurposedubbedcontent.WealsolookedatMOSforthetwodifferentsubsetstounderstandwhetheritwasdifficultforthesubmittedsystemstodubthevideos.Asitturnsout,Subset1hasansignificantlyhigherMOSof3.54(±0.11)comparedtoSubset2withaMOSof3.31(±0.11).Thisshowsitissignifi-cantlymoredifficultforallsystemstodubSubset2thanSubset1.10.5ResultsResultsareshowninTable12.Allthreedubbingsystemsoutperformthenon-isochronicText2Phonebaseline(Chronopoulouetal.,2023),asexpected.Thefactoredbaselineimprovesovertheinterleavedbaseline,consistentwiththeauto-maticmetricresultsreportedbyPaletal.(2023).TheHW-TSCsystem(Raoetal.,2023)outper-formsallthebaselinesintermsofmeanopinionscore,makingittheclearwinneroftheIWSLT2023dubbingsharedtask.Unfortunately,sinceHW-TSCsystemwasunconstrained(ittrainsonadditionalbitextcomparedtothebaselines)andusesfundamentallydifferentapproachesthanthebaselines,itisnotpossibletoattributeit’sperfor-mancetoanysinglefactor.Lip-syncisanimportantfeatureofdubbing,itisimportantthatthefinalgeneratedaudioisinsyncwiththelipmovementsoftheon-screenspeakerintheoriginalvideo.Asananaly-sis,welookedatLip-SyncErrorDistance(LSE-D)(ChungandZisserman,2016)followingtheevaluationmethodologyinHuetal.(2021).LSE-Disnotaperfectmetricbutitisanindicationto
3.34
3.74
Annotator
11.64
±0.16
Interleaved
Interleaved
3.16±0.19
3.53
13.31
12.48
A2
46https://mundus-web.coli.uni-saarland.de/
Table12:Meanopinionscoreforbaselines1)Text2Phone2)Interleaved(Chronopoulouetal.,2023)3)Factored(Paletal.,2023)and4)submittedsystemofHW-TSC(Raoetal.,2023).
12.11
Constrained?
LSE-D↓
7.67
HW-TSC
HW-TSC
annotator(judge)level,thenumbersshowedthateachannotatorhadabiastowardsdubbing,somelikeddubbingmorethanotherswhichisintuitivebuthasnotbeenstudiedbeforeinthecontextofautomaticdubbing.AsshowninTable11,itisclearthatannotatorA2hadasignificantlyhigherpreferencefordubbingascomparedtoannotatorA4intermsofMOS.
7.39
CI
36


(NEUREM3)oftheCzechScienceFoundation.AntoniosAnastasopoulosandMilindAgarwalaresupportedbytheUSNationalScienceFoundationCCRI-Planning2234895award,aswellasaNa-tionalEndowmentfortheHumanitiesPR-276810-21award.ReferencesBasilAbraham,DanishGoel,DivyaSiddarth,Ka-likaBali,ManuChopra,MonojitChoudhury,PratikJoshi,PreethiJyoti,SunayanaSitaram,andVivekSeshadri.2020.Crowdsourcingspeechdataforlow-resourcelanguagesfromlow-incomeworkers.InProceedingsofthe12thLanguageResourcesandEvaluationConference,pages2819–2826.YasuhiroAkiba,MarcelloFederico,NorikoKando,Hi-romiNakaiwa,MichaelPaul,andJun’ichiTsujii.2004.OverviewoftheIWSLT04EvaluationCam-paign.InProceedingsoftheInternationalWork-shoponSpokenLanguageTranslation,pages1–12,Kyoto,Japan.AntoniosAnastasopoulos,Lo¨ıcBarrault,LuisaBen-tivogli,MarcelyZanonBoito,OndˇrejBojar,RoldanoCattoni,AnnaCurrey,GeorgianaDinu,KevinDuh,MahaElbayad,ClaraEmmanuel,Yan-nickEst`eve,MarcelloFederico,ChristianFed-ermann,SouhirGahbiche,HongyuGong,Ro-manGrundkiewicz,BarryHaddow,BenjaminHsu,D´avidJavorsk´y,V˘eraKloudov´a,SurafelLakew,XutaiMa,PrashantMathur,PaulMcNamee,KentonMurray,MariaNˇadejde,SatoshiNakamura,Mat-teoNegri,JanNiehues,XingNiu,JohnOrtega,JuanPino,ElizabethSalesky,JiatongShi,MatthiasSperber,SebastianSt¨uker,KatsuhitoSudoh,MarcoTurchi,YogeshVirkar,AlexanderWaibel,Chang-hanWang,andShinjiWatanabe.2022.FindingsoftheIWSLT2022EvaluationCampaign.InProceed-ingsofthe19thInternationalConferenceonSpokenLanguageTranslation(IWSLT2022),pages98–157,Dublin,Ireland(in-personandonline).AssociationforComputationalLinguistics.AntoniosAnastasopoulos,OndˇrejBojar,JacobBre-merman,RoldanoCattoni,MahaElbayad,MarcelloFederico,XutaiMa,SatoshiNakamura,MatteoNe-gri,JanNiehues,JuanPino,ElizabethSalesky,SebastianSt¨uker,KatsuhitoSudoh,MarcoTurchi,AlexanderWaibel,ChanghanWang,andMatthewWiesner.2021.FINDINGSOFTHEIWSLT2021EVALUATIONCAMPAIGN.InProceedingsofthe18thInternationalConferenceonSpokenLanguageTranslation(IWSLT2021),pages1–29,Bangkok,Thailand(online).AssociationforComputationalLinguistics.PierreAndrews,GuillaumeWenzek,KevinHeffernan,OnurC¸elebi,AnnaSun,AmmarKamran,YingzheGuo,AlexandreMourachko,HolgerSchwenk,andAngelaFan.2022.stopes-modularmachinetrans-lationpipelines.InProceedingsoftheThe2022ConferenceonEmpiricalMethodsinNaturalLan-guageProcessing:SystemDemonstrations,pages258–265.EbrahimAnsari,AmittaiAxelrod,NguyenBach,On-drejBojar,RoldanoCattoni,FahimDalvi,NadirDurrani,MarcelloFederico,ChristianFedermann,JiataoGu,FeiHuang,KevinKnight,XutaiMa,AjayNagesh,MatteoNegri,JanNiehues,JuanPino,Eliz-abethSalesky,XingShi,SebastianSt¨uker,MarcoTurchi,andChanghanWang.2020.FindingsoftheIWSLT2020EvaluationCampaign.InProceedingsofthe17thInternationalConferenceonSpokenLan-guageTranslation(IWSLT2020),Seattle,USA.RosanaArdila,MeganBranson,KellyDavis,MichaelHenretty,MichaelKohler,JoshMeyer,ReubenMorais,LindsaySaunders,FrancisMTyers,andGregorWeber.2019.Commonvoice:Amassively-multilingualspeechcorpus.arXivpreprintarXiv:1912.06670.RosanaArdila,MeganBranson,KellyDavis,MichaelHenretty,MichaelKohler,JoshMeyer,ReubenMorais,LindsaySaunders,FrancisMTyers,andGregorWeber.2020a.Commonvoice:Amassively-multilingualspeechcorpus.InLREC.RosanaArdila,MeganBranson,KellyDavis,MichaelKohler,JoshMeyer,MichaelHenretty,ReubenMorais,LindsaySaunders,FrancisTyers,andGre-gorWeber.2020b.Commonvoice:Amassively-multilingualspeechcorpus.InProceedingsofThe12thLanguageResourcesandEvaluationConfer-ence,pages4218–4222.MikelArtetxeandHolgerSchwenk.2019.Mas-sivelymultilingualsentenceembeddingsforzero-shotcross-lingualtransferandbeyond.Transac-tionsoftheAssociationforComputationalLinguis-tics,7:597–610.ArunBabu,ChanghanWang,AndrosTjandra,KushalLakhotia,QiantongXu,NamanGoyal,KritikaSingh,PatrickvonPlaten,YatharthSaraf,JuanPino,etal.2021.XLS-R:Self-supervisedcross-lingualspeechrepresentationlearningatscale.arXivpreprintarXiv:2111.09296.AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli.2020a.wav2vec2.0:Aframe-workforself-supervisedlearningofspeechrepre-sentations.InAdvancesinNeuralInformationPro-cessingSystems,volume33,pages12449–12460.AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli.2020b.wav2vec2.0:Aframe-workforself-supervisedlearningofspeechrepre-sentations.AdvancesinNeuralInformationPro-cessingSystems,33:12449–12460.ParniaBahar,PatrickWilken,JavierIranzo-S´anchez,MattiaDiGangi,EvgenyMatusov,andZolt´anT¨uske.2023.SpeechTranslationwithStyle:AppTek’sSubmissionstotheIWSLTSubtitlingand
37


FormalityTracksin2023.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).LuisaBentivogli,MauroCettolo,MarcoGaido,AlinaKarakanta,AlbertoMartinelli,andMarcoTurchiMatteoNegri.2021.CascadeversusDirectSpeechTranslation:DotheDifferencesStillMakeaDif-ference?InProceedingsofthe59thAnnualMeet-ingoftheAssociationforComputationalLinguis-tics,Bangkok,Thailand.AssociationforComputa-tionalLinguistics.MarcelyZanonBoito,FethiBougares,FlorentinBar-bier,SouhirGahbiche,Lo¨ıcBarrault,MickaelRou-vier,andYannickEst´eve.2022a.Speechresourcesinthetamasheqlanguage.LanguageResourcesandEvaluationConference(LREC).MarcelyZanonBoito,JohnOrtega,HugoRiguidel,AntoineLaurent,Lo¨ıcBarrault,FethiBougares,Fi-rasChaabani,HaNguyen,FlorentinBarbier,SouhirGahbiche,andYannickEst`eve.2022b.ON-TRACConsortiumSystemsfortheIWSLT2022DialectandLow-resourceSpeechTranslationTasks.InProceedingsofthe19thInternationalConferenceonSpokenLanguageTranslation(IWSLT).WilliamBrannon,YogeshVirkar,andBrianThomp-son.2023.DubbinginPractice:ALargeScaleStudyofHumanLocalizationWithInsightsforAu-tomaticDubbing.TransactionsoftheAssociationforComputationalLinguistics,11:419–435.EleftheriaBriakou,SwetaAgrawal,JoelTetreault,andMarineCarpuat.2021.Evaluatingtheevaluationmetricsforstyletransfer:Acasestudyinmulti-lingualformalitytransfer.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1321–1336,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.RonaldCardenas,RodolfoZevallos,ReynaldoBaquer-izo,andLuisCamacho.2018.Siminchik:Aspeechcorpusforpreservationofsouthernquechua.ISI-NLP2,page21.MauroCettolo,MarcelloFederico,LuisaBen-tivogli,JanNiehues,SebastianSt¨uker,K.Su-doh,K.Yoshino,andChristianFedermann.2017.OverviewoftheIWSLT2017EvaluationCampaign.InProceedingsofthe14thInternationalWorkshoponSpokenLanguageTranslation(IWSLT2017),pages2–14,Tokyo,Japan.MauroCettolo,JanNiehues,SebastianSt¨uker,LuisaBentivogli,RoldanoCattoni,andMarcelloFederico.2015.TheIWSLT2015EvaluationCampaign.InProceedingsofthe12thInternationalWorkshoponSpokenLanguageTranslation(IWSLT2015),DaNang,Vietnam.MauroCettolo,JanNiehues,SebastianSt¨uker,LuisaBentivogli,andMarcelloFederico.2013.Reportonthe10thIWSLTEvaluationCampaign.InProceed-ingsoftheTenthInternationalWorkshoponSpokenLanguageTranslation(IWSLT2013),Heidelberg,Germany.MauroCettolo,JanNiehues,SebastianSt¨uker,LuisaBentivogli,andMarcelloFederico.2014.Reportonthe11thIWSLTEvaluationCampaign,IWSLT2014.InProceedingsoftheEleventhInternationalWorkshoponSpokenLanguageTranslation(IWSLT2014),LakeTahoe,USA.MauroCettolo,JanNiehues,SebastianSt¨uker,LuisaBentivogli,andMarcelloFederico.2016.TheIWSLT2016EvaluationCampaign.InProceedingsofthe13thInternationalWorkshoponSpokenLan-guageTranslation(IWSLT2016),Seattle,USA.MingdaChen,Paul-AmbroiseDuquenne,PierreAn-drews,JustineKao,AlexandreMourachko,HolgerSchwenk,andMartaR.Costa-juss`a.2022.Blaser:Atext-freespeech-to-speechtranslationevaluationmetric.SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,ZhuoChen,JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,JianWu,LongZhou,ShuoRen,YanminQian,YaoQian,MichealZeng,andFuruWei.2021.Wavlm:Large-scaleself-supervisedpre-trainingforfullstackspeechprocessing.IEEEJournalofSelectedTop-icsinSignalProcessing,16:1505–1518.ColinCherryandGeorgeFoster.2019.Thinkingslowaboutlatencyevaluationforsimultaneousmachinetranslation.arXivpreprintarXiv:1906.00048.KyunghyunChoandMashaEsipova.2016.Canneu-ralmachinetranslationdosimultaneoustranslation?arXivpreprintarXiv:1606.02012.AlexandraChronopoulou,BrianThompson,PrashantMathur,YogeshVirkar,SurafelM.Lakew,andMar-celloFederico.2023.JointlyOptimizingTransla-tionsandSpeechTimingtoImproveIsochronyinAutomaticDubbing.ArXiv:2302.12979.J.S.ChungandA.Zisserman.2016.Outoftime:au-tomatedlipsyncinthewild.InWorkshoponMulti-viewLip-reading,ACCV.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzm´an,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.InProceedingsofthe58thAnnualMeetingoftheAsso-ciationforComputationalLinguistics,pages8440–8451,Online.AssociationforComputationalLin-guistics.AlexisConneau,MinMa,SimranKhanuja,YuZhang,VeraAxelrod,SiddharthDalmia,JasonRiesa,ClaraRivera,andAnkurBapna.2023.Fleurs:Few-shotlearningevaluationofuniversalrepresentationsofspeech.In2022IEEESpokenLanguageTechnol-ogyWorkshop(SLT),pages798–805.IEEE.
38


MartaR.Costa-juss`a,MarcosZampieri,andSantanuPal.2018.Aneuralapproachtolanguagevarietytranslation.InProceedingsoftheFifthWorkshoponNLPforSimilarLanguages,VarietiesandDi-alects(VarDial2018),pages275–282,SantaFe,NewMexico,USA.AssociationforComputationalLinguistics.PanDeng,ShihaoChen,WeitaiZhang,JieZhang,andLirongDai.2023.TheUSTC’sDialectSpeechTranslationSystemforIWSLT2023.InProceed-ingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).MattiaA.DiGangi,RoldanoCattoni,LuisaBentivogli,MatteoNegri,andMarcoTurchi.2019a.MuST-C:aMultilingualSpeechTranslationCorpus.InPro-ceedingsofthe2019ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Vol-ume1(LongandShortPapers),pages2012–2017,Minneapolis,Minnesota.MattiaA.DiGangi,RoldanoCattoni,LuisaBentivogli,MatteoNegri,andMarcoTurchi.2019b.MuST-C:aMultilingualSpeechTranslationCorpus.InPro-ceedingsofthe2019ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Vol-ume1(LongandShortPapers),pages2012–2017,Minneapolis,Minnesota.AssociationforComputa-tionalLinguistics.YichaoDu,GuoZhengsheng,JinchuanTian,ZhiruiZhang,XingWang,JianweiYu,ZhaopengTu,TongXu,andEnhongChen.2023.TheMineTransSys-temsforIWSLT2023OfflineSpeechTranslationandSpeech-to-SpeechTranslationTasks.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).MatthiasEckandChioriHori.2005.OverviewoftheIWSLT2005evaluationcampaign.InProceedingsoftheInternationalWorkshoponSpokenLanguageTranslation,pages1–22,Pittsburgh,PA.ELRAcatalogue.2016a.Tradpashtobroadcastnewsspeechcorpus.https://catalogue.elra.info/en-us/repository/browse/ELRA-S0381/.ISLRN:918-508-885-913-7,ELRAID:ELRA-S0381.ELRAcatalogue.2016b.Tradpashto-frenchparallelcorpusoftranscribedbroadcastnewsspeech-train-ingdata.http://catalog.elda.org/en-us/repository/browse/ELRA-W0093/.ISLRN:802-643-297-429-4,ELRAID:ELRA-W0093.AngelaFan,ShrutiBhosale,HolgerSchwenk,ZhiyiMa,AhmedEl-Kishky,SiddharthGoyal,Man-deepBaines,OnurCelebi,GuillaumeWenzek,VishravChaudhary,NamanGoyal,TomBirch,Vi-taliyLiptchinsky,SergeyEdunov,EdouardGrave,MichaelAuli,andArmandJoulin.2020.Beyondenglish-centricmultilingualmachinetranslation.MarcelloFederico,LuisaBentivogli,MichaelPaul,andSebastianSt¨uker.2011.OverviewoftheIWSLT2011EvaluationCampaign.InProceedingsoftheInternationalWorkshoponSpokenLanguageTrans-lation,pages11–27,SanFrancisco,USA.MarcelloFederico,MauroCettolo,LuisaBen-tivogli,MichaelPaul,andSebastianSt¨uker.2012.OverviewoftheIWSLT2012EvaluationCampaign.InProceedingsoftheInternationalWorkshoponSpokenLanguageTranslation,pages11–27,HongKong,HK.F.Feng,Y.Yang,D.Cer,N.Arivazhagan,andW.Wang.2022.Language-agnosticBERTSentenceEmbedding.InProceedingsofthe60thACL.CameronShawFordyce.2007.OverviewoftheIWSLT2007evaluationcampaign.InProceedingsoftheInternationalWorkshoponSpokenLanguageTranslation,pages1–12,Trento,Italy.MarkusFreitag,GeorgeFoster,DavidGrangier,VireshRatnakar,QijunTan,andWolfgangMacherey.2021a.Experts,errors,andcontext:Alarge-scalestudyofhumanevaluationformachinetranslation.TransactionsoftheAssociationforComputationalLinguistics,9:1460–1474.MarkusFreitag,RicardoRei,NitikaMathur,Chi-kiuLo,CraigStewart,GeorgeFoster,AlonLavie,andOndˇrejBojar.2021b.ResultsoftheWMT21met-ricssharedtask:Evaluatingmetricswithexpert-basedhumanevaluationsonTEDandnewsdomain.InProceedingsoftheSixthConferenceonMachineTranslation,pages733–774,Online.AssociationforComputationalLinguistics.RyoFukuda,YutaNishikawa,YasumasaKano,YukaKo,TomoyaYanagita,KosukeDoi,ManaMaki-nae,SakrianiSakti,KatsuhitoSudoh,andSatoshiNakamura.2023.NAISTSimultaneousSpeech-to-speechTranslationSystemforIWSLT2023.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).MercedesGarc´ıa-Mart´ınez,Lo¨ıcBarrault,andFethiBougares.2016.Factoredneuralmachinetransla-tionarchitectures.InProceedingsofthe13thInter-nationalConferenceonSpokenLanguageTransla-tion,Seattle,WashingtonD.C.InternationalWork-shoponSpokenLanguageTranslation.JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin.2017.Convolutionalsequencetosequencelearning.KarthikGopalakrishnan,BehnamHedayatnia,Qin-langChen,AnnaGottardi,SanjeevKwatra,AnuVenkatesh,RaeferGabriel,andDilekHakkani-T¨ur.2019.Topical-Chat:Towardsknowledge-groundedopen-domainconversations.InProc.Interspeech2019,pages1891–1895.
39


EdwardGow-Smith,AlexandreBerard,MarcelyZanonBoito,andIoanCalapodescu.2023.NAVERLABSEurope’sMultilingualSpeechTranslationSystemsfortheIWSLT2023Low-ResourceTrack.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).NamanGoyal,CynthiaGao,VishravChaudhary,Peng-JenChen,GuillaumeWenzek,DaJu,SanjanaKr-ishnan,Marc’AurelioRanzato,FranciscoGuzm´an,andAngelaFan.2022.TheFlores-101evaluationbenchmarkforlow-resourceandmultilingualma-chinetranslation.TransactionsoftheAssociationforComputationalLinguistics,10:522–538.AnmolGulati,JamesQin,Chung-ChengChiu,NikiParmar,YuZhang,JiahuiYu,WeiHan,ShiboWang,ZhengdongZhang,YonghuiWu,andRuomingPang.2020.Conformer:Convolution-augmentedtransformerforspeechrecognition.Interspeech,pages5036–5040.JiaxinGuo,DaimengWei,ZhanglinWu,ZongyaoLi,ZhiqiangRao,MinghanWang,HengchaoShang,XiaoyuChen,ZhengzheYu,ShaojunLi,YuhaoXie,LizhiLei,andHaoYang.2023.TheHW-TSC’sSi-multaneousSpeech-to-TextTranslationsystemforIWSLT2023evaluation.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).YuchenHan,XiaoqianLiu,HaoChen,YuhaoZhang,ChenXu,TongXiao,andJingboZhu.2023.TheNiuTransEnd-to-EndSpeechTranslationSystemforIWSLT23English-to-ChineseOfflineTask.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).FeiHe,Shan-HuiCathyChu,OddurKjartansson,ClaraRivera,AnnaKatanova,AlexanderGutkin,IsinDemirsahin,CibuJohny,MartinJansche,SupheakmungkolSarin,andKnotPipatsrisawat.2020.Open-sourcemulti-speakerspeechcor-poraforbuildingGujarati,Kannada,Malayalam,Marathi,TamilandTeluguspeechsynthesissys-tems.InProceedingsoftheTwelfthLanguageRe-sourcesandEvaluationConference,pages6494–6503,Marseille,France.EuropeanLanguageRe-sourcesAssociation.KevinHeffernan,OnurC¸elebi,andHolgerSchwenk.2022.Bitextminingusingdistilledsentencerep-resentationsforlow-resourcelanguages.InFind-ingsoftheAssociationforComputationalLinguis-tics:EMNLP2022,pages2101–2112,AbuDhabi,UnitedArabEmirates.AssociationforComputa-tionalLinguistics.CarlosDanielHernandezMena,AlbertGatt,AndreaDeMarco,ClaudiaBorg,LonnekevanderPlas,AmandaMuscat,andIanPadovani.2020.MASRI-HEADSET:AMaltesecorpusforspeechrecogni-tion.InProceedingsoftheTwelfthLanguageRe-sourcesandEvaluationConference,pages6381–6388,Marseille,France.EuropeanLanguageRe-sourcesAssociation.OleksiiHrinchuk,VladimirBataev,EvelinaBakhtu-rina,andBorisGinsburg.2023.NVIDIANeMoOf-flineSpeechTranslationSystemsforIWSLT2023.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai,KushalLakhotia,RuslanSalakhutdinov,andAbdelrahmanMohamed.2021.Hubert:Self-supervisedspeechrepresentationlearn-ingbymaskedpredictionofhiddenunits.IEEE/ACMTrans.Audio,SpeechandLang.Proc.,29:3451–3460.ChenxuHu,QiaoTian,TingleLi,WangYuping,Yux-uanWang,andHangZhao.2021.Neuraldubber:Dubbingforvideosaccordingtoscripts.InThirty-FifthConferenceonNeuralInformationProcessingSystems.WuweiHuang,MenggeLiu,XiangLi,YanzhiTian,FengyuYang,WenZhang,JianLuan,BinWang,YuhangGuo,andJinsongSu.2023.TheXiaomiAILab’sSpeechTranslationSystemsforIWSLT2023OfflineTask,SimultaneousTaskandSpeech-to-SpeechTask.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT).AmirHussein,CihanXiao,NehaVerma,MatthewWiesner,ThomasThebaud,andSanjeevKhudanpur.2023.JHUIWSLT2023DialectSpeechTranslationSystemDescription.InProceedingsofthe20thIn-ternationalConferenceonSpokenLanguageTrans-lation(IWSLT).MuhammadHuzaifah,KyeMinTan,andRichengDuan.2023.I2R’sEnd-to-EndSpeechTranslationSystemforIWSLT2023OfflineSharedTask.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).HirofumiInaguma,BrianYan,SiddharthDalmia,PengchengGuo,JiatongShi,KevinDuh,andShinjiWatanabe.2021.ESPnet-STIWSLT2021OfflineSpeechTranslationSystem.InProceedingsofthe18thInternationalConferenceonSpokenLanguageTranslation(IWSLT).JavierIranzo-S´anchez,JoanAlbertSilvestre-Cerd`a,JavierJorge,NahuelRosell´o,Adri`aGim´enez,Al-bertSanchis,JorgeCivera,andAlfonsJuan.2020.Europarl-st:Amultilingualcorpusforspeechtrans-lationofparliamentarydebates.InProc.of45thIntl.Conf.onAcoustics,Speech,andSignalProcess-ing(ICASSP2020),pages8229–8233,Barcelona(Spain).D´avidJavorsk´y,DominikMach´aˇcek,andOndˇrejBo-jar.2022.Continuousratingasreliablehumanevaluationofsimultaneousspeechtranslation.InProceedingsoftheSeventhConferenceonMachine
40


Translation(WMT),pages154–164,AbuDhabi,UnitedArabEmirates(Hybrid).AssociationforComputationalLinguistics.JapanTranslationFederationJTF.2018.JTFTransla-tionQualityEvaluationGuidelines,1stEdition(inJapanese).YasumasaKano,KatsuhitoSudoh,andSatoshiNaka-mura.2023.AverageTokenDelay:ALatencyMet-ricforSimultaneousTranslation.InProceedingsofInterspeech2023.Toappear.AlinaKarakanta,LuisaBentivogli,MauroCettolo,MatteoNegri,andMarcoTurchi.2022a.Post-editinginautomaticsubtitling:Asubtitlers’per-spective.InProceedingsofthe23rdAnnualCon-ferenceoftheEuropeanAssociationforMachineTranslation,pages261–270,Ghent,Belgium.Euro-peanAssociationforMachineTranslation.AlinaKarakanta,Franc¸oisBuet,MauroCettolo,andFranc¸oisYvon.2022b.Evaluatingsubtitleseg-mentationforend-to-endgenerationsystems.InProceedingsoftheThirteenthLanguageResourcesandEvaluationConference,pages3069–3078,Mar-seille,France.EuropeanLanguageResourcesAsso-ciation.AlinaKarakanta,MatteoNegri,andMarcoTurchi.2020.MuST-cinema:aspeech-to-subtitlescorpus.InProceedingsoftheTwelfthLanguageResourcesandEvaluationConference,pages3727–3734,Mar-seille,France.EuropeanLanguageResourcesAsso-ciation.SantoshKesiraju,KarelBeneˇs,MaksimTikhonov,andJanˇCernock´y.2023.BUTSystemsforIWSLT2023Marathi-HindiLowResourceSpeechTranslationTask.InProceedingsofthe20thInternationalCon-ferenceonSpokenLanguageTranslation(IWSLT).SameerKhurana,AntoineLaurent,andJamesGlass.2022.Samu-xlsr:Semantically-alignedmultimodalutterance-levelcross-lingualspeechrepresentation.IEEEJournalofSelectedTopicsinSignalProcess-ing,pages1–13.GuillaumeKlein,YoonKim,YuntianDeng,JeanSenellart,andAlexanderRush.2017.OpenNMT:Open-sourcetoolkitforneuralmachinetranslation.InProceedingsofACL2017,SystemDemonstra-tions,pages67–72,Vancouver,Canada.AssociationforComputationalLinguistics.SurafelMLakew,YogeshVirkar,PrashantMathur,andMarcelloFederico.2021.Isometricmt:Neuralmachinetranslationforautomaticdubbing.arXivpreprintarXiv:2112.08682.SurafelMelakuLakew,MattiaDiGangi,andMarcelloFederico.2019.Controllingtheoutputlengthofneuralmachinetranslation.InProc.IWSLT.AntoineLaurent,SouhirGahbiche,HaNguyen,HarounElleuch,FethiBougares,AntoineThiol,HugoRiguidel,SalimaMdhaffar,Ga¨elleLaperri`ere,LucasMaison,SameerKhurana,andYannickEst`eve.2023.ON-TRACconsortiumsystemsfortheIWSLT2023dialectalandlow-resourcespeechtranslationtasks.InProceedingsofthe20thInter-nationalConferenceonSpokenLanguageTransla-tion(IWSLT).SeugnjunLee,HyeonseokMoon,ChanjunPark,andHeuiseokLim.2023.ImprovingFormality-SensitiveMachineTranslationusingData-CentricApproachesandPromptEngineering.InProceed-ingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).ZongyaoLi,ZhanglinWu,ZhiqiangRao,XieYuHao,GuoJiaXin,DaimengWei,HengchaoShang,WangMinghan,XiaoyuChen,ZhengzheYU,LiShao-Jun,LeiLiZhi,andHaoYang.2023.HW-TSCatIWSLT2023:BreaktheQualityCeilingofOfflineTrackviaPre-TrainingandDomainAdaptation.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).XiVictoriaLin,TodorMihaylov,MikelArtetxe,TianluWang,ShuohuiChen,DanielSimig,MyleOtt,Na-manGoyal,ShrutiBhosale,JingfeiDu,etal.2021.Few-shotlearningwithmultilinguallanguagemod-els.arXivpreprintarXiv:2112.10668.PierreLisonandJ¨orgTiedemann.2016.OpenSub-titles2016:ExtractinglargeparallelcorporafrommovieandTVsubtitles.InProceedingsoftheTenthInternationalConferenceonLanguageResourcesandEvaluation(LREC’16),pages923–929,Por-toroˇz,Slovenia.EuropeanLanguageResourcesAs-sociation(ELRA).DanniLiu,ThaiBinhNguyen,SaiKoneru,EnesYavuzUgan,Ngoc-QuanPham,TuanNamNguyen,TuAnhDinh,CarlosMullov,AlexanderWaibel,andJanNiehues.2023.KIT’sMultilingualSpeechTranslationSystemforIWSLT2023.InProceed-ingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).YinhanLiu,JiataoGu,NamanGoyal,XianLi,SergeyEdunov,MarjanGhazvininejad,MikeLewis,andLukeZettlemoyer.2020.Multilingualdenoisingpre-trainingforneuralmachinetranslation.Trans-actionsoftheAssociationforComputationalLin-guistics,8:726–742.ArleLommel,HansUszkoreit,andAljoschaBur-chardt.2014.MultidimensionalQualityMet-rics(MQM):AFrameworkforDeclaringandDescribingTranslationQualityMetrics.RevistaTradum`atica:tecnologiesdelatraducci´o,12:455–463.MingboMa,LiangHuang,HaoXiong,RenjieZheng,KaiboLiu,BaigongZheng,ChuanqiangZhang,ZhongjunHe,HairongLiu,XingLi,HuaWu,and
41


HaifengWang.2019.STACL:Simultaneoustrans-lationwithimplicitanticipationandcontrollablela-tencyusingprefix-to-prefixframework.InProceed-ingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages3025–3036,Florence,Italy.AssociationforComputationalLin-guistics.ShumingMa,LiDong,ShaohanHuang,Dong-dongZhang,AlexandreMuzio,SakshamSinghal,HanyHassanAwadalla,XiaSong,andFuruWei.2021.DeltaLM:Encoder-decoderpre-trainingforlanguagegenerationandtranslationbyaugmentingpretrainedmultilingualencoders.arXiv.XutaiMa,MohammadJavadDousti,ChanghanWang,JiataoGu,andJuanPino.2020a.SIMULEVAL:Anevaluationtoolkitforsimultaneoustranslation.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations,pages144–150,Online.Associa-tionforComputationalLinguistics.XutaiMa,JuanPino,andPhilippKoehn.2020b.SimulMTtoSimulST:Adaptingsimultaneoustexttranslationtoend-to-endsimultaneousspeechtrans-lation.InProceedingsofthe1stConferenceoftheAsia-PacificChapteroftheAssociationforCompu-tationalLinguisticsandthe10thInternationalJointConferenceonNaturalLanguageProcessing,pages582–587,Suzhou,China.AssociationforComputa-tionalLinguistics.DominikMach´aˇcek,OndˇrejBojar,andRajDabre.2023.MTMetricsCorrelatewithHumanRatingsofSimultaneousSpeechTranslation.InProceed-ingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).EvgenyMatusov,GregorLeusch,OliverBender,andHermannNey.2005a.Evaluatingmachinetransla-tionoutputwithautomaticsentencesegmentation.InProc.oftheInternationalWorkshoponSpokenLanguageTranslation(IWSLT),pages138–144.EvgenyMatusov,GregorLeusch,OliverBender,andHermannNey.2005b.Evaluatingmachinetransla-tionoutputwithautomaticsentencesegmentation.InProceedingsoftheSecondInternationalWork-shoponSpokenLanguageTranslation,Pittsburgh,Pennsylvania,USA.EvgenyMatusov,PatrickWilken,andYotaGeor-gakopoulou.2019.Customizingneuralmachinetranslationforsubtitling.InProceedingsoftheFourthConferenceonMachineTranslation(Volume1:ResearchPapers),pages82–93,Florence,Italy.AssociationforComputationalLinguistics.JonathanMbuyaandAntoniosAnastasopoulos.2023.GMUSystemsfortheIWSLT2023DialectandLow-resourceSpeechTranslationTasks.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).MariaNadejde,AnnaCurrey,BenjaminHsu,XingNiu,MarcelloFederico,andGeorgianaDinu.2022.CoCoA-MT:Adatasetandbenchmarkforcon-trastivecontrolledMTwithapplicationtoformality.InFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pages616–632,Seattle,UnitedStates.AssociationforComputationalLin-guistics.J.Niehues,R.Cattoni,S.St¨uker,M.Negri,M.Turchi,T.Ha,E.Salesky,R.Sanabria,L.Barrault,L.Spe-cia,andM.Federico.2019.TheIWSLT2019Eval-uationCampaign.InProceedingsofthe16thInter-nationalWorkshoponSpokenLanguageTranslation(IWSLT2019),HongKong,China.JanNiehues,RoldanoCattoni,SebastianSt¨uker,MauroCettolo,MarcoTurchi,andMarcelloFed-erico.2018.TheIWSLT2018EvaluationCam-paign.InProceedingsofthe15thInternationalWorkshoponSpokenLanguageTranslation(IWSLT2018),pages2–6,Bruges,Belgium.XingNiu,MariannaMartindale,andMarineCarpuat.2017.Astudyofstyleinmachinetranslation:Con-trollingtheformalityofmachinetranslationoutput.InProceedingsofthe2017ConferenceonEmpiri-calMethodsinNaturalLanguageProcessing,pages2814–2819,Copenhagen,Denmark.AssociationforComputationalLinguistics.NLLBTeam,MartaR.Costa-juss`a,JamesCross,OnurC¸elebi,MahaElbayad,KennethHeafield,KevinHeffernan,ElaheKalbassi,JaniceLam,DanielLicht,JeanMaillard,AnnaSun,SkylerWang,GuillaumeWenzek,AlYoungblood,BapiAkula,LoicBarrault,GabrielMejia-Gonzalez,PrangthipHansanti,JohnHoffman,SemarleyJar-rett,KaushikRamSadagopan,DirkRowe,Shan-nonSpruit,ChauTran,PierreAndrews,NecipFazilAyan,ShrutiBhosale,SergeyEdunov,AngelaFan,CynthiaGao,VedanujGoswami,FranciscoGuzm´an,PhilippKoehn,AlexandreMourachko,ChristopheRopers,SafiyyahSaleem,HolgerSchwenk,andJeffWang.2022.Nolanguageleftbe-hind:Scalinghuman-centeredmachinetranslation.arXivpreprint.JohnEOrtega,RichardCastroMamani,andKyunghyunCho.2020.Neuralmachinetranslationwithapolysyntheticlowresourcelanguage.Ma-chineTranslation,34(4):325–346.JohnE.Ortega,RodolfoZevallos,andWilliamChen.2023.QUESPASubmissionfortheIWSLT2023DialectandLow-resourceSpeechTranslationTasks.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).ProyagPal,BrianThompson,YogeshVirkar,PrashantMathur,AlexandraChronopoulou,andMarcelloFederico.2023.Improvingisochronousmachinetranslationwithtargetfactorsandauxiliarycounters.SaraPapi,MarcoGaido,AlinaKarakanta,MauroCet-tolo,MatteoNegri,andMarcoTurchi.2023a.Direct
42


speechtranslationforautomaticsubtitling.Transac-tionsoftheAssociationforComputationalLinguis-tics,11(inproduction).SaraPapi,MarcoGaido,andMatteoNegri.2023b.Di-rectModelsforSimultaneousTranslationandAuto-maticSubtitling:FBK@IWSLT2023.InProceed-ingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).SaraPapi,MarcoGaido,MatteoNegri,andMarcoTurchi.2022.Over-generationcannotberewarded:Length-adaptiveaveragelaggingforsimultaneousspeechtranslation.InProceedingsoftheThirdWorkshoponAutomaticSimultaneousTranslation,pages12–17,Online.AssociationforComputationalLinguistics.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002a.Bleu:amethodforautomaticeval-uationofmachinetranslation.InProceedingsofthe40thannualmeetingonassociationforcompu-tationallinguistics.AssociationforComputationalLinguistics.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002b.Bleu:amethodforautomaticeval-uationofmachinetranslation.InProceedingsofthe40thAnnualMeetingoftheAssociationforCom-putationalLinguistics,pages311–318,Philadelphia,Pennsylvania,USA.AssociationforComputationalLinguistics.DanielS.Park,WilliamChan,YuZhang,Chung-ChengChiu,BarretZoph,EkinD.Cubuk,andQuocV.Le.2019.SpecAugment:ASimpleDataAugmentationMethodforAutomaticSpeechRecognition.Interspeech2019.MichaelPaul.2006.OverviewoftheIWSLT2006EvaluationCampaign.InProceedingsoftheIn-ternationalWorkshoponSpokenLanguageTrans-lation,pages1–15,Kyoto,Japan.MichaelPaul.2008.OverviewoftheIWSLT2008EvaluationCampaign.InProceedingsoftheIn-ternationalWorkshoponSpokenLanguageTrans-lation,pages1–17,Waikiki,Hawaii.MichaelPaul.2009.OverviewoftheIWSLT2009EvaluationCampaign.InProceedingsoftheIn-ternationalWorkshoponSpokenLanguageTrans-lation,pages1–18,Tokyo,Japan.MichaelPaul,MarcelloFederico,andSebastianSt¨uker.2010.OverviewoftheIWSLT2010EvaluationCampaign.InProceedingsoftheInternationalWorkshoponSpokenLanguageTranslation,pages3–27,Paris,France.SimonePerone.2023.Matesub:theTranslatedSub-titlingToolattheIWSLT2023Subtitlingtask.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).PeterPol´ak,DanniLiu,Ngoc-QuanPham,JanNiehues,AlexanderWaibel,andOndˇrejBojar.2023.TowardsEfficientSimultaneousSpeechTransla-tion:CUNI-KITSystemforSimultaneousTrackatIWSLT2023.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT).PeterPol´ak,Ngoc-QuanPham,TuanNamNguyen,DanniLiu,CarlosMullov,JanNiehues,OndˇrejBo-jar,andAlexanderWaibel.2022.CUNI-KITsystemforsimultaneousspeechtranslationtaskatIWSLT2022.InProceedingsofthe19thInternationalCon-ferenceonSpokenLanguageTranslation(IWSLT2022),pages277–285,Dublin,Ireland(in-personandonline).AssociationforComputationalLinguis-tics.MajaPopovi´c.2015a.chrF:charactern-gramF-scoreforautomaticMTevaluation.InProceedingsoftheTenthWorkshoponStatisticalMachineTranslation,pages392–395,Lisbon,Portugal.AssociationforComputationalLinguistics.MajaPopovi´c.2015b.chrf:charactern-gramf-scoreforautomaticmtevaluation.InProceedingsofthetenthworkshoponstatisticalmachinetranslation,pages392–395.MattPost.2018.AcallforclarityinreportingBLEUscores.InProceedingsoftheThirdConferenceonMachineTranslation:ResearchPapers,pages186–191,Brussels,Belgium.AssociationforComputa-tionalLinguistics.VineelPratap,AwniHannun,QiantongXu,JeffCai,JacobKahn,GabrielSynnaeve,VitaliyLiptchin-sky,andRonanCollobert.2019.Wav2letter++:Afastopen-sourcespeechrecognitionsystem.InICASSP2019-2019IEEEInternationalConfer-enceonAcoustics,SpeechandSignalProcessing(ICASSP),pages6460–6464.IEEE.AlecRadford,JongWookKim,TaoXu,GregBrock-man,ChristineMcLeavey,andIlyaSutskever.2022.Robustspeechrecognitionvialarge-scaleweaksu-pervision.BalajiRadhakrishnan,SaurabhAgrawal,RajPrakashGohil,KiranPraveen,AdvaitVinayDhopesh-warkar,andAbhishekPandey.2023.SRI-B’ssys-temsforIWSLT2023DialectalandLow-resourcetrack:Marathi-HindiSpeechTranslation.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).ZhiqiangRao,HengchaoShang,JinlongYang,DaimengWei,ZongyaoLi,LizhiLei,andHaoYang.2023.Length-AwareNMTandAdaptiveDu-rationforAutomaticDubbing.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).RicardoRei,Jos´eGCdeSouza,DuarteAlves,ChrysoulaZerva,AnaCFarinha,Taisiya
43


Glushkova,AlonLavie,LuisaCoheur,andAndr´eFTMartins.2022.Comet-22:Unbabel-ist2022submissionforthemetricssharedtask.InProceedingsoftheSeventhConferenceonMachineTranslation(WMT),pages578–585.RicardoRei,CraigStewart,AnaCFarinha,andAlonLavie.2020a.Comet:Aneuralframeworkformtevaluation.arXivpreprintarXiv:2009.09025.RicardoRei,CraigStewart,AnaCFarinha,andAlonLavie.2020b.COMET:AneuralframeworkforMTevaluation.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguagePro-cessing(EMNLP),pages2685–2702,Online.Asso-ciationforComputationalLinguistics.YiRen,ChenxuHu,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.2022.Fastspeech2:Fastandhigh-qualityend-to-endtexttospeech.AnthonyRousseau,PaulDel´eglise,andYannickEs-teve.2014.Enhancingtheted-liumcorpuswithselecteddataforlanguagemodelingandmoretedtalks.InLREC.ElizabethSalesky,KareemDarwish,MohamedAl-Badrashiny,MonaDiab,andJanNiehues.2023.EvaluatingMultilingualSpeechTranslationUnderRealisticConditionswithResegmentationandTer-minology.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT2023).AssociationforComputationalLin-guistics.ElizabethSalesky,MatthewWiesner,JacobBremer-man,RoldanoCattoni,MatteoNegri,MarcoTurchi,DouglasW.Oard,andMattPost.2021.TheMul-tilingualTEDxCorpusforSpeechRecognitionandTranslation.InProc.Interspeech2021,pages3655–3659.GabrieleSarti,PhuMonHtut,XingNiu,Ben-jaminHsu,AnnaCurrey,GeorgianaDinu,andMariaNadejde.2023.RAMP:Retrievalandattribute-markingenhancedpromptingforattribute-controlledtranslation.HolgerSchwenk,GuillaumeWenzek,SergeyEdunov,EdouardGrave,ArmandJoulin,andAngelaFan.2021.CCMatrix:Miningbillionsofhigh-qualityparallelsentencesontheweb.InProceedingsofthe59thAnnualMeetingoftheAssociationforCompu-tationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Vol-ume1:LongPapers),pages6490–6500,Online.As-sociationforComputationalLinguistics.ThibaultSellam,DipanjanDas,andAnkurParikh.2020.BLEURT:Learningrobustmetricsfortextgeneration.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLin-guistics,pages7881–7892,Online.AssociationforComputationalLinguistics.HengchaoShang,ZhiqiangRao,ZongyaoLi,ZhanglinWu,JiaxinGuo,MinghanWang,DaimengWei,ShaojunLi,ZhengzheYu,XiaoyuChen,LizhiLei,andHaoYang.2023.TheHW-TSC’sSimultaneousSpeech-to-SpeechTranslationsystemforIWSLT2023evaluation.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT).YaoShi,HuiBu,XinXu,ShaojiZhang,andMingLi.2020.Aishell-3:Amulti-speakermandarinttscorpusandthebaselines.arXivpreprintarXiv:2010.11567.KunSong,YiLei,PeikunChen,YiqingCao,KunWei,YongmaoZhang,LeiXie,NingJiang,andGuoqingZhao.2023.TheNPU-MSXFSpeech-to-SpeechTranslationSystemforIWSLT2023Speech-to-SpeechTranslationTask.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-manGoyal,VishravChaudhary,JiataoGu,andAn-gelaFan.2020.Multilingualtranslationwithexten-siblemultilingualpretrainingandfinetuning.arXivpreprintarXiv:2008.00401.J¨orgTiedemann.2012.Paralleldata,toolsandinter-facesinOPUS.InProceedingsoftheEighthIn-ternationalConferenceonLanguageResourcesandEvaluation(LREC’12),pages2214–2218,Istanbul,Turkey.EuropeanLanguageResourcesAssociation(ELRA).IoannisTsiamas,GerardI.G´allego,JoseFonollosa,andMartaR.Costa-juss`a.2023.SpeechTransla-tionwithFoundationModelsandOptimalTrans-port:UPCatIWSLT23.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).IoannisTsiamas,GerardI.G´allego,Jos´eA.R.Fonol-losa,andMartaR.Costa-juss`a.2022.SHAS:ApproachingoptimalSegmentationforEnd-to-EndSpeechTranslation.InProc.Interspeech2022,pages106–110.PriyeshVakharia,ShreeVigneshS,PranjaliBas-matkar,andIanLane.2023.Low-ResourceFor-malityControlledNMTUsingPre-trainedLM.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017.AttentionisAllYouNeed.InProceedingsofNIPS2017.AkshayaVishnu,KudluShanbhogue,RanXue,SoumyaSaha,DanielZhang,andAshwinkumarGanesan.2023.AmazonAlexaAI’sLow-ResourceSpeechTranslationSystemforIWSLT2023.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).
44


ChanghanWang,JuanPino,AnneWu,andJiataoGu.2020a.Covost:Adiversemultilingualspeech-to-texttranslationcorpus.InProceedingsofThe12thLanguageResourcesandEvaluationConference,pages4197–4203.ChanghanWang,YunTang,XutaiMa,AnneWu,DmytroOkhonko,andJuanPino.2020b.fairseqs2t:Fastspeech-to-textmodelingwithfairseq.arXivpreprintarXiv:2010.05171.ChanghanWang,AnneWu,JiataoGu,andJuanPino.2021.CoVoST2andMassivelyMultilin-gualSpeechTranslation.InProc.Interspeech2021,pages2247–2251.MinghanWang,YingluLi,JiaxinGuo,ZongyaoLi,HengchaoShang,DaimengWei,MinZhang,ShiminTao,andHaoYang.2023a.TheHW-TSC’sSpeech-to-SpeechTranslationSystemforIWSLT2023.InProceedingsofthe20thInternationalCon-ferenceonSpokenLanguageTranslation(IWSLT).ZhipengWang,YuhangGuo,andShuoyingChen.2023b.BIT’sSystemforMultilingualTrack.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).PatrickWilken,PanayotaGeorgakopoulou,andEvgenyMatusov.2022.SubER-ametricforau-tomaticevaluationofsubtitlequality.InProceed-ingsofthe19thInternationalConferenceonSpokenLanguageTranslation(IWSLT2022),pages1–10,Dublin,Ireland(in-personandonline).AssociationforComputationalLinguistics.AidenWilliams.2022.TheapplicabilityofWav2Vec2.0forlow-resourceMalteseASR.B.S.thesis,Uni-versityofMalta.AidenWilliams,KurtAbela,RishuKumar,MartinB¨ar,HannahBillinghurst,KurtMicallef,AhnafMozibSamin,AndreaDeMarco,LonnekevanderPlas,andClaudiaBorg.2023.UM-DFKIMalteseSpeechTranslation.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT).ZhanglinWu,ZongyaoLi,DaimengWei,HengchaoShang,JiaxinGuo,XiaoyuChen,ZhiqiangRao,ZhengzheYU,JinlongYang,ShaojunLi,YuhaoXie,BinWei,JiaweiZheng,MingZhu,LizhiLei,HaoYang,andYanfeiJiang.2023.ImprovingNeuralMachineTranslationFormalityControlwithDomainAdaptationandReranking-basedTransduc-tiveLearning.InProceedingsofthe20thInterna-tionalConferenceonSpokenLanguageTranslation(IWSLT).ZhihangXie.2023.TheBIGAIOfflineSpeechTrans-lationSystemsforIWSLT2023Evaluation.InPro-ceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).HenryLiXinyuan,NehaVerma,BismarckBamfoOdoom,UjvalaPradeep,MatthewWiesner,andSanjeevKhudanpur.2023.JHUIWSLT2023Mul-tilingualSpeechTranslationSystemDescription.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).ChenXu,BojieHu,YanyangLi,YuhaoZhang,shenhuang,QiJu,TongXiao,andJingboZhu.2021.Stackedacoustic-and-textualencoding:Integratingthepre-trainedmodelsintospeechtranslationen-coders.WendaXu,XianQian,MingxuanWang,LeiLi,andWilliamYangWang.2022.Sescore2:Retrievalaug-mentedpretrainingfortextgenerationevaluation.arXivpreprintarXiv:2212.09305.BrianYan,JiatongShi,SoumiMaiti,WilliamChen,XinjianLi,YifanPeng,SiddhantArora,andShinjiWatanabe.2023.CMU’sIWSLT2023Simultane-ousSpeechTranslationSystem.InProceedingsofthe20thInternationalConferenceonSpokenLan-guageTranslation(IWSLT).ZhengdongYang,ShuichiroShimizu,ShengLiWangjinZhou,andChenhuiChu.2023.TheKyotoSpeech-to-SpeechTranslationSystemforIWSLT2023.InProceedingsofthe20thInternationalCon-ferenceonSpokenLanguageTranslation(IWSLT).ZhuoyuanYao,DiWu,XiongWang,BinbinZhang,FanYu,ChaoYang,ZhendongPeng,XiaoyuChen,LeiXie,andXinLei.2021.Wenet:Produc-tionorientedstreamingandnon-streamingend-to-endspeechrecognitiontoolkit.arXivpreprintarXiv:2102.01547.RongYe,ChengqiZhao,TomKo,ChutongMeng,TaoWang,MingxuanWang,andJunCao.2023.Gigast:A10,000-hourpseudospeechtranslationcorpus.InInterspeech2023.XinyuanZhou,JianweiCui,ZhongyiYe,YichiWang,LuzhenXu,HanyiZhang,WeitaiZhang,andLirongDai.2023.SubmissionofUSTC’ssystemfortheIWSLT2023-OfflineSpeechTranslationTrack.InProceedingsofthe20thInternationalConferenceonSpokenLanguageTranslation(IWSLT).AdrianŁa´ncucki.2021.Fastpitch:Paralleltext-to-speechwithpitchprediction.InICASSP2021-2021IEEEInternationalConferenceonAcous-tics,SpeechandSignalProcessing(ICASSP),pages6588–6592,Toronto,Canada.IEEE.
45


AppendixA.HumanEvaluation
46


47NotethattheratingscouldbealsoweightedwithrespecttothedurationoftimesegmentsbetweentheratingsbutMach´aˇceketal.(2023)documentedon2022datathatthedifferenceisnegligible.
AHumanEvaluationHumanevaluationwascarriedoutfortheSimultaneousandOfflineSLTsharedtasks.Atthetimeofwriting,onlytheformerevaluationhasbeencompletedwhichisreportedhere.ThehumanevaluationoftheOfflineTaskwillberecountedduringtheconferenceandpossiblyinanupdateversionofthisreport.A.1SimultaneousSpeechTranslationTaskSimultaneousSpeechTranslationTaskrantwodifferenttypesofmanualevaluation:“continuousrating”forEnglish-to-GermanandMQMforEnglish-to-Japanese.A.1.1HumanEvaluationfortheEnglish-to-GermanSimultaneousTaskWeusedavariantof“continuousrating”aspresentedbyJavorsk´yetal.(2022).Theevaluationprocessandtheguidelinespresentedtoannotatorswerethesameasduringthelastyearevaluation(consultSectionA.1.1inAnastasopoulosetal.(2022)formoredetails).TimeShiftforBetterSimultaneityLastyear,wereducedthedelaybyshiftingthesubtitlesaheadintimetoeasethememoryoverloadoftheevaluators.Sincethisyearonlyalowlatencyregimewasused,weleftthesubtitlesintactforthesystemoutputs.Forinterpreting,weusedthesameshiftaslastyear.TwoTestSets:CommonandNon-NativeThemainpartofthetestsetfortheEnglish-to-GermantaskwastheCommontestset.TheCommontestsetisanewinstance(differentfrompreviousyears)consistingofselectedTEDtalksanditservesbothintheOfflineSpeechTranslationtaskaswellasintheSimultaneousTranslationtask.Followingthelastyear,wealsoaddedtheNon-NativepartthatwascreatedandisinusesinceIWSLT2020Non-NativeTranslationTask.TheNon-NativepartisdescribedinAnsarietal.(2020)AppendixA.6.Weshowthesizeofthecorpus,aswellastheamountofannotationcollectedinTable21.ProcessingofCollectedRankingsOncetheresultsarecollected,theyareprocessedasfollows.Wefirstinspectthetimestampsontheratings,andremoveanyratingsthathavetimestampsmorethan20secondsgreaterthanthelengthoftheaudio.Becauseofthenaturaldelay(evenwiththetime-shift)andbecausethecollectionprocessissubjecttonetworkandcomputationalconstraints,therecanberatingsthataretimestampedgreaterthantheaudiolength.Ifthedifferenceishowevertoohigh,wejudgeittobeanannotationerror.Wealsoremoveanyannotatedaudiowherethereisfewerthanoneratingper20seconds,sincetheannotatorswereinstructedtoannotateevery5-10seconds.ObtainingFinalScoresTocalculateascoreforeachsystem,weaveragetheratingsacrosseachannotatedaudio,47thenaverageacrossthemultipleannotationsforeachaudiotoobtainasystemscoreforthataudio.Finallyweaverageacrossallaudiostoobtainascoreforeachsystem.Thistypeofaveragingrendersallinputspeechesequallyimportantanditisnotaffectedbythespeechlength.WeshowtheresultsinTable22.WeobservethatallsystemsperformbetterontheCommonpartofthetestsetthanontheNon-Nativeone.Thedifferenceinscoresbetweenthebestandtheworstsystemisnotsosignificant:Itmakesonly∼0.3.WhenexaminingtheevaluationofNon-Nativeaudios,wecanseethatbestsystemsontheCommonpartareworstonNon-Native.Giventhatthequalityoftherecordingsinthenon-nativepartislowonaverageandthespeakersarenotnative,wehypothesizethatsystemswithworseperformanceonCommonpartaremorerobust.Suchsystemsthenachieveanincreasedperformancegivennoisyinputs.A.1.2HumanEvaluationfortheEnglish-to-JapaneseSimultaneousTaskFortheEnglish-to-JapaneseSimultaneousTranslationTask,weconductedahumanevaluationusingavariantofMultidimensionalQualityMetrics(MQM;Lommeletal.,2014).MQMhasbeenusedinrecentMTevaluationstudies(Freitagetal.,2021a)andWMTMetricssharedtask(Freitagetal.,2021b).FortheevaluationofJapanesetranslations,weusedJTFTranslationQualityEvaluationGuidelines(JTF,
47


2018),distributedbyJapanTranslationFederation(JTF).TheguidelinesarebasedonMQMbutincludesomemodificationsinconsiderationofthepropertyoftheJapaneselanguage.WehiredaJapanese-nativeprofessionalinterpreterastheevaluator,whiletheevaluatorwasatrans-latorinthelastyear(Anastasopoulosetal.,2022).Theevaluatorcheckedtranslationhypothesesalongwiththeirsourcespeechtranscriptsandchosethecorrespondingerrorcategoryandseverityforeachtranslationhypothesisusingaspreadsheet.Here,weaskedtheevaluatortofocusonlyonAccuracyandFluencyerrors,becauseothertypesoferrorsinTerminology,Style,andLocaleconventionwouldnotbesoseriousintheevaluationofsimultaneoustranslation.Finally,wecalculatedthecumulativeerrorscoreforeachsystembasedontheerrorweightingpresentedbyFreitagetal.(2021a),whereCriticalandMajorerrorsarenotdistinguished.
48


AppendixB.AutomaticEvaluationResultsandDetails
49


0.8205
0.7666
0.3506
41.1
16.5
18.1
53.8
32.4
32.4
TED
TED
TED
0.8032
0.3784
0.8063
I2R
30.6
0.3555
36.6
16.9
30.7
30.7
22.6
22.6
0.8123
0.8519
0.7874
0.8406
0.8034
New
New
New
New
New
New
0.8181
0.8443
0.7473
38.1
0.8114
28.5
22.4
0.8255
0.7070
0.8499
18.7
18.7
38.5
0.7428
18.6
0.8463
XIAOMI
0.8208
22.7
NEMO
0.8210
52.9
B.1OfflineSLT⋅SystemsareorderedaccordingtotheBLEUscorecomputedontheconcatenationofthethreetestsets(JointBLEU
0.7977
0.8404
0.7171
0.7999
32.2
0.8376
13.3
0.6841
60.6
46.5
46.5
16.8
16.8
D
D
D
10.7
10.7
0.3819
20.4
53.9
0.8134
0.8627
0.7260
USTC
USTC
0.7575
0.6534
32.1
ACL
ACL
ACL
33.0
29.0
0.8111
0.8331
0.7759
0.8357
0.5976
0.7201
0.6375
0.8648
0.6295
29.5
34.5
34.5
34.5
47.2
42.4
0.7871
57.9
57.9
58.0
28.7
30.9
0.7733
,thirdcolumn).⋅The“D”columnindicatesthedataconditioninwhicheachsubmittedrunwastrained,namely:Con-strained(C),constrained+LLM(C+),Unconstrained(U).⋅FortheBLEUscorescomputedontheTEDtestset,“Orig”and“New”respectivelyindicatetheresultscomputedontheoriginal(subtitle-like)TEDtranslationsandtheunconstrained(exact,moreliteral)translationsasreferences.⋅Directsystemsareindicatedbygraybackground.⋅“*”indicatesalatesubmission.⋅“+”indicatesanunofficialsubmission.
0.8073
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
BLEU
54.7
0.8401
62.1
34.4
0.6371
29.8
Table15:OfficialresultsoftheautomaticevaluationfortheOfflineSpeechTranslationTask,EnglishtoJapanese.
19.3
NeuroDub+
System
System
System
0.3680
24.9
30.4
C
C
C
C
C
C
C
C
EPTV
0.7870
0.7920
23.3
34.8
30.2
0.8497
0.3746
13.2
27.7
26.4
0.7404
0.7128
0.7172
Joint
Joint
Joint
57.8
57.8
38.6
COMET
COMET
COMET
COMET
COMET
COMET
COMET
COMET
COMET
COMET
30.5
10.4
Table14:OfficialresultsoftheautomaticevaluationfortheOfflineSpeechTranslationTask,EnglishtoGerman.
0.8090
0.8042
NIUTRANS
0.6769
Orig
Orig
Orig
Orig
Orig
Orig
0.7546
22.3
0.8375
0.7769
0.8082
37.2
MINETRANS
MINETRANS
27.4
27.4
0.7055
0.8279
0.8177
0.8177
0.8177
45.0
51.1
51.1
0.8098
0.7014
0.7228
0.8213
0.8209
16.7
47.1
47.1
0.7798
Ref
Ref
Ref
U
U
U
U
U
U
U
27.9
0.8332
0.8089
37.7
C+
C+
C+
C+
C+
C+
C+
C+
C+
0.8473
34.9
49.4
46.3
0.8230
15.4
0.8514
29.1
0.3829
0.7992
0.8535
0.8327
36.9
0.8223
51.0
Both
Both
Both
50.6
50.6
50.6
23.9
0.7734
31.8
31.8
31.8
Table16:OfficialresultsoftheautomaticevaluationfortheOfflineSpeechTranslationTask,EnglishtoChinese.
0.7892
25.5
0.7876
32.3
19.6
42.1
0.7248
39.9
52.5
0.7866
0.7773
52.8
52.8
54.1
36.8
0.6354
BIGAI∗
BIGAI∗
BIGAI∗
20.3
20.3
0.8029
0.8521
11.5
15.6
15.6
24.0
0.7740
25.6
0.6945
0.6997
30.1
NeMo
NeMo
35.5
UPC
21.0
28.9
41.9
25.8
32.0
45.6
34.3
0.3823
53.2
0.7985
31.9
31.9
31.9
0.7741
41.8
0.7956
0.8494
20.9
20.9
50.0
HW-TSC
HW-TSC
HW-TSC
HW-TSC
HW-TSC
HW-TSC
HW-TSC
HW-TSC
HW-TSC
18.8
0.7122
0.8439
53.0
53.0
50


2.35(3.63)
0.89(1.54)XIAOMI
14.66
2.26(2.96)FBK
2.26(3.93)
3.44(6.61)
Table18:SimultaneousSpeech-to-TextTranslation,EnglishtoChinese.ExceptforAP,thelatencyismeasuredinseconds.Numbersinbracketsarecomputationawarelatency.
2.20(3.55)
0.83(5.12)CUNI-KIT
4.14(5.87)
0.83(1.59)
Table19:SimultaneousSpeech-to-TextTranslation,EnglishtoJapanese.ExceptforAP,thelatencyismeasuredinseconds.Numbersinbracketsarecomputationawarelatency.
0.99(5.31)CUNI-KIT
17.91
2.06(3.76)
0.77(1.49)
B.2SimultaneousSLT
NAIST
0.845(1.02)
Common
19.94
0.53(1.50)NAIST
3.21(8.66)
3.77(5.47)CUNI-KIT
3.94(5.22)
2.52(3.43)
2.23(2.98)
2.43(3.52)
14.92
3.37(4.71)
25.78
28.51
3.24(4.87)
LAAL
LAAL
LAAL
16.63
2.70(3.65)
2.00(2.80)
2.25(3.06)CMU
BLEU
BLEU
BLEU
Team
Team
Team
28.38
2.13(3.30)
1.99(3.39)
ATD
ATD
ATD
2.47(3.74)
2.93(4.08)
2.28(6.77)CUNI-KIT
0.90(1.47)NAIST
Non-Native
2.30(3.03)
15.19
3.57(6.67)
0.89(1.12)
0.705(1.65)
2.74(5.17)
0.84(1.03)
2.56(4.36)
3.82(4.84)HW-TSC
1.92(3.33)
AP
AP
AP
44.95
2.66(4.22)
3.13(3.92)CMU
2.88(4.50)
1.88(2.74)
3.17(8.99)
2.24(3.56)
4.53(5.85)
0.79(1.11)
2.09(2.88)
3.62(9.07)
4.04(11.13)FBK
2.78(4.38)
0.68(1.06)
29.63
0.80(1.08)
0.798(1.16)
4.39(12.91)
2.45(3.39)
3.78(6.56)
2.25(2.99)
1.95(3.22)
2.36(3.30)
2.46(4.63)
3.05(4.45)
0.82(1.07)
DAL
DAL
DAL
36.80
2.13(3.80)
AL
AL
AL
3.42(5.00)
2.22(3.21)
0.60(1.57)
4.54(6.77)
0.78(1.48)
2.11(3.86)
3.24(5.16)
Table17:SimultaneousSpeech-to-TextTranslation,EnglishtoGerman.ExceptforAP,thelatencyismeasuredinseconds.Numbersinbracketsarecomputationawarelatency.
0.77(1.08)
2.06(3.25)
0.71(1.31)
2.60(4.38)
0.744(1.04)
2.16(3.53)
0.82(1.31)
44.16
2.15(2.48)NAIST
22.96
0.76(1.03)
0.75(1.03)
43.69
4.10(5.34)
2.36(3.63)
26.05
22.84
HW-TSC
HW-TSC
HW-TSC
3.76(4.65)
51


5.09
Interpreter–2.792.71→2.87
Numberofaudios4243Meanaudiolength(seconds)400.3208.8Meanratingsperaudio65.636.5
0.122
98CUNI-KIT
31.68
398
34Table23:Humanevaluationresultsontwotalks(107lines)intheEnglish-to-JapaneseSimultaneousspeech-to-texttranslationtask.Errorweightsare5forCriticalandMajorerrorsand1forMinorerrors.
1.68
BLEU(ontwotalks)
18.71
Numberoferrors
NAIST
Major
3.12
384
0.442
3.48
StartOffset
Critical
7.69
Table20:SimultaneousSpeech-to-SpeechfromEnglishSpeech.Thelatencyismeasuredinseconds.TheBLEUscoresarecomputedbasedontranscriptfromthedefaultWhisper(Radfordetal.,2022)ASRmodelforeachlanguagedirection.
383
1,074
3
Minor
22.62
Team
4.32
ATD
0
0
17.95
1
2.04
TEDref.
56
56
CommonNon-native
CommonNon-native
1.92
2.37
2.37
Chinese
BLASER
15.53
Table21:HumanevaluationfortheEnglish-to-Germantaskontwotestsets:theCommonone(usedalsoinautomaticscoring)andtheNon-nativeone.Weshowthesizeofthetestsets,andthenumberofratingscollected.Onaverage,ourannotatorsprovideaqualityjudgementever6seconds.
CUNI-KIT3.103.04→3.161.631.54→1.72FBK3.083.02→3.141.261.20→1.30HWTSC2.912.85→2.982.041.92→2.15NAIST2.842.78→2.912.272.18→2.34CMU2.792.72→2.872.382.30→2.46
0.696
Additionalref.
6.27
German
104NAIST
4.22
16.75
CMU
26.59
3.56
93Baseline
ASRBLEU
Japanese
3.75
10.19
5.21
EndOffset
TargetLanguage
61
1.70
25.10
3.23
Errorscore
3.49
19.74
Table22:HumanevaluationresultsforEnglish-to-GermanSimultaneoustaskonthe1–5(worst-to-best)scale,with95%confidenceintervals.Wecalculateameanscoreforeachannotatedaudiofile,thenameanacrossannotators(foreachaudio),thenameanacrossallaudiofilesforeachsystem.Tocomputeconfidenceintervals,wetakethescoresforannotatedaudios,perform10,000xbootstrapresampling,computethemeanscoreforeachresample,thencompute[2.5,97.5]percentilesacrosstheresampledmeans.Team
205
HW-TSC
HW-TSC
HW-TSC
HW-TSC
2.58
24.21
52


59.6174.29
71.3979.83100.00
89.1799.29100.00
76.2568.49
teamcon-systemdomain
teamcon-systemdomain
45.8174.50
81.6966.36
68.4769.63
95.42100.00100.00
80.7269.56
Table25:AutomaticevaluationresultsfortheSubtitlingTask:en→es.LegendainTable24.
Translationquality
Translationquality
74.4759.59
17.7939.54.3419
15.3838.36.4376
82.6777.17
40.3665.72.7047
81.9399.51100.00
75.4165.22
88.5999.20100.00
11.2233.32.3172
Table24:AutomaticevaluationresultsfortheSubtitlingTask:en→de.CandUstandforconstrainedanduncon-strainedtrainingcondition,respectively;prmryandcntrstvforprimaryandcontrastivesystems.
14.8139.50.4591
77.0091.3499.99
Subtitlecompliance
Subtitlecompliance
80.8772.62
14.9237.13.4501
97.20100.00100.00
11.4639.25.4150
21.4850.31.5511
SubERSigma
SubERSigma
23.5751.94.5379
80.2199.47100.00
86.65100.00100.00
78.9588.14100.00
13.2042.69.4722
14.4342.37.4604
86.0799.52100.00
90.1599.44100.00
94.67100.00100.00
77.6372.79
9.7330.51.2914
77.7992.6799.96
10.0634.46.4264
90.5598.61100.00
88.98100.00100.00
CPSCPLLPB
CPSCPLLPB
91.50100.00100.00
77.0572.50
6.7023.85.2204
68.4772.97
14.4335.27.4028
78.4575.78
97.33100.00100.00
9.4031.20.3419
87.0457.73
22.3447.38.5059
90.53100.00100.00
APPTEKCprmryALL
APPTEKCprmryALL
88.2799.60100.00
73.3174.44
93.35100.00100.00
66.6073.31
APPTEKUprmryALL
8.0526.10.2255
92.6299.48100.00
86.01100.00100.00
12.8435.89.3513
67.7585.12100.00
71.2571.06
92.58100.00100.00
83.5370.39
18.6740.21.3637
MATESUBUprmryALL
MATESUBUprmryALL
eptv
eptv
eptv
eptv
eptv
eptv
eptv
eptv
BleuChrFBleurt
BleuChrFBleurt
12.0843.59.4705
79.7075.73
70.6473.35
15.9641.86.4666
14.0336.95.3664
73.1167.04
86.85100.00100.00
7.0827.89.2780
73.9867.09
87.2568.29
10.4733.18.4069
26.7850.93.5539
9.3327.14.2063
21.0654.11.5728
19.0746.17.4921
pltn
pltn
pltn
pltn
pltn
pltn
pltn
pltn
45.9466.85
91.14100.00100.00
70.0284.2099.96
9.0827.74.2612
Subtitlequality
Subtitlequality
59.7274.33
15.8145.21.5229
79.7268.27
96.27100.00100.00
89.60100.00100.00
dition
dition
FBKCprmryALL
FBKCprmryALL
63.8576.79
APPTEKCcntrstvALL
93.45100.00100.00
86.3769.79
74.8770.99
76.0074.63
11.3729.75.2487
68.1168.37
79.8188.05100.00
itv
itv
itv
itv
itv
itv
itv
itv
67.7062.01
83.7174.02
20.3750.05.5500
84.9799.25100.00
80.2075.90
95.76100.00100.00
69.9883.5099.98
23.7449.14.5683
79.7669.04
12.7434.31.3420
69.8374.48
ted
ted
ted
ted
ted
ted
ted
ted
74.9582.0899.91
87.74100.00100.00
23.9252.19.5490
B.3AutomaticSubtitling
18.5041.07.4592
39.3762.11.6562
74.6792.94100.00
82.0076.16
45.6874.31
94.57100.00100.00
7.7330.17.3137
71.6874.99
40.2165.09.6737
95.18100.00100.00
53


I2R
76.433.7—6KITcontrastive3✓+LLM55.9
63.5(17)
—10JHUconstrained✓+LLM48.1
31.8(17)
9
MINE-Trans
MINE-Trans
27.4(18)
51.7
✓
✓
✓
✓
✓
SystemConstrained?chrF
10.4(13)
73.9(11)
73.9(11)
NiuTrans
KITc5
JHUMult.✓+LLM59.0(15)23.7(15)69.3(11)18.9(12)67.9(15)37.4(16)
78.7(9)
8
41.8(14)
55.7(18)
BIT
BITprimary
63.0(14)
68.4(13)
KITc7
46.0(12)
25.6(10)
56.2(13)
53.9
65.324.534.1
79.1(6)
76.333.5—
USTC
65.3(16)
76.7
74.7(7)
KITc6
11
19.6(16)
11.7
30.9
COMETBLEUEnglishWER
31.9(13)
46.5(11)
78.9(7)
74.0(13)
80.0(4)
70.0(14)
77.3(12)
31.1
78.8(8)
32.9(11)
32.9(11)
8.0(14)
7
80.0(9)
52.5(4)KITprMult.✓+LLM74.9(6)37.5(4)82.0(4)35.7(1)79.3(5)49.4(6)KITc1Mult.✓+LLM74.6(8)36.5(7)82.0(4)35.2(2)79.3(5)49.7(5)KITc2Mult.✓+LLM74.3(9)36.5(7)81.6(6)34.0(3)78.6(10)49.4(6)KITc3Mult.✓+LLM74.7(7)36.1(9)81.4(7)33.3(5)78.4(11)48.6(7)KITc4Mult.✓+LLM74.2(10)36.4(8)81.7(5)33.9(4)78.4(11)48.2(8)
77.034.923.73KITcontrastive1✓+LLM57.5
Table26:Overalltaskrankingwithmetricsaveragedacrossalltenlanguagepairsontheevaluationset.Weshowtheofficialtaskmetric(chrF)aswellastheunofficialmetrics(COMET,BLEU,andEnglishWER).Allmetricsarecalculatedafterresegmentationtoreferencetranscriptsandtranslations.Direct/end-to-endsystemsarehighlightedingray.
67.7(12)
32.1(12)
1JHUunconstrained61.1
USTCOff.85.4(1)58.0(1)HW-TSCOff.✓80.9(2)38.1(3)84.4(3)30.1(7)84.0(2)53.0(2)JHUMult.81.3(1)41.2(1)84.7(1)33.9(4)82.0(3)46.5(11)HW-TSCOff.80.7(3)36.9(6)84.7(1)30.7(6)84.0(2)52.8(3)HW-TSCOff.✓+LLM80.6(4)37.2(5)84.6(2)30.7(6)84.0(2)53.0(2)NeuroDubOff.79.6(5)41.1(2)
54.5
11.1(17)
77.7(10)
Xiaomi
27.3(8)
Table27:SubmissionsfromalltracksontheACL60-60evaluationsetsonthethreelanguagepairssharedacrosstracks(En→De,Ja,Zh),orderedbyaveragemetricranking.Direct/end-to-endsystemsarehighlightedingray.
82.339.316.92KITprimary✓+LLM57.5
75.9
26.6(9)
71.7(12)
SystemTaskConstrained?COMETBLEUCOMETBLEUCOMETBLEU
74.9(6)
KITcontrastive6
53.7
—
—
—
Off.
Off.
Off.
Off.
Off.
Off.
Off.
Off.
Off.
47.2(16)
B.4MultilingualSpeechTranslationBelowweshowtheMultilingualtask(§5)resultsandoverallrankings,orderedaccordingtotheaveragechrFacrossall10targetlanguagesafterresegmentationtothereferencetranslations.WealsocomparetotheOfflinesubmissionsontheACL60-60evaluationsetonthe3languagepairsusedfortheOfflinetask.Finally,weshowthescoresforeachmetric(chrF,COMET,BLEU)perlanguagepairforallsystems.
BIGAI*
76.534.0—5KITcontrastive4✓+LLM56.2
45.7(13)
80.3(8)
80.3(8)
31.0
Mult.
Mult.
Mult.
Mult.
76.834.8—4KITcontrastive2✓+LLM56.4
NeMo
UPC
KITcontrastive7
47.1(9)
19.8(19)
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
✓+LLM
31.7
dejazh
24.9(11)
33.8(10)
46.7(10)
39.9(15)
KITcontrastive5
76.6
23.9(14)
54


56.4KITcontrastive4
52.2
39.2
53.8
53.8
53.8
46.0
65.2
45.2
39.7
26.6
19.8
51.7
54.2
67.3
76.3
76.3
43.0
18.6
19.1
19.1
76.9
56.964.655.667.842.067.669.651.256.742.7
78.9
54.8
Table29:COMETwithresegmentationforeachtargetlanguageontheevaluationset,sortedbythesystemaverage.Direct/end-to-endsystemsarehighlightedingray.
BIT
BIT
BIT
30.9JHUconstrained15.023.721.933.118.931.333.217.212.837.424.5
74.9
JHUunconstrained
JHUunconstrained
39.6
KITconstrastive6
KITconstrastive6
20.1
40.7
52.7
75.4
55.9
48.1
77.774.675.774.582.077.678.472.276.479.3
80.3
80.3
80.3
71.3
53.9
61.1KITprimary
75.7
19.7
19.7
17.2
76.5KITconstrastive4
71.6
55.7
38.4
76.7
76.7
16.3
40.9
34.5
66.4
47.2
11.7
48.7
28.7
73.3
54.7
80.0
64.3
48.6
75.9JHUconstrained
73.9
73.9
54.9
14.5
78.8
78.074.975.874.482.077.778.472.576.679.3
47.7
54.0
31.1
57.5KITcontrastive2
28.8
35.0
47.8
82.781.380.681.484.784.184.978.982.582.0
8.0
46.7
76.6KITconstrastive2
48.2
56.163.652.967.340.866.569.250.655.641.3
77.7
67.959.066.163.269.366.267.862.064.067.9
32.9
32.9
11.1
77.8
54.6
25.9
Submission
Submission
56.2KITcontrastive3
75.2
78.5
78.5
JHUunconstrained33.441.235.050.033.944.851.727.928.146.539.3KITprimary25.937.529.841.335.740.444.322.421.849.434.9KITconstrastive125.637.530.141.135.240.644.522.621.349.734.8KITconstrastive224.736.528.042.434.038.843.821.920.649.434.0KITconstrastive424.436.428.442.133.938.943.021.620.348.233.7KITconstrastive324.036.127.641.933.338.243.621.520.148.633.5
55.563.752.166.940.366.068.950.055.240.6
56.964.855.467.842.367.669.651.257.342.5
54.5
6.3
56.2
Table28:chrFwithresegmentationforeachtargetlanguageontheevaluationset,sortedbythesystemaverage.Direct/end-to-endsystemsarehighlightedingray.
62.467.657.873.442.071.675.056.862.542.2
77.274.275.074.381.777.378.272.075.578.4
75.9
45.253.444.562.426.862.162.246.846.330.8
77.0KITconstrastive1
ardefafrjanlptrutrzhAvg.
20.6
79.1
Table30:BLEUwithresegmentationforeachtargetlanguageontheevaluationset,sortedbythesystemaverage.BLEUscoresingreyarecalculatedusinglanguage-specifictokenization(ja)oratthecharacter-level(zh);see§5.2forspecifictokenizationdetails.Direct/end-to-endsystemsarehighlightedingray.
27.3
KITcontrastive6
70.8
41.7
28.3
Avg.
Avg.
KITconstrastive5
KITconstrastive5
35.9
35.9
33.8
53.7JHUconstrained
63.4
23.0
76.2
19.6
ardefafrjanlptrutrzh
ardefafrjanlptrutrzh
77.374.374.974.381.677.378.472.175.878.6
38.2
38.8
52.8
76.4KITconstrastive3
76.8
76.8
36.8
63.7
57.5KITcontrastive1
38.9
31.0
5.7
28.6
7.4
48.0
65.3
74.6
60.3
60.3
55.3
25.6
KITconstrastive7
KITconstrastive7
23.7
82.3KITprimary
35.5
28.9
64.4
KITcontrastive7
56.263.353.067.240.766.568.850.455.140.3
4.1
74.2
23.4
62.7
31.7
78.2
36.0
KITcontrastive5
61.3
45.7
28.4
76.974.774.674.281.476.978.271.875.778.4
67.2
55


E2E(contrastive2)
E2E(contrastive2)
25.0
22.03
3.21
76.96
31.3
32.4
23.41
3.58
3.58
3.50
SEScore2
SEScore2
SEScore2
73.47
67.49
Test-primary
34.0
29.6
19.32
79.90
28.5
Table32:OfficialresultsofthehumanevaluationfortheEnglishtoChineseSpeech-to-SpeechTranslationTask.
XIAOMI
XIAOMI
12.21
73.81
17.07
24.7
22.77
78.68
40.7
3.54
28.2
69.09
79.91
NPU-MSXF
NPU-MSXF
72.90
73.00
27.2
38.4
29.5
29.5
44.4
34.5
SpeechQualityScore
38.3
19.18
BLEU
BLEU
BLEU
73.28
47.4
19.40
65.65
E2ESystems
E2ESystems
System
System
43.2
3.01
31.1
31.1
34.8
Overall
Overall
27.7
26.4
26.4
26.4
74.40
76.43
COMET
COMET
COMET
3.72
E2E(contrastive1)
34.7
chrF
chrF
chrF
13.62
13.86
44.5
KU
KU
E2E(primary)
MINETRANS
MINETRANS
MINETRANS
MINETRANS
MINETRANS
MINETRANS
45.0
Cascade
Cascade
3.98
Ref
3.70
3.70
21.5
3.16
22.20
37.7
3.26
74.14
34.9
41.0
24.3
35.3
Table31:OfficialresultsoftheautomaticevaluationfortheEnglishtoChineseSpeech-to-SpeechTranslationTask.
21.7
3.67
3.67
26.34
36.9
74.83
47.9
12.27
31.8
20.15
20.23
64.71
3.75
79.07
32.3
36.7
2.97
79.35
33.9
17.48
17.68
38.0
38.0
CascadeSystems
CascadeSystems
29.2
2.92
13.92
31.0
Test-expanded
3.84
19.12
28.6
30.1
21.61
25.68
22.12
B.5Speech-to-SpeechTranslation
23.4
68.33
79.09
73.32
76.61
17.52
14.23
HW-TSC
HW-TSC
67.94
TranslationQualityScore
56


44.3
44.3
24.2
24.2
69.8
52.2
18.1
39.2
contrastive5
86.9
B.6DialectalSLTTunisianArabic→English(UnconstrainedCondition)
contrastive2
contrastive2
46.0
.96
USTC/unconstrained/primary
.99
.99
.99
43.6
7.0
36.6
36.6
30.7
35.4
83.9
39.7
39.7
44.8
79.6
79.6
40.1
66.9
51.7
.88
67.7
50.7
18.7
18.7
43.0
pr1
pr1
pr1
pr1
bp
bp
bp
bp
19.1
19.1
69.3
28.1
22.7
40.8
40.8
42.9
102.2
81.4
GMU
GMU
GMU
GMU
21.1
21.1
4.5
74.9
test2WER↓
Table33:AutomaticevaluationresultsfortheDialectSpeechTranslationtask,UnconstrainedCondition.SystemsareorderedintermsoftheofficialmetricBLEUontest3.Wealsoreportbrevitypenalty(bp)andunigramprecision(pr1)ofBLEU,chrF,andTER.TunisianArabic→English(ConstrainedCondition)
70.3
19.9
29.9
52.7
64.5
Norm
Norm
Norm
Norm
20.4
USTC
USTC
USTC
68.4
73.7
17.1
17.1
29.0
USTC/constrained/primary
ON-TRAC
ON-TRAC
38.4
16.3
48.4
44.2
42.4
42.4
contrastive3
20.8
67.6
20.0
BLEU
BLEU
BLEU
BLEU
Team
Team
47.4
19.4
84.7
TER
TER
TER
TER
test2
test2
23.6
39.3
65.7
65.7
45.9
27.1
2022best:CMU
2022best:CMU
System
System
46.6
46.6
43.2
.93
6.2
47.7
49.5
22.8
18.3
65.4
41.5
.94
.94
49.0
49.0
Table34:AutomaticevaluationresultsfortheDialectSpeechTranslationtask,ConstrainedCondition.TunisianArabicASRAutomaticEvaluationResults
52.3
22.1
10.4
40.3
46.7
chrF
chrF
chrF
chrF
48.2
43.8
43.8
73.8
40.4
12.9
Orig
Orig
Orig
Orig
72.9
73.1
73.1
73.1
69.4
20.7
20.7
20.7
11.1
70.7
21.6
21.9
22.9
test3CER↓
77.8
baseline
74.1
74.1
44.5
44.7
.97
37.2
17.6
105.5
41.4
JHU
JHU
JHU
JHU
JHU
JHU
JHU
45.0
JHU/unconstrained/primary
primary
primary
primary
primary
primary
primary
primary
2022best:ON-TRAC/unconstrained
40.5
40.5
40.6
43.7
37.5
5.0
36.4
.90
21.2
44.9
16.6
25.2
.98
41.6
41.6
23.8
27.3
ASRSystem
contrastive4
13.4
41.3
49.3
68.7
45.5
45.5
15.0
31.5
test3WER↓
92.0
49.9
36.1
71.9
Table35:WordErrorRate(WER)andCharacterErrorRate(CER)oftheASRcomponentofsubmittedcascadedsystemsontest2andtest3.Theoriginalversion(Orig)matchestheminimaltextpre-processingprovidedbytheorganizer’sdatapreparationscripts,andresultsinrelativelyhighWER.Asdiagnosis,weranadditionalArabic-specificnormalization(Norm)fore.g.Alif,Ya,Ta-MarbutaonthehypothesesandtranscriptsbeforecomputingWER/CER.WearegratefultoAhmedAliforassistanceonthis.
39.9
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
14.6
69.0
23.1
20.3
20.5
contrastive1
contrastive1
contrastive1
37.6
33.1
18.4
40.0














50.5
JHU/constrained/primary
41.9
41.9
34.3
64.6
post-eval
18.2
18.2
31.9
test2CER↓
78.2
41.8
20.9
74.0
50.0
49.2
72.3
79.8
14.1
45.7
45.7
test3
test3
53.1
24.8
42.7
20.2
67.2
57


contrastive5
32.4
contrastive2
contrastive2
contrastive2
7.7
49.1
Table36:AutomaticevaluationresultsfortheIrishtoEnglishtask,ConstrainedCondition.Irish→English(UnconstrainedCondition)
B.7Low-ResourceSLTIrish→English(ConstrainedCondition)
54.8
GMU
GMU
GMU
GMU
GMU
GMU
GMU
GMU
24.7
39.6
74.5
16.8
23
contrastive3
31.2
BLEU
BLEU
BLEU
BLEU
Team
Team
Team
Team
Table37:AutomaticevaluationresultsfortheIrishtoEnglishtask,UnconstrainedCondition.Marathi→Hindi(ConstrainedCondition)
29.8
SRI-B
SRI-B
SRI-B
SRI-B
63.3
68.5
System
System
System
System
25.3
8.6
54.4
55.5
3.3
15.1
15.1
25.7
contrastive
contrastive
contrastive
BUT
BUT
primary
primary
primary
primary
primary
primary
primary
primary
26.5
26.5
49.4
49.4
46.3
46.3
23.8
AlexaAI
AlexaAI
AlexaAI
AlexaAI
AlexaAI
AlexaAI
contrastive4
5.9
chrF2
chrF2
chrF2
chrF2
19.6
39.9
20.3
contrastive1
contrastive1
contrastive1
77.4
28.6
28.6
25.6
41.9
53.2
81.6
Table39:AutomaticevaluationresultsfortheMarathitoHinditask,UnconstrainedCondition.
28.4
Table38:AutomaticevaluationresultsfortheMarathitoHinditask,ConstrainedCondition.Marathi→Hindi(UnconstrainedCondition)
58


11.27
9.72
contrastive2
contrastive2
contrastive2
contrastive2
15.18
15.56
15.07
19.57
15.29
0.3
5.92
GMU
GMU
GMU
GMU
GMU
106.23
13.32
0.48
Pashto→French(UnconstrainedCondition)
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
contrastive3
contrastive3
contrastive3
BLEU
BLEU
BLEU
BLEU
Team
Team
Team
Team
TER
24.87
11.99
System
System
System
System
14.52
15.06
24.82
15.24
10.5
11.11
0.7
primary
primary
primary
primary
primary
primary
test
test
23.87
23.38
0.6
contrastive4
Table43:AutomaticevaluationresultsfortheTamasheqtoFrenchtask,ConstrainedCondition.
chrF2
UM-DFKI
UM-DFKI
UM-DFKI
UM-DFKI
UM-DFKI
16.87
Table41:AutomaticevaluationresultsforthePashtotoFrenchtask,ConstrainedCondition.Maltese→English(UnconstrainedCondition)
12.26
9.2
Table42:AutomaticevaluationresultsfortheMaltesetoEnglishtask,UnconstrainedCondition.Tamasheq→French(ConstrainedCondition)
11.06
contrastive1
contrastive1
contrastive1
contrastive1
2.66
valid
valid
Table40:AutomaticevaluationresultsforthePashtotoFrenchtask,UnconstrainedCondition.Pashto→French(ConstrainedCondition)
0.4
0.4
12.16
59


9.50
1.25
NAVER
NAVER
NAVER
NAVER
NAVER
NAVER
contrastive5
contrastive2
contrastive2
contrastive2
contrastive2
contrastive2
contrastive2
contrastive2
contrastive2
81.03
75.30
15.54
33.67
48.15
32.86
Tamasheq→French(UnconstrainedCondition)
81.25
23.59
1.86
Table44:AutomaticevaluationresultsfortheTamasheqtoFrenchtask,UnconstrainedCondition.Quechua→Spanish(ConstrainedCondition)
74.26
74.26
75.08
1.30
Table45:AutomaticevaluationresultsfortheQuechuatoSpanishtask,ConstrainedCondition.ChrF2scoreswereonlytakenintoaccountforthosesystemsthatscoredlessthan5pointsBLEU.Quechua→Spanish(UnconstrainedCondition)
8.03
75.07
GMU
GMU
GMU
GMU
GMU
GMU
GMU
44.11
0.11
9.30
15.70
15.46
1.63
Table46:AutomaticevaluationresultsfortheQuechuatoSpanishtask,UnconstrainedCondition.ChrF2scoreswereonlytakenintoaccountforthosesystemsthatscoredlessthan5pointsBLEU.
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
ON-TRAC
15.27
33.03
46.11
21.46
70.32
8.87
10.63
96.72
contrastive3
contrastive3
73.85
BLEU
BLEU
BLEU
Team
Team
Team
64.00
2.10
1.78
TER
44.22
System
System
System
23.63
80.85
10.75
43.74
15.49
0.13
43.59
QUESPA
QUESPA
QUESPA
QUESPA
QUESPA
QUESPA
primary
primary
primary
primary
primary
primary
primary
primary
primary
15.55
15.88
13.17
82.33
43.91
32.29
AlexaAI
AlexaAI
AlexaAI
AlexaAI
66.41
contrastive4
24.33
1.46
chrF2
chrF2
15.36
10.53
9.28
16.35
25.35
32.04
contrastive1
contrastive1
contrastive1
contrastive1
contrastive1
contrastive1
contrastive1
contrastive1
43.88
18.73
21.31
16.25
87.81
94.58
49.84
60


cACCCONSTRAINEDCOCOA(baseline)F11.10.504428.55543.20.61899999IF11.10.512580.45841.50.60219899HW-TSCF25.60.75128910051.30.7522100100IF26.10.736710010049.80.7209100100UNCONSTRAINEDUMD(baseline)F4.90.2110789926.70.36299695IF4.90.1697989925.30.34529798HW-TSCF25.40.73478710048.20.7214100100IF26.20.721810010048.30.7102100100KUXUPSTAGEF26.60.72698710047.00.668599100IF27.10.7145989545.60.637399100UCSCF23.30.5210869844.60.67719998IF22.80.4724989643.50.628199100Table47:ResultsfortheFormalityTrack(SupervisedSetting).Mostsystemsperformwellinthissetting,thoughMTqualityonformal(F)tendstobehigherthaninformal(IF)Model
EN-VI
cACC
cACC
EN-PT
cACCCONSTRAINEDHW-TSCF47.40.733710010036.50.6472100100IF47.90.744210010035.60.6442100100UNCONSTRAINEDUMD(baseline)F27.30.4477969821.30.34929692IF30.90.4161939121.00.34758485APPTEKF34.60.6089999935.40.61659998IF42.40.6776646533.30.60269897HW-TSCF45.40.773710010033.70.5804100100IF49.10.784510010032.40.5558100100KUXUPSTAGEF31.00.525110010025.80.4446100100IF19.90.2486689026.30.4181100100UCSCF26.60.4048909118.4-0.17139979IF28.40.4252584214.9-0.27665267Table48:ResultsfortheFormalityTrack(Zero-shotSetting).Appreciabledifferencesinformalitycontrolexistbetweenformal(F)andinformal(IF),suggestingthatformalitybiasexistsinparticipantsystems.
BLEU
BLEU
BLEU
BLEU
B.8FormalityControlforSLTModel
COMET
COMET
COMET
COMET
EN-RU
EN-KO
mACC
mACC
mACC
mACC
61