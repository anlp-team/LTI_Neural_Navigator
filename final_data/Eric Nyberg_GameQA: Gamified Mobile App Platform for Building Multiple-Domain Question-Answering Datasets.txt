1https://developers.google.com/custom-search/v1/introduction
GameQA:GamifiedMobileAppPlatformforBuildingMultiple-DomainQuestion-AnsweringDatasetsNjállSkarphéðinsson1,2,BrekiGuðmundsson2,SteinarÞ.Smári1,MartaK.Lárusdóttir1,HafsteinnEinarsson3,AbuzarKhan2,EricNyberg2,HrafnLoftsson11DepartmentofComputerScience,ReykjavikUniversity,Reykjavik,Iceland2LanguageTechnologiesInstitute,CarnegieMellonUniversity,PA,USA3DepartmentofComputerScience,UniversityofIceland,Reykjavik,Iceland{nskarphe,abuzark,en09}@andrew.cmu.edu,hafsteinne@hi.is{brekig18,steinars21,marta,hrafn}@ru.isAbstractThemethodsusedtocreatemanyofthewell-knownQuestion-Answering(QA)datasetsarehardtoreplicateforlow-resourcelanguages.Acommonalityamongstthesemethodsishiringannotatorstosourceanswersfromtheinternetbyqueryingasingleanswersource,suchasWikipedia.Applyingthesemethodsforlow-resourcelanguagescanbeproblematicsincethereisnosinglelargeanswersourcefortheselanguages.Consequently,thiscanresultinahighratioofunansweredquestions,sincetheamountofinformationinanysinglesourceislimited.Toaddressthisproblem,wedevel-opedanovelcrowd-sourcingplatformtogathermultiple-domainQAdataforlow-resourcelan-guages.Ourplatform,whichconsistsofamobileappandawebAPI,gamifiesthedatacollectionprocess.WesuccessfullyreleasedtheappforIcelandic(alow-resourcelanguagewithabout350,000nativespeakers)tobuildadatasetwhichrivalslargeQAdatasetsforhigh-resourcelanguagesbothintermsofsizeandratioofansweredquestions.Wehavemadetheplatformopensourcewithinstructionsonhowtolocalizeanddeployittogatherdataforotherlow-resourcelanguages.1IntroductionReplicatingwellknownQuestion-Answering(QA)datacollectionmethods,suchasthoseusedtocreatetheSQuAD(Rajpurkaretal.,2016)andTyDi(Clarketal.,2020)datasets,forlow-resourcelanguagesposesafewproblems.First,manylarge-scaleQAdatasetsaregatheredusingasin-glesourceforanswers,e.g.Wikipedia.Thisisproblematicsincelow-resourcelanguagesdonothaveaccesstoanysingle,largeknowledgebasefromwhichinformationcanbeextractedtocreatesuchadataset.Second,QAdatasetsgatheredinaninformation-seekingmanner(wheretheques-tionisaskedpriortofindingtheanswer),willhavequestionsthatcannotbeansweredbytheanswersource(s).Aswewillshow,theratioofanswerablequestionsispositivelycorrelatedwiththeamountofcontentinananswersource.Third,manyofthesemethodsrelyonpaidworkerstoperformthelaborioustaskofannotatingdataandthenec-essaryfundsmaynotbeavailableinregionsoflow-resourcelanguages.Inthispaper,weintroduceGameQA,acrowd-sourcingplatformtobuildQAdatasets.GameQAconsistsofamobiletriviaapp(foriOSandAn-droid)andawebAPI.GameQA,whichisopensource,isspecificallydesignedtogatherQAdataforlow-resourcelanguages.Itcanbetriviallylo-calizedandpublishedforspecificgeographicalre-gions.ThemaincontributionsofGameQAareasfollows:•Gamification:Itincorporatesnumerousas-pectsofgamificationtoincreasethenumberofannotationsprovidedperuser.Thisin-cludesrewardingpoints,level-ups,streaks,avatarupgrades,andprestigetokenstousersastheycontributetothedatacollection.•Socialfeatures:Theusersaremadeawareoftheircontributionsrelativetootherusers.Thisincludesaleaderboardandnotifyingusersonceanotheruserhasansweredtheirques-tion.•Culturalrelevance:OurresultsshowthatGameQAgathersquestionswhicharerelevanttotheculture,history,andgeographyoftheregioninwhichitisemployed.•MultipleAnswerSources:Theplat-form’sAPIintegratesGoogle’sProgrammableSearchEngine1toallowuserstofindanswers
152 Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics System Demonstrations, pages 152–160 May 2-4, 2023 ©2023 Association for Computational Linguistics


2https://github.com/cadia-lvl/GameQA2017),andNaturalQuestions(Kwiatkowskietal.,2019).TyDiisanexampleofaninformation-seekingdatasetconstructedusinganswerpara-graphsfromtheencyclopedicdomain(Clarketal.,2020).Clarketal.showedcrowd-workersapara-graphfromWikipedia,butinstructedthemtoaskaquestionthatwasnotanswerablebytheparagraph.JustasQAdatasetscandifferintermsofhowtheysourcetheirquestions(e.g.,squad-likeorinformation-seeking),theycanalsobecategorizedintermsofwheretheiranswerparagraphsaresourced,i.e.thedomainthatcontainstheanswers.Averycommonpracticeistoconstrainadatasettoasingledomain–thisisthecaseforthema-jorityofover80QAdatasetsreviewedbyRogersetal.(2020).SQuAD,WikiQA,TyDi,andNatu-ralQuestionsareexamplesofsuchsingle-sourcedatasets,i.e.theyallsourceanswerparagraphsfromWikipediaonly.Examplesofdatasetssourc-inganswersfromanothernotabledomain,thenewsdomain,areNewsQAandCNN/DailyMail(Her-mannetal.,2015).However,low-resourcelan-guagesareunlikelytohaveaccesstoasinglesourcethatcontainsenoughinformationtocon-structalargeQAdataset.Multiple-domainQAdatasetshavealsobeencreated.MS-Marco,whichutilizedBing3searchqueries,usedaproprietarystate-of-the-artpas-sageretrievalsystematBingtomatchquerieswithanswerparagraphsontheinternet.SinceMS-Marcoreliesonsuchanalgorithm,replicat-ingtheirmethods(i.e.forotherlanguages)isim-possible.MMQA,amultiple-domain,squad-likedatasetinEnglishandHindi,wascreatedbyweb-crawlingandsubsequentlyhavingannotatorswritequestionsaboutthecrawledarticles(Guptaetal.,2018).However,itislikelythatitsuffersfromthesameproblemsasothersquad-likedatasets.Tothebestofourknowledge,thereexistsnoeasilyreproduciblemethodintheliteraturetogatheramulti-domaindatasetwherethequestionsreflectinformation-seekingintent.2.1QAforIcelandicInthelastfewyears,Icelandichasbeengrowingconsiderablywithregardtolanguageresources(Nikulásdóttiretal.,2022).However,formanynaturallanguageprocessingtasksitstilllacksthenecessaryresources.ForreadingcomprehensionandopenQAtasks,thereonlyexistsonedataset
onmultiplewebsites,thusseamlesslycon-structingamultiple-domainQAdataset.TospurQAresearchforlow-resourcelanguages,wehavemadetheGameQAplatformopensourcewithinstructionsonhowtolocalizeitandsubse-quentlyreleaseitforanygeographicalarea2.2RelatedWorkInrecentyears,theliteraturehasseenanexplo-sioninthenumberanddiversityofQAdatasets(Cambazogluetal.,2021).ThemostprevalenttypeofQAdatasetsaresentenceclassificationandspan-predictiondatasets.Theseincludedoc-uments,questions,anddemarcatedanswerspansthatamachinelearningmodelmustlearntopre-dictforagivenquestion.Rajpurkaretal.(2016)introducedSQuAD,oneofthefirstlargespan-predictiondatasets.Theycrowd-sourcedthecre-ationofthedatasetbyshowingcrowd-workersanexcerptfromWikipediaandtaskingthemwithwrit-ingaquestionwhoseansweriscontainedwithintheexcerpt.ThisresultsinQAdatawithahighlexicaloverlapbetweenquestionsandanswerparagraphs(Ribeiroetal.,2019;GanandNg,2019)whichcanleadtobiaseddata(Shinodaetal.,2021).Inthispaper,wewillrefertospan-predictiondatasetscon-structedinthismannerasbeingsquad-like.CoQa(Reddyetal.,2019),NarrativeQA(Koˇciskýetal.,2018),andNewsQA(Trischleretal.,2016)areotherexamplesofdatasetsfallingintothiscate-gory.Toaddresstheproblemsassociatedwithsquad-likedatasets,researchershavedevelopedwaysthatencourageinformation-seekingbehaviordur-ingquestionelicitation.Theaimistoemulatehumancuriositybyhavingannotatorsaskques-tionsaboutsomethingtheydonotknowtheanswerto.WikiQA(Yangetal.,2015),whichposestheproblemasasentenceclassificationproblemin-steadofspanprediction,isordersofmagnitudesmallerthanSQuAD(3,047vs.100,000ques-tions,respectively).However,WikiQAbringsforthinterestingideas,suchascollectingQAdatainaninformation-seekingmannerandusingweb-searchqueriesasameanstocapturethecurios-ityofinformation-seekingusers.Thisweb-searchquery-basedapproachwaslateradoptedbylargerinformation-seekingQAdatasets,suchasMS-Marco(Bajajetal.,2016),SearchQA(Dunnetal.,
3https://www.bing.com
153


4Atthetime,theIcelandicWikipediahadonly3,730pageswithmorethan250characters.4GamificationandSocialFeaturesGatheringandannotatingQAdataisalaboriousandrepetitivetask.Sincethecrowd-workersofGameQAarenotfinanciallycompensatedfortheircontributions,andthushavelittleincentivetopar-ticipate,GameQAleveragesgamificationtoincen-tivizetheusersandtogivethempositivefeedbackwhentheycontributetothedatacollection.4.1UserlevelsandavatarsEveryusercollectspointsbycompletingtasks.Foreachcompletedtask,theuserisrewardedwith1point.Uponcompletingacertainnumberoftasks,theuserisawardedwitha“level-up”.Weusedanad-hocformula(seeEquation1)tocalculatethenumberoftasksTinordertocompleteaspecificlevelL:TL=⌊2.5×L1.1⌋(1)HereTL∈N,∀L∈N.Forexample,auserwouldhavetocomplete⌊2.5×11.1⌋=2tasksforthefirst“level-up”,andP20l=1Tl=667taskstocompleteall20levels.Usersarealsogivenavatarswhichchangeastheuserslevelup.Sinceuserscanseeeachother’savatars,theyareasignalfromausertothecommu-nityabouttheirstatus.4.2PrestigetokensOncewerolledoutthedatacollection,weweredoubtfulthatanyuserwouldfinishthe667tasksrequiredtocompleteall20levels.Afterthefirstday,however,werealizedthatafewcompleted667taskswithin24hoursand,subsequently,stoppedplaying.Wehypothesizedthatthiswasbecauseusershadlittlemotivationtocontinueannotatingoncetheyhadreachedthemaximumlevel.Asaresult,takinginspirationfromgamingfranchiseslikeCallofDuty5,weaddedPrestigeTokens.Theprestigetokensworkasfollows:Usersarepromptedwhentheyfinishlevel20torestartthegameatlevel1,butwithatokenthatappearsnexttotheiravatarwhichsignalstootherusersthattheyhavefinishedthegameonceover.Theprestigeto-kensthenchangecolor,everytimetheuserreacheslevel20.4.3LeaderboardWeimplementedaliveleaderboardwithinGameQAwhichallowsuserstoseehowtheyare
5https://www.activision.com/
forIcelandic(SnæbjarnarsonandEinarsson,2022).Itwascreatedusingthesameinformation-seekingprocessaswasintroducedwithTyDi.Furthermore,theauthorsspecificallymentionedthattheyex-haustedtheIcelandicWikipedia4whencreatingquestionsforthedataset,therebyhighlightingtheneedtoincludemoredomainsbothforquestionelicitationandanswerannotation.3TheGameQAPlatformOurcrowd-sourcingplatformconsistsofamobileappandwebAPI.ThemobileappwaswritteninReactNative,thewebserverinNode.js,andtheunderlyingdatabaseisMongoDB.WerecruitedusersbysendinganemailtoallstudentsatReykjavikUniversityandbyadvertisingtheapponsocialmediaplatforms.TheappwasdistributedthroughAppleAppStore(iOSversion)andGooglePlayStore(Androidversion),and,inbothcases,onlymadeaccessibleinIceland.Theusersformacommunitywheretheyhelpeachotherfindinganswerstousergeneratedques-tions.Forexample,theappmightaskausertowriteaquestion.Lateron,anotheruserwouldbetaskedwithreviewingit.Onceitpassespeerre-view,athirduserwouldbetaskedwithfindingaspecificparagraphonawebpagecontainingtheanswer,usinganintegratedweb-searchinterface.Lastly,anotheruserwouldverifytheanswer.Thesetasksareservedrandomlyandusersarenotabletoreviewtheirowncontent.3.1UsercentereddevelopmentInthedesignanddevelopmentofGameQA,weapplieduser-centreddesignmethodologythroughiterativedevelopmentandthreeprototypes.Inthefirstiteration,aweb-basedinterfacewasevaluated.Asaresult,intheseconditeration,theinterfacewassimplifiedandgamificationwasadded.Whenevaluatingthesecondversion,userspointedouttheneedforamobilephoneinterface.Inthefi-naliteration,themobileappwasthusdevelopedandevaluated.Involvingusersinthedesignandthedevelopmentoftheapplicationimprovedthefinalresultandtheuserexperience.Byqualita-tivelyanalysingtheuserinterfacepriortolaunch,wewereabletounderstandwhichgamificationfeaturescouldincreaseadoptionandusageoftheapp.
154


6DemonstratedinaYouTubevideo:https://www.youtube.com/watch?v=PmCR7v_KDhQ7https://github.com/cadia-lvl/RUQuAD8https://is.wikipedia.org/wiki/9https://www.visindavefur.is/10https://www.stjornarradid.is/5.2QuestionreviewSinceweseektogatherquestionsbasedontheusers’curiositywithminimalguidanceandinflu-ence,wepurposefullyplacelittlerestrictionsonthenatureofthequestions.Usersareaskedtoratequestionsgiventhefollowingcriteria:ClarityIfitisclearwhattheauthorofthequestionisaskingfor.ConsistencyIftheanswerisunlikelytochangedependingonwhomorwhenyouask.AnswerlengthIfitseemslikethisquestioncouldbeansweredinthreesentencesorless.WechosetoincludetheAnswerLengthcriteriainordertosimplifyotherannotationtaskssuchasanswerreviews.Eachquestionhastopassallofthesecriteriaintwoseparatereviewsperformedbytwoseparateusers.ResearchersseekingtolocalizeGameQAcanmodifythesecriteriaifneeded.5.3WebsearchandparagraphselectionAdistinguishingfeatureofourdatacollectionistheusers’abilitytofindanswersinvariousdiffer-entsourcesanddomainsinsteadofonlylinkingaquestiontoaWikipediaarticle.Whensearch-ingforanarticleonlinecontainingananswertoagivenquestion,theusersformasearchstringthattheybelievewillleadtosuccess,i.e.forwhichananswerwillbefound(seeFigure1).Thisiscarriedoutinverymuchthesamewayasauserofasearchengineperformsawebsearch.Oncetheusersfindawebsitethatcontainstheinformationnecessarytoanswerthequestion,theyselecttheexactparagraphthatcontainstheanswer.Ifannotatorsarenotabletofindananswer,theycanmarkitasunanswerable.5.4AnswerspanmarkingOnceaquestionhasbeenlinkedtoananswerpara-graph,thequestionandtheattachedparagraphisshowntousers.First,theyareaskedwhetherornottheythinkthattheansweriscontainedwithintheparagraph.Iftheuserrespondsintheaffirmative,theyarethentaskedwithselectingthefirstandlastword(thespan)oftheanswer(seeFigure2).How-ever,ifthequestionisaYESorNOquestion,thentheuserwillmarkitassuchwiththerightanswer.
performingrelativetootherusers.Weobservedsig-nificantcompetitivenessamongstsomeusersafteraddingthisfeature.Forexample,someusersspentseveralhoursperdayannotatingdata,inordertoachievethehighestrank.Theavatarofthehighestrankinguserwasgivenacrowntofurtherincentiveuserstocompeteforthehighestrank.Itisworthmentioningthateventhoughuserscompetedtoachievehighrankstheywereinformedthatthedatacollectionwasacollaborativeeffort,forthepurposeofcompilingatrainingcorpusforIcelandicQAmodels.4.4UsernotificationsOnceaquestionhasbeenansweredbythecom-munity,anotificationissenttotheauthorofthequestiontellingthemthattheycanseetheanswer(andwhoansweredit)intheapp.Thisservesasanimportantwayforuserstoseethattheircontribu-tionisimpactingthedatacollection.5DataCollectionStepsIntotal,thedatacollectionconsistsoffivedifferentstagesthateachQApairhastopass:1)questionelicitation,2)questionreview,3)websearchandanswerparagraphselection,4)answerspanmark-ing,and5)answerreview6.Tasksarerandomlyservedtousers–subsequenttasksarethusindepen-dentofoneanother.ForourIcelandicQAdataset,whichwecallRUQuAD(ReykjavikUniversityQuestionAn-sweringDataset)7,wesourcedanswersfromfivesourcesinfourseparatedomains:TheIcelandicWikipedia8,“Vísindavefurinn”(TheIcelandicWebofScience)9,thenewswebsitesmbl.isandvisir.is,and“Stjórnarráðið”(TheIcelandicGov-ernmentInformationwebsite)10.5.1QuestionelicitationUsersareshownanimageandaskedtowriteaquestionthatcomestomind.However,usersarenotconstrainedtoaskaquestionabouttheimageitself.Instead,theimageservesasastimulusforcuriosity.Togatherthesetofimages,wefirstconstructedalistof78broadtopics.Fromthere,wefoundoneimagerelatedtoeachtopic.
155


3ofthose(1,024users)contributedcon-tenttothecreationoftheRUQuADdataset.Bytheend,theyhadgenerated23,036questions,20,730(90%)ofwhichpassedthedoublepeerreview.12,772answerswereannotatedandreviewed,re-sultinginanunanswerableratioof38.4%.Apre-liminaryanalysissuggeststhatapproximately30%ofthequestionsthateitherfailedthepeerrevieworweremarkedasunanswerablemighthavebeenmis-labeledassuch.Asaresult,theunanswerableratemightbecomeconsiderablylowerwithadditionallabelingaftercrowd-sourcingthedata.Thereisaremarkablediversityinthenumberofanswerarticles.7,835articlesweregatheredintotalforthe12,722answers,i.e.1.64answersperarticle.Thisratioisroughly2.05and200forTyDiandSQuAD,respectively.Weexpectthatmoredi-verseanswerparagraphswillhelpamachinelearn-
Figure2:Ascreenshotfromthemobileappdemonstrat-ingtheinterfacefortheAnswerspanmarkingtask.Thequestion(inIcelandic)presentedtotheuseris“ÍhvaðaheimsálfuerPerú”(InwhichcontinentisPeru).Theuserhasmarked“Suður-Ameríku”(SouthAmerica)astheanswer.Roughly2
Figure1:Ascreenshotfromthemobileappdemon-stratingtheinterfacefortheWebsearchandparagraphselectiontask.Thequestion(inIcelandic)presentedtotheuseris“HverdrapFrankenstein”(WhokilledFrankenstein).Theuserhasformedthesearchstring“Frankenstein”,andalistofsearchresultsfromthefivesourcesappearsbelow.Atthetop,theuserscanseetheiravatar,level,positionontheleaderboard,andtheirprogresstowardstheirnextlevel.5.5AnswerreviewThelaststepinthispipelineistheanswerreviewstep.Similartothequestionreviewstep,eachan-swerhastopasstwoseparatereviewsfromtwoseparateusers.Thereviewstepconsistsofasinglequestion,askingusersiftheybelieveananswershowntothemtobecorrectornot.Theusersarenotrequiredtoknowthepreciseanswertotheques-tion,insteadtheyusetheirreadingcomprehensionskillsandjudgementtodetermineiftheanswerseemscorrect.6ResultsandDataAnalysisThroughoutourQAcollectionprocessforIcelandicusingGameQA,1,524userscreatedanaccount.
156


Figure3:Taskscompletedperuserfollowaparetodis-tributionwhereaminorityofthe1,524userscontributedamajorityofthecontent.6.2UnanswerableQuestionsAsmentionedinSection2,aparticularproblemwithinformation-seekingmethodsforlow-resourcelanguagesisthehighratioofunanswerableques-tions–thiscanbeobservedinTable1.IcelandichasfewerWikipediaarticles(54,121)thanalllanguagesinTyDi.Yet,byleveragingmul-tipleanswersourceswithGameQA,weachievedanunanswerablerateof38%whichislowerthanalllanguagesinTyDi,exceptArabic.6.3SpanlengthdistributionOutofthefiveannotationsteps,markinganswerspansisthestepthatrequiresthehighestdegreeofstandardizationofannotation.Withoutsuchstan-dards(orpreciseguidelines)andawayofenforcingthem,theannotatorswillnotmarkanswerspansinaconsistentmanner.
ingmodel,trainedonthedata,togeneralizebetter.Thedistributionofarticlesoverthesourcesisasfollows:68.3%camefromtheIcelandicWikipedia,18.4%fromTheIcelandicWebofScience,13.1%fromthetwonewswebsites,and0.2%fromtheGovernmentInformationwebsite.6.1UnderstandingusercontributionsSincethecrowd-workersweren’tpaid,butratherusersplayingagameintheirownfreetime,thestrengthofeachuserscontributionwasmostlyim-pactedbythetimetheywerewillingtospendontheapp.Intotal,theusersperformed137,972an-notationtasks(elicitquestions,reviewquestions,findanswers,labelanswers,reviewanswers).AsFigure3shows,theamountofworkperformedperuserfollowsaparetodistribution.
Russian1,816,91651%Japanese1,324,30432%Arabic1,165,57569%Indonesian620,86334%Korean587,57322%Finnish530,42041%Thai147,37843%Bengali122,04135%Telugu76,25927%Kiswahili71,57022%
LanguageNumberofRatioofquestionsWikipediawithanarticlesanswerspan
Figure4:Acomparisonoftheminimumanswerspanwiththeannotatedandpredictedspans.Theunderlyingsubsentenceis:“hófstsnemmaaðmorgni4.apríl2010ogstóðtil23.maísamaár”(startedearlyinthemorningonApril4,2010andlasteduntilMay23ofthesameyear).Figure4showsacomparisonoftheminimumanswerspan(neededtoansweraquestion)withthespanannotatedbyauserandthespanpredictedbytheIceBERTmodel(Snæbjarnarsonetal.,2022),fine-tunedonourdataset.Weexpectthisdiscrep-ancybetweengroundtruthlabelsandthepredictiontobearesultoflackofalignmentamongstannota-torswhenmarkinganswerspans.WeproposethatresearchersthatlocalizeGameQAstandardizeandshortentheanswerspanswhereneeded,oncethecrowd-sourcinghasconcluded.Table2showssummarystatisticsfortheanswerspanlengthsforthreedifferentdatasets.HighervarianceinanswerspanlengthsinRUQuADistobeexpectedsinceenforcingannotationstandardsacrossthousandsofcrowd-workersisnon-trivial.
Table1:Acomparisonoftheratioofquestions,whichhadananswerspan,withthenumberofWikipediaarticles,foreachofthe10non-EnglishlanguagesintheTyDidataset.ThePearsoncorrelationcoefficientisp=0.54.
157


Table2:Summarystatisticsofanswerspanlengths(charactercount)6.4Cost-effectivedatacollectionAclearadvantageoflocalizingGameQAfordatacollectionforotherlanguagesisthepossibilityofgatheringQAdatainacost-effectivemanner.Bygamifyingthedatacollection,wewereabletocre-atealarge-scaleQAdataset,gatheredbythousandsofcrowd-workerswithouttheneedofhiring,train-ing,andmanagingannotators.ThemajorityofthecostweincurredwithGameQAwasthecostofdevelopingtheplatform.Bymakingthecodeopensource,wehopetoenableresearchersaroundtheworldtogathercost-effectivelarge-scalemultiple-domainQAdataforlow-resourcelanguages.6.5CulturalrelevanceAsaresultofhavingthousandsofannotators,weobserveaconsiderablediversityintermsoftherangeoftopicsusersaskedabout.Furthermore,wenoticethatourproposedmethodisabletogatherquestionswhicharerepresentativeofthelocalhis-toryandculture.InordertobuildQAsystems,researchersforlow-resourcelanguagesmightbetemptedtotranslatelargeEnglishdatasets.How-ever,translationofEnglishdatasetswillnotpro-ducequestionsrelevanttolocalculture,history,ge-ography,etc.Outof100questionsfromourdataset,sampleduniformlyatrandom,33weredirectlyaskingaboutlocal(Icelandic)culture,history,orgeography.Thisemphasisonculture-relatedques-tionscanpossiblybeattributedtosomeextenttotheimagesusedinthepromptingstep,butitalsohighlightshowtherightcombinationofannotatorsandpromptscanleadtogreaterculturefocusintheresultingdata.7ConclusionInthispaper,wehavepresentedGameQA–anovelmobiletriviagameplatformforcollectingQAdataforlow-resourcelanguages.Wesuccessfullygam-ifiedtheexperiencetoincreasethenumberofan-notationstasksperformedperuserandconducteddifferentiterationsofuserexperiencetesting.TheQAdatagatheredbyGameQA’susersisculturallyrelevantforthelanguageand/orgeographicalre-gioninquestion.WehavemadeGameQAopensource,withinstructionsonhowtolocalizeandsubsequentlyreleaseitforparticulargeographicalareas.Webelievethatourplatformcanhelptore-ducethecostandtimeassociatedwithcollectingQAdataforlow-resourcelanguages.Ourmethodopensupnewareasofresearche.g.comparingdif-ferentpromptingmethods,suchasimagevs.textprompts,aswellaspossibleadvancementsforQAresearchinlanguageswheretraditionalmethodsmightfailtogatheralarge-scaleQAdataset.Giventhesuccessofthegamificationforcollect-ingQAdata,weproposethatgamifiedcrowdsourc-ingcanbeleveragedtogatherdataforotherNLPtasksaswell.ForanappsuchasGameQA,thereisatargetuserbasethatisinterestedintrivaandknowledgeandthuswillingtoannotatedatainthismanner.Similarly,forotherNLPtasks,suchasma-chinetranslation,thereexistsapotentialuserbaseofmultilingualpersonsthataregreatlyinterestedinlanguagesandtranslation.Weseegreatpotentialinapplyingtheknowledgelearnedthroughimple-mentingGameQAforsuchtasks.LimitationsThequestionelicitationpartofGameQAisdiffer-entfrompriorwork.Animageisshowntotheuserinsteadofatextualprompttoinspireques-tions.Itisunclearwhateffectthatdecisionhasonthechancesofthequestionbeinganswerablesinceuserscouldmakemoreorlesschallengingques-tionswhenpromptedwithimagesinsteadoftext.Furthermore,thesourcefromwhichtheimagesaretakencouldfurtherinfluenceanswerability.Futureworkwillneedtorevealthedifferencebetweenpromptingwithtextorimages.InGameQA,theuserisresponsibleforfindingthearticlethatcouldcontainananswertoagivenquestion.Thisstepwasautomatedinpriorworkbyselectingtopsearchengineresults.Thisapproachgivestheusermorefreedomwhenlookingfortheanswer.Still,itcouldalsolimittheirabilitytofindanswerssincetheyareresponsibleforperformingGooglesearchqueriesthemselves.Althoughananswermightexist,theirqueriesmightnotsufficetoidentifyrelevantcandidatepages.Furthermore,itislikelythatsomeoftheques-tionsaskedareambiguous,i.e.thatforagiven
DatasetSpanLengthStandardDeviation
SQuAD19.7520.73TyDi25.7746.12RUQuAD75.6491.52
158


questionmorethanonecorrectanswerispossi-ble.Insuchcases,arewriteofthequestion,forthepurposeofclarifyingitsinterpretation,mightbebeneficial(Minetal.,2020).InGameQA,thiswouldrequireanadditionaltaskinthequestionreviewstep(seeSection5.2).EthicsStatementThedatacollectionprocessinGameQAcon-sistsofcollectingparagraphs,fromasetofsources/domains(seeSection5),inwhichanswerscanbefoundtogivenquestions.BeforestartingourRUQuADcorpuscollectionprocess,weob-tainedformalpermissionsfromTheIcelandicWebofScience,thenewscitesmbl.isandvisir.is,andtheIcelandicGovernmentInformationweb-site,tofreelyincludeparagraphsfromtheirsourcesinourcorpus.Forthelastdomain,theIcelandicWikipedia,formalpermissionwasnotneededbe-causeitsmaterialisalreadyfreelylicensed.Asapartofthedatacollection,wedidnotcol-lectanyinformationabouttheusersasidefromtheiremailaddresswhichwasnecessarytoverifyanaccountafterregistration.ThedatacollectionwasGDPRcompliantandweofferedtoremoveanyannotationsordatapointsbelongingtoausersshouldtheyrequestthat.However,nousermadesucharequest.AsdiscussedinSection4,GameQAisagameopentoanyuserinaparticulargeographicareaanddoesnotcompensatecrowd-workersfinancially.AcknowledgementsWethankTheIcelandicWebofScience,mbl.is,visir.is,andtheGovernmentInformationweb-siteforallowingustoincludeparagraphs,fromarticlesontheirwebsites,inourRuQuADcorpus.ReferencesPayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,RanganMajumder,An-drewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,AlinaStoica,SaurabhTiwary,andTongWang.2016.MSMARCO:AHumanGen-eratedMAchineReadingCOmprehensionDataset.arXiv:1611.09268.B.BarlaCambazoglu,MarkSanderson,FalkScholer,andBruceCroft.2021.AReviewofPublicDatasetsinQuestionAnsweringResearch.SIGIRForum,54(2).JonathanH.Clark,EunsolChoi,MichaelCollins,DanGarrette,TomKwiatkowski,VitalyNikolaev,andJennimariaPalomaki.2020.TyDiQA:Abenchmarkforinformation-seekingquestionansweringintypo-logicallydiverselanguages.TransactionsoftheAs-sociationforComputationalLinguistics,8:454–470.MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyunCho.2017.SearchQA:ANewQ&ADatasetAugmentedwithContextfromaSearchEngine.arXiv:1704.05179.WeeChungGanandHweeTouNg.2019.Improv-ingtherobustnessofquestionansweringsystemstoquestionparaphrasing.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages6065–6075,Florence,Italy.Asso-ciationforComputationalLinguistics.DeepakGupta,SurabhiKumari,AsifEkbal,andPush-pakBhattacharyya.2018.MMQA:Amulti-domainmulti-lingualquestion-answeringframeworkforEn-glishandHindi.InProceedingsoftheEleventhIn-ternationalConferenceonLanguageResourcesandEvaluation(LREC2018),Miyazaki,Japan.EuropeanLanguageResourcesAssociation(ELRA).KarlMoritzHermann,TomášKoˇciský,EdwardGrefen-stette,LasseEspeholt,WillKay,MustafaSuleyman,andPhilBlunsom.2015.Teachingmachinestoreadandcomprehend.InProceedingsofthe28thInterna-tionalConferenceonNeuralInformationProcessingSystems-Volume1,NIPS’15,page1693–1701,Cam-bridge,MA,USA.MITPress.TomášKoˇciský,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,GáborMelis,andEd-wardGrefenstette.2018.TheNarrativeQAreadingcomprehensionchallenge.TransactionsoftheAsso-ciationforComputationalLinguistics,6:317–328.TomKwiatkowski,JennimariaPalomaki,OliviaRed-field,MichaelCollins,AnkurParikh,ChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-tonLee,KristinaToutanova,LlionJones,MatthewKelcey,Ming-WeiChang,AndrewM.Dai,JakobUszkoreit,QuocLe,andSlavPetrov.2019.Natu-ralquestions:Abenchmarkforquestionansweringresearch.TransactionsoftheAssociationforCompu-tationalLinguistics,7:452–466.SewonMin,JulianMichael,HannanehHajishirzi,andLukeZettlemoyer.2020.AmbigQA:Answeringam-biguousopen-domainquestions.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNat-uralLanguageProcessing(EMNLP),pages5783–5797,Online.AssociationforComputationalLin-guistics.AnnaBjörkNikulásdóttir,ÞórunnArnardóttir,StarkaðurBarkarson,JónGuðnason,ÞorsteinnDaðiGunnarsson,AntonKarlIngason,HaukurPállJónsson,HrafnLoftsson,HuldaÓladóttir,EiríkurRögnvaldsson,EinarFreyrSigurðssona,AtliÞórSigurgeirsson,VésteinnSnæbjarnarson,Steinþór
159


Steingrímsson,andGunnarThorÖrnólfsson.2022.HelpYourselffromtheBuffet:NationalLanguageTechnologyInfrastructureInitiativeonCLARIN-IS.InSelectedPapersfromtheCLARINAnnualConference2021,LinköpingElectronicConferenceProceedings189.PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.2016.SQuAD:100,000+questionsformachinecomprehensionoftext.InProceedingsofthe2016ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages2383–2392,Austin,Texas.AssociationforComputationalLinguistics.SivaReddy,DanqiChen,andChristopherD.Manning.2019.CoQA:Aconversationalquestionansweringchallenge.TransactionsoftheAssociationforCom-putationalLinguistics,7:249–266.MarcoTulioRibeiro,CarlosGuestrin,andSameerSingh.2019.Areredrosesred?evaluatingconsis-tencyofquestion-answeringmodels.InProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages6174–6184,Flo-rence,Italy.AssociationforComputationalLinguis-tics.AnnaRogers,OlgaKovaleva,MatthewDowney,andAnnaRumshisky.2020.GettingClosertoAICom-pleteQuestionAnswering:ASetofPrerequisiteRealTasks.ProceedingsoftheAAAIConferenceonArtifi-cialIntelligence,34(05):8722–8731.KazutoshiShinoda,SakuSugawara,andAkikoAizawa.2021.Canquestiongenerationdebiasquestionan-sweringmodels?acasestudyonquestion–contextlexicaloverlap.InProceedingsofthe3rdWorkshoponMachineReadingforQuestionAnswering,pages63–72,PuntaCana,DominicanRepublic.Associa-tionforComputationalLinguistics.VésteinnSnæbjarnarsonandHafsteinnEinarsson.2022.NaturalQuestionsinIcelandic.InProceedingsoftheThirteenthLanguageResourcesandEvaluationConference,pages4488–4496,Marseille,France.Eu-ropeanLanguageResourcesAssociation.VésteinnSnæbjarnarson,HaukurBarriSímonarson,PéturOrriRagnarsson,SvanhvítLiljaIngólfsdót-tir,HaukurJónsson,VilhjalmurThorsteinsson,andHafsteinnEinarsson.2022.AWarmStartandaCleanCrawledCorpus-ARecipeforGoodLan-guageModels.InProceedingsoftheThirteenthLan-guageResourcesandEvaluationConference,pages4356–4366,Marseille,France.EuropeanLanguageResourcesAssociation.AdamTrischler,TongWang,XingdiYuan,JustinHar-ris,AlessandroSordoni,PhilipBachman,andKaheerSuleman.2016.NewsQA:AMachineComprehen-sionDataset.arXiv:1611.09830.YiYang,Wen-tauYih,andChristopherMeek.2015.WikiQA:Achallengedatasetforopen-domainques-tionanswering.InProceedingsofthe2015Con-ferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2013–2018,Lisbon,Portugal.As-sociationforComputationalLinguistics.
160