3 2 0 2
b e F 7 2
] L C . s c [
1 v 2 3 1 4 1 . 2 0 3 2 : v i X r a
STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS FOR SPEECH RECOGNITION AND UNDERSTANDING
Yifan Peng2∗, Kwangyoun Kim1, Felix Wu1, Prashant Sridhar1, Shinji Watanabe2
1ASAPP Inc., Mountain View, CA, USA 2Carnegie Mellon University, Pittsburgh, PA, USA {yifanpen,swatanab}@andrew.cmu.edu {kkim,fwu,psridhar}@asapp.com
ABSTRACT
Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in ac- curacy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we pro- pose three task-speciﬁc structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.
Index Terms— Structured pruning, self-supervised models,
speech recognition, spoken language understanding
1. INTRODUCTION
Self-supervised speech representation learning (SSL) has achieved great success in a variety of speech processing tasks [1, 2, 3, 4, 5, 6, 7]. However, SSL pre-trained models (e.g., wav2vec2 [8], Hu- BERT [9] and WavLM [10]) usually require large memory and high computational cost. Hence, it is difﬁcult to deploy them in real- world applications. Recent studies have utilized model compression techniques to reduce the model size and computation without degra- dation in accuracy. One common approach is knowledge distilla- tion [11], which trains a small student model with a pre-speciﬁed ar- chitecture to match the soft targets generated by a large pre-trained model. Distillation has shown to be effective in natural language processing (NLP) [12, 13] and speech processing [14, 15, 16, 17], but it usually performs general distillation using large amounts of unlabeled data before task-speciﬁc distillation or ﬁne-tuning. This can make the training procedure computationally expensive.
Another compression technique is pruning, which extracts a compact and accurate subnetwork from the original model. Pruning has been widely used in computer vision (CV) [18, 19, 20, 21] and NLP [21, 22, 23, 24]. For speech models, [25, 26] prune recurrent neural networks (RNNs) for resource-limited applications. Another work [27] prunes deep neural networks (DNNs) based speech en- hancement models using the sparse group lasso regularization [28]. These studies do not consider SSL models. PARP [29] is one of the ﬁrst pruning methods designed for SSL speech models, which prunes individual weights based on magnitudes. Despite its good performance in low-resource automatic speech recognition (ASR),
∗Work done during an internship at ASAPP.
PARP is a type of unstructured pruning and thus cannot achieve an actual speedup without an efﬁcient sparse matrix computation li- brary, which is not usually available in many deployment scenarios. Another limitation is that PARP only prunes the Transformer layers while keeping the convolutional feature extractor (CNN) ﬁxed. As discussed in [30], although the CNN has much fewer parameters than the Transformer, its computational cost is large and cannot simply be ignored. For example, in wav2vec2-base, the CNN has less than 5% of the total parameters but its computational cost is nearly 33% in terms of multiply-accumulate (MAC) operations for a 10-second audio.
In this work, we propose HJ-Pruning (Heterogeneous Joint Pruning) where both CNN and Transformer components are pruned jointly. We consider three variants: (a) HJ-Pruning based on the overall model size sets a single sparsity for the entire model size. (b) HJ-Pruning based on separate model sizes introduces a sepa- rate sparsity hyperparameter for each component which allows ﬁne- grained tuning to ﬁnd a trade-off between CNN and Transformer. (c) HJ-Pruning based on the overall MACs uses multiply–accumulate (MAC) operations as the sparsity criterion to ﬁnd the best allocation of the computation budget across different components. We evaluate our methods in the ASR and spoken language understanding (SLU) tasks. Experiments on LibriSpeech and SLURP show that all HJ- Pruning methods outperform the previous Transformer-only pruning strategy. Our HJ-Pruning-MAC is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.
2. BACKGROUND
2.1. Self-supervised pre-trained speech models
We evaluate the pruning algorithms mainly using wav2vec2 [8], but our proposed methods can be easily applied to other SSL models with as a similar architecture such as HuBERT [9] (see Sec. 4.5), SEW-D [30], and WavLM [10]. The wav2vec2-base model (pre- trained on Librispeech 960h [31]) consists of a convolutional feature extractor (CNN) and a Transformer [32] encoder. The CNN con- tains seven temporal convolutions with 512 channels and GeLU [33] activations. The Transformer encoder is a stack of 12 Transformer layers with a hidden dimension of 768 and 12 attention heads.
2.2. Structured pruning using L0 regularization
We follow [22, 23, 34] to formulate the structured pruning task as a regularized learning problem, which aims to learn a sparse model. Let f (·; θ) be a model with parameters θ = {θj }n j=1, where each θj is a group of parameters (e.g., an attention head) and n is the num- ber of groups. The pruning decisions are given by a set of binary


variables called gates: z = {zj }n j=1 where zj ∈ {0, 1}. The model parameters after pruning are ˜θ = {˜θj}n j=1 such that ˜θj = θj zj. We usually sample gates from some distributions (e.g., Bernoulli) and update their parameters during training. Suppose the gates follow a distribution q(z; α) with parameters α = {αj }n j=1, then our train- ing objective is:
min θ,α
Ez∼q
"
1 D
D
i=1 X
L(f (xi; ˜θ), yi) + λk˜θk0
#
,
where {(xi, yi)}D i=1 is the training data containing D samples, L is the training loss (i.e., CTC loss for ASR, cross entropy loss for SLU), and λ > 0 is a hyperparameter to control the sparsity. However, it is intractable to optimize Eq. (1) using gradient descent because the gates are discrete. Louizos et al. [34] propose a reparameterization trick to make the loss differentiable, which has been widely used in sparse model learning. Here we only introduce their ﬁnal approach. Please refer to [34] for the derivation. Louizos et al. adopt the Hard Concrete Distribution [34] to model the gates z:
1 β
u 1 − u
+ log α
u ∼ U(0, 1), v(α) = σ
log
(cid:18) (cid:19)(cid:19) ¯v(α) = (r − l) · v(α) + l, z = min(1, max(0, ¯v(α))),
(cid:18)
where U(0, 1) is a uniform distribution over the interval [0, 1], σ(·) is the sigmoid function and β is a temperature constant. The actual parameters are α. l < 0 and r > 0 are two constants to stretch the output of sigmoid to [l, r], which is ﬁnally rectiﬁed to [0, 1]. It is proven that the ﬁrst term in Eq. (1) now becomes differentiable w.r.t. all parameters. We can write the second term in a closed-form expression based on the distribution of z shown in Eq. (2):
Ez
k˜θk0 h
i
=
n
j=1 X
P (zj 6= 0) =
n
j=1 X
σ
(cid:18)
log αj − β log
−l r
which is also differentiable. P (·) denotes the probability.
Now we can train a sparse model using Eq. (1). However, it is difﬁcult to exactly control the pruned model size [22, 23]. Instead of adding a regularizer λk˜θk0, prior studies [22, 23] suggest optimizing the training loss subject to an explicit equality constraint on sparsity:
D
1 D
L(f (xi; ˜θ), yi)
Ez
s.t. s(α) = t,
min θ,α
#
"
i=1 X
where s(α) is the current sparsity and t is a pre-speciﬁed target spar- sity. The sparsity is deﬁned as the percentage of parameters that are pruned. Similar to Eq. (3), given the current parameters α, we can calculate the expected number of nonzero gates in every module of the model. Recall that each gate is associated with a group of pa- rameters. Hence, we know the expected number of parameters that are still kept, which further gives us the sparsity s(α). Eq. (4) can be rewritten as an adversarial game according to the augmented La- grangian method [22]:
D
1 D
L(f (xi; ˜θ), yi)
Ez
+ g(λ, α),
max λ
min θ,α
"
#
i=1 X
2
g(λ, α) = λ1(s(α) − t) + λ2(s(α) − t)
(6) where λ1, λ2 ∈ R are two Lagrange multipliers that are jointly trained with other parameters. Once the game reaches equilibrium, the equality constraint will be satisﬁed. Hence, we can precisely control the sparsity of the pruned model. To facilitate training, we linearly increase the target sparsity t from zero to the desired value.
,
,
(cid:19)
(1)
(2)
,
(3)
(4)
(5)
2.3. Structured pruning of Transformer layers
A Transformer [32] layer consists of a multi-head self-attention (MHA) block and a position-wise feed-forward network (FFN). We consider three pruning units, i.e., attention heads (12 per layer), intermediate size of FFN (3072 per layer), and the model’s hidden size (768). We deﬁne a gate for each pruning unit. Given an input sequence X ∈ RT ×d of length T and feature size d, the MHA and FFN at a particular layer are the following:
h
MHA(X) =
(zhead k
ATT(X; Watt k )),
k=1 X FFN(X) = GeLU(XWffn
1 ) · diag(zint) · Wffn 2 ,
where ATT(·; Watt k ) denotes the k-th attention head parameterized by Watt k , and zhead is a scalar gate. There are h heads in total. zint is a dint-dimensional gate for the FFN intermediate size. diag(·) creates a diagonal matrix with its argument vector on the diago- nal. GeLU is an activation [33]. FFN has two linear layers Wffn 1 ∈ Rd×dint 2 ∈ Rdint×d. Each Transformer layer has its own gates and their parameters are independent. For the main hidden size, we deﬁne a gate zhid of size d and share it across layers as in [23].
k
, Wffn
3. PROPOSED METHODS
3.1. Joint pruning based on the model size
As introduced in Sec. 1, the convolutional feature extractor (CNN) in SSL models is small in size but heavy in computation. To optimize the overall computation, we propose to jointly prune the CNN and Transformer. We have introduced the pruning units for Transformer in Sec. 2.3. For CNN, we prune convolution channels by introduc- ing gate variables for every channel in every CNN layer, i.e., each output channel is multiplied with a gate. To train the model using Eq. (5), we need to deﬁne the model sparsity s(α). Our ﬁrst pro- posed method is HJ-Pruning-Size (HJ-Pruning based on the overall model size), which can be viewed as a direct extension from prior work [22, 23]. Speciﬁcally, given the current distribution parame- ters α, we can calculate the probability of each gate being nonzero (i.e., the corresponding module is kept) as in Eq. (3). We then know the current sizes of all modules, including the model’s hidden size, CNN channels, attention heads, and FFN intermediate sizes. Based on these sizes, we can compute the percentage of parameters that are pruned, which is the overall size sparsity sall
size(α).
However, Sec. 4.2 shows that this approach does not work well in practice, because the CNN has much fewer parameters than the Transformer. If we simply set an overall sparsity, parameters will be pruned mainly from Transformer. To solve this problem, we propose the second method, i.e., HJ-Pruning-SepSize (HJ-Pruning based on separate model sizes). We calculate the size sparsity separately for size (α)). We also specify sepa- CNN (scnn rate target sparsities tcnn size and extend Eq. (6) to have two terms:
size(α)) and Transformer (strans
size, ttrans
2
gsize = λcnn + λtrans 1
size) + λcnn size ) + λtrans
size(α) − tcnn 1 (scnn size (α) − ttrans (strans
2 (scnn
size(α) − tcnn size) (strans
size (α) − ttrans size )
2
2
.
As shown in Sec. 4.2, this method achieves strong performance. However, it requires careful tuning of the separate target sparsities. We always need to search over the two sparsities to meet a particular budget, which is computationally expensive.
(7)
(8)
(9)


3.2. Joint pruning based on the MACs
The third method we propose is HJ-Pruning-MAC (HJ-Pruning based on the overall MACs). Unlike prior methods, we prune the entire model to directly meet a computational budget measured by MACs. We follow the formulas used in the DeepSpeed ﬂops proﬁler to calculate MACs. 1 For an input sequence of length T and hidden size d, the MACs for each MHA and FFN block are as follows:
MACmha = 4T hddhead + 2T 2hdhead, MACffn = 2T ddint,
where h is the number of attention heads and dhead is the size per head. dint denotes the intermediate size of FFN. The MACs of a 1-D convolution can be computed by
MACcnn = T outC outC inK,
where T out is the output length and K is the kernel size. C in and C out are the input and output channels, respectively. Note that h, d, dint, C in, C out are calculated from the current gate distributions (similar to Eq. (3)). They are differentiable functions of α. We deﬁne the percentage of MACs that are pruned as the MACs-based macs(α). 2 It is differentiable w.r.t. parameters α. Hence, sparsity sall we can still train the model using Eq. (5).
4. EXPERIMENTS
4.1. Experimental setup
We focus on task-speciﬁc structured pruning of SSL speech models. We mainly prune wav2vec2-base, but we also show that our methods can be directly applied to HuBERT-base in Sec. 4.5. We conduct ex- periments using PyTorch [35] and HuggingFace’s transformers [36]. Our implementation of the basic pruning algorithm is based on prior work in NLP [23]. For each task, we add a linear layer on top of the pre-trained SSL model and ﬁne-tune the entire model to obtain an unpruned model. Then, we prune this ﬁne-tuned model to reach a speciﬁc sparsity using Eq. (5). We employ an AdamW optimizer and a linear learning rate scheduler for all experiments.
ASR: The 100-hour clean set of LibriSpeech [31] is utilized. In Sec. 4.3, the Tedlium [37] test set is used as out-of-domain data to demonstrate the robustness of structured pruning. The training loss is CTC [38]. We ﬁne-tune a pre-trained model for 25 epochs and prune for 30 epochs with a learning rate of 1.5e-4 and a batch size of 64. The target sparsity is linearly increased to the desired value during the ﬁrst 5 epochs. The learning rate of α and λ is selected from {0.02, 0.05}. The pruned model is ﬁne-tuned for another 10 epochs with a learning rate of 5e-5. The learning rate warmup steps are 3k, 3k, and 1k for training, pruning, and ﬁne-tuning, respectively. SLU: The SLURP corpus [39] is used for intent classiﬁcation. A pre-trained SSL model is ﬁne-tuned for 50 epochs and pruned for 50 epochs with a learning rate of 1e-4 and a batch size of 80. The ﬁnal ﬁne-tuning has 10 epochs with a learning rate of 1e-5. The learning rate warmup is performed for 4k, 4k, 1k steps for training, pruning, and ﬁne-tuning, respectively. Other conﬁgs are the same as ASR.
1https://github.com/microsoft/DeepSpeed 2The computation of MACs also depends on the sequence length T , be- cause MHA has quadratic complexity w.r.t. T . We use 10 seconds to compute MACs in our experiments. This is a “virtual” length used only for computing MACs. We do not modify any training utterances.
(10)
(11)
(12)
) ↓ (
e t a R
r o r r E d r o W %
11
9
7
Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC
5.77
20
30
40 MACs (×109)
50
60
70
Fig. 1: Word Error Rate (%) vs. Multiply-Accumulate Operations on LibriSpeech test-clean. Our proposed HJ-Pruning methods con- sistently outperform the baseline.
86.1 85
) ↑ (
y c a r u c c A %
80
Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC
75
10
20
30
50 MACs (×109)
40
60
70
Fig. 2: Intent Classiﬁcation Accuracy (%) vs. Multiply-Accumulate Operations on the SLURP test set. Our proposed HJ-Pruning meth- ods consistently outperform the baseline.
4.2. Main results
Fig. 1 compares various pruning methods for LibriSpeech ASR. The unpruned model has good performance (5.77% WER) but is computationally expensive (74.4 GMACs). At a low sparsity (>55 GMACs), all pruned models achieve similar WERs which are even better than the original result, because the pruning target can regu- larize the training. As the sparsity increases, the baseline method which only prunes Transformer drastically degrades. Our proposed three algorithms which jointly prune CNN and Transformer consis- tently outperform the baseline by a large margin. We can reduce over 40% of the total computation without degradation in WER. HJ- Pruning-MAC has similar performance with HJ-Pruning-SepSize, both outperforming HJ-Pruning-Size. This is because the CNN has much fewer parameters than Transformer. If we simply set an overall size sparsity, the pruned parameters are mainly from Transformer, while CNN still has high computational overhead. To prune them based on separate sizes (Eq. (9)), we have to search for the best com- bination of the two target sparsities. This model selection procedure is presented in Fig. 3a, where we perform a grid search and select the Pareto frontiers. This requires much more computation than the other methods. Hence, the HJ-Pruning-MAC is probably the best method in terms of performance and complexity.
Fig. 2 shows the results of intent classiﬁcation on SLURP. The overall trend is very similar to that of ASR. Our joint pruning meth- ods outperform the baseline by a large margin, especially at a high sparsity (low MACs). HJ-Pruning-SepSize is comparable with HJ- Pruning-MAC, but again, it requires a grid search over the two target sparsities as shown in Fig. 3b. This high complexity hinders its usage in practice. Compared to ASR, we can achieve a higher compression rate (over 55%) without loss in accuracy. This is probably because


) ↓ (
e t a R
r o r r E d r o W %
7
6.5
6
5.5
Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Selected Points
30
40
50 MACs (×109)
60
70
(a) Word Error Rates (%) on LibriSpeech test-clean.
87
) ↑ (
86
y c a r u c c A %
85
84
Unpruned wav2vec2-base Transformer Sparsity = 0.1 Transformer Sparsity = 0.2 Transformer Sparsity = 0.3 Transformer Sparsity = 0.4 Transformer Sparsity = 0.5 Transformer Sparsity = 0.6 Selected Points
30
40
50 MACs (×109)
60
70
(b) Intent Classiﬁcation Accuracy (%) on the SLURP test set. Fig. 3: Model selection for HJ-Pruning-SepSize. As described in Sec. 3.1, we perform grid search over the Transformer sparsity (0.1 to 0.4/0.6) and CNN sparsity (0.1 to 0.95). The Pareto frontiers are shown in blue, which are also presented in Fig. 1 and Fig. 2.
26
) ↓ (
e t a R
r o r r E d r o W %
24
22
20
Unpruned wav2vec2-base Pruning Transformer Only HJ-Pruning-Size HJ-Pruning-SepSize HJ-Pruning-MAC
18
20
30
40
50 MACs (×109)
60
70
Fig. 4: Robustness analysis. All models are trained on LibriSpeech 100h and then directly evaluated on the out-of-domain Tedlium test set. The trend is similar to that of the in-domain evaluation in Fig. 1.
the classiﬁcation task is easier and thus requires less information than the sequence-to-sequence task.
4.3. Robustness of structured pruning
To investigate the robustness of the proposed structured pruning methods, we test the ASR models using an out-of-domain corpus, TED-LIUM [37]. Note that these models are trained only with Lib- riSpeech data. As shown in Fig. 4, again, our joint pruning methods consistently outperform the baseline, and the trend is very similar to that of the in-domain evaluation (see Fig. 1). This demonstrates that our pruning methods are robust.
4.4. Architectures of pruned models
Fig. 5 shows the remaining CNN channels, attention heads and FFN intermediate sizes after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%. For CNN, the sequence length gradually de- creases due to downsampling. The ﬁrst few layers have higher com- putational cost, so they tend to be pruned more. For MHA and FFN,
10% 20% 30% 40%
s l e n n a h C
500 400 300 200
s d a e H
12 8 4 0
1
2
3
4 CNN Layer
5
6
7
1
2
3
4
5
6
7
8
9
10
11
MHA Layer
s e z i S
.
m r e t n I
3,000 2,000 1,000
1
2
3
4
5
6
7
8
9
10
11
FFN Layer
Fig. 5: ASR model architectures after HJ-Pruning-MAC. The target sparsity ranges from 10% to 40%.
) ↓ (
e t a R
r o r r E d r o W %
13
11
9
7 5.73
Unpruned HuBERT-base DistilHuBERT∗ LightHuBERT-small∗ LightHuBERT-base∗ HJ-Pruning-MAC
20
30
40 MACs (×109)
50
60
70
Fig. 6: Results of pruning HuBERT-base on LibriSpeech test-clean. ∗ WERs from SUPERB [1]. See Sec. 4.5 for discussions.
the upper layers are pruned the most, indicating that upper layers are more redundant. Prior studies had similar observations by analyz- ing the self-attention patterns in speech encoders [40, 41, 42]. The overall trend is also consistent with a prior work in NLP [23].
4.5. Comparison with other compression methods
As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and ﬁne-tune the entire model (same as our set- ting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then ﬁne-tuned on the 100h labeled data, but our task- speciﬁc pruning only utilizes the 100h data. This comparison shows that our task-speciﬁc pruning method is highly effective.
5. CONCLUSION
In this paper, we propose HJ-Pruning to jointly prune heteroge- neous components of SSL speech models, which achieves strong performance-efﬁciency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5].
12
12


6. REFERENCES
[1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. In- terspeech, 2021.
[2] A. Mohamed, H.-y. Lee, L. Borgholt, et al.,
“Self- Supervised Speech Representation Learning: A Review,” arXiv:2205.10643, 2022.
[3] X. Chang, T. Maekaku, P. Guo, et al., “An exploration of self- supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021.
[4] Z. Huang, S. Watanabe, S.-w. Yang, et al.,
“Investigating Self-Supervised Learning for Speech Enhancement and Sep- aration,” in Proc. ICASSP, 2022.
[5] S. Shon, A. Pasad, F. Wu, et al., “SLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Nat- ural Speech,” in Proc. ICASSP, 2022.
[6] S. Arora, S. Dalmia, P. Denisov, et al., “ESPnet-SLU: Ad- vancing Spoken Language Understanding Through ESPnet,” in Proc. ICASSP, 2022.
[7] Y. Peng, S. Arora, Y. Higuchi, et al., “A Study on the Integra- tion of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding,” in Proc. SLT, 2022.
[8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech repre- sentations,” in Proc. NeurIPS, 2020.
[9] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “HuBERT: Self- supervised speech representation learning by masked predic- tion of hidden units,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021.
[10] S. Chen, C. Wang, Z. Chen, et al., “WavLM: Large-scale self- supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [11] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling the knowl-
edge in a neural network,” arXiv:1503.02531, 2015.
[12] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,” arXiv:1910.01108, 2019.
[13] X. Jiao, Y. Yin, et al., “TinyBERT: Distilling BERT for natural language understanding,” in Findings of EMNLP, 2020. [14] Z. Peng, A. Budhkar, I. Tuil, et al., “Shrinking bigfoot: Reduc-
ing wav2vec 2.0 footprint,” in SustaiNLP, 2021.
[15] H.-J. Chang, S.-w. Yang, and H.-y. Lee,
“DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,” in Proc. ICASSP, 2022.
[16] R. Wang, Q. Bai, J. Ao, et al., “LightHuBERT: Lightweight and Conﬁgurable Speech Representation Learning with Once- for-All Hidden-Unit BERT,” in Proc. Interspeech, 2022. [17] Y. Lee, K. Jang, J. Goo, et al., “FitHuBERT: Going Thin- ner and Deeper for Knowledge Distillation of Speech Self- Supervised Models,” in Proc. Interspeech, 2022.
[18] S. Han, J. Pool, et al., “Learning both weights and connections
for efﬁcient neural network,” in Proc. NeurIPS, 2015.
[19] H. Li, A. Kadav, I. Durdanovic, et al., “Pruning Filters for
Efﬁcient ConvNets,” in Proc. ICLR, 2017.
[20] Z. Liu, J. Li, Z. Shen, et al., “Learning Efﬁcient Convolutional Networks Through Network Slimming,” in Proc. ICCV, 2017. [21] Q. Zhang, S. Zuo, C. Liang, et al., “PLATON: Pruning large transformer models with upper conﬁdence bound of weight im- portance,” in Proc. ICML, 2022.
[22] Z. Wang, J. Wohlwend, and T. Lei, “Structured Pruning of
Large Language Models,” in Proc. EMNLP, 2020.
[23] M. Xia, Z. Zhong, and D. Chen, “Structured Pruning Learns
Compact and Accurate Models,” in Proc. ACL, 2022.
[24] C. Liang, S. Zuo, M. Chen, et al., “Super tickets in pre-trained language models: From model compression to improving gen- eralization,” in Proc. ACL, 2021.
[25] P. Dong, S. Wang, W. Niu, et al., “Rtmobile: Beyond real- time mobile acceleration of rnns for speech recognition,” in ACM/IEEE Design Automation Conference (DAC), 2020. [26] S. Wang, P. Lin, R. Hu, et al., “Acceleration of LSTM With
Structured Pruning Method on FPGA,” IEEE Access, 2019.
[27] K. Tan and D.L. Wang, “Compressing Deep Neural Networks
for Efﬁcient Speech Enhancement,” in Proc. ICASSP, 2021.
[28] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini, “Group sparse regularization for deep neural networks,” Neu- rocomputing, vol. 241, pp. 81–89, 2017.
[29] C.-I J. Lai, Y. Zhang, A. H. Liu, et al., “PARP: Prune, Ad- just and Re-Prune for Self-Supervised Speech Recognition,” in Proc. NeurIPS, 2021.
[30] F. Wu, K. Kim, J. Pan, et al., “Performance-Efﬁciency Trade- offs in Unsupervised Pre-training for Speech Recognition,” in Proc. ICASSP, 2022.
[31] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib- rispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015.
[32] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you
need,” in Proc. NeurIPS, 2017.
[33] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units
(GELUs),” arXiv:1606.08415, 2016.
[34] C. Louizos, M. Welling, and D. P. Kingma, “Learning Sparse Neural Networks through L0 Regularization,” in ICLR, 2018. “Pytorch: An imperative style, high-
[35] A. Paszke et al.,
performance deep learning library,” Proc. NeurIPS, 2019. [36] T. Wolf et al., “Huggingface’s transformers: State-of-the-art natural language processing,” arXiv:1910.03771, 2019. [37] A. Rousseau et al., “TED-LIUM: an automatic speech recog-
nition dedicated corpus.,” in Proc. LREC, 2012.
[38] A. Graves, S. Fern´andez, F. Gomez, et al., “Connectionist tem- poral classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006.
[39] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, “SLURP: A Spoken Language Understanding Resource Pack- age,” in Proc. EMNLP, 2020.
[40] S. Zhang, E. Loweimi, P. Bell, and S. Renals, “On the use- fulness of self-attention for automatic speech recognition with transformers,” in Proc. SLT, 2021.
[41] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe,
“Branch- former: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022.
[42] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, “Attention Weight Smoothing Using Prior Distributions for Transformer- Based End-to-End ASR,” in Proc. Interspeech, 2022.
[43] F. Wu, K. Kim, S. Watanabe, et al.,
“Wav2seq: Pre- training speech-to-text encoder-decoder models using pseudo languages,” arXiv:2205.01086, 2022.
[44] L. Lugosch, M. Ravanelli, P. Ignoto, et al., “Speech model pre-training for end-to-end spoken language understanding,” in Interspeech, 2019.