David R. Mortensen15104975
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages', '2023', ['Conference on Machine Translation', 'WMT', 'Conf Mach Transl'], 'Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.', ['Nathaniel R. Robinson', 'Perez Ogayo', 'David R. Mortensen', 'Graham Neubig']]
['Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.", ['Orevaoghene Ahia', 'Sachin Kumar', 'Hila Gonen', 'Jungo Kasai', 'David R. Mortensen', 'Noah A. Smith', 'Yulia Tsvetkov']]
['SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Spéc Interest Group Comput Morphol Phonol Workshop'], 'In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.', ['Taiqi He', 'Lindia Tjuatja', 'Nathaniel R. Robinson', 'Shinji Watanabe', 'David R. Mortensen', 'Graham Neubig', 'L. Levin']]
['Construction Grammar Provides Unique Insight into Neural Language Models', '2023', ['', ''], 'Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.', ['Leonie Weissweiler', 'Taiqi He', 'Naoki Otani', 'David R. Mortensen', 'L. Levin', 'Hinrich Schütze']]
['Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', '2023', ['Special Interest Group on Computational Morphology and Phonology Workshop', 'SIGMORPHON', 'Spéc Interest Group Comput Morphol Phonol Workshop'], 'Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.', ['David R. Mortensen', 'Ela Gulsen', 'Taiqi He', 'Nathaniel R. Robinson', 'Jonathan D. Amith', 'Lindia Tjuatja', 'L. Levin']]
['Transformed Protoform Reconstruction', '2023', ['Annual Meeting of the Association for Computational Linguistics', 'Annu Meet Assoc Comput Linguistics', 'Meeting of the Association for Computational Linguistics', 'ACL', 'Meet Assoc Comput Linguistics'], 'Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.', ['Young Min Kim', 'Kalvin Chang', 'Chenxuan Cui', 'David R. Mortensen']]
['PWESuite: Phonetic Word Embeddings and Tasks They Facilitate', '2023', ['arXiv.org', 'ArXiv'], 'Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.', ['Vilém Zouhar', 'Kalvin Chang', 'Chenxuan Cui', 'Nathaniel Carlson', 'Nathaniel R. Robinson', 'Mrinmaya Sachan', 'David R. Mortensen']]
