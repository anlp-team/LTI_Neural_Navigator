3 2 0 2
n u J
0 3
] L C . s c [
1 v 1 0 1 0 0 . 7 0 3 2 : v i X r a
Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models
Harnoor Dhingra
Preetiha Jayashanker
Sayali Moghe
Emma Strubell
Carnegie Mellon University {hdhingra, pjayasha, smoghe, estrubel}@cs.cmu.edu
Abstract
quantify the detected biases. Hence, in this work we aim to answer the following research questions:
Large Language Models (LLMs) are trained pri- marily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ com- munity. In this paper, we perform a compara- tive study of how LLMs generate text describ- ing people with different sexual identities. Ana- lyzing bias in the text generated by an LLM us- ing regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising ap- proach towards debiasing the output of LLMs in this setting.
RQ1: Does a pre-trained LLM perpetuate mea- surable, quantifiable bias against queer people?
RQ2: Can we mitigate the said bias in the LLM output while preserving the context using a post- hoc debiasing method?
1
Introduction
A large number of current Natural Language Pro- cessing (NLP) models, especially Large Language Models (LLMs), yield biased predictions. The out- put of an LLM is contextually associated with the input prompt (Liang et al., 2021). However, in some cases, the generated text can be biased against one or more human identities such as gender, sex- ual identity, or race. These biases arise due to the prejudices inherent in the datasets on which these LLMs are trained.
Because of biased results generated by LLMs, these models inadvertently perpetuate stereotypes towards marginalized groups including women, people from certain racial and ethnic groups, peo- ple from the LGBTQIA+ community, people with disabilities, etc. (Lucy and Bamman, 2021; Hassan et al., 2021; Nozza et al., 2021; Smith et al., 2022). While it is known that LLMs can reflect and per- petuate biases, the extent of these biases is not well measured. Also, in order to effectively gauge the impact of bias reduction efforts, we need a way to
Figure 1: An illustrative example for generating a gender-neutral prompt. The biographical information about Hussain Dawood is sourced from the WikiBio dataset then made gender-neutral and anonymized. We then prepend this text with trigger words indicating sex- ual identity of the subject.
In this paper, the kind of bias that we will focus on is representational bias (Blodgett et al., 2020; Liang et al., 2021). As defined in the aforemen- tioned papers, a particular demographic group ex- periences representational harm when the system negatively portrays them. To quantify this bias, we use the regard score introduced in Sheng et al. (2019). The regard metric helps identify biases against certain minority groups that experience a lower social perception compared to other minori- ties.
In order to answer the first research question, we use gender-neutral biographies of people as prompts for the LLM as shown in Figure 1. The gender-neutral biographies provide different con- textual information to the LLM, such as personality traits and characteristics of the individual, to yield a diverse set of outputs. In order to analyze bias
1


in the outputs of the LLM, we prepend the gender- neutral biographies with trigger words indicating the sexual identity of the subject of the biogra- phy. We find qualitatively and quantitatively that these trigger words drive the LLM to yield biased outputs, measured as having low regard score, for queer people. For example, the LLM generates out- put that acknowledges the success of the subject’s business pursuits when the subject of the gender neutral biography is indicated to identify as straight. However, when the trigger word is replaced by a queer sexual identity, the output of the LLM fo- cuses more on the queer struggle and philanthropic side of the subject, rather than acknowledging their business savvy nature. Based on such observations, it was found that there are qualitative differences in the outputs of LLMs. The authors acknowledge and value the recognition of the queer struggle by LLMs. Considering the quantitative angle, the se- lection of words and linguistic patterns observed in diverse outputs of LLMs influence the subjects’ regard score, reflecting a measure of their social perception.
After establishing measurable bias in the outputs of LLMs in this setting, the second contribution of this work is to mitigate this representational bias in LLM outputs using a post-hoc technique. As mentioned above, the prompts with queer trigger words yield outputs that have lower regard in con- trast with their straight counterparts. Hence, we formulate our debiasing technique as a text-to-text style transfer problem. Our approach is inspired by that described by Ma et al. (2020), in which the au- thors introduce controllable debiasing to increase the power and agency of female characters. Our pri- mary focus is to increase the regard of such outputs produced by queer trigger words while preserv- ing the contextual information encoded in the low- regard output of an LLM. Using SHAP analysis (Lundberg and Lee, 2017) and the regard classifier, we detect the low-regard words and formulate the problem of rewriting the text without those words as text-to-text neural style transfer, as done by Yang (2022). By employing a post-hoc method to en- hance the regard of sentences while maintaining LLMs’ recognition of queer struggle, we demon- strate the potential for positive change in societal attitudes. This approach represents a promising first step towards fostering a more affirming future for the LGBTQIA+ community, given the increas- ing prominence of LLM-generated text.
2
2 Related Work
The proposed pipeline for bias detection has three major steps: generating gender-neutral text, lan- guage model prompting, and computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below.
2.1 Generating gender-neutral text
Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text.
Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case.
2.2 LLM prompting for bias detection
Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task.


In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task.
There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information.
2.3 Fairness analysis of LLM output
In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model’s outputs.
Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts.
Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias.
2.4 Debiasing LLM Output
Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto
3
the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights.
The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods.
Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the words that lower the regard.
3 Bias Statement
LLMs often depict queer individuals as struggling and perpetuate harmful stereotypes that create an unfavorable representation of them compared to their straight counterparts. Hence, there are qualita- tive differences in the outputs of LLMs for different sexual identities. The authors of this paper agree that it is important to acknowledge the queer strug- gle. However, it is equally important to look at the other facets of an individual’s personality. Hence, the focus of our work is to study this representa- tional bias (Blodgett et al., 2020; Liang et al., 2021) against queer people.
As illustrated in Dodge et al. (2021), big datasets like C4.EN on which LLMs are trained on exhibit a higher occurrence of document removal when they contain references to words such as ‘gay’, ‘lesbian’,


‘bisexual’, etc. Moreover, LLMs trained predomi- nantly on heteronormative (Vásquez et al., 2022) and cisnormative (Dev et al., 2021) language have an adverse effect on downstream tasks as these per- petuate harmful representations that negatively af- fect individuals belonging to minority groups such as the LGBTQIA+ community.
It should be noted that we explore a limited set of sexual and gender identities (straight man, straight woman, gay man, and lesbian woman) in this work. It is important to note that our intention is not to disregard other queer identities. We celebrate and respect the richness and complexity of all sexual and gender identities. Our focus on these specific identities is meant to serve as a foundation for ex- ploring diverse experiences within the scope of this conversation, and demonstrate a proof-of-concept with respect to these queer identities, under a lim- ited computational budget.
Further, while variations in the outputs for dif- ferent sexual identities do exist, it is important to note that the difference in quantitative metrics such as regard score primarily stems from the use of words that tend not to be identity-specific, but that diminish the overall regard for queer individuals.
4 Data
One aim of this work is to generate gender-neutral prompts to be used for LLMs. To automate the process of getting different contextual information for different people, we used the WikiBio dataset (Lebret et al., 2016). This dataset contains around 700k biographies extracted from Wikipedia con- taining the first paragraph of the biography. These biographies help give personas of different people. For our experimental setup, we randomly select approximately 200 biographies from the dataset, ensuring that the selected biographies contain a suitable number of sentences ranging from 4 to 9. An example biography from this dataset is shown in Figure 1.
One of the primary rationales for employing a dataset that contains biographical information lies in its inherent characteristic of predominantly fo- cusing on a single individual. These biographies will have general information about the personality traits for a given individual. Hence, these can be used to create prompts for the language model. We append sexual identity trigger words to the gen- der neutral biographies to generate the required prompts. The creation of this diverse contextual
4
corpus helps bridge the gap in the existing research and our work.
5 Proposed Approach
As the study focuses on two research questions, we will discuss the methodology into two parts.
5.1 Bias Detection
The proposed approach to answer the first research question is depicted in Figure 2. It involves the following three steps:
1. Gender-neutralizing WikiBio biographies.
2. Generating prompts for bias detection.
3. Quantitatively analyzing the outputs of the LLM.
Figure 2: Proposed pipeline for bias detection in LLM. The biographies from WikiBio dataset are made gender- neutral. We then prepend these with trigger words in- dicating sexual identity of the subject of the biography. We conduct fairness analysis on the output generated by the LLM during text-to-text generation task using gender-neutral prompts.
For the first step, we use the methodology as described by Sun et al. (2021). That is, we focus first on replacing gendered words like gendered pronouns, and words such as mother, sister, fire- man, etc. with their gender-neutral form. Then, in order to make sure the prompt is well-structured se- mantically and grammatically, we use the language model as described in the paper for corrections. Next, to reduce the implicit bias because of gen- dered names of famous people on the output of the LLM, we replace the names with <PER> token using named entity recognition.
Next, we want to detect the bias in LLM out- put with a set of prompts. In our work, we use a popular autoregressive language model, GPT-3.5


davinci, to perform a text completion task. The gender-neutral biographies from the previous step are used to generate prompts. We append a sen- tence of the type “The person being talked about here is a XX”, where XX = straight man, straight woman, gay man, lesbian woman to the gender neu- tral biographies. In addition to these prompts, we use the gender-neutral biography without a trigger word as the control prompt. This diverse contextual corpus of prompts helps us detect the bias in the LLM output. To perform a text completion task using the API for GPT-3.5 davinci, we append the line “Write two more lines.” to the prompt.
Hence, the process of prompt generation has two major components: gender neutralized biogra- phies to provide different contextual information and sexual identity trigger words to induce bias in the output of the LLM.
In order to detect the variations in outputs that were generated for different target groups, like het- erosexual individuals (‘straight man’ and ‘straight woman’), queer individuals (‘gay man’ and ‘les- bian woman’), and the control group, we use the following qualitative and quantitative metrics and analyses:
Word clouds: Hassan et al. (2021) used a hier- archical dirichlet process to analyze the abstract topics in LLM generated outputs. In similar spirit, we perform a simple frequency-based word cloud visualization for the LLM generated outputs for control prompts and sexual identity trigger word prompts. The most frequently occurring words in each case show the words that have a higher chance of being in the output of an LLM when a particular trigger word is appended to the prompt. That is, it helps us to closely examine the bias association between the prompt and the output generated by LLM.
Pointwise mutual information: Similar to the above frequency based word cloud analysis, point- wise mutual information (PMI) analysis helps us to analyze those words that occur more often with one type of trigger word as compared to other trigger words. Usually, PMI is calculated be- tween 2 words. For our analysis, we append the tags LABEL_CONTROL, LABEL_STRAIGHT_MAN, LABEL_STRAIGHT_WOMAN, LABEL_GAY_MAN, LABEL_LESBIAN_WOMAN to the five types of gen- erated outputs, respectively. We then calculate the PMI of each word occurring in all the LLM outputs
5
with those label words individually to look at the top few words for each label.
t-SNE visualizations: We compute TF-IDF sen- tence embeddings for all the outputs of the LLM. We then use t-SNE to plot these embeddings in a two-dimensional space. The points in the plot are color coded by their label. The rationale behind this plot lies in the fact that the proximity of data points indicates similarity in embeddings.
Average cosine similarity: We calculated the av- erage cosine similarity between the output embed- dings of prompts that had sexual identity trigger words with those of control prompts to see how similar or dissimilar the outputs are.
Regard score: The regard score or regard metric (Sheng et al., 2019) is a measure of how society perceives a person. In other words, it measures how powerful/weak or high-regard/low-regard words are used to describe an individual. The regard score for outputs of sexual identity trigger words were compared with those of the control sentence. The proximity of regard scores to that of the control group is an indication of the current societal norms.
5.2 Debiasing the LLM Output
The outputs of the LLM when the prompt in- cluded queer trigger words (‘gay man’ and ‘les- bian woman’) had lower regard than those prompts with their straight counterparts. As can be seen in Section 6, the words/phrases that describe the queer struggle are common in the outputs for queer trigger words prompts. To find a solution for the second research question, we do not undermine the queer struggle that is being acknowledged in the LLM outputs. Rather, we prioritize the eleva- tion of queer individuals’ status and visibility in these outputs. Consequently, we employ a post-hoc approach to mitigate bias in the generated output. We formulate the problem as a text-to-text neural style transfer task in order to preserve the semantic meaning (acknowledging the queer struggle) and increase the overall regard of the sentence (elevat- ing the status of the queer individual).
Our methodology is based on the idea introduced by Yang (2022) as this paper also tries to solve a text-to-text style transfer problem. SHAP can be used with a trained classifier to detect the words that drive the output of a classifier to a particular label more than other words. Hence, in our work, we used the trained regard classifier from Sheng


Figure 3: Proposed pipeline for debiasing the output of the LLM. We begin by prompting the LLM to identify the reasons for the low regard of a sentence, utilizing low-regard words identified through SHAP analysis. Using the original sentence and the reason generated by the LLM, we then prompt the LLM again to generate a high regard sentence by replacing the low-regard words.
et al. (2019). Using SHAP word level analysis with this classifier, we found the words that drive a sentence towards its lower regard. Yang (2022) mask out the words detected by SHAP and use a language model to predict the words in place of In our case, we take the idea of chain-of- that. thought prompting (CoT) (Wei et al., 2023). We first query the LLM for a possible reason why the words detected by the SHAP analysis would lower the regard of the given sentence. We then take that reason and re-prompt the LLM to rephrase the given sentence to keep the meaning intact and choose different words for the low-regard words. This is shown in Figure 3.
6 Results
6.1 Detecting Bias in the Language Model
A walk-through of the methodology using an ex- ample is shown in Figure (1). The resulting outputs for the LLM are shown in Table (4) in Appendix. From Table (4), we notice that the outputs of control, straight man and straight woman acknowl- edge the fact that person being discussed was an accomplished figure known for their business pur- suits. However, the outputs for gay man and lesbian woman include words that indicate the queer strug- gle – which is justified. However, for the output of lesbian woman, the LLM fails to adequately emphasize the individual’s business mindset, in- stead primarily focusing on their contributions to philanthropy and promotion of inclusivity.
The t-SNE plot in Figure (6) shows that the out- puts of control and straight men are similar to each other as they are closer to each other. However, the
6
Measure Cosine similarity Regard score
SM SW GM LW 0.33 0.43 0.27 0.01
0.39 -0.05
0.35 0.31
Table 1: Cosine similarity and regard score difference of sentences with sexual identity trigger words with those of control sentences.
outputs of gay men and lesbian women are closer to each other but afar from control outputs. Based on these embeddings, we computed the average cosine similarity between the embeddings between sexual identity trigger words outputs and the con- trol as shown in Table 1. The same conclusion can be drawn from these average cosine values. Please note SM stands for straight man and control, SW stands for straight woman and control, GM stands for gay man and control, LW stands for lesbian woman and control.
A similar notation as above is used for regard score difference with control sentence in Table 1. As can be seen from this table, the regard of straight men and straight women is very similar to the con- trol. However, the regard of gay men and lesbian women is significantly less than the control. This, in a way, shows the heteronormative nature of his- torical discourse on which the LLM is trained on.
6.2 Debiasing the LLM Output
The premise to debias the LLM Output is to use SHAP to detect the low-regard words. In Figure (4), we show an example with net positive regard of 0.95. In Figure (5), we show an example with net negative regard of 0.85. The words like dis- crimination and challenges lower the regard of the person.
In Table (2), we can see that the reason described by the LLM makes sense for the low-regard sen- tence. Hence, as we formulated it as a text-to- text style transfer problem, the LLM changed the words/phrases accordingly.
The baseline for this was just prompting the LLM to increase the regard of the person. The results for regard score difference after debiasing are shown in Table (3). In Figure (7), we can see that the points for original low regard sentences and debiased sentences are overlapping (keeping contextual meaning intact). So, the sentences are almost similar whereas the regard has increased significantly.


Figure 4: SHAP word analysis for positive regard sentence. The highlighted words drive the sentence towards a higher regard with the opacity as an indication of its greater importance.
Figure 5: SHAP word analysis for negative regard sentence. The highlighted words drive the sentence towards a lower regard with the opacity as an indication of its greater importance.
Figure 6: t-SNE plot for LLM output embeddings. The output sentence embeddings for straight men and straight women demonstrate close proximity to the con- trol group, while the sentence embeddings for gay men and lesbian women exhibit greater distance from the control group, suggesting a qualitative distinction in the LLM output.
7 Discussion
Most of the previous works have tried to study the explicit bias in LLM output because of marked trigger words describing demographic features of an individual. Our work builds up on the work to incorporate more contextual information. As was seen in the results above, the LLM gets influenced by the bias association even when the context is changed.
The methodology described above to detect bi- ased outputs in an LLM is limited to when the prompt includes a trigger word as this language markedness leads to explicit bias. Because we need to quantitatively measure the differences in outputs for different sexual identities, this is a necessity for our study. The results in the Table (1) indicate a notable distinction in the regard score of outputs
Figure 7: t-SNE plot for LLM output embeddings before and after debiasing. Substantial overlap of points sug- gests high lexical similarity between the embeddings.
between queer individuals and their straight coun- terparts, suggesting that the described methodology is effective. This can be extended to cases when the prompt implicitly exhibits bias based on a person’s sexual identity, even in the absence of explicit trig- ger words. The assumption is that even in this case, the language model would lead to lower regard out- puts for queer individuals. Additional research is necessary to validate this assumption, considering that the majority of historical discourse tends to reflect a heteronormative perspective which con- strains the examination of linguistic cues present in queer discourse (Cheshire, 2007; Kitzinger, 2005; CH-Wang and Jurgens, 2021). Our findings cor- roborate this fact. As illustrated in the Figure 6, the points representing straight individuals are positioned in proximity to the control sentences. Conversely, the queer sentences are noticeably dis- tanced from the control, providing further evidence that the prevailing norm is heteronormative.
In the context of debiasing, our methodology en-
7


Low Regard Sentence
Reason
High Regard Sentence
he was known for his artistic depictions of water and light, often incorporating sensual and homoerotic elements into his work. he was openly gay in a time when homosexuality was heavily stigmatized and criminalized. The words like criminalized, stigmatized and gay suggest that the person may have been subjected to negative societal attitudes due to their sexual identity. Additionally, the words sensual and homoerotic may be seen as taboo, further contributing to a lower social perception. he was known for his artistic depictions of water and light, often incorporating beautiful and intimate moments into his work. despite the societal norms of the time, he was true to himself and openly expressed his same-sex attraction.
Table 2: Chain-of-thought based debiasing using SHAP analysis. The words marked in red indicate the top few words that drive the sentence towards a lower regard score. The phrases marked in green indicate the rephrased parts of the original sentence based on the reason.
GM LW
Original Baseline Our Method 0.15 0.21 0.06 0.16
0.31 0.27
Table 3: Regard score difference of sentences with sex- ual identity trigger words with those of control sentences after debiasing.
sures that the overall semantic meaning of the sen- tence remains largely unchanged, while effectively replacing low regard words. This importance stems from the fact that, despite qualitative differences in the outputs of LLMs for straight and queer subjects, the regard score mainly depends on the selection of words in the output. Therefore, our objective is not to disregard the struggles faced by the queer com- munity, but rather to quantitatively enhance regard score, ultimately reducing the bias. CoT prompt- ing helped remove the low regard words which had spurious correlations with the regard score. Further, CoT prompting focused on replacing low-regard words while leaving other aspects of the sentence intact as is shown in the example in Table (2). The debiasing methodology outlined in our work can be generalized to broader range of problems which can be formulated as text-to-text style transfer tasks. There is a need of trained classifier which is able to detect linguistic differences between source and target styles.
over, the methodology used to gender-neutralize the prompts (Sun et al., 2021) is not flawless. In some cases, the sentences are not semantically and grammatically correct. Also, gender-neutralizing in this way does not remove the implicit bias in the prompts which might inadvertently have an effect on the output of the LLM. The premise underlying the statement appended to the gender-neutral bi- ographies, which assumes that biographies should solely talk about a single person, may not hold true in certain instances.
Another avenue where improvement might be needed in the future is at looking at SHAP de- tected low-regard words using the regard classifier (Sheng et al., 2019) used in this study. In some cases the word ‘gay’ correlates to sentence hav- ing low-regard. This might be because of short- cut learning in the trained regard classifier (Sheng et al., 2019). Moreover, the methodology described in this paper is a post-hoc way to debias the low- regard sentences. In order to make sure that LLMs consider all humans equal, the training data should be non-cisnormative and non-heteronormative. So, research in the field of datasets for LLMs is another avenue which can help understand the origins of such biases.
8 Limitations
Our methodology focuses on detecting explicit bias by identifying sexual identity trigger words, while it may not directly address the potential presence of implicit bias within the prompt itself. More-
Finally, the authors would like to emphasize that the study’s focus on four sexual identities should not be interpreted as a suggestion that these are the new ‘norm’. The authors recognize that sexuality exists on a diverse and fluid spectrum, and that every individual’s unique experiences and identities should be celebrated and respected.
8


Acknowledgements
We express our gratitude to Dr. Maarten Sap for his invaluable support and constructive feedback provided during the development of this work. We would also like to thank Athiya Deviyani for her insights which helped shape the final outcome of this undertaking.
References
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 5454– 5476, Online. Association for Computational Lin- guistics.
Sky CH-Wang and David Jurgens. 2021. Using so- ciolinguistic variables to reveal changing attitudes towards sexuality and gender. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9918–9938, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Jenny Cheshire. 2007. Style and sociolinguistic varia-
tion (review).
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicog- raphy. Computational Linguistics, 16(1):22–29.
Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar- jun Subramonian, Jeff Phillips, and Kai-Wei Chang. 2021. Harms of gender exclusivity and challenges in non-binary representation in language technologies. In Proceedings of the 2021 Conference on Empiri- cal Methods in Natural Language Processing, pages 1968–1994, Online and Punta Cana, Dominican Re- public. Association for Computational Linguistics.
Jesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colos- In Proceedings of the sal clean crawled corpus. 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286–1305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxi- cityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.
Michael Gira, Ruisu Zhang, and Kangwook Lee. 2022. Debiasing pre-trained language models via efficient
9
fine-tuning. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 59–69.
Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and Aram Galstyan. 2022. Mitigating gender bias in distilled language models via counterfactual role re- versal.
Saad Hassan, Matt Huenerfauth, and Cecilia Ovesdotter Alm. 2021. Unpacking the interdependent systems of discrimination: Ableist bias in NLP systems through an intersectional lens. In Findings of the Associa- tion for Computational Linguistics: EMNLP 2021, pages 3116–3123, Punta Cana, Dominican Republic. Association for Computational Linguistics.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2018. Toward con- trolled generation of text.
Hamed Jelodar, Yongli Wang, Chi Yuan, and Xia Feng. 2017. Latent dirichlet allocation (LDA) and topic modeling: models, applications, a survey. CoRR, abs/1711.04305.
Celia Kitzinger. 2005.
"speaking as a heterosex- (how) does sexuality matter for talk-in- ual": interaction? Research on Language and Social Inter- action, 38(3):221–265.
Rémi Lebret, David Grangier, and Michael Auli. 2016. Generating text from structured data with application to the biography domain. CoRR, abs/1603.07771.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to senti- ment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865–1874, New Orleans, Louisiana. Associa- tion for Computational Linguistics.
Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards understand- ing and mitigating social biases in language models. In International Conference on Machine Learning, pages 6565–6576. PMLR.
Li Lucy and David Bamman. 2021. Gender and rep- resentation bias in GPT-3 generated stories. In Pro- ceedings of the Third Workshop on Narrative Un- derstanding, pages 48–55, Virtual. Association for Computational Linguistics.
Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Ad- vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.
Xinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020. Powertransformer: Unsupervised con- trollable revision for biased language correction.


Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A chal- lenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. As- sociation for Computational Linguistics.
Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion In Proceedings of the 2021 in language models. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2398–2406, Online. Association for Computational Linguistics.
Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, and Yejin Choi. 2017. Connota- tion frames of power and agency in modern films. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2329–2334, Copenhagen, Denmark. Association for Computational Linguistics.
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3407– 3412, Hong Kong, China. Association for Computa- tional Linguistics.
Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. “I’m sorry to hear that”: Finding new biases in language models with a holistic descriptor dataset. In Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9180–9211, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Tony Sun, Kellie Webster, Apu Shah, William Yang Wang, and Melvin Johnson. 2021. They, them, theirs: Rewriting with gender-neutral english.
Ewoenam Kwaku Tokpo and Toon Calders. 2022. Text style transfer for bias mitigation using masked lan- guage modeling. In Proceedings of the 2022 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies: Student Research Workshop, pages 163–171, Hybrid: Seattle, Washington + On- line. Association for Computational Linguistics.
10
Eva Vanmassenhove, Chris Emmery, and Dimitar Shte- rionov. 2021. Neutral rewriter: A rule-based and neural approach to automatic rewriting into gender- neutral alternatives.
Juan Vásquez, Gemma Bel-Enguix, Scott Thomas An- dersen, and Sergio-Luis Ojeda-Trueba. 2022. Hetero- Corpus: A corpus for heteronormative language de- tection. In Proceedings of the 4th Workshop on Gen- der Bias in Natural Language Processing (GeBNLP), pages 225–234, Seattle, Washington. Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elic- its reasoning in large language models.
Xusheng Yang. 2022. Transferring styles between sar- castic and unsarcastic text using shap, gpt-2 and pplm. In 2022 4th International Conference on Natural Lan- guage Processing (ICNLP), pages 390–394.


A Appendix
Figure 8: Word Cloud for Control Outputs
Figure 9: Word Cloud for Straight Man Outputs
Figure 10: Word Cloud for Gay Man Outputs
11
Prompt Control
Straight Man
Straight Woman
Outputs <PER> has actively contributed to various social causes in Pakistan, in- cluding healthcare and education. They have also been involved in several initiatives to promote en- trepreneurship and innovation in the country. As successful businessman, <PER> has also invested in various other companies and ventures. In addition to his business pursuits, he is also dedicated to improving ed- ucation and healthcare in Pakistan through his philanthropic work. As a prominent figure in the busi- ness community, <PER> has re- ceived numerous awards and ac- colades for their accomplishments. They are also actively involved in various initiatives aimed at expand- ing access to education and health- care in their community.
a
Gay Man
Lesbian Woman
Despite facing discrimination and prejudice for their sexual orienta- tion, <PER> has continued to make several contributions to their com- munity through their philanthropic endeavors. Their success in busi- ness and dedication to social causes has earned them widespread re- spect. <PER>’s extensive philanthropic work includes supporting organiza- tions that promote LGBTQ+ rights and advocacy efforts for the commu- nity. As a lesbian woman, <PER> is dedicated to creating more inclu- sive workplaces and communities.
Table 4: Example outputs of the LLM