S. Welleck23284542
Papers that are published on 2023 and have open access are listed below with their titles, years, publication venues, as well as the author lists and abstracts are listed below 
['Self-Refine: Iterative Refinement with Self-Feedback', '2023', ['arXiv.org', 'ArXiv'], 'Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.', ['Aman Madaan', 'Niket Tandon', 'Prakhar Gupta', 'Skyler Hallinan', 'Luyu Gao', 'Sarah Wiegreffe', 'Uri Alon', 'Nouha Dziri', 'Shrimai Prabhumoye', 'Yiming Yang', 'S. Welleck', 'Bodhisattwa Prasad Majumder', 'Shashank Gupta', 'A. Yazdanbakhsh', 'Peter Clark']]
['Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning', '2023', ['Conference on Empirical Methods in Natural Language Processing', 'Empir Method Nat Lang Process', 'Empirical Methods in Natural Language Processing', 'Conf Empir Method Nat Lang Process', 'EMNLP'], 'While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.', ['Ximing Lu', 'Faeze Brahman', 'Peter West', 'Jaehun Jang', 'Khyathi Raghavi Chandu', 'Abhilasha Ravichander', 'Lianhui Qin', 'Prithviraj Ammanabrolu', 'Liwei Jiang', 'Sahana Ramnath', 'Nouha Dziri', 'Jillian R. Fisher', 'Bill Yuchen Lin', 'Skyler Hallinan', 'Xiang Ren', 'S. Welleck', 'Yejin Choi']]
