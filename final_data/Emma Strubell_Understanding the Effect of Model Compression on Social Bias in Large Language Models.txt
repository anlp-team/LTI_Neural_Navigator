1https://github.com/gsgoncalves/EMNLP2023_llm_compression_and_social_biasprohibitivelyexpensivebothfinanciallyandenvi-ronmentally(Hessenthaleretal.,2022).Atthesametime,thecompressionofLLMshasbeenintenselystudied.Pruning,quantization,anddis-tillationareamongthemostcommonstrategiestocompressLLMs.Pruningreducestheparametersofatrainedmodelbyremovingredundantcon-nectionswhilepreservingequivalentperformancetotheiroriginalcounterparts(Liebenweinetal.,2021;Ahiaetal.,2021).Quantizationreducestheprecisionofmodelweightsandactivationstoimproveefficiencywhilepreservingperformance(Ahmadianetal.,2023).Finally,knowledgedistil-lation(Hintonetal.,2015)trainsasmallermoreefficientmodelbasedonalargerpre-trainedmodel.Whilemuchresearchhasbeendoneonmea-suringandmitigatingsocialbiasinLLMs,andmakingLLMssmallerandmoreefficient,byusingoneoracombinationofmanycompressionmeth-ods(Xuetal.,2021),littleresearchhasbeendoneregardingtheinterplaybetweensocialbiasesandLLMcompression.Existingworkhasshownthatpruningdisproportionatelyimpactsclassificationaccuracyonlow-frequencycategoriesincomputervisionmodels(Hookeretal.,2021),butthatprun-ingtransformermodelscanhaveabeneficialeffectwithrespecttobiaswhenmodelingmultilingualtext(Hookeretal.,2020;Oguejietal.,2022).Fur-ther,XuandHu(2022)haveshownthatcompress-ingpretrainedmodelsimprovesmodelfairnessbyworkingasaregularizeragainsttoxicity.Unlikepreviouswork,ourworkfocusesontheimpactsofwidelyusedquantizationanddistilla-tiononthesocialbiasesexhibitedbyavarietyofbothencoder-anddecoder-onlyLLMs.Wefo-cusontheeffectsofsocialbiasoverBERT(Devlinetal.,2019),RoBERTa(Liuetal.,2019)andPythiaLLMs(Bidermanetal.,2023).WeevaluatethesemodelsagainstBiasBench(Meadeetal.,2022),acompilationofthreesocialbiasdatasets.Inourexperimentalresultswedemonstrateacor-
UnderstandingtheEffectofModelCompressiononSocialBiasinLargeLanguageModelsGustavoGonçalves1,2andEmmaStrubell1,31LanguageTechnologiesInstitute,CarnegieMellonUniversity,Pittsburgh,PA,USA2NOVALINCS,UniversidadeNOVAdeLisboa,Lisbon,Portugal3AllenInstituteforArtificialIntelligence,Seattle,WA,USA{ggoncalv,estrubel}@cs.cmu.eduAbstractLargeLanguageModels(LLMs)trainedwithself-supervisiononvastcorporaofwebtextfittothesocialbiasesofthattext.Withoutintervention,thesesocialbiasespersistinthemodel’spredictionsindownstreamtasks,lead-ingtorepresentationalharm.Manystrategieshavebeenproposedtomitigatetheeffectsofinappropriatesocialbiaseslearnedduringpre-training.Simultaneously,methodsformodelcompressionhavebecomeincreasinglypop-ulartoreducethecomputationalburdenofLLMs.Despitethepopularityandneedforbothapproaches,littleworkhasbeendonetoexploretheinterplaybetweenthesetwo.Weperformacarefullycontrolledstudyoftheim-pactofmodelcompressionviaquantizationandknowledgedistillationonmeasuresofsocialbiasinLLMs.Longerpretrainingandlargermodelsledtohighersocialbias,andquantiza-tionshowedaregularizereffectwithitsbesttrade-offaround20%oftheoriginalpretrainingtime.11IntroductionLargeLanguageModels(LLMs)aretrainedonlargecorporausingself-supervision,whichallowsmodelstoconsidervastamountsofunlabelleddata,andlearnlanguagepatternsthroughmask-ingtasks(Devlinetal.,2019;Radfordetal.,2019).However,self-supervisionallowsLLMstopickupsocialbiasescontainedinthetrainingdata.Whichisamplifiedbylargermodels,moredata,andlongertraining(Kanekoetal.,2022;KanekoandBollegala,2022;Kuritaetal.,2019;DelobelleandBerendt,2022).SocialbiasesinLLMsareanongoingprob-lemthatispropagatedfrompretrainingtofinetun-ing(Ladhaketal.,2023;Giraetal.,2022).Biasedpretrainedmodelsarehardtofix,asretrainingis
2663 Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2663–2675 December 6-10, 2023 ©2023 Association for Computational Linguistics


relationbetweenlongerpretraining,largermodels,andincreasedsocialbias,andshowthatquantiza-tionanddistillationcanreducebias,demonstratingthepotentialforcompressionasapragmaticap-proachforreducingsocialbiasinLLMs.2MethodologyWewereinterestedinunderstandinghowdynamicPost-TrainingQuantization(PTQ)anddistillationinfluencesocialbiascontainedinLLMsofdifferentsizes,andalongtheirpretraining.IndynamicPTQ,full-precisionfloatingpointmodelweightsarestat-icallymappedtolowerprecisionsaftertraining,withactivationsdynamicallymappedfromhightolowprecisionduringinference.Tothisend,inSection2.1wepresentthedatasetsoftheBiasBenchbenchmark(Meadeetal.,2022)thatenableustoevaluatethreedifferentlanguagemodelingtasksacrossthethreesocialbiascategories.InSection2.2welayoutthemodelswestudied.WeexpandontheBiasBenchoriginalevaluationbylookingattheLargeversionsoftheBERTandRoBERTamodels,andthePythiafamilyofau-toregressivemodels.Thechosenmodelscoverdifferentlanguagemodelingtasksandspanacrossawiderangeofparametersizes,thusprovidingacomprehensiveviewofthevariationsofsocialbias.2.1MeasuringBiasWeusetheBiasBenchbenchmarkforevaluatingmarkersofsocialbiasinLLMs.BiasBenchcom-pilesthreedatasets,CrowS-Pairs(Nangiaetal.,2020),StereoSet(SS)(Nadeemetal.,2021),andSEAT(KanekoandBollegala,2021),formeasur-ingintrinsicbiasacrossthreedifferentidentitycate-gories:GENDER,RACE,andRELIGION.Whilethesetofidentitiescoveredbythisdatasetisfarfromcomplete,itservesasausefulindicatorasthesemodelsareencodingcommonsocialbiases;how-ever,thelackofbiasindicatedbythisbenchmarkdoesnotimplyanoveralllackofinappropriatebiasinthemodel,forexamplewithrespecttoothergroups.Webrieflydescribeeachdatasetbelow;refertotheoriginalworksformoredetail.CrowS-Pairsiscomposedofpairsofminimallydistantsentencesthathavebeencrowdsourced.Aminimallydistantsentenceisdefinedasasmallnumberoftokenswapsinasentence,thatcarrydifferentsocialbiasinterpretations.Anunbiasedmodelwillpickanequalratioofbothstereotypi-calandanti-stereotypicalchoices,thusanoptimalscoreforthisdatasetisaratioof50%.StereoSetiscomposedofcrowdsourcedsamples.Eachsampleiscomposedofamaskedcontextsentence,andasetofthreecandidateanswers:1)stereotypical,2)anti-stereotypical,and3)un-related.UndertheSSformulation,anunbiasedmodelwouldgiveabalancednumberofclassifica-tionsoftypes1)and2),thustheoptimalscoreisalso50%.TheSSdatasetalsomeasuresifwearechangingthelanguagemodelingpropertiesofourmodel.Thatis,ifourmodelpicksahighpercent-ageofunrelatedchoices3)itcanbeinterpretedaslosingitslanguagecapabilities.ThisisdefinedastheLanguageModel(LM)Score.SEATevaluatesbiasesinsentences.ASEATtaskisdefinedbytwosetsofattributesentences,andtwoothersetsoftargetsentences.Theobjec-tiveofthetaskistomeasurethedistanceofthesentenceembeddingsbetweentheattributeandtar-getsetstoassessapreferencebetweenattributesandtargets(bias).WeprovidemoredetailofthisformulationinAppendixA.1.2.2ModelsInthiswork,wefocusontwopopularmethodsformodelcompression:knowledgedistillationandquantization.Wechoosethesetwomethodsgiventheircompetitiveperformance,widedeploymentgiventheavailabilityofdistributionsundertheHug-gingFaceandPytorchlibraries,andthelackofunderstandingoftheimpactofthesemethodsonsocialbiases.Weleavethestudyofmoreelaboratemethodsforimprovingmodelefficiencysuchaspruning(Chenetal.,2020),mixturesofexperts(Kuduguntaetal.,2021),andadaptivecomputation(Elbayadetal.,2020)tofuturework.Sincemodelcompressionaffectsmodelsize,weareparticularlyinterestedinunderstandinghowpretrainedmodelsizeimpactsmeasuresofsocialbias,andhowthatchangesasafunctionofhowwellthemodelfitsthedata.Wearealsointer-estedininvestigatinghowthenumberoftokensobservedduringtrainingimpactsalloftheabove.WeexperimentwiththreedifferentbaseLLMs:BERT(Devlinetal.,2019),RoBERTa(Liuetal.,2019),andPythia(Bidermanetal.,2023),withuncompressedmodelsizesrangingfrom70Mpa-rametersto6.9Bparameters.BERTandRoBERTa
2664


↓0.1962.14
↓1.1456.11
↓3.8356.32
↓6.6756.19+SENTDEBIAS(Liangetal.,2020)110M
↓4.7658.10
↑1.9162.86+SELF-DEBIAS(Schicketal.,2021)110M
↓5.6356.70
↓5.6356.70
↑1.5565.12
↓0.9559.05+DROPOUT(Websteretal.,2020)110M
↓0.9560.00DistilRoBERTa82M329
↓1.5255.73
↓9.5251.43+SENTDEBIAS(Liangetal.,2020)110M
Table1:CrowS-PairsstereotypescoresforGENDER,RACE,andRELIGIONforBERTandRoBERTamodels.Stereotypescorescloserto50%indicatelessbiasedmodelbehavior.Boldvaluesindicatethebestmethodperbiascategory.ResultsontheotherdatasetsdisplayedsimilartrendsandwereincludedinAppendixBforspace.representtwosimilarsetsofwidelyusedandstud-iedpretrainedarchitectures,trainedondifferentdatawithasmalloverlap.RoBERTapretrainingwasdoneover161GBoftext,whichcontainedthe16GBusedtotrainBERT,approximatelyaten-foldincrease.RoBERTaalsotrainedforlonger,withlargerbatchsizeswhichhaveshowntodecreasetheperplexityoftheLLM(Liuetal.,2019).ThesetofcheckpointsreleasedforthePythiamodelfamilyallowsustoassessanevenwidervarietyofmodelsizesandnumberoftrainingto-kens,includingintermediatecheckpointssaveddur-ingpretraining,sothatwecanobservehowbiasvariesthroughoutpretraining.Weusedthemod-elspretrainedonthededuplicatedversionofThePile(Gaoetal.,2021)containing768GBoftext.Knowledgedistillation(Hintonetal.,2015)isapopulartechniqueforcompressingtheknowledgeencodedinalargerteachermodelintoasmallerstudentmodel.Inthiswork,weanalyzeDistil-BERT(Sanhetal.,2019)andDistilRoBERTa2distilledLMs.Duringtrainingthestudentmodelminimizesthelossaccordingtothepredictionsof
↓1.1762.40
↓1.1762.40
↓2.8657.14+INLP(Ravfogeletal.,2020)110M
↑2.8663.81
↓9.3246.99
↓8.0452.11
↓1.9155.34
↑4.7667.62+DYNAMICPTQint8345M432
ModelParamsSize(MB)GENDERRACERELIGION
2https://huggingface.co/distilroberta-basetheteachermodel(soft-targets)andthetruelabels(hard-targets)tobettergeneralizetounseendata.Quantizationcompressesmodelsbyreducingtheprecisionoftheirweightsandactivationsduringinference.WeusethestandardPyTorchimplemen-tation3toapplydynamicPTQoverthelinearlayersofthetransformerstack,fromfp32full-precisiontoquantizedint8precision.ThisworkanalyzesquantizedBERT,RoBERTa,andPythiamodelsofacomprehensiverangeofsizes.3ResultsDynamicPTQanddistillationlowersocialbias.InTable1weanalyzetheeffectsofdynamicPTQanddistillationintheCrowSdataset,whereBERTBaseandRoBERTaBaseareourbaselines.Tocomparequantizationanddistillation,weaddthreedebiasingbaselinesalsoreferencedbyMeadeetal.(2022)thatarecompetitivestrategiestoreducebias.TheINLP(Ravfogeletal.,2020)baselineconsistsofalinearclassifierthatlearnstopredictthetargetbiasgroupgivenasetofcontextwords,suchas
↓7.6255.24+INLP(Ravfogeletal.,2020)110M
↓0.7659.39
↓9.5346.67+CDA(Websteretal.,2020)110M
↑0.1963.76
↓7.6255.24DistilBERT66M268
↓3.4960.08
↓3.0657.09
↓4.9652.29
↓4.9652.29
RoBERTaBase123M49860.1563.5760.95+DYNAMICPTQint8123M242
↓6.5153.64
↑0.9561.90+DYNAMICPTQint8354M513
↓0.2063.37
↓1.9460.39
↓6.1051.15
↓6.1051.15
↓1.940.95RoBERTaLarge354M142260.15
↓2.8660.00+DROPOUT(Websteretal.,2020)110M
BERTBase110M43857.2562.3362.86+DYNAMICPTQint8110M18157.25
↓7.2852.87
↓4.9855.17
↓5.0458.53
↓6.8750.38
↑0.5864.15
↑0.9563.81BERTLarge345M1341
↑0.7863.11
↓3.3059.03
↑0.3962.72
↓10.4749.52+CDA(Websteretal.,2020)110M
↓2.6857.47
↓1.9160.95+SELF-DEBIAS(Schicketal.,2021)110M
↓1.7561.82
3https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html
↑5.6367.96
2665


91.650K64.1/60.2/61.96.9B
89.221K59.8/58.4/58.6160M
91.6114K65.2/60.7/64.51.4B
Figure1:LMscorevs.GENDER,RACE,andRELIGIONbiasontheSSdatasetacrossallPythiamodels.Darkerdatapointsshowlaterpretrainingsteps,andmoretransparentpointstoearliersteps.TheincludedtableshowstheKendallTauC,forthecorrelationacross"All"modelsizes,full-precision"Original",and"int8"modelsizes.
92.7129K69.0/64.0/68.4
87.729K57.5/54.8/58.0160M
70M
70M
ModelSize
ModelSize
90.550K64.2/58.4/63.61.4B
91.421K67.3/60.1/67.3
91.429K66.1/59.7/63.32.8B
89.021K61.1/56.3/57.7410M
BestLMScoreStepNr.BiasG./RA./RE.
BestLMScoreStepNr.BiasG./RA./RE.
92.9114K67.1/63.7/66.86.9B
Table2:BiasmeasuredusingSSforthefull-precisionPythiamodelshavingthebestLMscorepermodelsize.
90.236K61.4/57.6/59.4410M
Table3:BiasmeasuredusingSSforint8quantizedPythiamodelshavingthebestLMscorepermodelsize.’he/she’.TheSelf-DebiasbaselinewasproposedbySchicketal.(2021),andusespromptstoencouragemodelstogeneratetoxictextandlearnstogivelessweighttothegeneratetoxictokens.Self-Debiasdoesnotchangethemodel’sinternalrepresentation,thusitcannotbeevaluatedontheSEATdataset.NotabletrendsinTable1arethereductionofsocialbiaseswhenapplyingdynamicPTQanddis-tillation,whichcancompeteonaveragewiththespecificallydesigneddebiasmethods.AdditionalresultsininAppendixBalsodisplaysimilartrends.OntheSSdatasetinTable4wearealsoabletoobservethattheapplicationofdistillationprovidesremarkabledecreasesinsocialbiases,atthegreatexpenseofLMscore.However,dynamicPTQshowsabettertrade-offinprovidingsocialbiasreductions,whilepreservingLMscore.Onemodelsizedoesnotfitallsocialbiases.InTable1andtheequivalentTablesinAppendixBwecanseethatsocialbiascategoriesresponddif-ferentlytomodelsize,acrossthedifferentdatasets.WhileBERTBase/LargeoutperformsRoBERTainGENDER,thebestmodelforRACEandRELIGIONvariesacrossdatasets.Thiscanbeexplainedbythedifferentdatasettasksandthepretraining.InAppendixBweshowthesocialbiasscoresasafunctionofthepretrainingofthePythiamodelsinFigures2to7,9,10and11.TheBERT/RoBERTaBaseandLargeversionsareroughlycomparablewiththe160Mand410MPythiamodels.FortheSSdataset,the160Mmodelisconsistentlylessbiasedthanthe410Mmodel.However,thisisnotthecasefortheothertwodatasetswherethe160MstrugglesintheRACEcategorywhileassess-ingthedistanceofsentenceembeddings(SEAT);andintheRELIGIONcategorywhileswappingmin-imallydistantpairs(CrowS).Thisillustratesthedifficultyofdistinguishingbetweensemanticallyclosewords,andshowstheneedforlargermodelspretrainedforlongerandonmoredata.Longerpretrainingandlargermodelsleadtomoresociallybiasedmodels.Westudytheef-fectsoflongerpretrainingandlargermodelsonsocialbias,byestablishingthecorrelationofthesevariablesinFigure1.HerewecanobservethatasthemodelsizeincreasessodoestheLMmodelscoreandsocialbiasacrosstheSSdataset.More-over,laterstagesofpretraininghaveahigherLMmodelscore,wherethesocialbiasscoretendstobehigh.TheapplicationofdynamicPTQshowsaregularizereffectonallmodels.TheKendallTauCacrossthemodelsandcategoriesshowsastrong
92.6129K66.6/63.2/66.22.8B
2666


correlationbetweenLMscoreandsocialbias.Sta-tisticalsignificanttestswereperformedusingaone-sidedt-testtoevaluatethepositivecorrelation.Tables2and3showatwhatstep,outofthe21wetested,thebestLMscoresoccurontheSSdataset.InTable2thebestLMscoreincreasesmonotonicallywithmodelsizeandsodothesocialbiases.Interestingly,asthemodelsizeincreasesthebestLMscoreappearsafteraround80%ofthepretraining.Inopposition,inTable3,withdynamicPTQthebestLMscoreoccursaround20%ofthepretrainingandmaintainsthetrendofhigherLMscoreandsocialbias,albeitatlowerscoresthantheoriginalmodels.ThisshowsaninterestingpossibilityofearlystoppingdependingonthedeploymenttaskoftheLLM.4LimitationsWhilethisworkprovidesthreedifferentdatasets,whichhavedifferentviewsonsocialbiasandallowforanindicativeviewofLLMs,theysharesomelimitationsthatshouldbeconsidered.ThedatasetsSSandCrowSdefineanunbiasedmodelasonethatmakesanequalamountofstereotypicalandanti-stereotypicalchoices.Whileweagreethatthismakesagooddefinitionofanimpartialmodelitisalimiteddefinitionofanunbiasedmodel.ThishasalsobeennotedbyBlodgettetal.(2021),showingthatCrowSisslightlymorerobustthanSSbytak-ing"extrastepstocontrolforvaryingbaseratesbe-tweengroups."(Blodgettetal.,2021).WeshouldconsiderthatthesedatasetsdepictmostlyWesternbiases,andthedatasetconstructionsinceitisbasedonassessorsitisdependentontheassessor’sviews.Moreover,Blodgettetal.(2021)hasalsonotedtheexistenceofunbalancedstereotypepairsinSSandCrowS,andthefactthatsomesamplesinthedatasetarenotconsensualstereotypes.Alldatasetsonlyexplorethreegroupsofbiases:GENDER,RACE,andRELIGION,whicharenotbyanymeansexhaustiverepresentationsofsocialbias.Theexperimentsinthispapershouldbeconsideredindicativeofsocialbiasandneedtobefurtherstud-ied.Additionally,theGENDERcategoryisdefinedasbinary,whichweacknowledgethatdoesnotreflectthetimelysocialneedsofLLMs,butcanbeextendedtoincludenon-binaryexamplesbyimprovingonexistingdatasets.WebenefitedfromaccesstoaclusterwithtwoAMDEPYC766264-CoreProcessors,wherethequantizedexperimentsranforapproximately4days.ACPUimplementationwasusedgiventhequantizationbackendsavailableinPyTorch.Exper-imentsthatdidnotrequirequantizationranusinganNVIDIAA10040GBGPUandtookapproxi-mately5hourstorun.EthicsStatementWereiteratethatthisworkprovidesalimitedWest-ernviewofSocialbiasfocusingonlyonthreemaincategories:GENDER,RACE,andRELIGION.OurworkisfurtherlimitedtoabinarydefinitionofGENDER,whichweacknowledgethatdoesnotre-flectthecurrentsociety’sneeds.Moreover,wemustalsoreiteratethatthesemodelsneedtobefur-therstudiedandarenotreadyforproduction.Theeffectsofquantizationalongpretrainingshouldbeconsideredaspreliminaryresults.5AcknowledgmentsThisworkhasbeenpartiallyfundedbytheFCTprojectNOVALINCSRef.UIDP/04516/2020,bytheAmazonScience-TaskBotPrizeChal-lengeandtheCMU|PortugalprojectsiFetchRef.LISBOA-01-0247-FEDER-045920andGoLocalRef.CMUP-ERI/TIC/0046/2014,andbytheFCTPh.D.scholarshipgrantRef.SFRH/BD/140924/2018.Wewouldliketoac-knowledgetheNOVASearchgroupforprovidingcomputeresourcesforthiswork.Anyopinions,findings,andconclusionsinthispaperaretheau-thors’anddonotnecessarilyreflectthoseofthesponsors.ReferencesOrevaogheneAhia,JuliaKreutzer,andSaraHooker.2021.TheLow-ResourceDoubleBind:AnEmpir-icalStudyofPruningforLow-ResourceMachineTranslation.InEMNLP(Findings),pages3316–3333.AssociationforComputationalLinguistics.ArashAhmadian,SaurabhDash,HongyuChen,BharatVenkitesh,StephenGou,PhilBlunsom,AhmetÜstün,andSaraHooker.2023.IntriguingPropertiesofQuantizationatScale.CoRR,abs/2305.19268.StellaBiderman,HaileySchoelkopf,QuentinAnthony,HerbieBradley,KyleO’Brien,EricHallahan,Mo-hammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,AviyaSkowron,LintangSutawika,andOskarvanderWal.2023.Pythia:ASuiteforAnalyzingLargeLanguageModelsAcrossTrainingandScaling.CoRR,abs/2304.01373.
2667


SuLinBlodgett,GilsiniaLopez,AlexandraOlteanu,RobertSim,andHannaM.Wallach.2021.Stereo-typingNorwegianSalmon:AnInventoryofPitfallsinFairnessBenchmarkDatasets.InACL/IJCNLP(1),pages1004–1015.AssociationforComputationalLinguistics.TianlongChen,JonathanFrankle,ShiyuChang,SijiaLiu,YangZhang,ZhangyangWang,andMichaelCarbin.2020.TheLotteryTicketHypothesisforPre-trainedBERTNetworks.InNeurIPS.PieterDelobelleandBettinaBerendt.2022.FairDistil-lation:MitigatingStereotypinginLanguageModels.InECML/PKDD(2),volume13714ofLectureNotesinComputerScience,pages638–654.Springer.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUn-derstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,NAACL-HLT2019,Minneapolis,MN,USA,June2-7,2019,Volume1(LongandShortPapers),pages4171–4186.AssociationforComputationalLinguistics.MahaElbayad,JiataoGu,EdouardGrave,andMichaelAuli.2020.Depth-AdaptiveTransformer.InICLR.OpenReview.net.LeoGao,StellaBiderman,SidBlack,LaurenceGold-ing,TravisHoppe,CharlesFoster,JasonPhang,HoraceHe,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy.2021.ThePile:An800GBDatasetofDiverseTextforLanguageModel-ing.CoRR,abs/2101.00027.MichaelGira,RuisuZhang,andKangwookLee.2022.DebiasingPre-TrainedLanguageModelsviaEffi-cientFine-Tuning.InLT-EDI,pages59–69.Associa-tionforComputationalLinguistics.MariusHessenthaler,EmmaStrubell,DirkHovy,andAnneLauscher.2022.BridgingFairnessandEnvi-ronmentalSustainabilityinNaturalLanguagePro-cessing.InEMNLP,pages7817–7836.AssociationforComputationalLinguistics.GeoffreyE.Hinton,OriolVinyals,andJeffreyDean.2015.Distillingtheknowledgeinaneuralnetwork.InNIPSWorkshoponDeepLearning.SaraHooker,AaronCourville,GregoryClark,YannDauphin,andAndreaFrome.2021.WhatDoCom-pressedDeepNeuralNetworksForget?SaraHooker,NyallengMoorosi,GregoryClark,SamyBengio,andEmilyDenton.2020.CharacterisingBiasinCompressedModels.CoRR,abs/2010.03058.MasahiroKanekoandDanushkaBollegala.2021.De-biasingPre-trainedContextualisedEmbeddings.InEACL,pages1256–1266.AssociationforComputa-tionalLinguistics.MasahiroKanekoandDanushkaBollegala.2022.Un-maskingtheMask-EvaluatingSocialBiasesinMaskedLanguageModels.InAAAI,pages11954–11962.AAAIPress.MasahiroKaneko,DanushkaBollegala,andNaoakiOkazaki.2022.DebiasingIsn’tEnough!-ontheEffectivenessofDebiasingMLMsandTheirSocialBiasesinDownstreamTasks.InCOLING,pages1299–1310.InternationalCommitteeonComputa-tionalLinguistics.SnehaKudugunta,YanpingHuang,AnkurBapna,MaximKrikun,DmitryLepikhin,Minh-ThangLu-ong,andOrhanFirat.2021.BeyondDistillation:Task-levelMixture-of-ExpertsforEfficientInference.InEMNLP(Findings),pages3577–3599.Associa-tionforComputationalLinguistics.KeitaKurita,NidhiVyas,AyushPareek,AlanW.Black,andYuliaTsvetkov.2019.MeasuringBiasinContextualizedWordRepresentations.CoRR,abs/1906.07337.FaisalLadhak,EsinDurmus,MiracSuzgun,TianyiZhang,DanJurafsky,KathleenR.McKeown,andTatsunoriHashimoto.2023.WhenDoPre-TrainingBiasesPropagatetoDownstreamTasks?ACaseStudyinTextSummarization.InEACL,pages3198–3211.AssociationforComputationalLinguistics.PaulPuLiang,IreneMengzeLi,EmilyZheng,YaoChongLim,RuslanSalakhutdinov,andLouis-PhilippeMorency.2020.TowardsDebiasingSen-tenceRepresentations.InACL,pages5502–5515.AssociationforComputationalLinguistics.LucasLiebenwein,CenkBaykal,BrandonCarter,DavidGifford,andDanielaRus.2021.LostinPruning:TheEffectsofPruningNeuralNetworksbeyondTestAccuracy.InMLSys.mlsys.org.YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.RoBERTa:ARobustlyOptimizedBERTPretrain-ingApproach.CoRR,abs/1907.11692.NicholasMeade,ElinorPoole-Dayan,andSivaReddy.2022.AnEmpiricalSurveyoftheEffectivenessofDebiasingTechniquesforPre-trainedLanguageMod-els.InACL(1),pages1878–1898.AssociationforComputationalLinguistics.MoinNadeem,AnnaBethke,andSivaReddy.2021.StereoSet:Measuringstereotypicalbiasinpretrainedlanguagemodels.InACL/IJCNLP(1),pages5356–5371.AssociationforComputationalLinguistics.NikitaNangia,ClaraVania,RasikaBhalerao,andSamuelR.Bowman.2020.CrowS-Pairs:AChal-lengeDatasetforMeasuringSocialBiasesinMaskedLanguageModels.InEMNLP(1),pages1953–1967.AssociationforComputationalLinguistics.
2668


σ({s(t,X,Y)}t∈A∪B)(3)Whereµandσarethemeanandstandardde-viationrespectively.Equation(3)isdesignedsothatscoresclosertozeroindicatethesmallestpos-sibledegreeofbias.SEATextendsthepreviousformulationbyconsideringthedistancesentenceembeddingsinsteadofwordembeddings.BAdditionalPlotsandTables
KelechiOgueji,OrevaogheneAhia,GbemilekeOnilude,SebastianGehrmann,SaraHooker,andJuliaKreutzer.2022.IntriguingPropertiesofCompres-siononMultilingualModels.InEMNLP,pages9092–9110.AssociationforComputationalLinguis-tics.AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.2019.Languagemodelsareunsupervisedmultitasklearners.ShauliRavfogel,YanaiElazar,HilaGonen,MichaelTwiton,andYoavGoldberg.2020.NullItOut:GuardingProtectedAttributesbyIterativeNullspaceProjection.InACL,pages7237–7256.AssociationforComputationalLinguistics.VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.2019.DistilBERT,adistilledversionofBERT:Smaller,faster,cheaperandlighter.CoRR,abs/1910.01108.TimoSchick,SahanaUdupa,andHinrichSchütze.2021.Self-DiagnosisandSelf-Debiasing:AProposalforReducingCorpus-BasedBiasinNLP.Trans.Assoc.Comput.Linguistics,9:1408–1424.KellieWebster,XuezhiWang,IanTenney,AlexBeu-tel,EmilyPitler,ElliePavlick,JilinChen,andSlavPetrov.2020.MeasuringandReducingGen-deredCorrelationsinPre-trainedModels.CoRR,abs/2010.06032.CanwenXu,WangchunshuZhou,TaoGe,KeXu,Ju-lianJ.McAuley,andFuruWei.2021.BeyondPre-servedAccuracy:EvaluatingLoyaltyandRobustnessofBERTCompression.InEMNLP(1),pages10653–10659.AssociationforComputationalLinguistics.GuangxuanXuandQingyuanHu.2022.CanModelCompressionImproveNLPFairness.CoRR,abs/2201.08542.ADetailsofMetricCalculationA.1SEATTheSEATtasksharesthesametaskasWEATtask,whichisdefinedbyfourwordsets,twoattributesets,andtwotargetsets.Forexample,todecidethepresenceofgenderbiasthetwoattributesetsaredisjointsetsgivenby:1)amasculinesetofwords,suchas{’man’,’boy’,’he’,...},and2)asetoffemininewords{’woman’,’girl’,’her’,...}.Thetargetsetswillcharacterizeconceptssuchas’sports’and’culinary’.WEATevaluateshowclosearetheattributesetsfromthetargetsetstodeterminetheexistenceofbias.Mathematicallythisisgivenby:s(A,B,X,Y)=Xx∈Xs(x,A,B)−Xy∈Ys(y,A,B)(1)WhereAandBrepresenttheattributesets,andXandYarethetargetsetsofwords.Thesfunc-tioninEquation(1)denotesmeancosinesimilaritybetweenthetargetwordembeddingsandtheat-tributewordembeddings:s(w,A,B)=1
|A|Xa∈Acos(w,a)−1
|B|Xb∈Bcos(w,b).(2)Thereportedscoreofthebenchmark(effectsize)isgivenby:d=µ({s(x,A,B)}x∈X)−µ({s(y,A,B)}y∈Y)
2669


Figure4:CrowsRELIGIONbiaswithQuantizedResults
Figure3:CrowsRACEbiaswithQuantizedResults
Figure2:CrowsGENDERbiaswithQuantizedResults
2670


Figure7:StereosetRELIGIONbiaswithQuantizedResults
Figure5:StereosetGENDERbiaswithQuantizedResults
Figure6:StereosetRACEbiaswithQuantizedResults
2671


↑0.2484.41+DYNAMICPTQint8
↓9.0660.82
↓8.7351.55
↑0.2164.49
↓2.2062.08
↓3.9262.40
↓3.2956.99
↑1.1665.44
↓0.0666.26
↓1.3760.30
↑0.5166.83
↓0.9159.37
↓0.4063.88
↑0.2459.94
ModelGENDERbiasRACEbiasRELIGIONbiasLMScore
↓1.0983.08+DROPOUT(Websteretal.,2020)
↓0.7088.23+SELF-DEBIAS(Schicketal.,2021)
↓3.1558.52
↓3.7455.96
↓30.3053.87
↑0.3860.66
↓2.8958.78
↓9.5749.87
↓0.8259.46
↓0.1083.83+DROPOUT(Websteretal.,2020)
↓1.4462.84
↑0.2489.19
↓0.0884.09+SENTENCEDEBIAS(Liangetal.,2020)
Figure8:StereosetLMScorewithQuantizedResultsTable4:SSstereotypescoresandlanguagemodelingscores(LMScore)forBERT,andRoBERTamodels.Stereotypescorescloserto50%indicatelessbiasedmodelbehavior.BoldvaluesindicatethebestmethodperbiasandLMScore.ResultsareontheSStestset.Arandommodel(whichchoosesthestereotypicalcandidateandtheanti-stereotypicalcandidateforeachexamplewithequalprobability)obtainsastereotypescoreof50%inexpectation.
↓2.7354.30
RoBERTaBase66.3261.6764.2888.95+DYNAMICPTQint8
BERTBase60.2857.0359.7084.17+DYNAMICPTQint8
↑0.1489.09+DYNAMICPTQint8
↓1.8964.43
↓0.6888.27DistilRoBERTaBase
↓0.5759.13
↓2.4457.26
↓2.4457.26
↓3.9460.34
↓1.2865.04
↑0.0188.94RoBERTaLarge
↓0.3056.73
↓3.4158.26
↓0.6788.26+SENTENCEDEBIAS(Liangetal.,2020)
↓0.2364.51
↓3.1281.05DistilBERTBase
↑2.9663.24
↓0.1188.81+INLP(Ravfogeletal.,2020)
↓6.4050.63
↑0.7557.78
↓3.5480.63+SELF-DEBIAS(Schicketal.,2021)
↓2.9481.23+CDA(Websteretal.,2020)
↓0.7360.95
↓1.1483.04+INLP(Ravfogeletal.,2020)
↑0.0457.07
↑0.0457.07
↓0.3763.91
↑0.0384.20BERTLarge
↓2.8756.83
↓3.0357.25
↓0.6759.61
↑0.2657.29
↓5.7583.20+CDA(Websteretal.,2020)
↓1.3358.37
↑1.0562.72
↓0.0364.25
↓3.5562.77
↓2.3654.67
↓2.7263.60
↓2.1059.57
↓2.0464.28
↓0.3661.31
↓1.8655.17
↓1.2760.41
↓0.9758.73
↓0.9459.34
2672


88.336K59.4/54.7/57.3410M
88.7114K63.3/57.8/60.91.4B
LMScoreStepNr.BiasG./RA./RE.
LMScoreStepNr.BiasG./RA./RE.
70M
70M
Table6:LMScoresvs.BiasesontheSSdatasetoftheoriginal(full-precision)models,atthesamestepswiththebestLMScorefortheint8models(Table3).
ModelSize
ModelSize
89.821K62.7/57.7/57.0410M
88.429K58.9/55.4/58.0160M
91.550K67.2/60.5/63.31.4B
90.5114K64.3/58.3/62.06.9B
92.450K65.3/63.5/63.86.9B
Figure9:SeatGENDERbiaswithQuantizedResults
92.221K67.0/61.0/64.9
91.829K65.9/61.2/64.92.8B
90.5129K66.6/62.2/64.7
Figure10:SeatRACEbiaswithQuantizedResults
90.1129K65.5/60.0/62.52.8B
87.721K55.4/56.8/58.8160M
Table5:LMScoresvs.BiasesontheSSdatasetoftheint8models,atthesamestepswiththebestLMScorefortheoriginal(full-precision)models(Table2).
2673


RoBERTaBase0.922∗0.2080.979∗1.460∗0.810∗1.261∗0.940+DYNAMICPTQint80.3500.1770.389∗1.038∗0.3490.897∗
↓0.1860.434BERTLarge0.370-0.0150.418∗0.221-0.2590.710∗
↑0.2260.846DistilBERT0.061-0.2220.093-0.1200.2220.112
↑0.1341.074+INLP0.812∗0.0590.604∗1.407∗0.812∗1.246∗
↓0.2880.332+DYNAMICPTQint80.905∗0.2731.097∗0.894∗0.728∗1.180∗
↓0.1580.462+CDA0.846∗0.186-0.2781.342∗0.831∗0.849∗
BERTBase0.931∗0.090-0.1240.937∗0.783∗0.858∗0.620+DYNAMICPTQint80.614∗0.000-0.4960.711∗0.4010.549∗
↓0.1170.823+SENTENCEDEBIAS0.755∗0.0680.869∗1.372∗0.774∗1.239∗
↓0.4160.204+SENTENCEDEBIAS0.350-0.298-0.6260.458∗0.4130.462∗
↓0.0940.846RoBERTalarge0.849∗0.170-0.2370.900∗0.510∗1.102∗
↑0.1020.722+DROPOUT1.136∗0.3170.1381.179∗0.879∗0.939∗
↓0.4060.533+CDA0.976∗0.0130.848∗1.288∗0.994∗1.160∗
↑0.1440.765+INLP0.317-0.354-0.2580.1050.187-0.004
↑0.0590.999
↓0.4820.138
↓0.0600.880+DROPOUT1.134∗0.2091.161∗1.482∗1.136∗1.321∗
↓0.6400.300DistilRoBERTa1.229∗0.1920.859∗1.504∗0.748∗1.462∗
↓0.3120.628+DYNAMICPTQint80.446∗0.218-0.3680.423∗-0.0400.303
Figure11:SeatRELIGIONbiaswithQuantizedResultsTable7:GENDERbiasonSEATdataset.Effectsizescloserto0areindicativeoflessbiasedmodelrepresentations.Boldvaluesindicatethebestmethodpertest.Statisticallysignificanteffectsizesatp<0.01aredenotedby*.ThefinalcolumnreportstheaverageabsoluteeffectsizeacrossallsixgenderSEATtestsforeachmodel.
Modelweat6weat6bweat7weat7bweat8weat8bAvg.Effect
2674


↑0.0760.383+INLP0.2220.4450.354∗0.1300.1250.636∗0.301∗
↑0.1150.735+CDA0.2310.619∗0.824∗0.510∗0.896∗0.418∗0.486∗
↑0.1440.271RoBERTaLarge-0.163-0.685-0.158-0.542
↓0.0950.397DistilBERT0.1720.529∗0.3180.076
ModelABWSABWS-bweat3weat3bweat4weat5weat5bAvg.Effect
Modelreligion1religion1breligion2religion2bAvg.Abs.Effect.
Table8:RACEbiasonSEATdataset.ABWS:angry-black-woman-stereotype.Effectsizescloserto0areindicativeoflessbiasedmodelrepresentations.Boldvaluesindicatethebestmethodpertest.Statisticallysignificanteffectsizesatp<0.01aredenotedby*.ThefinalcolumnreportstheaverageabsoluteeffectsizeacrossallsevenraceSEATtestsforeachmodel.
↓0.0340.273RoBERTaLarge-0.0900.2740.869∗-0.0210.943∗0.767∗0.061
↓0.3050.295DistilBERT1.081∗-0.9270.441∗0.2020.358∗0.726∗-0.076
↓0.0760.544
↑0.1190.246+SENTENCEDEBIAS0.002-0.088-0.516-0.477
↑0.1720.298+CDA0.3410.148-0.222-0.269
↑0.1060.232
↑0.1190.245+DROPOUT0.2430.152-0.115-0.159
↓0.0080.612BERTLarge-0.2190.953∗0.420∗-0.3750.415∗0.890∗-0.345
Table9:RELIGIONbiasonSEATdataset.Effectsizescloserto0areindicativeoflessbiasedmodelrepresentations.Boldvaluesindicatethebestmethodpertest.Statisticallysignificanteffectsizesatp<0.01aredenotedby*.ThefinalcolumnreportstheaverageabsoluteeffectsizeacrossallfourreligionSEATtestsforeachmodel.
↓0.0670.554+INLP0.2950.565∗0.799∗0.370∗0.976∗1.039∗0.432∗
RoBERTaBase0.1320.018-0.191-0.1660.127+DYNAMICPTQint80.527∗0.567∗0.0790.020
RoBERTaBase0.395∗0.159-0.114-0.003-0.3150.780∗0.386∗0.307+DYN.PTQint80.660∗-0.118-0.1730.093-0.3180.337∗0.364∗
↑0.1250.432+DYN.PTQint8-0.065-0.0140.587∗-0.1900.572∗0.580∗-0.173
↑0.0520.179DistilRoBERTa0.490∗0.0190.291-0.131
↑0.0410.167+INLP-0.309-0.347-0.191-0.135
↑0.0090.316+SENTDEBIAS0.407∗0.084-0.1030.015-0.3000.728∗0.274∗
BERTBase0.744∗-0.0671.009∗-0.1470.492+DYNAMICPTQint80.524∗-0.1710.689∗-0.205
↓0.1150.377+INLP0.473∗-0.3010.787∗-0.280
↓0.0510.569+DROPOUT0.415∗0.690∗0.698∗0.476∗0.683∗0.417∗0.495∗
↓0.3060.186+DYNAMICPTQint80.524∗-0.1710.689∗-0.205
↓0.2180.274
↑0.0150.322+DROPOUT0.499∗0.392-0.1620.044-0.3670.841∗0.379∗
↓0.1520.339+DROPOUT0.535∗0.1090.436∗-0.428
BERTBase-0.0790.690∗0.778∗0.469∗0.901∗0.887∗0.539∗0.620+DYN.PTQint80.772∗0.4250.835∗0.548∗0.970∗1.076∗0.517∗
↓0.0120.295+CDA0.455∗0.300-0.0800.024-0.3080.716∗0.371∗
↓0.1040.517+DYN.PTQint80.660∗-0.118-0.1730.093-0.3180.337∗0.364∗
↓0.0950.397+CDA0.355-0.1040.424∗-0.474
↑0.2600.387+DYNAMICPTQint80.117-0.2920.2930.015
↓0.0530.439BERTLarge0.0110.144-0.160-0.426
↑0.0190.639+SENTDEBIAS-0.0670.684∗0.776∗0.451∗0.902∗0.891∗0.513∗
↑0.0040.312DistilRoBERTa0.774∗0.112-0.062-0.012-0.4100.843∗0.456∗
↑0.0740.381
↓0.0310.460+SENTENCEDEBIAS0.728∗0.0030.985∗0.038
2675