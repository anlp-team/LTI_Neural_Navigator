{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_New_Benchmark_of_Aphasia_Speech_Recognition_and_Detection_Based_on_E-Branchformer_and_Multi-task_Learning_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the beam search in the encoder and decoder output in this text?,answer: The purpose of the beam search is to produce the final hypothesis during inference.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " How is the system often evaluated in the context of speech recognition?,answer: The system is often evaluated with word error rate (WER).", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " What is the purpose of the Intermediate CTC (InterCTC) in this text?,answer: The purpose of the Intermediate CTC is to regularize deep encoder networks and support multi-task learning.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " How is the InterCTC loss calculated in this text?,answer: The negative log likelihood of generating the InterCTC target sequence is used as the InterCTC loss.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " What does the InterCTC weight \u03b1 represent?,answer: The InterCTC weight \u03b1 represents a hyper-parameter in the loss function.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " Describe the architecture of the Conformer in the context of speech processing tasks.,answer: The Conformer architecture combines convolution and self-attention, allowing for capturing both the global and local context of input sequences.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " What is the purpose of SSLR in enhancing acoustic feature extraction?,answer: SSLR leverages unlabeled speech data to learn a universal representation from speech signals, improving the generalizability of acoustic feature extraction.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " How does the tag-based Aphasia detector train the ASR model?,answer: The tag-based Aphasia detector trains the ASR model using Aphasia tag tokens in the ground truth to detect Aphasia based on linguistic and acoustic information.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " What is the purpose of using InterCTC in the second type of Aphasia detector?,answer: The second type of Aphasia detector uses InterCTC to classify input speech as either Aphasia or healthy speech during inference.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}, {"question": " How is the speaker-level Aphasia prediction obtained for both the tag-based and InterCTC-based detectors?,answer: The speaker-level Aphasia prediction is obtained via majority voting of the predictions made for all segments.", "ref_chunk": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}], "doc_text": "|X) where the CTC weight \u03bb is an hyper-parameter. The output of the encoder and decoder is jointly decoded using beam search to produce the \ufb01nal hypothesis during inference [17]. The system is often evaluated with word error rate (WER). 2.2. Intermediate CTC Intermediate CTC (InterCTC) was proposed to regularize deep encoder networks and to support multi-task learning [22, 25, 26]. To achieve this, the existing CTC module is applied to the output of an intermediate encoder layer with index e. Then sub- sequent encoder layers incorporate the intermediate predictions into their input. Equation 1 can be reorganized as: He = Enc1:e(X) P (ZInter|X) = CTC(He) H = Ence+1:E(NRM(He) + LIN(ZInter)) where E is the total number of encoder layers, and ZInter is the latent sequence of the InterCTC target sequence TInter = (t\u2032 ). NRM(\u00b7) and LIN(\u00b7) refer to a normal- ization layer and a linear layer respectively. The negative log likelihood of generating TInter is used as the InterCTC loss: k|k = 1, . . . , K \u2032 LInter = \u2212 log PInter(TInter|X) The choice of TInter is dependent on the task. During training, the intermediate layer is optimized to correctly predict TInter by including LInter in the loss function: L \u2032 CTC = \u03b1LInter + (1 \u2212 \u03b1)LCTC where the InterCTC weight \u03b1 is a hyper-parameter. The up- dated overall loss function is obtained by inserting Equation 11 into Equation 5: L \u2032 = \u03bbL \u2032 CTC + (1 \u2212 \u03bb)LDec Note that it is possible to apply CTC to multiple encoder layers while having different target sequences for each. In that case, the average of all InterCTC losses is used as LInter [22, 26]. (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) 2.3. Speech Recognizer follows the design of a hybrid Our speech recognizer CTC/Attention architecture described in Section 2.1. It tran- scribes the acoustic feature sequence Xij belonging to speaker sj to the corresponding text token sequence Tij . We experiment with recently proposed encoder architec- tures that enhance acoustic modeling ability over the original Transformer. One of these architectures, called Conformer, se- quentially combines convolution and self-attention. This al- lows for capturing both the global and the local context of in- put sequences [27]. On the other hand, Branchformer mod- els these contexts using parallel branches and merges them together. Both architectures demonstrate competitive perfor- mance in speech processing tasks [28]. In subsequent studies, E-Branchformer is proposed to enhance Branchformer further. It comprises a better method for merging the branches, and it achieves the new state-of-the-art ASR performance [18]. Meanwhile, self-supervised learning representation (SSLR) has been developed to improve the generalizability of acoustic feature extraction. SSLR leverages a large amount of unlabeled speech data to learn a universal representation from speech sig- nals. Studies show signi\ufb01cant performance improvement in ASR and other downstream tasks by using SSLR as the input of the encoder [19, 21, 29, 30]. 2.4. Aphasia Detectors We present two types of Aphasia detectors based on the speech recognizer, the tag-based detector and the InterCTC-based de- tector. Inspired by the use of language identi\ufb01ers in multilingual speech recognition [31\u201333], we form an extended vocabulary V \u2032 by adding two Aphasia tag tokens to V : V \u2032 = V \u222a {[APH], [NONAPH]} ij = (tk \u2208 V \u2032 We then train the ASR model using T tag |k = 1, . . . , K) as the ground truth, where T tag ij is formed by insert- ing one or more Aphasia tags to Tij . Speci\ufb01cally, [APH] is in- serted if the speaker has Aphasia while [NONAPH] is inserted if the speaker is healthy. This method effectively trains the model to perform both tasks jointly. Moreover, the model leverages both linguistic and acoustic information to detect Aphasia, as the encoder \ufb01rst generates an initial tag prediction based on the acoustic features, and the decoder then re\ufb01nes the prediction based on prior textual context. During inference, the sentence- level prediction is obtained by taking out the tag token from the predicted sequence. Three tag insertion strategies will be tested in Section 3: prepending, appending, and using both. We note that all tag tokens are excluded from WER computation. InterCTC is proven to be effective at identifying language identity in multilingual ASR as part of a multi-task objective. By conditioning on its language identity predictions, the ASR model achieves state-of-the-art performance on FLEURS [22]. Inspired by this, the second type of Aphasia detector uses In- terCTC to classify input speech as either Aphasia or healthy speech. During training, the ground truth sequence T inter for In- terCTC contains an Aphasia tag token. During inference, the prediction \u02c6yij is generated by checking the tag produced by In- terCTC greedy search. This approach allows us to select which encoder layer to use for the best speaker-level accuracy. ij For both the tag-based and InterCTC-based detectors, the speaker-level Aphasia prediction yj is obtained via majority voting of yij for all i. (13) 3. Experiments In this section, we \ufb01rst explore the impact of state-of-the-art encoder architectures and SSLR on Aphasia speech recognition. We then analyze the performance of the proposed method for recognition and detection tasks. All of our experiments were conducted using ESPnet2 [34]. 3.1. Datasets 3.1.1. CHAT Transcripts CHAT [35] is a standardized transcription format for describ- ing conversational interactions, used by both AphasiaBank and DementiaBank. Besides the textual representations of spoken words, it includes a set of notations that describes non-speech sounds, paraphasias, phonology, morphology, and more. The transcript cleaning procedures differ between prior works, mak- ing it dif\ufb01cult to fairly compare their machine learning systems. Therefore, we derive a pipeline based on previous work [5] in the hope of standardizing this process for future research. The speci\ufb01c steps of our pipeline are as follows. (1) Keep the textual representations of retracing, repetitions, \ufb01ller words, phonological fragments, and IPA annotations while removing their markers. (2) Replace laughter markers with a special to- ken <LAU>. (3) Remove pre-codes,"}