{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_InPars-Light:_Cost-Effective_Unsupervised_Training_of_Efficient_Rankers_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the paper mentioned in the text?", "answer": " InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " Who are the authors of the paper?", "answer": " Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What is the main focus of InPars-Light?", "answer": " Unsupervised training of neural rankers", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What model did InPars-Light use compared to the proprietary GPT-3 model?", "answer": " BLOOM", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " How many English retrieval collections did InPars-Light use in the study?", "answer": " Five", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What was the average gain over BM25 for the MiniLM-L6-30M ranker in the study?", "answer": " 1.13", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What question does the research question RQ2 address?", "answer": " Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model?", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What is the importance of carrying out a reproducibility study of InPars?", "answer": " To determine if key findings of InPars can be reproduced using open-source and community-trained models", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What is the significance of InPars-Light in the field of neural ranking models?", "answer": " It is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}, {"question": " What method was employed to filter out generated queries for consistency checking in the study?", "answer": " DeBERTA-v3-435M model", "ref_chunk": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}], "doc_text": "4 2 0 2 b e F 1 2 ] R I . s c [ 2 v 8 9 9 2 0 . 1 0 3 2 : v i X r a Published in Transactions on Machine Learning Research (MM/YYYY) InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers Leonid Boytsov\u2217 Amazon AWS AI Labs Pittsburgh USA Preksha Patel Vivek Sourabh Riddhi Nisar Sayani Kundu Ramya Ramanathan Eric Nyberg Carnegie Mellon University Pittsburgh USA leo@boytsov.info Reviewed on OpenReview: https: // openreview. net/ forum? id= sHSKFYyINO Abstract We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which\u2014as we found out\u2014produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.https://github.com/searchivarius/inpars_light/ 1 Introduction Training effective neural IR models often requires abundant in-domain training data, which can be quite costly to obtain: For a human annotator, judging a single document-query pair takes at least one minute on average (Han et al., 2020; Kwiatkowski et al., 2019) and a single query may need as many as 50 of such judgements (Buckley et al., 2007).1 Models trained on out-of-domain data and/or fine-tuned using a small number of in-domain queries often perform worse or marginally better than simple non-neural BM25 rankers Work done outside of the scope of employment. 1Robust04 and TREC-COVID collections used in our study have about 1K judgements per query. 1 Published in Transactions on Machine Learning Research (MM/YYYY) Table 1: Average Gains over BM25 for different Models and Training Recipes Model name and training recipe Avg. gain over BM25 # of \u201cwins\u201d over BM25s (\u2264 7) Unsupervised: InPars-based Training Data (three-shot prompting) MiniLM-L6-30M (InPars-light) DeBERTA-v3-435M (InPars-light) 1.13 1.30 7 7 monoT5-220M (InPars) (Bonifacio et al., 2022) monoT5-3B (InPars) (Bonifacio et al., 2022) 1.07 1.32 3 7 Supervised transfer learning with optional unsupervised fine-tuning: transfer from MS MARCO with optional fine-tuning on consistency-checked InPars data MiniLM-L6-30M (MS MARCO) MiniLM-L6-30M (MS MARCO \u25b6 consist. checked queries) 1.21 1.24 5 7 DeBERTA-v3-435M (MS MARCO) DeBERTA-v3-435M (MS MARCO \u25b6 consist. checked queries) 1.42 1.36 7 7 monoT5-220M (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO) (Bonifacio et al., 2022) monoT5-3B (MS MARCO+InPars) (Bonifacio et al., 2022) 1.46 1.59 1.59 7 7 7 Consist. checked queries denotes a set of generated queries filtered out (via consistency checking) using the DeBERTA-v3- 435M model trained on InPars-generated data. (Thakur et al., 2021; Mokrii et al., 2021). Good transferability requires (1) large impractical models (Rosa et al., 2022; Ni et al., 2021), and (2) datasets with large and diverse manually annotated query sets. A recent trend to deal with these problems consists in gen- erating synthetic in-domain training data via prompting of Large Language Models (LLMs). This trend was spear- headed by a recent InPars study (Bonifacio et al., 2022). However, proposed solutions are not cost effective because they require either querying the costly generative models or training impractically large rankers. Although follow up studies, in particular by Dai et al. (2022), claimed improvements upon InPars, these improvements were not demonstrated under the same experimental setting. More- over, researchers used primarily proprietary LLMs whose training procedure was not controlled by the scientific community. Thus, outcomes could have been affected by data leakage, i.e., training of models on publicly available and popular IR collections whose copies could have ended up in the LLMs training data. As such, there is an im- portant question of whether we can obtain comparable or better results using only open-source models trained by the scientific community. Figure 1: Average relative improvement over BM25 for different model types/sizes and train- ing recipes. Higher and to the left is better. We compare InPars with InPars-Light for the unsu- pervised training scenario, where training data is generated by an LLM using a three-shot prompt. This study is driven by two high-level inquiries: (1) Does InPars work? (2) Can it be made more accurate and cost effective? To address these inquiries, we carry out a rigorous reproducibility study of InPars (Bonifacio et al., 2022). In that, we use open-source and community-trained generative LLMs (Scao et al., 2022; Wang & Komatsuzaki, 2021), train rankers using multiple seeds, and use statistical testing when measuring improvements. Because efficiency is an important consideration, we also evaluate much smaller ranking models (see Figure 1 and Table 1) compared to those used by Bonifacio et al. (2022). More specifically, we ask the following research questions: RQ1: Can we reproduce key findings of InPars (Bonifacio et al., 2022) using open-source and community-trained LLMs as well as smaller ranking models? 2 Published in Transactions on Machine Learning Research (MM/YYYY) RQ2: Are open-source models more or less useful for generation of synthetic IR training data compared to the similar-sized GPT-3 Curie model (Brown et al., 2020)? RQ3: Does consistency checking proposed by Dai et al. (2022) improve the"}