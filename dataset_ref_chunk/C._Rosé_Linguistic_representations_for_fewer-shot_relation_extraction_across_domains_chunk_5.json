{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/C._Rose\u0301_Linguistic_representations_for_fewer-shot_relation_extraction_across_domains_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What percentage of the variation in F1 scores does the ANOVA model explain?,answer: 98%", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the effect of transfer setting on performance according to the results?,answer: In-domain performance on the entire dataset is better than transfer performance in a few-shot setting.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " How does the number of target domain examples affect performance?,answer: Larger numbers of target domain examples are associated with higher performance.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What did the student-t analysis reveal about the pairwise comparisons?,answer: All pairwise comparisons were found to be significant.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the significant interaction between transfer setting and few shot setting?,answer: The effect of the few-shot setting is restricted to the transfer setting.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the significant effect of representation case according to the results?,answer: +Dep and +AMR cases are better than plain text.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the effect of representation case on transfer setting performance?,answer: The effect of case is only significant in the transfer setting.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " At what shot setting does the effect of representation case become significant?,answer: The effect is only significant for the 5- and 10-shot settings.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the impact of source domain on the utility of linguistic representations?,answer: Transfer between datasets for text-only models is of limited utility, while incorporating linguistic formalisms proves to be more robust.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}, {"question": " What is the key takeaway regarding the choice of source domain on transfer performance?,answer: The linguistic representations, regardless of source domain, are never worse than the baseline trained on that source domain and frequently outperform the baseline trained from scratch.", "ref_chunk": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}], "doc_text": "of individual, pairwise comparisons. We expect that the similarity between source and target datasets, the variation in the target dataset, and the few-shot setting could all either dampen or magnify any effect of representation on the perfor- mance. We therefore include pairwise interaction terms in the ANOVA model for case by source dataset, case by target dataset, case by transfer setting, and case by few-shot setting. The exam- ples added for the few shot setting in the transfer case are sampled from the training split of the tar- get dataset. Thus, while we expect for the cross- domain case the few shot setting has an effect, we do not expect an effect in the in-domain case, since the target domain examples added to the training data simply replicate examples that were already part of the dataset. To account for this, we include a final interaction term between few shot setting and transfer setting in the ANOVA model. The ANOVA model explains 98% of the varia- tion in F1 scores. The results align well with our intuitions. First, as expected we find a significant effect of transfer setting such that in-domain perfor- mance on the entire dataset is better than transfer performance in a few-shot setting: F(1, 679) = 10356.25, p < .0001. In these cases, the original (a) +AMR vs baseline (b) +Dep vs baseline Figure 2: Differences in F1 over baseline from incorporating linguistic graphs in models. dataset for in-domain training is between 5 and 15 times the size of the target training dataset. We also find a significant effect of the few-shot setting, such that larger numbers of target domain examples are associated with higher performance, F(5, 679) = 716.79, p < .0001. A post-hoc student-t analysis re- veals that all pairwise comparisons are significant. Notably, there is a significant interaction between transfer setting and few shot setting: F(5, 679) = 733.83, p < .0001, such that the effect of the few- shot setting is restricted to the transfer setting, as expected. Our hypothesis is primarily related to the impor- tance of the representation of the data for efficiently enabling transfer between domains. We find a sig- nificant effect of representation case: F(2, 679) = 5.26, p < .01. A student-t post-hoc analysis reveals that both +Dep and +AMR cases are bet- ter than plain text, but there is no significant difference between the two. There is also a signif- icant interaction between representation case and transfer setting: F(2, 679) = 8.19, p < .0005. In particular, the effect of case is only significant in the transfer setting. There is also a significant inter- action between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. representation cases in the transfer setting. Ad- ditionally, we investigate the impact of source do- main on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as \"From Scratch\" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differ- ences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited util- ity, if not outright harmful. While we see one in- stance (the EFGC to RISeC transfer) in which in- troducing a transfer source dataset improves the baseline model\u2019s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small differ- ence, or even hurts the performance of the baseline model. In the cases of transfer between MSCor- pus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorpo- rating linguistic formalisms proves to be far more robust to the choice of source domain: the linguis- tic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the base- line trained from scratch, even when the choice of source domain imposes a performance penalty. We present all of our few-shot results in Table 3. Significance testing was performed on the differ- ence in results between the baseline and lingusitic Fewshot Setting Target Source Case 1 5 10 20 50 100 RISeC From Scratch Baseline MSCorpus EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR MSCorpus From Scratch Baseline RISeC EFGC +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR EFGC From Scratch Baseline RISeC MSCorpus +Dep +AMR Baseline +Dep +AMR Baseline +Dep +AMR 18.6 (2.9) 19.3 (4.5) 19.8 (7.1) 19.7 (5.5) 19.4 (2.1) 21.9 (2.6) 25.8 (5.0) 28.8 (7.7) 27.0 (7.6) 25.0 (4.9) 30.6 (2.8) 26.7 (4.3) 24.4 (2.2) 30.6 (0.5) 25.3 (3.1) 26.9 (4.6) 31.7 (4.0) 31.9 (3.8) 16.2 (1.5) 17.2 (4.1) 14.2 (2.3) 16.0 (1.7) 18.2 (4.5) 18.1 (1.5) 17.4 (4.4) 17.0 (3.8) 17.1 (2.4) 36.5 (3.2) 40.0 (2.8) 39.3 (5.2) 35.1 (5.4) 39.7 (5.2) 39.4 (4.2) 42.0 (4.0) 50.5 (3.9) 47.7 (8.9) 46.9 (2.7) 49.5 (1.0) 45.3 (0.9) 43.4"}