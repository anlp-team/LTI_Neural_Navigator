{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_Reasoning_about_the_Unseen_for_Efficient_Outdoor_Object_Navigation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the ability to avoid objects depend on, according to the text?", "answer": " The ability to avoid objects depends on the graph\u2019s edge length and the precision of the perception model.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " How does the text suggest smaller obstacles could potentially be handled more adeptly?", "answer": " With a more advanced perception model that can accurately determine the relative positions of objects.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " What is the main reason for the significant disparity between OSR and SR, as mentioned in the text?", "answer": " The divergence predominantly emerges because the VLM often fails to acknowledge its goal, leading the agent off target.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " Why does the VLM have a propensity to hallucinate object positions, particularly in outdoor environments?", "answer": " It becomes challenging to provide accurate descriptions of every object in outdoor environments.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " According to the text, what does a consistent CASR despite varying CT indicate?", "answer": " A balance between computation and task time, indicating that other factors such as motion planning dominate.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " What new avenues have emerged in the embodied agent domain, according to the text?", "answer": " The emergence of LLMs has opened new avenues in the embodied agent domain.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " What did the paper introduce in terms of using LLMs for reasoning about robot plans in unseen environments?", "answer": " The paper introduced a novel, general mechanism for using LLMs to reason about robot plans in unseen environments.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " What does the proposed CASR metric aim to assess?", "answer": " The CASR metric aims to assess the balance between reasoning and action for embodied agents.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " According to the text, what do humans trade off between when navigating and exploring?", "answer": " Humans trade off between thinking and acting to leverage what they know in general and can see in the specific.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}, {"question": " What is the OUTDOOR task introduced in the text aimed at?", "answer": " The OUTDOOR task is a pioneering approach aimed at propelling embodied agents into challenging outdoor settings.", "ref_chunk": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}], "doc_text": "The ability to avoid these objects hinges on the graph\u2019s edge length and the preci- sion of the perception model. We posit that with a more advanced perception model, which can accurately determine Vis + Eval CASR GPT3.5+ 3.5 GPT3.5+ 4 GPT4 + 4 0.449 0.732 - TABLE IV CHOICE OF MODEL Fig. 7. Comparison between SR and OSR for Reasoned Explorer the relative positions of objects, the method holds potential to adeptly handle smaller obstacles as well. B. Current Limitations of VLM As illustrated in Figure 7, there is a significant disparity between OSR and SR, especially with harder tasks. This divergence predominantly emerges because the VLM, upon achieving its goal, often fails to acknowledge it. This mis- recognition subsequently leads the agent off target. Further- more, the VLM has a propensity to hallucinate the positions of objects. In outdoor environments, it becomes particularly challenging to provide accurate descriptions of every object. Consequently, future work may consider the direct use of embeddings in lieu of caption data. Such observations point towards the potential of improving VLM\u2019s performance, ideally bridging the gap and enabling SR to align more closely with OSR. C. Observation Derived from CASR The CASR metric serves as both a metric for method comparison and a tool for understanding the balance between computational time and task efficiency. Key observations are: 1) Positive Correlation with CT: An increase in CT leading to a higher CASR suggests that more computa- tion can reduce travel time (TT), enhancing efficiency. 2) Negative Correlation with CT: A drop in CASR with increased CT indicates diminishing returns from additional computation (e.g. saturation). 3) Steady CASR despite CT Variation: A consistent CASR, despite varying CT, indicates a balance be- tween computation and task time, so other factors, such as motion planning dominate. VII. CONCLUSIONS The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUT- DOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and pro- posed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific. REFERENCES [1] P. Anderson, A. Chang, D. S. Chaplot, et al., On evaluation of embodied navigation agents, 2018. [2] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdi- nov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d in In Neural Information Processing Systems (NeurIPS), 2020. [3] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, 2022. [4] A. Nanavati, X. Z. Tan, and A. Steinfeld, \u201cCoupled indoor navigation for people who are blind,\u201d in Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, ser. HRI \u201918, Chicago, IL, USA: Association for Computing Machinery, 2018, 201\u2013202. [5] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, \u201cLearning to explore using active neural slam,\u201d in International Conference on Learning Represen- tations (ICLR), 2020. [6] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in The Tenth International Confer- ence on Learning Representations, 2022. [7] M. G. Castro, S. Triest, W. Wang, et al., \u201cHow does it feel? self-supervised costmap learning for off-road vehicle traversability,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 931\u2013938. [8] T. B. Brown, B. Mann, N. Ryder, et al., Language models [9] are few-shot learners, 2020. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018. [10] OpenAI, Gpt-4 technical report, 2023. [11] M. Ahn, A. Brohan, N. Brown, et al., \u201cDo as i can and not as i say: Grounding language in robotic affordances,\u201d in arXiv preprint arXiv:2204.01691, 2022. [12] D. Shah, B. Osi\u00b4nski, S. Levine, et al., \u201cLm-nav: Robotic navigation with large pre-trained models of language, vision, and action,\u201d in Conference on Robot Learning, PMLR, 2023, pp. 492\u2013504. [13] A. Brohan, N. Brown, J. Carbajal, et al., \u201cRt-1: Robotics transformer for real-world control at scale,\u201d arXiv preprint arXiv:2212.06817, 2022. J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, 2023. [14] [15] K. Zhou, K. Zheng, C. Pryor, et al., Esc: Exploration with soft commonsense constraints for zero-shot object naviga- tion, 2023. [16] G. Zhou, Y. Hong, and Q. Wu, Navgpt: Explicit reasoning in vision-and-language navigation with large language models, 2023. J. Wei, X. Wang, D. Schuurmans, et al., Chain-of-thought prompting elicits reasoning in large language models, 2023. [18] S. Yao, D. Yu, J. Zhao, et al., Tree of thoughts: Deliberate problem solving with large language models, 2023. [19] Y. Xie, K. Kawaguchi, Y. Zhao, et al., Decomposition enhances reasoning via self-evaluation guided decoding, 2023. [17] [20] S. Hao, Y. Gu, H. Ma, et al., \u201cReasoning with lan- guage model is planning with world model,\u201d arXiv preprint arXiv:2305.14992, 2023. [21] Z. Zhao, W. S. Lee, and D. Hsu, \u201cLarge language models as commonsense knowledge for large-scale task planning,\u201d in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [22] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, \u201cPlanning with large language models for code generation,\u201d in The Eleventh International Conference on Learning Representations, 2023. [23] L. Wang, W. Xu, Y. Lan, et al., Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023. [24] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, Describe, explain, plan and select: Interactive planning with large lan- guage models enables open-world multi-task agents, 2023. [25] W. Huang, F. Xia, T. Xiao, et al., \u201cInner monologue: Em- bodied reasoning through planning with language models,\u201d in"}