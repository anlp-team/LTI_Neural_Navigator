{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_I3D:_Transformer_Architectures_with_Input-Dependent_Dynamic_Depth_for_Speech_Recognition_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Why can the encoder learn powerful representations for the ASR task?,        answer: Auxiliary CTC losses inserted at intermediate layers facilitate gradient propagation to lower parts of the deep encoder, effectively improving its capacity and final performance.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " What role does gate analysis play in interpreting the behavior of deep networks?,        answer: Gate analysis provides a way to interpret the layer-wise behavior of deep networks.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " How do I3D models adjust the encoder depth based on the characteristics of an input utterance?,        answer: It has been shown that I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " What factor affects the inference architecture based on the number of blocks used by an I3D-GlobalGP model during inference?,        answer: The speech length generally affects the inference architecture.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " What is the probable reason that longer utterances tend to use more blocks, especially the MHA blocks, in processing?,        answer: Longer utterances contain more complex information and longer-range dependency among frames, requiring more blocks for processing.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " Is there a clear relationship observed between the difficulty of utterances, measured by WERs, and the number of layers used for inference?,        answer: In general, no clear relationship was observed between the difficulty of utterances and the number of layers used for inference.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " How is the generalizability of the proposed I3D encoders demonstrated?,        answer: The proposed I3D encoders can be directly applied to other datasets and ASR frameworks.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " In the context of ASR models, what performance trade-off does the I3D encoder dynamically adjust its depth for?,        answer: The I3D encoder dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and efficiency.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " What are the two types of gate predictors designed for I3D-based models in the study?,        answer: The study designed a global gate predictor and a local gate predictor for I3D-based models.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}, {"question": " What is the key aspect highlighted in the future plans mentioned in the conclusion of the study?,        answer: The future plans include applying the method to large pre-trained models and exploring fine-tuning gate predictors to reduce training cost significantly.    ", "ref_chunk": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}], "doc_text": "encoder can learn powerful representations for the ASR task. This is probably because auxiliary CTC losses inserted at intermediate layers can facilitate the gradient propagation to lower parts of a deep encoder, which effectively improves its capacity and also the \ufb01nal performance. We believe this gate analysis can provide a way to interpret the layer-wise behavior of deep networks. 3.4. Analysis of input-dependency It has been shown that our I3D models can dynamically adjust the encoder depth based on the characteristics of an input utterance, which achieves strong performance even with reduced computation. But it is unclear which features are important for the gate predic- tor to determine the modules used during inference. We have found that the speech length generally affects the inference architecture. Fig. 7 shows the speech length distributions categorized by the num- ber of MHA or FFN blocks used by an I3D-GlobalGP model during inference. We observe that utterances using more blocks tend to be longer. This is probably because longer utterances contain more complex information and longer-range dependency among frames, which require more blocks (especially MHA) to process. We also considered two other factors that may affect the infer- y c n e u q e r F 0.4 0.2 0 18 blocks 20 blocks 19 blocks 21 blocks 0 5 10 15 20 25 Input speech length in seconds (a) MHA y c n e u q e r F 0.1 0 19 blocks 21 blocks 20 blocks 22 blocks 0 5 10 15 20 25 Input speech length in seconds (b) FFN Fig. 7: Distributions of speech lengths categorized by the number of MHA or FFN blocks used for inference. This is an I3D-GlobalGP model evaluated on LibriSpeech test other. Utterances using more blocks tend to be longer. ence architecture, namely the dif\ufb01culty of utterances measured by WERs, and the audio quality measured by DNSMOS scores [39]. However, in general, we didn\u2019t observe a clear relationship between these metrics and the number of layers used for inference. 3.5. Generalizability We demonstrate that the proposed I3D encoders can be directly ap- plied to other datasets and ASR frameworks. Fig. 4 shows the results of CTC-based models on Tedlium2. Our I3D models consistently achieve lower WERs than the standard Transformer with similar or even fewer layers during inference. 2 We further apply I3D to the attention-based encoder-decoder (AED) framework. Only the en- coder is changed while the decoder is still a standard Transformer decoder. Table 1 presents the results on LibriSpeech 100h. With around 27 layers on average during inference, our I3D models out- perform the 27-layer Transformer trained from scratch on both dev clean and test clean sets. The I3D with a global gate predictor is slightly better than that with a local gate predictor. 4. CONCLUSION In this work, we propose I3D, a Transformer-based encoder which dynamically adjusts its depth based on the characteristics of input utterances to trade off performance and ef\ufb01ciency. We design two types of gate predictors and show that I3D-based models consis- tently outperform the vanilla Transformer trained from scratch and the static pruned model. I3D can be applied to various end-to-end ASR frameworks and corpora. We also present interesting analy- sis on the predicted gate probabilities and the input-dependency to better interpret the behavior of deep encoders and the effect of in- termediate loss regularization techniques. In the future, we plan to apply this method to large pre-trained models. We will explore only \ufb01ne-tuning gate predictors to signi\ufb01cantly reduce training cost. 5. ACKNOWLEDGEMENTS This work used Bridges2 at PSC and Delta at NCSA through allo- cation CIS210014 from the Advanced Cyberinfrastructure Coordi- nation Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 2We have also evaluated I3D on LibriSpeech 960h. Observations are con- sistent with LibriSpeech 100h and Tedlium2. 6. REFERENCES [1] A. Graves, S. Fern\u00b4andez, et al., \u201cConnectionist temporal clas- si\ufb01cation: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., \u201cLearning phrase representa- tions using RNN encoder-decoder for statistical machine trans- lation,\u201d in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recogni- tion,\u201d in Proc. ICASSP, 2016. [5] A. Graves, \u201cSequence transduction with recurrent neural net- works,\u201d arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you need,\u201d in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolution- augmented Transformer for Speech Recognition,\u201d in Proc. In- terspeech, 2020. [8] Y. Peng, S. Dalmia, et al., \u201cBranchformer: Parallel MLP- attention architectures to capture local and global context for speech recognition and understanding,\u201d in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., \u201cE-branchformer: Branch- former with enhanced merging for speech recognition,\u201d arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., \u201cA comparative study on transformer vs rnn in speech applications,\u201d in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., \u201cDistilling the knowl- edge in a neural network,\u201d arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, \u201cDistilhubert: Speech rep- resentation learning by layer-wise distillation of hidden-unit bert,\u201d in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., \u201cLightHuBERT: Lightweight and Con- \ufb01gurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,\u201d in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., \u201cRTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,\u201d in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, \u201cCompressing deep neural networks for ef\ufb01cient speech enhancement,\u201d in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., \u201cParp: Prune, adjust and re-prune for self-supervised speech recognition,\u201d in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., \u201cDynamic neural networks: A survey,\u201d IEEE"}