{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Surveying_(Dis)Parities_and_Concerns_of_Compute_Hungry_NLP_Research_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What immediate action could be taken to improve peer reviewing according to the text?", "answer": " Adapt the ARR reviewing guidelines and instruct reviewers to consider the compute budget reported in a paper when asking for more experiments.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " Why is there a high emphasis on the release of artifacts?", "answer": " Because it facilitates future research and helps reproducibility.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " What are some examples of artifacts mentioned in the text?", "answer": " Code, trained models, system outputs, training checkpoints, and proper documentation of training data.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " Why do some respondents wish for a high quality release of artifacts complemented by code and documentation?", "answer": " To ensure proper understanding and reproducibility of the research.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " How should the release of artifacts be integrated into the reviewing process according to the text?", "answer": " Submitting artifacts together with the paper before reviewing.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " Why do some respondents suggest that artifact release should be mandatory for acceptance?", "answer": " To ensure transparency, reproducibility, and facilitate future research.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " What were the two most prominent topics discussed in the suggestions that do not fit into other topics?", "answer": " Evaluation and emphasizing research over engineering.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " How do respondents propose to address the issue of model comparability?", "answer": " By reporting performance based on Pareto frontiers and considering the compute budget along with model performance.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " According to some respondents, what trend has the field of study seemingly drifted towards?", "answer": " Primarily chasing high performance rather than producing meaningful scientific insights.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}, {"question": " What were some of the suggestions from respondents to combat the trend towards engineering over research?", "answer": " Authors should clearly state their scientific hypothesis and conduct research that tests this hypothesis using appropriate resources.", "ref_chunk": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}], "doc_text": "model release could boost inclusiveness and reproducibility.6 To im- prove peer reviewing, one immediate action could be to adapt the ARR reviewing guidelines and in- struct reviewers to consider the compute budget reported in a paper when asking for more experi- ments.7 Among the 22 additional suggestions for \ud835\udc4418, we find a high emphasis (68.2%) towards the re- lease of artifacts\u2014both because this facilitates fu- ture research and helps reproducibility. Moreover, 22 of the 67 general suggestion (\ud835\udc4419) also touched upon issues about model release and reviewing, highlighting the importance of both topics. The responses mentioned a remarkably wide variety of artifacts: code; trained models; system outputs (to facilitate comparative evaluations without rerun- ning the code); training checkpoints (to study the training dynamics); and proper documentation of training data (including crowdsourcing questions). In addition to simply releasing trained models, sev- eral respondents also wished for a sufficiently high quality of the released models complemented by code and documentation. One particular concern was how the release of artifacts should be integrated into the reviewing process. On the one hand, it seems useful to submit artifacts together with the paper before reviewing, so that reviewers can ac- cess them and to prevent breaking promises of fu- ture code release. On the other hand, this needs to happen within the constraints of double-blind 6https://naacl2022-reproducibility-track. github.io/results/ 7https://aclrollingreview.org/ reviewertutorial reviewing. Finally, 12 of the free-text responses of \ud835\udc4418 and \ud835\udc4419 suggested that artifact release should be mandatory for acceptance. 6 Further Considerations Finally, we discuss suggestions (\ud835\udc4419) that do not fit into any of the previously discussed topics. From the 67 free-text responses (21.5%), the two most prominent topics were evaluation (11 respondents) and emphasizing research over engineering (7 re- spondents). Evaluation. 16.4% of the free-text respondents touched upon the issue of evaluation and model comparability; as current benchmarks often focus on improving a single metric. One measure to counter this trend would be to report performance based on Pareto frontiers and to consider the com- pute budget along with the model performance. To promote such curves, it would also be important to release metadata including preprocessing and hyperparameter choices that allows future research to draw proper comparisons as well as to provide concrete guidelines for reviewers. 10.4% of the free-text Research vs. engineering. respondents further noted that the field seemed to have drifted more towards engineering by primar- ily chasing high performance; straying away from producing meaningful scientific insights. The re- spondents brought forward various suggestions to combat this; for instance that authors should clearly state their scientific hypothesis and then report re- search that tests this hypothesis using the lowest appropriate amount of resources. Other suggestions were to actively promote more theoretical, or more non deep learning work. Other suggestions. Another suggestion worth mentioning was the creation of a separate track (four respondents); either specifically for small models or for industry that cannot publish their models. Fi- nally, there was also a call for more shared tasks with limited resources such as the efficient NMT challenge (Heafield et al., 2022) or the efficient in- ference task (Moosavi et al., 2020). 7 Conclusion We presented a first attempt to capture and quantify existing concerns about the environmental impact and equity within the *CL community. We further investigated the resulting implications on peer re- viewing considering the increasing computational (a) Potential changes to reviewing Q14: Would consider submitting to efficiency track? Q15: Authors to justify budget allocation? Q16: Reviewers to justfy petition for more experiments? Q17: Benefit from releasing smaller models? (b) Model release Q18: How to encourage model release? s t n a p i c i t r a P % 100 80 60 40 20 0 1 0 9 3 2 6 3 1 4 4 3 1 4 8 8 7 1 5 7 s t n a p i c i t r a P % 60 50 40 30 20 10 0 Q14 Q15 Q16 Q17 Possible answers: yes (\u25fc), not sure (\u25fc), no (\u25fc). 0 Branding proceedings in artifact ward A best for Reviewers release reward Other None (c) Model release (by seniority) (d) Model release (by job sector) Q18: How to encourage model release? Q18: How to encourage model release? s t n a p i c i t r a P % 80 60 40 20 1\u20135 6\u201310 11\u201315 16+ s t n a p i c i t r a P % 80 60 40 20 Student Aca. PD Aca. PI Ind. (s) Ind. (l) 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None 0 Branding proceedings in ward artifact A best for Reviewers release reward Other None Figure 9: Analysis of responses on how to improve the reviewing process. In (a), we show the distribution of our participants\u2019 responses for \ud835\udc4414\u2013\ud835\udc4417 (in %). A majority of our participants would submit to an efficiency track (\ud835\udc4414) and would prefer reviewers to justify a request for more experiments (\ud835\udc4416). They further would benefit from a release of smaller models (\ud835\udc4416). In contrast, the responses are more mixed about the authors justifying the compute budget (\ud835\udc4415). In (b\u2013d), we show our participants\u2019 responses on how to encourage the release of models (in %): (b) overall, (c) by seniority, (d) by job sector. Multiple responses were allowed for \ud835\udc4418. demand. A majority of our respondents were con- cerned regarding the environmental footprint of NLP experiments with model training and model selection being the most pressing issues. We also found a high disparity among our respondents with students and small industry researchers suffering most from a lack of resources. There was a large support for measures to improve equity and accessi- bility across all respondents; most prominently for an efficiency track, asking reviewers to justify the petition for additional experiments, and the release of small versions of pretrained models. Considering the continuous increase of param- eters in PLMs (Zhao et al., 2023), one danger we face is that existing disparities may intensify even further."}