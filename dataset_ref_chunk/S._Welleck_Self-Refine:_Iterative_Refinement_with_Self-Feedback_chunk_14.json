{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Self-Refine:_Iterative_Refinement_with_Self-Feedback_chunk_14.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the FED dataset?", "answer": " The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities to evaluate interactive dialog systems.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " How many instances were used in the automatic evaluation with text-davinci-003?", "answer": " 342 instances.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " How do the outputs generated by SELF-REFINE compare to those generated by INIT?", "answer": " Outputs generated by SELF-REFINE are more engaging, interesting, and elaborate than the outputs generated by INIT.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " What is the primary objective of Code Optimization Performance-Improving Code Edits (PIE)?", "answer": " The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " What does SELF-REFINE do after generating an optimization through PIE?", "answer": " SELF-REFINE first generates a natural language feedback on possible improvements, which is then fed to REFINE for refinement.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " What dataset is used for evaluating SELF-REFINE on math reasoning?", "answer": " Grade School Math 8k (GSM-8k) dataset.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " How is SELF-REFINE used to refine mathematical problem-solving outputs?", "answer": " SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " What does SELF-REFINE do in the task of sentiment reversal?", "answer": " SELF-REFINE is instantiated to re-write a passage to flip its sentiment (positive to negative or vice-versa) in long-form text style transfer.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " How does the feedback mechanism of SELF-REFINE contribute to refining generated code?", "answer": " The introspective feedback mechanism of SELF-REFINE contributes to refining the generated code through iterative feedback and improvement.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}, {"question": " What benchmark is shown in Figure 14 related to the GSM-8k math reasoning dataset?", "answer": " Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the number of iterations of SELF-REFINE.", "ref_chunk": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}], "doc_text": "we chose the response that gets the highest total score from the FEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 for all the experiments. 32 GPT-3.5 ChatGPT GPT4 54.0 48.0 16.0 18.0 30.0 50.0 36.0 23.0 41.0 Table 15: Human evaluation results for dialogue response generation SELF-REFINE wins INIT wins Both are equal Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead. Given a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \u201cboth\u201d when it is hard to show preference toward one response. Results Automatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF- REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT. N Code Optimization Performance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance. Given an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement. Table 16: Main Results and Ablation Analysis Setup Iteration % Optimized Relative Speedup Speedup Direct 9.7 62.29 3.09 SELF-REFINE \u2212 feedback SELF-REFINE \u2212 feedback 1 2 10.1 10.4 62.15 61.79 3.03 3.01 SELF-REFINE SELF-REFINE 1 2 15.3 15.6 59.64 65.60 2.90 3.74 Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework\u2019s multi-faceted feedback in refining the generated code. 33 O Math Reasoning We use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback. Following Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback: # Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost (cid:44)\u2192 # The cost of a cup is $1200 less than the total cost of half a dozen # wrong! The cost of a cup is not the same as the cost of a plate. plates sold at $6000 each. (cid:44)\u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost - 1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. 34 Solve rate of SELF-REFINE Over Iterations for GSM-8k ) % 80 ( y c a r u c c A 75.06% 75.74% 76.19% 73.39% 71.34% 70 0 1 2 Iteration 3 4 Figure 14: Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE. P Sentiment Reversal We consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018; Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements. Instantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task"}