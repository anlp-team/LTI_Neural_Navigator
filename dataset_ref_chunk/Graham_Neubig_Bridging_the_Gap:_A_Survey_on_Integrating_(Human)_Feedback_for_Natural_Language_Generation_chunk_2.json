{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Bridging_the_Gap:_A_Survey_on_Integrating_(Human)_Feedback_for_Natural_Language_Generation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the text?", "answer": " The main focus of the text is on leveraging human feedback for improving language generation.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What is discussed in \u00a72 of the text?", "answer": " In \u00a72, the text formalizes the notion of human feedback and creates a taxonomy of different types of feedback in the literature.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " How can feedback be described according to the text?", "answer": " Feedback can be described by its format and objective, in terms of the desired model behavior.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What is an approach mentioned in \u00a74 for optimizing models using human feedback?", "answer": " One approach mentioned in \u00a74 is directly optimizing models against human feedback on their outputs, for example, using reinforcement learning with human reward functions.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What is discussed in \u00a76 of the text?", "answer": " In \u00a76, the text discusses existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What is one of the NLG tasks mentioned in the text?", "answer": " Dialog Generation is mentioned as an NLG task, where X is the space of possible dialog histories, and Y is the space of possible responses.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " How are NLG models generally realized according to the text?", "answer": " NLG models are generally realized as a parameterized, conditional probability distribution.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " Why can evaluating the quality of generated text be challenging?", "answer": " Evaluating the quality of generated text can be challenging due to the complexity and subjectivity of natural language.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What are some automatic metrics proposed for evaluating generated text?", "answer": " Various automatic metrics have been proposed, traditionally relying on n-gram matching or other simple heuristics.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}, {"question": " What is human feedback considered as in the text?", "answer": " In the text, human feedback is considered the gold standard for assessing the quality of generated text.", "ref_chunk": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}], "doc_text": "of human feedback for improving language generation. We start by formalizing the notion of human feedback and creating a taxonomy of the different types of feedback in the literature, and of how they have been used (\u00a72). We discuss how we can describe feedback by its format and its objective, in terms of the desired model behavior (\u00a73). We discuss approaches that directly optimize models against human feedback on (their) outputs, for example, using reinforcement learning with human reward functions (\u00a74). We then move to approaches that circumvent the costs of direct feed- back optimization by first training feedback models to approximate human feedback, and then improv- ing generation using these proxy models (\u00a75). We discuss existing datasets for human-feedback data, how these datasets are typically collected, and the impact that the collection process might have on the behaviour of the models (\u00a76). Finally, we dis- cuss a recent line of work that reduces the need to collect human feedback by leveraging AI feedback from large language models (\u00a77). 2 A Taxonomy for Leveraging (Human) Feedback for Generation 2.1 Background Consider a model M : X \u2192 Y which, given an input of some type x \u2208 X , outputs text \u02c6y \u2208 Y. Importantly, while x can be of any format, we re- strict ourselves to cases where y is in the space of natural language (i.e., Y \u2286 \u03a3\u22c6 for some alphabet 2 \u03a3). This general formulation encompasses a wide range of NLG tasks. For example: Summarization: X is the space of docu- ments, and Y the space of possible summaries. Machine Translation: X and Y are the spaces of sentences in the source and target languages, respectively. Dialog Generation: X is the space of pos- sible dialog histories, and Y is the space of possible responses. Image Captioning: X is the space of images, and Y is the space of possible captions. These models are generally realized as a parameter- ized, conditional probability distribution P\u03b8(y|x), where \u03b8 are the model parameters. This distribution is often estimated autoregressively: the probability of a sentence y given an input x is decomposed into the product of the probabilities of each token in the sentence, conditioned on the previous tokens. These models are then trained by finding the pa- rameters \u03b8\u22c6 that maximize the likelihood of some training data D = {(xi, yi)}N i=1. Then, at inference time, given an input x, an output \u02c6y is decoded from the learned distribution. This decoding can be done, for example, by approximating the most-likely se- quence of tokens (M (x) \u2248 arg maxy P\u03b8\u22c6(y|x)) or by random sampling (M (x) \u223c P\u03b8\u22c6(y|x)). Evaluating the quality of generated text \u02c6y \u2208 Y can be challenging due to the complexity and subjectivity of natural language. Various auto- matic metrics have been proposed for various do- mains/tasks. These metrics traditionally rely on n-gram matching or other simple heuristics that cannot account for complex linguistic phenomena (such as paraphrasing or stylistic variations) and often fail to capture all the nuances of human judg- ment (Sai et al., 2022; Gehrmann et al., 2022a). For this reason, for many of these tasks, asking for human feedback is considered the gold standard for assessing the quality of the generated text, and newer learned metrics often aim to approximate the way humans provide feedback (see \u00a75.1). More formally, we consider human feedback to be a family of functions H such that each feedback function h \u2208 H takes an input1 x \u2208 X and one 1Although feedback can be provided independently of the input (for example for fluency), we assume some (potentially empty) input for simplicity of notation. Preprint or more outputs y1, \u00b7 \u00b7 \u00b7 , yn \u2208 Y and returns some feedback f \u2208 F: h : X \u00d7 Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Yn (cid:125) \u2192 F. (cid:123)(cid:122) n A simple example of a (human) feedback function is asking humans to say if, given an input, a partic- ular output is good or bad (h : X \u00d7 Y \u2192 {0, 1}). However, more complex feedback functions, such as rankings or natural language feedback, exist and are commonly used (see \u00a73.1). (cid:124) We note that this framing is a simplification of the real world: often, different humans might pro- vide different (and potentially contradicting) feed- back for the same outputs, and a single function might not be able to capture this variability in hu- man opinion (we discuss this further in \u00a76). Finally, while our formalization is flexible, it excludes other approaches where models interact with humans to improve learning, such as active learning and other human-in-the-loop approaches. 2.2 Taxonomy Having established a basic mathematical formula- tion, we now identify four key axes along which we can classify the uses of human feedback: What is the format of the feedback? The for- mat of human feedback can vary, including binary judgments, numerical scores, ordinal rankings, or qualitative natural language explanations. What is its objective? Depending on the use case of our model, the feedback can have a va- riety of purposes, ranging from assessing model performance and accuracy to preventing toxicity and harmful behavior. When is it used? Human feedback can be in- corporated into the training stage to optimize the model parameters directly. Alternatively, it can be used at inference time to guide the decoding process. How is it modeled? While ideally, we would use direct feedback from humans whenever possible, the prohibitive cost of its collection means that it is often useful to instead use surrogate models that approximate human preferences. 3 Describing Feedback 3.1 Format An important decision to make when we want to im- prove language generation systems through human 3 (1) Preprint Input Output(s) Feedback Type 0.7 Score A melhor comida do mundo \u00e9 a portuguesa. The worst food in the world are Portuguese. \u2019worst\u2019: major/accuracy \u2019are\u2019: minor/fluency MQM \u2019worst\u2019 \u2192 \u2019best\u2019, \u2019are\u2019 \u2192 \u2019is\u2019 Post-Edition Artificial intelligence has the potential to revolutionize industries (...) but ethical concerns need to be handled."}