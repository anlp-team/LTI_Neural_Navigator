{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Vector_Quantized_Approach_for_Text_to_Speech_Synthesis_on_Real-World_Spontaneous_Speech_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, what do the tokens [R], [S], and [E] refer to?,        answer: The tokens [R], [S], and [E] refer to the repetition token, start token, and end token respectively.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the mathematical representation of LF, the function the transformer is trained to maximize?,        answer: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D)    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What are \u03b3 and \u03bb in the mathematical expression provided in the text?,        answer: \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight respectively.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " How does the text describe the difference in architecture between the encoder-decoder used in TTS compared to NLP tasks?,        answer: The text mentions that in TTS, cross-attention is simply a unique alignment map and the model applies a single cross-attention layer only on the last layer of the decoder.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What technique is used to replace positional encoding in the model according to the text?,        answer: ALiBi is used to replace positional encoding in the model.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the purpose of the Sub-decoder module in the multi-output transformer model described in the text?,        answer: The Sub-decoder module is used to explicitly model the conditional distribution of codes, allowing for generation of codes from codebook groups.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the role of the repetition token in the transformer described in the text?,        answer: The repetition token is introduced to explicitly model transitions where consecutive repeated codes are replaced by this token and decoded back to the original token during inference.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What sampling technique is adopted for transformer inference in the text?,        answer: Nucleus sampling is adopted for transformer inference.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " How does the text describe the Inference Monotonic Alignment in the transformer?,        answer: The text describes Inference Monotonic Alignment as a process where the decoder attends only to a certain context window of encoder states at a time sequentially.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}, {"question": " What is the purpose of prepending 3 frames of codes encoded from a silence clip before synthesis in the model?,        answer: The purpose is to prompt the model to synthesize clean speech by encouraging it to continue the synthesis without background noise.    ", "ref_chunk": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}], "doc_text": "ing training. [R] refers to the repetition token. [S] and [E] are the start and end tokens respectively. SPK refers to the processed speaker embedding. tion for ESP K. The transformer is trained to maximize log- likelihood: LF = \u03bbLV Q(QE, QD, G) + LGAN (QE, QD, G, D) (3) LV Q(QE, QD, G) =Ex (cid:20) 1 T N (cid:88) i=1 T (cid:88) t=1 (||sg[zc t,i] \u2212 zq t,i||2 2 t,i \u2212 sg[zq + \u03b3||zc (cid:21) t,i]||2 2) where \u03b3 and \u03bb are hyper-parameters for the commitment loss weight and quantization loss weight. We leave the de- tailed de\ufb01nition of LGAN in the HiFi-GAN paper. Note that our LV Q is different from previous work in image genera- tion; we do not include the reconstruction loss as the mel- spectrogram loss in LGAN has already served this purpose. 3.2 Conditional Synthesis with Transformer In the second stage, we train a transformer to autoregres- sively generate ct given the past sequence c<t. As presented in Figure 2, the transformer is additionally conditioned on a global speaker embedding s = ESP K(x) and the phoneme sequence h. We use a pretrained speaker embedding ex- tractor2 followed by two linear layers with ReLU activa- (4) LT = Ec (cid:34) \u2212 (cid:88) log p(ct | c<t, s, h) (cid:35) t Architectural Design. Different from the commonly used encoder-decoder architecture which contains cross at- tention from all decoder layers with the output encoder state, we adopt a different con\ufb01guration. The typical encoder- decoder architecture assumes that the two inputs have com- plex semantic correlations that need modeling from low- level to high-level representations, which is true for many NLP tasks. However, in TTS, the cross-attention is simply a unique alignment map that bene\ufb01ts less from pre-estimating the alignment pattern using low-level audio representations. As a result, we apply a single cross-attention layer only on the last layer of the decoder and use single-head cross- attention for the unique alignment. More importantly, hav- ing a unique alignment map enables the use of monotonic alignment which we will introduce in Section 3.3. We use ALiBi (Press, Smith, and Lewis 2022) to replace positional encoding to enable our model to extrapolate to long input sequences and syntheses for both encoder and decoder. 2https://huggingface.co/pyannote/embedding sampling tech- niques (Holtzman et al. 2020; Fan, Lewis, and Dauphin Multi-output Transformer. While (5) 2018) play an important role in the performance of autore- gressive models, it is not straightforward to adopt these to our multi-code setting. If we simply use a linear layer to predict N codes at a time, we can only access the distribu- tion of each code individually. Nucleus sampling (Holtzman et al. 2020) will be theoretically infeasible, as it requires top-p candidates from the joint distribution of N codes. To address this, we use an additional transformer decoder to explicitly model the conditional distribution of codes, and name it the Sub-decoder module. Different from the main decoder operating across the time sequence, Sub-decoder operates sequentially over the codebook groups of the \ufb01xed size N . As shown in Figure 2, at time t, the Sub-decoder accepts Ot, the output state of the main decoder, as starting condition and sequentially generates ct,i given ct,<i. We use separate embeddings for each codebook group without positional encoding. This con\ufb01guration enables us to apply nucleus sampling at time t on each of the conditional dis- tributions p(ct,i | ct,<i, Ot) respectively. After generating the whole ct, ct is fed back autoregressively to the main decoder for the generation of the next step Ot+1. Repetition Token. Speech signals often show high tem- poral similarity in their representations, making the trans- former predict repeating codes consecutively. We model such transitions explicitly by introducing an additional to- ken, a repetition token. Consecutive repeated codes are re- placed by this token, and decoded back to the original token in inference. 3.3 For the transformer inference, we adopt nucleus sam- pling (Holtzman et al. 2020), and the fully decoded codes are passed to QD to reconstruct the raw waveform. Inference Monotonic Alignment. We denote A(i, j) as the cross attention value between encoder state at step i and decoder state at step j. Since our cross attention is single-head and single-layer, A(i, j) is unique. Then during inference, given a new decoder state at time t, we only calculate its cross attention with Nw encoder states at [bk, bk + 1, \u00b7 \u00b7 \u00b7 , bk + Nw] instead of the entire encoder state sequence, and bk is de\ufb01ned recursively by: b0 = 0, bk+1 = (cid:40) bk + 1, bk, exp{A(bk,k)} i=0 exp{A(bk,k+i)} if (cid:80)Nw otherwise < 1 Nw (6) Intuitively, the decoder can attend only to a certain context window Nw of encoder states at a time sequentially, and the context window steps forward only if the Softmax attention weight of the \ufb01rst encoder state (at bk) is lower than a given \u22121). In general, smaller Nw threshold (here we set it to Nw leads to stricter monotonic alignment at the cost of a smaller phonetic context. Practically, we found Nw ranging from 3 to 6 works well judging from qualitative listening. Such con- struction also enables us to use the alignment progress as a stopping criterion instead of predicting the stop token which is vulnerable to over\ufb01tting on the training utterance duration. Audio Prompt. When trained on a noisy corpus, it seems inevitable that undesired background noise will be sampled during inference. Here we use a simple trick to prompt the model to synthesize clean speech. We prepend 3 frames of codes that are encoded by QE from a silence clip before the actual synthesis. Clean speech is often preceded by clean silence, we expect it to encourage the model to continue the synthesis without background noise. 4 Experimental Setup 4.1 Datasets We train our model on GigaSpeech (Chen et al. 2021), an ASR corpus containing transcribed audio from audiobooks, Podcasts, and YouTube with a 16 kHz sampling rate. We use only the Podcast and"}