{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What datasets are used as source datasets in the embedder training process?", "answer": " CIFAR-10 for Swin and CONLL-2003 for RoBERTa", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " How many data points are sampled to compute the OTDD?", "answer": " 5000", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " What is the purpose of cross-modal fine-tuning?", "answer": " To align and refine for classification or dense tasks", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " What is the number of clusters set to for 2D tasks that use CIFAR-10 as the source dataset?", "answer": " 10", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " Where can the OTDD implementation of the original paper be found?", "answer": " https://github.com/microsoft/otdd", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " How many epochs are used for embedding learning?", "answer": " 60", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " What is the learning rate scheduler used during fine-tuning stage?", "answer": " Linear decay with min LR = 0 and 5 warmup epochs", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " What baseline is used with the same hyperparameter configuration as ORCA?", "answer": " Standard fine-tuning baseline", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " What is the main focus of the experiments on NAS-Bench-360?", "answer": " Performance evaluation and comparison", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}, {"question": " How are the results of the experiments reported?", "answer": " Through performance profiles", "ref_chunk": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}], "doc_text": "= 4 can be reused by the embedder. A.2.4. EMBEDDING LEARNING WITH OTDD After initializing the embedder architecture for each task, we train it to minimize the OTDD between the embedded target features and embedded source features. For source datasets, we use CIFAR-10 for Swin and CONLL-2003 for RoBERTa. We sample 5000 data points to compute OTDD. In practice, we can pass the source data through the pretrained embedder once and save all the embedded features, so we don\u2019t have to pay the cost of obtaining the source features each time we \ufb01ne-tune a new model. Cross-Modal Fine-Tuning: Align then Re\ufb01ne For classi\ufb01cation tasks, we directly use the labels provided by the end task to compute OTDD. For dense tasks, we perform K-Means clustering on the target data to obtain pseudolabels for OTDD computation. The number of clusters is set to the number of classes of the source dataset, e.g., 10 for 2D tasks that use CIFAR-10 as the source dataset. To compute the embedding learning objective, we use the OTDD implementation of the original paper provided here: https://github.com/microsoft/otdd. We use the searched hyperparameters in Section A.2.2. The others are \ufb01xed across different tasks: Embedding learning epochs: 60 Embedding learning stage rate scheduler: decay by 0.2 every 20 epochs Fine-tuning stage learning rate scheduler: we use the linear decay with min lr = 0 and 5 warmup epochs A.3. Baseline Implementation For the standard \ufb01ne-tuning baseline, we use the same hyperparameter con\ufb01guration (number of epochs, batch size, learning rate, etc) as ORCA, except for setting embedding learning epochs to 0. For the train-from-scratch baseline, everything is the same as standard \ufb01ne-tuning, except that the model weights are reinitialized at the beginning. A.4. Experiments on NAS-Bench-360 A.4.1. INFORMATION ABOUT THE BENCHMARK AND EXPERIMENT PROTOCOL Table 6: Summary about each task and the hand-designed expert models used in NAS-Bench-360 (Tu et al., 2022). Task name # Data Data dim. Type License Learning objective Expert arch. CIFAR-100 60K 2D Point CC BY 4.0 Classify natural images into 100 classes DenseNet-BC (Huang et al., 2017) Spherical 60K 2D Point CC BY-SA Classify spherically projected images into 100 classes S2CN (Cohen et al., 2018) NinaPro 3956 2D Point CC BY-ND Classify sEMG signals into 18 classes corresponding to hand gestures Attention Model (Josephs et al., 2020) FSD50K 51K 2D Point (multi-label) CC BY 4.0 Classify sound events in log-mel spectrograms with 200 labels VGG (Fonseca et al., 2021) Darcy Flow 1100 2D Dense MIT Predict the \ufb01nal state of a \ufb02uid from its initial conditions FNO (Li et al., 2021) PSICOV 3606 2D Dense GPL Predict pairwise distances between resi- duals from 2D protein sequence features DEEPCON (Adhikari, 2019) Cosmic 5250 2D Dense Open License Predict propablistic maps to identify cos- mic rays in telescope images deepCR-mask (Zhang & Bloom, 2020) ECG 330K 1D Point ODC-BY 1.0 Detect atrial cardiac disease from a ECG recording (4 classes) ResNet-1D (Hong et al., 2020) Satellite 1M 1D Point GPL 3.0 Classify satellite image pixels\u2019 time series into 24 land cover types ROCKET (Dempster et al., 2020) DeepSEA 250K 1D Point (multi-label) CC BY 4.0 Predict chromatin states and binding states of RNA sequences (36 classes) DeepSEA (Zhou & Troyanskaya, 2015) For experiments, each dataset is preprocessed and split using the script available on https://github.com/rtu715/ NAS-Bench-360, with the training set being used for hyperparameter tuning, embedding learning, and \ufb01ne-tuning. When training/\ufb01ne-tuning is \ufb01nished, we evaluate the performance of all models following the NAS-Bench-360 protocol. We \ufb01rst report results of the target metric for each task by running the model of the last epoch on the test data. Then, Cross-Modal Fine-Tuning: Align then Re\ufb01ne we report aggregate results via performance pro\ufb01les (Dolan & Mor\u00b4e, 2002), a technique that considers both outliers and small performance differences to compare methods across multiple tasks robustly. In such plots, each curve represents one method. The \u03c4 on the x-axis denotes the fraction of tasks on which a method is no worse than a \u03c4 -factor from the best. The performance pro\ufb01le for our experiments is shown in Figure 3. The code and con\ufb01guration \ufb01le for reproducing each experiment can be found in our of\ufb01cial GitHub repository. A.4.2. COMPLETE RESULTS FOR TABLE 2 WITH ERROR BARS Table 7: Prediction errors (\u2193) for 10 diverse tasks. \u201cNAS-Bench-360\u201d refers to the task-wise best of all AutoML baselines evaluated in the paper, including DARTS (Liu et al., 2019b), DenseNAS (Fang et al., 2020), AMBER (Zhang et al., 2020), Auto-DL (Liu et al., 2019a), WRN-ASHA (Li et al., 2020a), and XGBoost (Chen & Guestrin, 2016). \u201cFPT\u201d refers to \ufb01ne-tuning the layer norms of RoBERTa/Swin. On 7/10 problems, ORCA ranks the \ufb01rst among all competitors. Hand-designed CIFAR-100 0-1 error (%) 0-1 error (%) Spherical 19.39\u00b10.20 67.41\u00b10.76 Darcy Flow relative (cid:96)2 8E-3\u00b11E-3 PSICOV MAE8 3.35\u00b10.14 Cosmic 1-AUROC 0.127\u00b10.01 NinaPro 0-1 error (%) 8.73\u00b10.90 FSD50K 1- mAP 0.62\u00b10.004 ECG 1 - F1 score 0.28\u00b10.00 Satellite 0-1 error (%) 19.80\u00b10.00 NAS-Bench-360 DASH 23.39\u00b10.01 24.37\u00b10.81 48.23\u00b12.87 71.28\u00b10.68 2.6E-2\u00b11E-3 7.9E-3\u00b12E-3 2.94\u00b10.13 3.30\u00b10.16 0.229\u00b10.04 0.19\u00b10.02 7.34\u00b10.76 6.60\u00b10.33 0.60\u00b10.001 0.60\u00b10.008 0.34\u00b10.01 0.32\u00b10.007 12.51\u00b10.24 12.28\u00b10.5 Perceiver IO FPT 70.04\u00b10.44 10.11\u00b11.18 82.57\u00b10.19 76.38\u00b14.89 2.4E-2\u00b11E-2 2.1E-2\u00b11.3E-3 8.06\u00b10.06 4.66\u00b10.054 0.485\u00b10.01 0.23\u00b10.002 22.22\u00b11.80 15.69\u00b12.33 0.72\u00b10.002 0.67\u00b10.0068 0.66\u00b10.01 0.50\u00b10.0098 15.93\u00b10.08 20.83\u00b10.24 ORCA 6.53\u00b10.079 29.85\u00b10.72 7.3E-3\u00b16.8E-5 1.91\u00b10.038 0.152\u00b10.005 7.54\u00b10.39 0.56\u00b10.013 0.28\u00b10.0059 11.59\u00b10.18 A.4.3. COMPLETE RESULTS FOR TABLE 3 WITH ERROR BARS Table 8: Prediction errors (\u2193) of ORCA, vanilla \ufb01ne-tuning, and training RoBERTa/Swin from scratch. We consider \ufb01ne-tuning all parameters (full setting) vs. only the layer norms (FPT setting). ORCA is better in both settings. CIFAR-100 Spherical Darcy Flow PSICOV Cosmic NinaPro FSD50K ECG Satellite Train-from-scratch 50.87\u00b10.32 76.67\u00b10.21 8.0E-2\u00b11.3E-2 5.09\u00b10.014 0.50\u00b10.00 9.96\u00b11.67 0.75\u00b10.017 0.42\u00b10.011 12.38\u00b10.14 Fine-tuning ORCA 7.67\u00b10.55 6.53\u00b10.079 55.26\u00b11.63 29.85\u00b10.72 7.34E-3\u00b11.1E-4 7.28E-3\u00b16.8E-5 1.92\u00b10.039 1.91\u00b10.038 0.17\u00b10.011 0.152\u00b10.005 8.35\u00b10.75 7.54\u00b10.39 0.63\u00b10.014 0.56\u00b10.013 0.44\u00b10.0056 0.28\u00b10.0059 13.86\u00b11.47 11.59\u00b10.18 Fine-tuning (layernorm) ORCA (layernorm) 10.11\u00b11.18 7.99\u00b10.098 76.38\u00b14.89 42.45\u00b10.21 2.1E-2\u00b11.3E-3 2.1E-2\u00b17.4E-4 4.66\u00b10.054 4.97\u00b10.14 0.233\u00b10.002 0.227\u00b10.003 15.69\u00b12.33 15.99\u00b11.92 0.67\u00b10.0068 0.64\u00b10.0093 0.50\u00b10.0098 0.47\u00b10.007 20.83\u00b10.24 20.54\u00b10.49 A.4.4. ABLATION STUDY ON EMBEDDING LEARNING METRICS As motivated in Section 4.1, we present here an ablation study on the embedding learning metrics that we have"}