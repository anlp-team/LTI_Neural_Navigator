{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Crossing_the_Threshold:_Idiomatic_Machine_Translation_through_Retrieval_Augmentation_and_Loss_Weighting_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of using the Ted Talks corpus for the out-of-domain test set?", "answer": " To ensure that translation quality of unrelated sentences is not impacted by modifications meant to improve translation of idioms.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " Why is training the model on OpenSubtitles and testing on Ted Talks important?", "answer": " To evaluate model generalization since topics and vocabulary in Ted Talks may differ from movies or TV shows.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " How were sentences in the test sets length-matched to control for difficulty?", "answer": " Sentences were length-matched on the target side with corresponding sentences in the idiomatic set.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " What is the purpose of the non-compositional translation task described in the text?", "answer": " To understand how much data is required to memorize non-compositional patterns.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " How did the authors create synthetic training corpora for the non-compositional translation task?", "answer": " By generating corpora of different sizes containing occurrences of the non-compositional rule.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " What conditions were examined during the non-compositional translation experiment?", "answer": " Two informativity conditions: one where the context provides no information, and one where the context is perfectly informative.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " What varied among the different transformer sizes used in the experiment?", "answer": " The number of encoder and decoder layers varied, while the hidden dimension and embedding size remained constant.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " What trend was observed in the translation of non-compositional expressions by transformers?", "answer": " There was a trend of translating non-compositional phrases literally, especially for smaller models.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " Why might less data be required for translating idioms when the context is informative?", "answer": " Clear context clues reduce the amount of data required for translating idioms effectively.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}, {"question": " What was observed about the translation quality of idiomatic sentences by commercial systems?", "answer": " There were drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall.", "ref_chunk": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}], "doc_text": "which is in-domain and another which is out-of- domain. For the in-domain test set, we simply select sentences from the development set of Open- Subtitles (see subsection 6.2 for details on our split of OpenSubtitles). For the out-of-domain test set, we use the Ted Talks corpus (Reimers and Gurevych, 2020). This is to ensure that translation quality of other, unrelated sentences is not impacted by any modifications meant to improve translation of idioms. Topics discussed and vocabulary used in Ted Talks may be slightly different from what is discussed in movies or TV shows, so training the model on OpenSubtitles and testing on Ted Talks allows us to evaluate model generalization. For both test sets, to control for translation length as a source of difficulty, sentences were length-matched on the target side with corresponding sentences in the idiomatic set. This created the random set, which is the same size as the idiomatic test set. All three test sets are summarized in Table 2. 4 Evaluating Non-Compositional Translation 4.1 Artificial Language Translation We first use the definition of non-compositional translation in (\u00a72) to create a synthetic task. This Language fr Idiom matches 85632 Idiomatic 777 Literal Random (in) Random (out) 79 777 777 fi 51811 449 81 449 449 ja 23018 3253 389 3253 3253 Table 2: Size of test sets for each language. The id- iomatic and literal sentences contain strings matching known idioms (after lemmatization), and the in-domain random set contains unrelated sentences from Open- Subtitles, but the out-of-domain random set contains unrelated sentences from the Ted Talks corpus. allows us to gain an understanding of how much data is required to memorize non-compositional patterns. Although this experiment is not realistic to natural language (notably, there is no token-level ambiguity in this experiment), we note that using synthetic experiments allows us to easily extend the data generation setup and examine model be- haviour along many different conditions, such as informativity. The source language in these experiments was composed of tokens 0 through 9, X = {0, 1, 2, ..., 9}. The target language was produced by adding 10 to each token, Y = {10, ..., 19}. The translation rule was to add 10 to the value of each token in the source language, e.g. 0 \u2192 10, 1 \u2192 11. We add a single non-compositional rule that doesn\u2019t follow this trend, 0 1 \u2192 12 (rather than 0 1 \u2192 10 11). We limited the maximum sequence length to 6 tokens. We generated synthetic training corpora of sev- eral sizes containing different numbers of occur- rences of the non-compositional rule 0 1 \u2192 12 . The number of training sentences ranged from 100k to 10M, while the number of noncompositional oc- currences ranged from 10 to 1M. We examined two informativity conditions, corresponding to the case where the context provides no information (to- kens are randomized around the non-compositional expression), and the context being perfectly infor- mative. The perfect informativity condition was achieved by adding the canary token \u201c11\u201d to the source vocabulary, and only inserting this token prior to the non-compositional pattern \u201c0 1\u201d. We experimented with three different trans- former sizes (Vaswani et al., 2017), each of which had a hidden dimension and embedding size of 512, as well as 16 attention heads. Only the num- ber of encoder and decoder layers varied, such that the small transformer had 3 encoder and decoder layers, the medium transformer 8, and the large transformer 16. We fix the number of epochs for the small, medium and large models to respectively Total 2410 1428 10148 be 10, 20, and 30 in the non-informative case and 15, 15 and 25 in the informative case.5 Further training details can be found in Appendix A. Although this may seem like a simple task, we found it surprisingly difficult for models to learn this non-compositional pattern. Results in each set- ting, averaged across 5 random seeds, are presented in Figure 1. Especially for the small model, there is a sharp gradation from translating none of the non-compositional expressions correctly to translat- ing them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corrobo- rates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct trans- lations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic lan- guage, there may be a stronger bias toward translat- ing compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required. 4.2 Evaluation of Commercial Systems Although synthetic experiments provide intuition on the difficulty of translating idioms, one might ask whether similar results hold in natural lan- guage. To answer this, we examine the perfor- mance of commercial systems on the test sets in (\u00a73). Namely, we examine Google Translate and DeepL on Finnish, French, and Japanese idiomatic, literal, and random sentences. Results are in Ta- ble 3. We observe drops in translation quality on idiomatic sentences in all languages, with lower automatic metrics overall. 5The number of training epochs was determined by the number of epochs it took for the validation loss to plateau in the 100k size corpus with 1k non-compositional examples, rounded up to multiples of 5. This was done to mimic the typical training process for MT models, which are trained until loss or accuracy plateaus on a general dev set. Since idiomatic expressions tend to be uncommon compared to literal ones, there may not be many in the dev or train sets, and so the model\u2019s performance on idiomatic expressions may"}