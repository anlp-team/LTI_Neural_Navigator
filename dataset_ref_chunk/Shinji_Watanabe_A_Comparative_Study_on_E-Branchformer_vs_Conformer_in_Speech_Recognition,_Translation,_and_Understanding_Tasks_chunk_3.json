{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_A_Comparative_Study_on_E-Branchformer_vs_Conformer_in_Speech_Recognition,_Translation,_and_Understanding_Tasks_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two different configurations mentioned in Table 2 using AED with joint CTC?", "answer": " Conformer-Deep with 15 encoder layers and 1024 FFN units, and Conformer-Wide with 12 encoder layers and 2048 FFN units.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " In Table 4, what type of models are evaluated for CER or WER (%)?", "answer": " RNN-T models.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " What sizes are the hidden sizes for all models in the benchmarks?", "answer": " d = 256.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " How does E-Branchformer compare to Conformer-Deep in terms of CERs or WERs?", "answer": " E-Branchformer consistently achieves lower CERs or WERs than Conformer-Deep with a similar size.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " What type of encoders are applied to pure CTC and RNN-T models for investigation?", "answer": " Conformer and E-Branchformer encoders.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " How does the training stability of E-Branchformer compare to Conformer in the TEDLIUM2 experiment?", "answer": " E-Branchformer only failed twice out of 10 random trials, while Conformer failed 6 times.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " What recommendation is given for setting the hidden size and expansion in FFNs for medium-sized models?", "answer": " Setting the hidden size to d = 256 and using 4\u00d7 expansion in FFNs and cgMLP.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " Are the training hyperparameters usually changed between Conformer and E-Branchformer experiments?", "answer": " In most experiments, the training hyperparameters remain the same assuming their sizes are close.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " What can slightly improve the final results for datasets like FLEURS and LibriSpeech 960h?", "answer": " Stochastic depth.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}, {"question": " In early training stages, what can be used as a reliable metric for assessing the success of the training process?", "answer": " The auxiliary CTC loss.", "ref_chunk": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}], "doc_text": "\u2021 6.5 / 4.3 Table 2: CER or WER (%) of different con\ufb01gurations using AED with joint CTC. \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer-Wide\u201d has 12 encoder lay- ers with 2048 FFN units. Evaluation sets are the same as in Table 1, except that LibriSpeech 100h only shows test sets. Table 4: CER or WER (%) of RNN-T models. The decoder is a single-layer LSTM [50]. Beam search with beam size 10 is performed without a language model. Dataset Conformer E-Branchformer Dataset Conformer-Deep Conformer-Wide E-Branchformer Params Results \u2193 Params Results \u2193 Params Results \u2193 LibriSpeech 100h Switchboard TEDLIUM2 VoxForge 39.0 36.7 35.5 35.2 6.6 / 17.2 13.5 / 7.4 7.5 / 7.6 9.0 / 8.1 46.8 44.5 43.4 43.0 6.6 / 17.1 13.8 / 7.5 7.5 / 7.5 8.9 / 8.0 38.5 36.2 35.0 34.7 6.3 / 17.0 13.4 / 7.3 7.3 / 7.1 8.8 / 8.0 Table 3: CER or WER (%) of pure CTC-based models. Greedy search is performed without a language model. Dataset Conformer E-Branchformer Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 26.8 27.0 25.8 5.8 / 6.3 9.4 / 22.5 / 9.9 / 23.1 9.1 / 9.0 26.2 26.4 25.3 5.4 / 6.0 9.2 / 22.4 / 9.6 / 23.1 8.7 / 8.3 benchmarks. The hidden sizes are d = 256 for all models. E-Branchformer consists of 12 encoder layers with 1024 FFN units and 1024 MLP units (dmlp in Eq. (11)). \u201cConformer-Deep\u201d has 15 encoder layers with 1024 FFN units, while \u201cConformer- Wide\u201d has 12 layers with 2048 FFN units. E-Branchformer consistently achieves lower CERs or WERs than \u201cConformer- Deep\u201d with a similar size. It even shows better or similar per- formance than \u201cConformer-Wide\u201d whose size is 20% larger. Different E2E ASR frameworks (i.e., AED, CTC and RNN- T) typically share a similar encoder. To investigate the general- izability of Conformer and E-Branchformer encoders, we apply them to pure CTC and RNN-T models. Table 3 and Table 4 summarize the two sets of experiments, respectively. Similar to the AED results, E-Branchformer achieves consistent improve- ments over Conformer in various evaluation sets under the same training condition. This demonstrates that E-Branchformer en- coders are capable of extracting better contextualized speech representations which generally bene\ufb01t E2E ASR. Params Results \u2193 Params Results \u2193 AISHELL LibriSpeech 100h TEDLIUM2 29.9 30.5 26.8 4.9 / 5.3 6.6 / 17.9 / 6.9 / 18.1 8.1 / 7.7 29.4 30.0 26.3 4.9 / 5.2 6.6 / 17.6 / 6.8 / 18.0 7.6 / 7.4 (and Branchformer) can offer greater training stability than Conformer, as observed empirically. For instance, in an ex- periment with TEDLIUM2 where we set the peak learning rate to 2e-3 and used pure CTC with 33M parameter en- coders, Conformer failed in 6 out of 10 random trials, while E-Branchformer only failed twice. One reason for this may be that Conformer\u2019s sequential combination of mod- ules increases the encoder\u2019s depth and makes it harder to con- verge. Moreover, in the successful trials, E-Branchformer ex- hibited lower validation loss in an early stage of training. It is worth noting that although a lower learning rate could im- prove training stability, in our case, it degraded the WERs. \u2022 A similar con\ufb01guration of E-Branchformer performs well in many ASR corpora. Using Macaron-style FFNs like those in Conformer is generally bene\ufb01cial. For medium-sized mod- els, we recommend setting the hidden size to d = 256 and us- ing 4\u00d7 expansion in FFNs and cgMLP. For large models, we suggest following the original paper [13] and using d = 512 with 6\u00d7 expansion in cgMLP and 2\u00d7 expansion in FFNs. \u2022 The same training hyperparameters (e.g., batch size, learning rate, warmup steps, total epochs) used by Conformer usually work well for E-Branchformer assuming their sizes are close. In most experiments, we did not change these con\ufb01gurations. \u2022 Stochastic depth [51] is disabled in most of our experiments for fair comparison with prior baselines. However, we do \ufb01nd that it can slightly improve the \ufb01nal results for datasets like FLEURS and LibriSpeech 960h. Moreover, it can be applied to both encoder and decoder, e.g., with dropout probabilities 0.1 and 0.2, respectively. 3.3. Discussions The following are some observations and training tips based on our investigations. \u2022 When the model is large or the data is small, E-Branchformer With joint CTC training [31], the auxiliary CTC loss can be a reliable metric for assessing the success of the training pro- cess in an early stage. If the validation loss does not decrease in the \ufb01rst few epochs, we need to reduce the learning rate or increase warmup steps. This also applies to Conformer. Table 5: Speech translation results. Dataset Conformer E-Branchformer Params BLEU \u2191 Params BLEU \u2191 MuST-C [45] Fisher [41] Callhome [41] 74.5 69.8 69.8 28.6 55.5 21.2 71.4 66.8 66.8 28.7 55.6 21.9 4. Speech translation experiments 4.1. Setups Data. Two ST datasets are used. MuST-C [45] is a TED-Talk corpus for English-to-X translation. We use the 400 hour v2 set of English-to-German. Fisher-Callhome [41] is a 170 hour conversational corpus for Spanish-to-English translation. Models. We use the attention-based encoder-decoder (AED) with joint CTC training and decoding [31, 32]. Our ST mod- els are similar to our ASR models, except they use hierarchical CTC encoders [52] which consist of a 12-layer Conformer or E-Branchformer attached to an ASR CTC criterion followed by another 6-layer Conformer or 8-layer E-Branchformer attached to an ST CTC criterion. Given the greater number of layers for ST models, we use a slightly higher number of layers for our E-Branchformer models to keep parameter counts in a similar range \u2013 E-Branchformer models are still smaller. ST models use ASR pre-training for the \ufb01rst 12 encoder layers. Training. We follow the ESPnet2 recipes for data prepara- tion, model training and decoding. Speed perturbation with ra- tios {0.9, 1.0, 1.1} and SpecAugment [36] are performed. The medium-sized model with hidden size d = 256 is used. The Adam [37] optimizer with warmup [7]"}