{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_The_Framework_Tax:_Disparities_Between_Inference_Efficiency_in_Research_and_Deployment_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is recommended when batch sizes are small to reduce framework overhead?", "answer": " Usage of static or ahead-of-time inference runtimes", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " How can model capacity be increased without affecting latency?", "answer": " By doubling hidden dimensions in self-attention and fully connected layers", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " Why are proxy metrics like FLOPs not always predictive of real-world efficiency?", "answer": " They do not account for factors like parallelization and techniques that reduce parameter counts", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " What are some techniques used to reduce the computational cost of long text sequences?", "answer": " Compression of input sequences and intermediate representations", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " What is distillation used for in model design?", "answer": " To reduce model size", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " How are efficiency metrics like wall-clock latency and energy usage incorporated into neural architecture search?", "answer": " They are integrated into the objectives of NAS and AutoML methods", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " Why is it difficult to generalize conclusions about model efficiency across deployment settings?", "answer": " Because it is often impossible to control for the hardware systems used", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " What design paradigms do neural network frameworks generally fall into?", "answer": " Eager Execution, Deferred Execution, and Static", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " Give examples of frameworks that follow the Eager Execution design paradigm.", "answer": " PyTorch and Chainer", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}, {"question": " How do frameworks following the Static design paradigm operate?", "answer": " The computational graph is pre-defined, compiled, and executed inside a specialized runtime", "ref_chunk": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}], "doc_text": "recommend usage of static or ahead-of-time inference runtimes when batch sizes are small as they can substantially reduce framework overhead. Alternatively, when using eager execution-based frameworks for inference, we recommend increasing model width or batch size at no cost to latency and take advantage of per- formance gains associated with increased model capacity (Zagoruyko and Komodakis, 2016). For example, hidden dimensions in self-attention and fully connected layers can be doubled to increase model capacity without affecting latency, imply- ing that model designers have an extra degree of freedom often overlooked when designing around parameters or FLOPs. We hope that our analy- sis and recommendations will help bridge the gap between efficient NLP research and practice. 2 Related Work 2.1 Efficiency Metrics & Cost Indicators Previous efforts to report efficiency often utilize proxy metrics for amount of computation, such as the number of floating point (FLOPs) or multiply- accumulate (MACs) operations (Schwartz et al., 2020). Similarly, number of trainable parameters is a frequently reported as a proxy for memory utiliza- tion (Lan et al., 2019). Unfortunately, these proxy metrics are often not predictive of realworld effi- ciency. For example, total FLOPs does not account for the varying extent to which different operations can be parallelized and techniques such as weight tying can reduce parameter counts without reduc- ing the amount of required computation (Lan et al., 2019). From the perspective of device utilization, hardware and model FLOPs utilization (Chowdh- ery et al., 2022) are reported as the ratio between observed FLOPs per second and a hardware plat- forms peak theoretical FLOPs. Previous works examining the relationship be- tween efficiency measures showed that different cost indicators do not correlate well with each other during neural network training (Dehghani et al., 2021). In particular, it has been hypothesized that discrepancies between FLOPs and wallclock infer- ence latency is primarily are primarily compute bounded by kernel execution or memory-bound by data movement as opposed to framework bottle- necks (Langerman et al., 2020). These previous works have largely focused on convolutional neural networks (CNNs) in computer vision. We extend these analyses to the inference setting and study transformer-based neural networks for NLP, and show that FLOP-based proxy metrics breakdown for due to additional performance bottlenecks in- troduced by deep learning frameworks. 2.2 Efficient Model Design Desire to develop computationally efficient models for language processing has led to the development of a variety of model architectures that achieve comparable task performance under fixed FLOP budgets. For example, compression of input se- quences and intermediate representations has been used to reduce the computational cost of long text sequences (Dai et al., 2020; Goyal et al., 2020) and distillation has been used to reduce model size (Sanh et al., 2019; Hou et al., 2020). Other work has sought to design efficient model archi- tectures by developing low-FLOP substitutes for standard, dense self-attention and convolution op- erations (Iandola et al., 2016; Zhang et al., 2018; Sandler et al., 2018; Sun et al., 2020; Xiong et al., 2021; Wang et al., 2020b). Additionally, direct efficiency metrics, such as wall-clock latency and energy usage have been in- corporated into the objectives of neural architec- ture search (NAS) and AutoML methods (Wu et al., 2019; Tan et al., 2019; Wang et al., 2020a). Man- ual inspection of the learned models shows that NAS often implicitly learns to take advantage of supported parallelism, learning wider architectures on GPU devices and deeper models on CPU de- vices (Cai et al., 2018; Tan et al., 2019; Sandler et al., 2018). However, it is often impossible to control for the hardware systems used for collect- ing these metrics leading to conclusions that may not generalize across deployment settings. 2.3 Platform Performance Analysis Efforts to establish common benchmarks leverage reference models and hardware platforms with tar- get latency or accuracy (Reddi et al., 2020; Zhou et al., 2020). Although these efforts have led to improvement in end-to-end latency, they often ab- stract away the underlying frameworks, compilers, backends, and hardware platforms. While general improvements in hardware and software kernels may lead to improvements across all models, it has been argued that solely focusing on performance optimization of a limited set of model architec- tures and runtimes may lead to overspecialization (Hooker, 2021). Previous analysis of the computational proper- ties of hardware accelerators has largely focused on the training setting in which larger kernels and batch sizes hide framework overhead that emerges in the inference setting (Wang et al., 2020c; Zhu et al., 2020, 2018). Other analyses of end-to-end systems analyses has primarily focused on domain- specific applications in reinforcement learning and recommendation systems (Gleeson et al., 2021; Lin et al., 2022), where simulation and memory access dominate execution time. Additionally, these prior efforts are restricted to small sets of reference mod- els and have not directly examined the relationship between model architectures and platforms. 3 Preliminaries 3.1 Neural Network Frameworks To take advantage of massively parallel hardware accelerators, inference with variable length text and speech sequences are padded to fixed length tensors that are processed with neural network frameworks. These frameworks provide implementations and APIs for tensor operations, gradient calculation, and construction of neural network computational graphs. Frameworks generally fall into the follow- ing design paradigms (Kahn et al., 2022): Eager Execution: The computational graph is constructed from a series of operations that are executed as soon as called from an interpreter. Ex- amples include: PyTorch (Paszke et al., 2019) and Chainer (Tokui et al., 2015). Deferred Execution: A series of operations are defined and executed on sample data to generate a dataflow graph that can then be just-in-time (JiT) compiled. Examples include: TorchScript, Jax (Bradbury et al., 2018), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014). Static: The computational graph is pre-defined, compiled, and executed inside a specialized run- time; allowing for aggressive, global ahead-of-time (AoT) compiler optimizations. Examples include: ComputeKernel 2 ComputeKernel 2 Launch Layer 4 Launch Layer 4 Launch Layer 3 Launch Layer 3 ComputeKernel 3 ComputeKernel 3 Launch Layer 2 Launch"}