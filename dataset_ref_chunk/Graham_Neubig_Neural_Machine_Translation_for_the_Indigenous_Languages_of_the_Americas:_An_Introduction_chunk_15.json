{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Neural_Machine_Translation_for_the_Indigenous_Languages_of_the_Americas:_An_Introduction_chunk_15.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the paper by Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo?", "answer": " Language-aware interlingua for multilingual neural machine translation", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " Where was the paper by Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Liu presented?", "answer": " arXiv preprint for neural machine translation", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " What is the negative aspect of training a single model on a massive amount of language pairs?", "answer": " The size of the network becomes a challenge", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " What approach does Zhang et al. (2020a) propose to address the problems faced by multilingual NMT models?", "answer": " A language-aware input layer, a deep transformer architecture, and an online back-translation approach", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " How does Wang et al. (2020) propose to improve the problem of imbalanced and linguistically diverse training data?", "answer": " By proposing a differentiable data selection method that automatically learns to weight training data", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " In multilingual modeling, what shows improvements compared with sharing everything in an RNN NMT model?", "answer": " Sharing all parameters except for the attention mechanism", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " What special language embeddings does Zhu et al. (2020a) incorporate into the self-attention mechanism?", "answer": " Two special language embeddings - one encoding unique characteristics of each language and the other capturing common semantics across languages", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " What problem does Zhang et al. (2020b) address in multilingual NMT systems?", "answer": " Translation into the wrong language", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " What does Raganato et al. (2021) explore in order to improve NMT model performances on low-resource languages?", "answer": " Weighting the target language label with jointly training one cross attention head with word alignments", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}, {"question": " How does Zhou et al. (2019) extend the multi-task training approach?", "answer": " They extend it with a cascade architecture where the first decoder reads the encoder, and the second decoder reads both the encoder and the first decoder", "ref_chunk": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}], "doc_text": "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua Luo. 2020a. Language-aware interlingua for multi- lingual neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1650\u20131655, On- line. Association for Computational Linguistics. Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan Soft contextual data augmentation Liu. 2019. arXiv preprint for neural machine translation. arXiv:1905.10523. Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020b. Incorporating bert into neural machine trans- lation. arXiv preprint arXiv:2002.06823. Barret Zoph, Deniz Yuret, and Kevin Knight. 2016. Transfer learning for low- resource neural machine translation. arXiv preprint arXiv:1604.02201. Jonathan May, A Appendix In this appendix we expand the information re- garding current work on MT for LRL. A.1 Expanded LR work on Multilingual supervised training Arivazhagan et al. (2019a) introduce a represen- tational invariance training objective across lan- guages that achieves comparable results with piv- oting methods. Promising results of multilingual models have encouraged experiments with models trained on a massive amount of language pairs, re- sulting in large multilingual models: Aharoni et al. (2019) train a single model on 102 languages to and from English in contrast to the 58 languages used by Neubig and Hu (2018). The negative aspect of this approach is the size of the network. Arivazhagan et al. (2019b) per- form an extensive study on 102 language pairs to explore different settings and training setups and achieve good results for LRLs, while main- taining good performance for high-resource lan- guages. Related massively multilingual NMT systems have been trained for analytic proposes (Tiedemann, 2018; Malaviya et al., 2017) and general zero-shot transfer learning (Artetxe and Schwenk, 2019). mRASP (Lin et al., 2020) use for pretraining of the multilingual model and add a randomly aligned substitution loss that aims to bring words and phrases closer in the cross-lingual space. Zhang et al. (2020a) explores the main problems that arise for such models: multilingual NMT usu- ally underperforms bilingual models (Arivazha- gan et al., 2019b), the larger the number of lan- guages gets the more the performance drops (Aha- roni et al., 2019), languages in datasets used for multilingual training are unbalanced in size, and poor zero-shot performance compared to pivot models (cf. \u00a76.3). Zhang et al. (2020a) ad- dresses these problems with a language-aware in- put layer, a deep transformer architecture (Wang et al., 2019b), and an online back-translation approach. These modifications boost zero-shot translation performance for multilingual models. To improve the problem of imbalanced and lin- guistically diverse training data, mostly heuristic methods have been proposed: Arivazhagan et al. (2019b) samples training data from different lan- guages based on a data size scaled by temperature term. These heuristics have an impact on perfor- mance, and ignore other factors that are not size. Oversampling of data is used by Johnson et al. (2017); Neubig and Hu (2018); Conneau and Lam- ple (2019). Wang et al. (2020) proposes a differ- entiable data selection method that automatically learns to weight training data, optimizing transla- tion on all languages. Multilingual modeling Sharing all parameters except for the attention mechanism shows im- provements compared with sharing everything in an RNN NMT model (Blackwood et al., 2018). Sachan and Neubig (2018) explores parameter sharing in the transformer architecture for the de- coder in the one-to-many translation setting and shows that transformers are more suitable than RNNs for this task. Also, parameter sharing in the decoder and embedding layer further improves performance. Lu et al. (2018) proposes a shared layer intended to capture the interlingua knowl- edge and an extension to the typical RNN network with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks condi- tioned on the task at hand, input, and model state (Zaremoodi et al., 2018). Zhang et al. (2020a) pro- poses a language-aware layer to improve such ar- chitectures further. With a similar idea, Zhu et al. (2020a) incorporates two special language embed- dings into the self-attention mechanism. The first encodes the unique characteristics of each lan- guage, while the second captures common seman- tics across languages. One problem in multilingual NMT systems is the translation into the wrong language. To ad- dress this problem, Zhang et al. (2020b) add a language-aware layer normalization and a lin- ear transformation that is inserted between the encoder and the decoder to induce a language- specific translation. Raganato et al. (2021) explore to weight the target language label with jointly training one cross attention head with word align- ments. Other modifications of NMT model archi- tectures to improve their performance on low- resource languages include: deep RNNs (Miceli- Barone et al., 2017), normalization layers (Ba et al., 2016), direct lexical connections (Nguyen et al., 2015), word embedding layers conducive to lexical sharing (Wang et al., 2019c). A.2 Extended Multi-task training Zhou et al. (2019) uses this approach, but extends it with a cascade architecture: the first decoder reads the encoder, and the second decoder reads the encoder and the first decoder (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). The auxiliary task (first decoder) is a denoising de- coder. With RNN NMT architectures, one can further decide if the attention mechanism should be shared among tasks (Niehues and Cho, 2017). The authors compare all architectures and find that they perform similarly, with only sharing the en- coder being slightly better. Using linguistic information as an auxiliary task has not yet been explored exhaustively. Niehues and Cho (2017) studies the usage of part-of-speech (POS) and named entity (NE) tags, finding that training on named entity recognition (NER), POS tagging and MT together improves performance the most. For agglutinative languages, morpho- logical auxiliary tasks can be beneficial: Pan et al. (2020) uses stemming with fully shared parame- ters. As an alternative to linguistically informed aux- iliary tasks Srinivasan et al. (2019) uses multiple BPE vocabulary sizes to generate different seg- mentations. Each segmentation is treated as"}