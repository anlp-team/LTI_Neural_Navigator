{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_COBRA_Frames:_Contextual_Reasoning_about_Effects_and_Harms_of_Offensive_Statements_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What do humans tend to have better explanations of compared to machines?", "answer": " Humans tend to have better explanations of the implications of statements compared to machines.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " Why might there be a gap between the majority vote and unanimously approved examples in the text?", "answer": " The gap between the majority vote and unanimously approved examples might be due to annotators having different standards for what constitutes a good explanation.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " In Table 2, what do the basic statistics of the COBRACORPUS illustrate?", "answer": " The basic statistics of the COBRACORPUS illustrate the average length of explanations and the distribution of target groups.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " According to Figure 3a, which groups are among the most frequently targeted in the COBRACORPUS?", "answer": " Minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " What is the most frequent type of offensiveness found in the COBRACORPUS?", "answer": " The most frequent type of offensiveness found in the COBRACORPUS is microaggressions.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " What is COBRACORPUS-CF and what purpose does it serve?", "answer": " COBRACORPUS-CF is a challenge set of counterfactual context pairs that invert the offensiveness of statements, designed to examine the limits of context-aware explanations.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " What is one of the difficulties of collecting counterfactual data mentioned in the text?", "answer": " One of the difficulties of collecting counterfactual data is finding statements that are contextually ambiguous and can be interpreted in different ways.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " How do the counterfactual contexts in COBRACORPUS-CF invert the offensiveness of the statements?", "answer": " The counterfactual contexts in COBRACORPUS-CF invert the offensiveness of the statements by presenting different contexts that change perceptions of toxicity.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " How are the counterfactual context pairs classified in terms of the situations they represent?", "answer": " The counterfactual context pairs are classified based on the types of situations they represent, such as conversations occurring between friends, among strangers in public, at a workplace, and between family members.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}, {"question": " Based on the text, where are offensive language and stereotypes more likely to be considered acceptable?", "answer": " Offensive language and stereotypes might be considered more acceptable in informal settings among people who are familiar with each other, like in sports or social clubs.", "ref_chunk": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}], "doc_text": "marked as likely by all three annotators (unanimous).6 As illustrated in Table 1, humans tend to have bet- ter explanations of the implications of statements, whereas machines sometimes re-use words from the statement. This might explain the gap between the majority vote and unanimously approved exam- ples, as the annotators might have different stan- dards for what constitutes a good explanation. Analyzing COBRACORPUS We present some basic statistics of the COBRACORPUS in Table 2. The average length shows illustrates the level of nu- ance in some of the explanations (e.g., 22 words for cognitive reaction). Additionally, we analyze the distribution of target groups, finding that minority or marginalized groups like LGBTQIA+, people with disabilities, and women are among the most frequently targeted groups (see Figure 3a). Analyz- ing the distribution of the free-text offensiveness types, we find that microaggressions are the most frequent type of offensiveness (see Figure 3b). 4 COBRACORPUS-CF: Generating Counterfactual Contexts To examine the limits of context-aware ex- planations generate COBRACORPUS-CF, a challenge set of counterfac- tual context pairs that invert the offensiveness of statements, inspired by adversarial and counterfac- tual test sets in NLP (Gardner et al., 2020; Li et al., 2020; Chang et al., 2021). Illustrated in Figure 1, our motivating question asks, how does the toxicity of a statement change with a different context? of offensiveness, we Creating COBRACORPUS-CF One of the diffi- culties of collecting counterfactual data is finding 6Our annotation agreement is moderately high, on average, with 89.10% pairwise agreement and \u03ba=0.782. T5 EncoderT5 Decoder CobraCorpus SourceThis is conversation between a man and a woman in an initial interaction in workspace: Excuse me miss, are any of your bosses here? Training and InferenceTraining and InferenceInferenceInference CobraCorpus Target<intent> The speaker is trying to find someone in a position of authority </intent> <target-Group> Listener, women </targetGroup> <implica-tion> It implies that women are not capable of being in a position of authority, and that a man is needed to speak to someone in charge </impli-cation> <powerDynamics> gender power differen-tial between speaker and listener </powerDynam-ics> <emotionalReaction> listener and women could feel angry, feel condescended to, angry that they didn\u2019t think a woman was boss </emo-tionalReaction> <cognitiveReaction> might lead women bosses to change their behavior to appear more boss-like, might become more confrontation-al </cognitiveReaction> <type> microaggression </type>This is a conversation between friend who own the place and a friend who broke a cup in the friend\u2019s place: it\u2019s not a big deal, just let it goThis is a conversation between a male co-worker and a female co-worker in a conversation about sexual harass-ment: it\u2019s not a big deal, just let it go CHaRM CobraCorpus-Cf Source CobraCorpus-Cf Target<intent> The speaker is trying to console their friend </intent> ... <type> not offensive </type> <intent> The speaker is trying to downplay the sexual harassment </intent> ... <type> sexism </type> Figure 4: Experiment overview. CHARM is an encoder-decoder Transformer model based on pretrained FLAN-T5 checkpoints (Chung et al., 2022). During the training stage, the model is finetuned to generate the explanation dimensions in a linearized format given the statement and context in COBRACORPUS. We evaluate the quality of the generated explanation on COBRACORPUS and the accuracy of detecting offensiveness in COBRACORPUS-CF. The arrows indicate the flow of input and output. For COBRACORPUS-CF, we always have a pair of contexts deciding if the statement is offensive ((cid:254)) or harmless ((cid:10)). statements that are contextually ambiguous and can be interpreted in different ways. Statements such as microaggressions, compliments, criticism, and offers for advice are well-suited for this, as their interpretation can be highly contextual (Sue, 2010; Nadal et al., 2014). We scraped 1000 statements from a crowd- sourced corpus of microaggressions,7 including many contextually ambiguous statements. Follow- ing a similar strategy as in \u00a73.2, we manually craft 50 (statement, offensive context, harmless context) triples to use as in-context examples for generating counterfactual contexts. Then, for each microag- gression in the corpus, we generated both a harm- less and offensive context with GPT-3.5, prompted with five randomly sampled triples as in-context examples. This process yields 982 triples, as GPT- 3.5 failed to generate a harmless context for 18 statements. Human Verification We then verify that the counterfactual contexts invert the offensiveness of the statements. Presented with both contexts, the annotators (1) rate the offensiveness of the statement under each context (Individual) and, (2) choose the context that makes the statement more offensive (Forced Choice). We annotate all of the 982 triples in this manner. When we evaluate mod- els\u2019 performance on COBRACORPUS-CF (\u00a75.2), we use the Individual ratings. In our experiments, we use the 344 (statement, context) pairs where all three annotators agreed on the offensiveness, to ensure the contrastiveness of the contexts.8 Analyzing Counterfactual Contexts To com- pare with our likely contexts, we examine the types of situations that changed perceptions of toxicity using our human-verified offensive and harmless counterfactual contexts. We use the aforemen- tioned Forced Choice ratings here. We detect and classify the category of the situation in the coun- terfactual context pairs as conversations occurring between friends, among strangers in public, at a workplace, and between members of a family, us- ing keyword matching. We observe that contexts involving conversa- tions occurring among strangers in public and at the workplace are perceived as more offensive than those which occur between friends (see Table 3). This aligns with previous literature showing that offensive, familiar, or impolite language might be considered more acceptable if used in environments where people are more familiar.(Jay and Jansche- witz, 2008; Dynel, 2015; Kasper, 1990). Ethno- graphic research shows how crude language, in- cluding the use of offensive stereotypes and slurs, is often encouraged in informal settings like sports (Fine, 1979) or social clubs (Eliasoph and Lichter- man, 2003). But such speech is generally con- sidered less acceptable in a broader public sphere including in public and at the workplace. 8We have high average annotation agreement in this task 7https://www.microaggressions.com/ (\u03ba = 0.73). Intent Target group Power Dynamics Implication Emotional React. Cognitive React. Offensiveness Average BLEU ROUGE"}