{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_KD-DLGAN:_Data_Limited_Image_Generation_via_Knowledge_Distillation_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What evaluation metrics were used in the experiments mentioned in the text?", "answer": " Frechet Inception Distance (FID) and Inception Score (IS) were used.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What does including the proposed KD-DLGAN into DA achieve, as shown in Row 3 of Table 1?", "answer": " Including KD-DLGAN into DA achieves superior performance across all data settings consistently compared to DA alone.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What is the purpose of training the GAN discriminator to mimic the visual feature representation of CLIP in the vanilla knowledge distillation method?", "answer": " The purpose is to mimic the visual feature representation of CLIP.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What did the experiments show about the performance of KD-DLGAN compared to other methods over CIFAR-10 and CIFAR-100?", "answer": " KD-DLGAN outperforms and complements the state-of-the-art clearly.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What is the backbone employed by all the compared methods mentioned in the text?", "answer": " BigGAN [5] is employed as the backbone.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What are the training data percentages used for CIFAR-10 and CIFAR-100 experiments in Table 3?", "answer": " The training data percentages are 10%, 5%, and 2.5%.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What is the key factor attributed to the superior performance of KD-DLGAN according to the text?", "answer": " The generative knowledge distillation techniques in KD-DLGAN is the key factor.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " In Table 4, what is the conclusion regarding the ablation study of KD-DLGAN?", "answer": " Both AGKD and CGKD in KD-DLGAN improve the generation performance over the baseline DA [53].", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What do AGKD and CGKD in KD-DLGAN complement according to the text?", "answer": " AGKD and CGKD complement each other in KD-DLGAN.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}, {"question": " What is the result of the ablation study over 100-shot Obama in Figure 4?", "answer": " AGKD and CGKD can generate more realistic images than the baseline.", "ref_chunk": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}], "doc_text": "[39]. Datasets details are provided in the supplementary mate- rial. We perform evaluations with Frechet Inception Dis- tance (FID) [16] and inception score (IS) [38]. As Row 3 of Table 1 shows, including the proposed KD-DLGAN into DA [53] achieves superior performance across all data settings consistently as compared with DA alone (in Row 2), demonstrating the complementary rela- tion between KD-DLGAN and DA [53]. In addition, the vanilla knowledge distillation in DA + KD (CLIP [36]) (Row 1) trains the GAN discriminator to mimic the visual feature representation of CLIP. We can observe that KD- Method CIFAR-10 CIFAR-100 100% Data 20% Data 10% Data 100% Data 20% Data 10% Data DA [53] + KD (CLIP [36]) 8.70 \u00b1 0.02 13.70 \u00b1 0.08 22.03 \u00b1 0.07 11.74 \u00b1 0.02 21.76 \u00b1 0.06 33.93 \u00b1 0.09 DA [53] (Baseline) + KD-DLGAN (Ours) 8.75 \u00b1 0.03 8.42 \u00b1 0.01 14.53 \u00b1 0.10 11.01 \u00b1 0.07 23.34 \u00b1 0.09 14.20 \u00b1 0.06 11.99 \u00b1 0.02 10.28 \u00b1 0.03 22.55 \u00b1 0.06 15.60 \u00b1 0.08 35.39 \u00b1 0.08 18.03 \u00b1 0.11 APA [19] + KD-DLGAN (Ours) 8.28 \u00b1 0.02 8.26 \u00b1 0.02 15.31 \u00b1 0.04 11.15 \u00b1 0.06 25.98 \u00b1 0.06 13.86 \u00b1 0.07 11.42 \u00b1 0.04 10.23 \u00b1 0.02 23.50 \u00b1 0.06 19.22 \u00b1 0.07 45.79 \u00b1 0.15 27.11 \u00b1 0.10 LeCam-GAN [40] + KD-DLGAN (Ours) 8.46 \u00b1 0.06 8.19 \u00b1 0.01 14.55 \u00b1 0.08 11.45 \u00b1 0.07 16.69 \u00b1 0.02 13.22 \u00b1 0.03 11.20 \u00b1 0.09 10.12 \u00b1 0.03 22.45 \u00b1 0.09 18.70 \u00b1 0.05 27.28\u00b1 0.05 22.40 \u00b1 0.06 ADA [21] + KD-DLGAN (Ours) 8.99 \u00b1 0.03 8.46 \u00b1 0.02 19.87 \u00b1 0.09 14.12 \u00b1 0.10 30.58 \u00b1 0.11 16.88 \u00b1 0.08 12.22 \u00b1 0.02 10.48 \u00b1 0.04 22.65 \u00b1 0.10 19.26 \u00b1 0.06 27.08 \u00b1 0.15 20.62 \u00b1 0.09 Table 2. Comparison with the state-of-the-art over CIFAR-10 and CIFAR 100: KD-DLGAN outperforms and complements the state-of- the-art clearly. KD-DLGAN also performs better than vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. And we report FID(\u2193) averaged over three runs. Method 10% training data 5% training data 2.5% training data DA [53] + KD (CLIP [36]) IS\u2191 13.29 \u00b1 0.50 FID\u2193 26.58 \u00b1 0.21 IS\u2191 11.63 \u00b1 0.29 FID\u2193 38.11 \u00b1 0.33 IS\u2191 9.43 \u00b1 0.25 FID\u2193 57.95 \u00b1 0.41 DA [53] (Baseline) + KD-DLGAN (Ours) 12.76 \u00b1 0.34 14.25 \u00b1 0.66 32.82 \u00b1 0.18 19.99 \u00b1 0.11 9.63 \u00b1 0.21 12.71 \u00b1 0.34 56.75 \u00b1 0.35 24.70 \u00b1 0.14 8.17 \u00b1 0.28 13.45 \u00b1 0.51 63.49 \u00b1 0.51 30.27 \u00b1 0.16 LeCam-GAN [40] + KD-DLGAN (Ours) 11.59 \u00b1 0.44 13.98 \u00b1 0.23 30.32 \u00b1 0.24 22.12 \u00b1 0.12 10.53 \u00b1 0.22 13.86 \u00b1 0.45 39.33 \u00b1 0.27 23.85 \u00b1 0.21 9.99 \u00b1 0.26 13.22 \u00b1 0.44 54.55 \u00b1 0.46 31.33 \u00b1 0.15 ADA + KD-DLGAN (Ours) 12.67 \u00b1 0.31 14.14 \u00b1 0.32 31.89 \u00b1 0.17 20.32 \u00b1 0.10 9.44 \u00b10.25 14.06 \u00b1 0.39 43.21 \u00b1 0.37 22.35 \u00b1 0.11 8.54 \u00b1 0.26 14.65 \u00b1 0.47 56.83 \u00b1 0.48 28.79 \u00b1 0.14 Table 3. Comparison with the state-of-the-art over ImageNet [11]: KD-DLGAN achieves the best performance consistently and comple- ments the state-of-the-art. Besides, KD-DLGAN outperforms vanilla knowledge distillation in DA + KD (CLIP [36]) consistently as well. All the compared methods employ BigGAN [5] as backbone. We report IS(\u2191) and FID(\u2193) averaged over three runs. DLGAN outperforms DA + KD (CLIP [36]) consistently as well, indicating that the performance gain in KD-DLGAN is largely attributed to our generative knowledge distillation designs instead of solely from the powerful vision-language model. Table 1 also tabulates the results of KD-DLGAN when implementing over four state-of-the-art data-limited generation approaches including LeCam-GAN [40], Ins- Gen [44], APA [19] and ADA [21]. We can see that KD- DLGAN complement all the state-of-the-art consistently, demonstrating the superior generalization and complemen- tary property of our proposed KD-DLGAN. Fig. 3 shows qualitative comparison with DA [53]. It can be observed that KD-DLGAN clearly outperforms the state-of-the-art in the data-limited generation, especially in term of the generated shapes and textures. 4.3. Experiments with BigGAN Table 2 and Table 3 show the conditional image gener- ation results on CIFAR-10, CIFAR-100 and ImageNet, re- spectively. All models employ BigGAN [5] as the back- bone. CIFAR-10 and and CIFAR-100 are trained with 100% (50K images), 20% (10K images) or 10% (5K im- ages) training data where the FIDs are evaluated over the validation sets (10K images). ImageNet is trained with 10% (~100K images), 5% (~50K images) and 2.5% (~25K im- ages), where the evaluations are performed over the whole training set (~1.2M images). The experiments show that our KD-DLGAN outper- forms the state-of-the-art substantially. The superior per- formance is largely attributed to our designed generative knowledge distillation techniques in KD-DLGAN, which mitigates the discriminator over\ufb01tting and improves the generation performance effectively. We also show the re- sults of vanilla knowledge distillation from the powerful vision-language model CLIP [36] in Row 1 of Table 2 and Table 3. We can see that KD-DLGAN outperforms the vanilla knowledge distillation method by a large margin, in- dicating that the performance gain is largely attributed to our generative knowledge distillation design instead of the powerful vision-language model. In addition, Table 2 and Method AGKD CGKD CIFAR-10 20% data 10% data Obama 100-shot Grumpy Cat DA [53] (Baseline) 14.53 23.34 46.87 27.08 (cid:88) (cid:88) 12.97 \u00b1 0.08 12.77 \u00b1 0.08 15.85 \u00b1 0.06 18.66 \u00b1 0.09 35.51\u00b1 0.25 36.18 \u00b1 0.22 23.24 \u00b1 0.16 23.17 \u00b1 0.11 Ours (cid:88) (cid:88) 11.01 \u00b1 0.07 14.20 \u00b1 0.06 31.54 \u00b1 0.27 20.13 \u00b1 0.13 Table 4. Quantitative ablation study of KD-DLGAN: AGKD and CGKD in KD-DLGAN both improves the generation performance over the baseline DA [53]. KD-DLGAN performs the best as AGKD and CGKD complement each other. The FIDs (\u2193) are averaged over three runs. DAAGKDCGKDKD-DLGAN Figure 4. Qualitative ablation study over 100-shot Obama: AGKD (Row 2) and CGKD (Row 3) can generate more realistic images than the"}