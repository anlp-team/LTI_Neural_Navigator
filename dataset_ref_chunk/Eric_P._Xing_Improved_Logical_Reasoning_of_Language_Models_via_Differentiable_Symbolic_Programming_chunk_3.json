{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Improved_Logical_Reasoning_of_Language_Models_via_Differentiable_Symbolic_Programming_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the probabilistic relational symbols denoted as?,answer: Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What is the output vector r in the context of extracted relational symbols?,answer: The output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1).", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What are the main objectives of the symbolic inference modules P\u03c6 and Psl?,answer: The main objectives are to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What is the purpose of logic rules in the context of deducing new facts?,answer: Logic rules can be applied to known facts to deduce new ones, allowing logical deduction and reasoning.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " How is the propagation of probabilities achieved in the text?,answer: The propagation of probabilities is achieved by allowing the deduced facts to be associated with probabilities computed using probabilities predicted by the underlying relation extractor M\u03b8.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " How does DSR-LM alleviate the issue of expensive hand-crafted rules?,answer: DSR-LM applies LMs to help automatically extract rules, and further utilizes the differentiable pipeline to fine-tune the rules, avoiding the need for expensive or impossible hand-crafted rules.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What is semantic loss in the context of training with weak supervision labels?,answer: Semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules, detecting violations of integrity constraints.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " How does the text evaluate DSR-LM in experiments?,answer: The text evaluates DSR-LM on both CLUTRR and DBpedia-INF to show accurate and generalizable long-range reasoning capability.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What is the purpose of the integrity constraint regarding father and son/daughter relationships?,answer: The integrity constraint states that if A is B\u2019s father, then B should be A\u2019s son or daughter, ensuring the model predicts consistent relationships.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}, {"question": " What is the significance of composition rules in kinship reasoning?,answer: Composition rules are the only kind of rule needed for kinship reasoning and allow for deriving more facts based on known kinship relations.", "ref_chunk": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}], "doc_text": "noisy and redundant data. Overall, assum- ing that there are m windows in the context x, we extract mn(n \u2212 1)(\u2223R\u2223 + 1) probabilistic re- lational symbols. Each symbol is denoted as an atom of the form p(s, o), where p \u2208 R \u222a {n/a} is the relational predicate, and s, o are the two entities connected by the predicate. We denote the probability of such symbol extracted by the LM and relational classi\ufb01er as Pr(p(s, o) \u2223 \u03b8). All these probabilities combined form the output vector r = M\u03b8(x) \u2208 Rmn(n\u22121)(\u2223R\u2223+1). 3.4 Differentiable Symbolic Inference The symbolic inference modules P\u03c6 and Psl are responsible for processing the extracted relations to deduce 1) an expected output relation in R, and 2) a semantic loss encoding the probability of constraint violation. There are two main objectives for these modules. First, they need to logically reason about the output relation and the semantic loss based on the extracted relational symbols r, the query q, and the rule weights \u03c6. Second, they need to compute the gradients of \u02c6y and lsl with respect to \u03b8 and \u03c6, namely \u2202 \u02c6y \u2202\u03c6 , and \u2202lsl \u2202\u03c6 , \u2202lsl \u2202\u03b8 , in order for the \ufb01ne-tuning and rule learning to happen. \u2202\u03b8 , \u2202 \u02c6y Logical deduction. Logic rules can be applied to known facts to deduce new ones. For example, below is a horn clause, which reads \u201cif b is a\u2019s brother and c is b\u2019s daughter, then c is a\u2019s niece\u201d: niece(a, c) \u2190 brother(a, b) \u2227 daughter(b, c). Note that the structure of the above rule can be captured by a higher-order logical predicate called \u201ccomposite\u201d (abbreviated as comp ). This allows us to express many other similarly struc- For instance, we can tured rules with ease. have comp(brother, daughter, niece) and comp(father, mother, grandmother) . With this set of rules, we may derive more facts based on known kinship relations. In fact, composition is the only kind of rule we need for kinship reasoning. In general, there are many other useful higher-order predicates to reason over knowledge bases, which we list out in Table 2. Predicate transitive symmetric inverse implies Example transitive(relative) symmetric(spouse) inverse(husband, wife) implies(mother, parent) Table 2: Higher-order predicate examples. Probability propagation. We seek to have the deduced facts to also be associated with probabili- ties computed using probabilities predicted by the underlying relation extractor M\u03b8. This is achieved by allowing the propagation of probabilities. For example, we have the proof tree with probabilities: 0.9 \u2236\u2236 brother(D, R) 0.8 \u2236\u2236 daughter(R, K) 0.72 \u2236\u2236 niece(D, K) In practice, there could be multiple steps in the proof tree (multi-hop) and one fact can be derived by multiple proof trees. We employ the inference algorithms based on approximated weighted model counting (WMC) presented in (Manhaeve et al., 2018) to account for probabilistic inference under complex scenarios. Since the WMC procedure is augmented for differentiation, we can obtain the \u2202r . From here, we can obtain \u2202 \u02c6y gradient \u2202 \u02c6y \u2202r \u2202\u03b8 , where the second part can be automatically derived from differentiating M\u03b8. \u2202\u03b8 = \u2202 \u02c6y \u2202r Rule learning. Hand-crafted rules could be ex- pensive or even impossible to obtain. To allevi- ate this issue, DSR-LM applies LMs to help au- tomatically extract rules, and further utilizes the differentiable pipeline to \ufb01ne-tune the rules. Each rule such as comp(brother, daughter, niece) is attached a weight, initialized by prompting an underlying LM. For example, the prompt we use for extracting comp(r,p,q) is \u201cone\u2019s r\u2019s p is their <q:mask>\u201d. Given that the relations r, p, q \u2208 R, DSR-LM automatically enumerates r and p from R while querying for LM to unmask the value of q. LM then returns a distribution of words, which we take an intersection with R. The probabilities com- bined form the initial rule weights \u03c6. This type of rule extraction strategy is different from existing ap- proaches in inductive logic programming since we are exploiting LMs for existing knowledge about relationships. Note that LMs often make simple mistakes an- swering such prompt. In fact, with the above prompt, even GPT-3 can only produce 62% of composition rules correctly. While we can edit prompt to include few-shot examples, in this work we consider \ufb01ne-tuning such rule weights \u03c6 within our differentiable reasoning pipeline. The gradient with respect to \u03c6 is also derived with the WMC procedure, giving us \u2202 \u02c6y \u2202\u03c6 . In practice, we use two optimizers with different hyper-parameters to up- date the rule weights \u03c6 and the underlying model parameter \u03b8, in order to account for optimizing different types of weights. Semantic loss and integrity constraints. In general, learning with weak supervision label is hard, not to mention that the deductive rules are learnt as well. We thereby introduce an additional semantic loss during training. Here, semantic loss is derived by a set of integrity constraints used to regularize the predicted entity-relation graph as well as the learnt logic rules. In particular, we consider rules that detect violations of integrity constraints. For example, \u201cif A is B\u2019s father, then B should be A\u2019s son or daughter\u201d is an integrity constraint for relation extractor\u2014if the model pre- dicts a father relationship between A and B, then it should also predict a son or daughter relationship between B and A. Encoded in \ufb01rst order logic, it is \u2200a, b, father(a, b) \u21d2 (son(b, a) \u2228 daughter(b, a)). Through differentiable reasoning, we evaluate the probability of such constraint being violated, yield- ing our expected semantic loss. In practice, arbi- trary number of constraints can be included, though too many interleaving ones could hinder learning. 4 Experiments We evaluate DSR-LM on both CLUTRR and DBpedia-INF. We show that DSR-LM has accurate and generalizable long-range reasoning capability. 4.1 Datasets CLUTRR (Sinha et al., 2019) consists of kinship reasoning questions. Given a context that describes a family\u2019s routine activity, the goal is to deduce the relationship between two family members that is not explicitly mentioned in the story. Although the"}