{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the blue rectangles in spectrograms (a) and (b) mark?,answer: The region with frequency permutation.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " What are the SDR scores for speaker images 1 and 2 in spectrograms (a) and (b) respectively?,answer: 8.7 dB and 7.7 dB.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " What is the motivation behind the proposed mixture-constraint loss, LMC?,answer: To use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " How is the relative RIR relating a signal to its delayed version described?,answer: Causal.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " What is the physical interpretation provided for the intermediate DNN estimates \u02c6Z in the multi-channel-input case?,answer: Similar to a signal captured by a virtual microphone that is closer to the speaker c than all the P microphones.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " In the monaural-input case, how is each \u02c6Z(c) aligned?,answer: Aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone).", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " How does UNSSOR differ from RAS in terms of training approach?,answer: UNSSOR can be trained in an unsupervised way while RAS requires labeled mixtures for training.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " Why is RAS considered to be ineffective in a fully-unsupervised setup?,answer: Because the loss is computed only on the right-ear mixture, leading to an ill-posed problem.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " What is the window size for STFT in the default setup?,answer: 32 ms.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}, {"question": " What does RAS stand for in the context of the text?,answer: Monaural two-speaker separation given binaural (two-channel) training mixtures.", "ref_chunk": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}], "doc_text": "show the separation result of the model trained with LMC+ISMS in (11), which effectively addresses the frequency permutation problem. Figure 3: Example spectrograms of (a)-(b): FCP-estimated speaker image 1 and 2 with SDR scores of 8.7 and 7.7 dB (using LMC in (9) for training); (c)-(d): oracle speaker image 1 and 2; and (e)-(f): FCP-estimated speaker image 1 and 2 with SDR scores of 17.1 and 16.8 dB (using LMC+ISMS in (11) for training). The blue rectangles in (a) and (b) mark the region with frequency permutation. The mixture SDR scores of the two speakers are respectively 0.2 and \u22120.1 dB. Best viewed in color. 16 (15) (16) E Differences between mixture-constraint loss and mixture consistency The proposed mixture-constraint loss, LMC, has very different physical meanings and mathematical forms compared to the mixture consistency term proposed in [59]. First, in [59], DNN estimates are strictly constrained to add up to the mixture (see Eq. (7) and (9) in [59]), while our MC loss only encourages the filtered DNN estimates to add up to the mixture. Second, our MC loss is applied to filtered DNN estimates rather than directly to DNN estimates. Third, we deal with multi-microphone MC, while [59] only addresses single-channel cases. Fourth, the motivation of the proposed MC loss is to use mixtures as constraints to regularize DNN estimates so that the estimates can approximate source images. This motivation is completely different from that of the mixture consistency term. F Illustration of using causal and non-causal filtering This section illustrates that the relative RIR relating a signal to its delayed version is causal and the relative RIR relating a signal to its advanced version is non- causal. In Fig. 4, suppose that the blue signal is the DNN es- timate for speaker c, and the orange signal is speaker c\u2019s image at another microphone, which is a delayed version (i.e., reaching the microphone later). To filter the blue signal to approximate the oracle one, we only need a causal filter to delay the blue signal. Reversely, suppose that the orange signal is the DNN estimate for speaker c, and the blue signal is speaker c\u2019s image at another microphone, which is an ad- vanced version (i.e., reaching the microphone earlier). To filter the orange signal to approximate the blue signal, we need a non-causal filter to advance the orangle signal. 8 2 12Time steps 10 2 4 0 6 4 Figure 4: Example for illustrating causal and non- causal filtering. Best viewed in color. G Interpretation of intermediate DNN estimates \u02c6Z This section provides an interpretation of the physical meanings of the intermediate DNN estimates \u02c6Z in the cases of using multi-channel input and loss, and single-channel input and multi-channel loss. G.1 Multi-channel-input case In (9), \u02c6Z(c) is constrained such that it can be filtered by a causal filter \u02c6gp(c) to approximate Xp(c). Since there could be an infinite number of combinations of \u02c6Z(c) and \u02c6gp(c) whose convolution results would well approximate Xp(c), \u02c6Z(c) cannot be interpreted as the dry source signal and we think it more similar to a signal captured by a virtual microphone that is closer to speaker c than all the P microphones. See Fig. 5(a) for an example, where each virtual microphone captures the direct-path signal of a target speaker earlier than any other microphones so that we can use causal FCP filters. G.2 Monaural-input case In the monaural-input case, each \u02c6Z(c) would be aligned to the speaker\u2019s image at the input microphone (i.e., the reference microphone), since the DNN only has monaural input and in this case the DNN is not likely to align its outputs to a virtual microphone or any actual microphones. We give an example in Figure 5(b), where the reference microphone captures speaker 2\u2019s direct-path signal later than all the other microphones. In this case, we just need to use non-causal FCP filters when filtering \u02c6Z(c) (which is estimated based on the monaural signal at the reference microphone) to approximate speaker 2\u2019s images captured at the other microphones. 17 (a)(b) Speaker#2 Virtual mic 1Virtual mic 2 Speaker#2Reference mic Speaker#1 Speaker#1 Figure 5: Illustration of signal alignment in (a) multi-channel-input, multi-channel-loss case; and (b) single- channel-input, multi-channel-loss case. Best viewed in color. H Differences from RAS In many aspects, UNSSOR differs from RAS [36], which deals with monaural two-speaker separation given binaural (two-channel) training mixtures. RAS first performs DNN-based monaural separation on the left-ear mixture in the magnitude T-F domain, then linearly filters the DNN estimates at the left ear through time-domain Wiener filtering such that the filtered estimate can approximate the right-ear mixture, and the training loss is computed between the summation of the filtered DNN estimates and the right-ear mixture. Besides differences in, for example, the DNN architectures, linear filtering algorithms, how phase estimation (important for estimating relative RIRs) is handled, how frequency permutation problem is dealt with, and training data curation (difficult training examples need to be removed to train RAS), the key difference is that RAS fails to be trained in an unsupervised way [36]. It still needs labelled mixtures so that a supervised PIT-based model can be trained and then used as the starting point for their unsupervised algorithm. We think that the ineffectiveness of RAS in fully-unsupervised setup is likely because the loss is computed only on the right-ear mixture. Following our analysis in Section 3, in RAS there are N \u00d7 1 equations (because the loss is only computed on the right-ear time-domain signal, assumed N -sample) but N \u00d7 C + (2 \u2212 1) \u00d7 512 \u00d7 C unknowns (where the 512 term is because the filter is assumed 512-tap in [36], and the (2 \u2212 1) term is because there is only one filter for each speaker in the binaural setup, i.e., only one non-reference microphone). This is an ill-posed problem, not likely to be solved via the current RAS algorithm. I Miscellaneous system and DNN setup In default, for STFT, the window size is 32 ms, the hop size"}