{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_The_CHiME-7_DASR_Challenge:_Distant_Meeting_Transcription_with_Multiple_Devices_in_Diverse_Scenarios_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What approach is chosen in the text for diarization?", "answer": " A simple but reasonably effective approach built over the Pyannote diarization system.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What does Fig. 4 in the text show?", "answer": " Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What is the key difference in the front-end module in CHiME-7 DASR compared with the CHiME-6 challenge?", "answer": " The use of automatic channel selection.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What trade-off is observed in the text regarding the number of microphones used in ASR trained with GSS?", "answer": " There is a trade-off between computational complexity and performance.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " How does the best performance vary between Mixer 6 and DiPCo in terms of GSS result?", "answer": " Mixer 6 benefits from the use of all microphones, while the best performance for DiPCo is achieved by using the top 80%.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What are the results achieved by the proposed diarization system in terms of DER and JER?", "answer": " The results achieved are 28.8% DER and 38.5% JER with 250 ms collar.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " How does the baseline ASR model compare with Whisper large in acoustic robustness sub-track and main track?", "answer": " The baseline ASR model compares favorably with Whisper large in the acoustic robustness sub-track, but Whisper large appears slightly more robust in the main track.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What impact does diarization have on the final recognition score according to Table 3?", "answer": " Diarization has a big impact on the final recognition score.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " What is the hope regarding the CHiME-7 DASR challenge?", "answer": " The hope is that participants will devise novel and more effective ways to address the challenges posed by the scenario.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}, {"question": " Who was acknowledged in the text for their help and feedback?", "answer": " Marc Delcroix, Naoyuki Kamo, Christoph Boedekker, and Thilo von Neumann were acknowledged for their help and feedback.", "ref_chunk": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}], "doc_text": "use of embedding-based approaches, unless one wants to create exten- sive multi-channel synthetic data. For these reasons, here we opt for a simple but reasonably effective approach built over the Pyannote [46] diarization sys- tem, which is very popular and has been proven to be success- 5espnet/tree/chime7task1 diar/egs2/chime7 task1/diar asr1 6pyannote/speaker-diarization 7popcornell/pyannote-segmentation-chime6-mixer6 8popcornell/chime7 task1 asr1 baseline Figure 4: Effect of EV-based channel selection with GSS on WER performance (left) and inference time (right), on the dev set. We show scenario-wise results and the macro-average. 6. Results & Discussion 6.1. Front-End Processing As mentioned in Sec. 5.1, a key difference between the front- end module in CHiME-7 DASR, compared with the CHiME-6 challenge, is the use of automatic channel selection. In Fig. 4, we show WER results and inference time on the dev sets for different selection ratios when the ASR is trained using data containing GSS performed with all channels. We can observe two trends. First, the time employed to per- form selection+GSS appears to be approximately linear with the number of microphones used. Secondly, the error rate generally decreases as the number of microphones used increases. Thus there appears to be a trade-off between computational complex- ity and performance. In particular, the best performance for CHiME-6 and DiPCo is achieved by using the top 80%, while Mixer 6 will benefit from the use of all microphones. This could be related to the fact that Mixer 6 is less noisy and all channels might contribute significantly to improving the GSS result. In the baseline we always use the top 80% channels for GSS for both training and inference, as this seems to maximize the per- formance overall. 6.2. Diarization In Table 2 we report the results achieved by our proposed di- arization system in terms of DER and Jaccard error rate (JER) with 250 ms collar. We can see that, overall, the system, despite being fine-tuned only on CHiME-6 data, generalizes well to the other scenarios. Table 2: CHiME-7 DASR diarization baseline results. We show scenario-wise results as well as the macro-average. Scenario Dev Eval DER JER DER JER CHiME-6 DiPCo Mixer 6 40.0 29.8 16.6 51.1 41.4 22.8 56.3 27.9 9.3 62.5 40.9 11.0 Macro 28.8 38.5 31.2 38.2 CHiME-6 remains by far the most difficult scenarios among the three for what regards diarization, due to the challenging acoustical conditions and frequent overlapped speech. 6.3. Overall Results In Table 3 we present DA-WER figures obtained on the three CHiME-7 DASR scenarios, for both sub-track (sub) and main- track (main), together with their macro averages. We com- pare our baseline model against Whisper [24] large \u2014 note that Whisper cannot be used by participants as it was not included among the allowed pre-trained models (its training data may contain our evaluation). Nevertheless, it serves as an interesting and strong point of comparison. Table 3: CHiME-7 DASR sub-track results obtained using the best configuration from development set (80% channels GSS). Whisper results are reported for reference. ASR model Scenario Dev Eval DA-WER (%) DA-WER(%) sub main sub main Baseline CHiME-6 32.6 33.5 DiPCo 20.2 Mixer 6 62.4 56.6 22.5 35.5 36.3 28.6 77.4 54.7 33.7 Macro 28.8 47.2 33.4 55.3 Whisper CHiME-6 30.9 34.5 DiPCo 21.2 Mixer 6 58.4 52.5 23.7 36.6 35.7 25.2 74.0 49.7 35.8 Macro 28.8 44.8 32.5 53.2 Overall, our baseline ASR model compares favorably with Whisper large especially in the acoustic robustness sub-track where the final macro-average DA-WER scores are very close. In the main track the difference is more pronounced, and Whis- per appears slightly more robust to segmentation errors, due to its significantly larger training data. By comparing sub and main columns in Table 3, we can observe that diarization has a big impact on the final recogni- tion score. We discovered that the permutation was always as- signed correctly in these results during DA-WER computation, yet, sub-optimal segmentation and mis-attributed sentences de- graded considerably the performance, especially on CHiME-6. On the other hand, we can see how both Whisper and the base- line system fail, even with oracle diarization, to reach satisfac- tory performance suitable for real-world applications, even on the arguably easier Mixer 6 scenario. 7. Conclusions In this paper, we introduced the CHiME-7 DASR challenge, which builds upon the past CHiME-6 and extends it to multi- ple scenarios which feature diverse array topologies, number of participants, acoustical conditions and linguistic differences, to test meeting transcription systems cross-domain generalization capability. Other novelties regard the use of a novel DA-WER metric which also accounts implicitly for diarization accuracy and the allowance for pre-trained \u201cfoundation\u201d models and pop- ular external datasets. Despite the efforts in developing a solid baseline system, the proposed challenge scenario still poses sig- nificant obstacles in achieving accurate transcriptions; this is also demonstrated by results with Whisper large and oracle di- arization. Our hope is that participants will devise novel and more effective ways to address this important problem. 8. Acknowledgements We would like to thank Marc Delcroix, Naoyuki Kamo, Christoph Boedekker and Thilo von Neumann for their help and feedback. S. Cornell was partially supported by Marche Region within the funded project \u201cMiracle\u201d POR MARCHE FESR 2014-2020. D. Raj was partially funded by NSF CCRI Grant No. 2120435, and a fellowship from Amazon via the JHU-Amazon Initiative for Interactive Artificial Intelligence (AI2AI). 9. References [1] S. Watanabe, M. Mandel et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d in CHiME Workshop, 2020. we are,\u201d in Findings of EMNLP, 2020. [3] Z. Chen, T. Yoshioka et al., \u201cContinuous speech separation: Dataset and analysis,\u201d in IEEE ICASSP, 2020. [4] D. Raj, P. Denisov et al., \u201cIntegration of speech separation, di- arization, and recognition for multi-speaker meetings: System de- scription, comparison, and analysis,\u201d in IEEE SLT, 2021. [5] S. Araki, N. Ono et al., \u201cMeeting recognition with asynchronous distributed microphone array using block-wise refinement of mask-based MVDR beamformer,\u201d in IEEE ICASSP, 2018. [6] T. Gburrek, J. Schmalenstroeer et al., \u201cInformed vs. blind beam- forming in AD-HOC acoustic sensor networks for"}