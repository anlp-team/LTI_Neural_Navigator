{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Which models are compared in the text?", "answer": " HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021)", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What task interference is discussed in the text?", "answer": " Specialized skill-specific specialization and its effects on task interference", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What inference strategy is compared in Figure 4?", "answer": " Chain-of-Skills vs retrieval-only", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What did the analyses show contributes to the observed gains?", "answer": " Both Chain-of-Skills inference and pre-training", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " How does the model with Chain-of-Skills inference compare to retrieval-only inference on NQ, EntityQuestions, and SQuAD?", "answer": " Chain-of-Skills inference yields further improvements, particularly on those unseen datasets", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What benefits does the multi-task pretraining demonstrate?", "answer": " Improved performance across all considered datasets, especially on out-of-domain datasets", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What happens when swapping experts for different inputs on HotpotQA?", "answer": " Significant decrease in performance, each specialized expert acquires distinct knowledge", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What is the main advantage of the COS model over other baselines?", "answer": " Configurable multi-skill model performing better on multiple types of ODQA tasks", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What is the configuration of the SPAR-wiki model?", "answer": " Ensemble of two dense models - one pretrained using BM25 supervision on Wikipedia and the other fine-tuned on NQ", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}, {"question": " What are the main findings regarding the benefits of Chain-of-Skills inference?", "answer": " Retrieval-only inference suffers large drops in performance, validating the effectiveness of the proposed model", "ref_chunk": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}], "doc_text": "and ensemble of dense, sparse and hyper- link retrieval HopRetriever (Li et al., 2021) and AISO (Zhu et al., 2021). The results on OTT-QA and HotpotQA are summarized in Table 3 and Ta- ble 4. It is easy to see that COS outperforms all the baselines here, again showing the advantage of our configurable multi-skill model over multiple types of ODQA tasks. Later, our analyses show that both Chain-of-Skills inference and pre- training contribute to the observed gains. Cross-data Results Given that both EntityQues- tions and SQuAD are single-hop, we use baselines on NQ with improved robustness for comparison. EntityQuestions Top-20 Top-100 Top-20 Top-100 SQuAD BM25 DPR-multi (Karpukhin et al., 2020) SPAR-wiki (Chen et al., 2021b) 70.8 56.6 73.6 79.2 70.1 81.5 71.1 52.0 73.0 81.8 67.7 83.6 COS 76.3 82.4 72.6 81.2 Table 5: Cross-dataset top-k accuracy on test sets. #Params Top-20 Top-100 Chain-of-Skills inference No Expert FFN Expert(naive) MHA Expert(naive) MHA Expert(COS) 111M 252M 182M 182M 90.2 91.3 92.0 92.0 92.4 93.4 94.0 94.2 Retrieval-only inference Multi-hop Retriever MHA Expert(naive) MHA Expert(COS) 110M 182M 182M 85.1 82.8 85.9 88.9 87.0 89.6 Table 6: Ablation results on HotpotQA dev using top- k retrieval accuracy. All models are initialized from BERT-base and trained on HotpotQA only. Particularly, SPAR-wiki is an ensemble of two dense models with one pretrained using BM25 su- pervision on Wikipedia and the other fine-tuned on NQ. BM25 is included here, as it is found to achieve better performance than its dense counter- part on those two datasets. The evaluation results are shown in Table 5. Overall, our model achieves the largest gains over BM25 on both datasets, indi- cating that our multi-task fine-tuned model with Chain-of-Skills inference is more robust than previous retrieval-only approaches. 5 Analysis 5.1 Task Interference We conduct ablation studies on HotpotQA to com- pare different ways of implementing skill-specific specialization (discussed in \u00a73.2) and their effects on task interference. As MHA experts are used for our model, we consider two variants for compari- son: 1) the no-expert model where all tasks share one encoder, and 2) the FFN expert model where specialized FFN sub-layers are used. Then we also compare the proposed expert configuration with a variant where the expanded query retrieval inputs share the same expert as single retrieval, denoted as the naive setting. The results are shown in the upper half of Table 6. Compared with the no-expert model, both FFN and MHA experts can effectively reduce task interference, wherein MHA expert is more effective overall. Our proposed expert config- Figure 4: Top-100 retrieval accuracy on inference strat- egy: Chain-of-Skills vs retrieval-only. Figure 5: Comparison on the effect of pretraining using top-100 retrieval accuracy with COS inference. uration can further help. 5.2 Benefit of Chain-of-Skills Inference Here we explore the benefits of the chained skill inference over the retrieval-only version. We ad- ditionally train a multi-hop retriever following Xiong et al. (2021b), and compare it with the two MHA expert models using the same two rounds of retrieval-only inference. The comparison is shown in the lower part of Table 6. As we can see, retrieval-only inference suffers large drops in performance. Although our proposed and naive MHA expert configurations have similar perfor- mance using Chain-of-Skills inference, the naive configuration model shows severe degrada- tion caused by task interference compared with the multi-hop retriever, validating the effective- ness of our proposed model. We further com- pare our Chain-of-Skills inference with the retrieval-only inference on NQ, EntityQuestions and SQuAD in Figure 4. It is easy to see that our pretraining can benefit the retrieval-only ver- sion. However, using better skill configurations via Chain-of-Skills inference yields further im- provements, particularly on those unseen datasets. 5.3 Effect of Pretraining To further demonstrate the benefit of our proposed multi-task pretraining, we fine-tune another multi- Query Doc Top-20 Top-100 Single query* Single query Single query Single query 0 4 2 2 1 1 1 3 96.1 90.1 91.8 87.4 98.2 95.2 95.9 92.7 Expanded query Expanded query* Expanded query Expanded query 0 4 2 2 1 1 1 3 94.2 95.3 74.5 67.3 97.0 97.4 85.8 79.6 Table 7: Results of feeding the inputs to different ex- perts, where the first two columns represent the query expert id and document expert id. * denotes the pro- posed setup task model following the same training protocol as COS but BERT model weights are used for ini- tialization. Both COS and the model without pre- training are then using the same skill configuration for inference. The results are illustrated in Fig- ure 5. Similar to the retrieval-only version (Fig- ure 4), we find that COS consistently outperforms the multi-task model without pretraining across all considered datasets using Chain-of-Skills inference. Again, the pretrained model is found to achieve improvements across the board, espe- cially on out-of-domain datasets, which validates the benefits of our multi-task pretraining. 5.4 Swapping Experts To understand if different experts in our model learned different specialized knowledge, we experi- ment with swapping experts for different inputs on HotpotQA. In particular, we feed the single query input and expanded query input to different query experts and then retrieve from either the context passage index or the entity description index. For single query input, we measure if the model can retrieve one of the positive passages. For expanded query input, we compute the recall for the other positive passage as done in (\u00a74.3). The results are shown in Table 7. Although both the single query expert and the expanded query expert learn to re- trieve evidence using the [CLS] token, swapping the expert for either of these input types leads to a significant decrease in performance. Also, switch- ing to the entity query expert and retrieving from the entity description index results in a large drop for both types of inputs. This implies that each specialized expert acquires distinct knowledge and cannot be substituted for one another. Dev EM F1 Test EM F1 HYBRIDER (Chen et al., 2020) FR+CBR(Chen et al., 2021a) CARP (Zhong et al., 2022) OTTer"}