{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Paaploss:_A_Phonetic-Aligned_Acoustic_Parameter_Loss_for_Speech_Enhancement_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some methods that have been previously used in speech enhancement?", "answer": " Generative adversarial networks (GAN), reinforcement learning, and convex approximations of metrics.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What was found in a study regarding current methods in capturing acoustic parameters?", "answer": " Current methods were found to fail in capturing the acoustic parameters, and explicit supervision of retaining them improved model outputs.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " How did previous methods attempt to use phonetic information in enhancing perceptual quality?", "answer": " Previous methods attempted to use phonetic information in enhancing perceptual quality by implicitly capturing phonetic information in wav2vec embeddings.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What dataset were two techniques evaluated on, and how does it compare to the dataset used in the experiments discussed?", "answer": " The techniques were evaluated on the Valentini dataset, which is much smaller and less varied than the dataset used in the experiments discussed.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What did the method proposed in this text aim to use to fine-tune speech enhancement models?", "answer": " The method proposed in this text aimed to use a phonetic-aligned acoustic parameter loss to fine-tune speech enhancement models.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " How does the proposed method allow for interpretability in the experiments section?", "answer": " The proposed method allows interpretability through both acoustic parameters and phonemes in the experiments section.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What components are described in Subsections 3.1, 3.2, and 3.3 of the method proposed?", "answer": " Subsection 3.1 describes temporal acoustic parameter estimation, Subsection 3.2 describes phonetic-alignment and weighting, and Subsection 3.3 describes the overall fine-tuning process with the proposed PAAP Loss.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What is the role of the acoustic-phonetic weights in the PAAP Loss?", "answer": " The acoustic-phonetic weights in the PAAP Loss weigh the acoustic parameters differently based on their importance in predicting phoneme logits.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What does the fine-tuning process involve according to the proposed method?", "answer": " The fine-tuning process involves predicting the phoneme index for each frame, using the argmax of predicted phoneme logits from clean audio, and performing calculations with the acoustic-phonetic weights.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}, {"question": " What evaluation results are presented in Table 1 regarding the use of the PAAP Loss compared with noisy audios?", "answer": " The evaluation results in Table 1 show improvements in various metrics such as PESQ, STOI, and WER when using the PAAP Loss compared with noisy audios.", "ref_chunk": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}], "doc_text": "works (GAN) [15], reinforcement learning [14], and convex approxi- mations of metrics [13]. However, as shown in [21], current methods fail to capture the aforementioned acoustic parameters, and explicit supervision of retaining them improved model outputs. Other methods have attempted to use phonetic information in enhancing perceptual quality, such as [16]. However, their loss func- tion did not explicitly use domain knowledge of phonemes and the phonetic information was only implicitly captured in wav2vec em- beddings. Recently, [26] performed a study of phonetic-aware tech- niques for speech enhancement but relies on uninterpretable Hu- BERT features [27]. Both techniques are evaluated on the Valentini dataset, which is much smaller and less varied than in our experi- ments. Moreover, our method allows interpretability through both acoustic parameters and phonemes, as illustrated in the experiments section. Lastly, [21] also used the acoustic parameters for optimiza- tion of perceptual quality. However, it did not factor in temporal or phonetic information. As these acoustic parameters vary greatly over an utterance, and between phonemes, modeling this phoneme and temporal dependencies can be helpful for improved performance. 3. METHOD We propose to use a phonetic-aligned acoustic parameter loss to \ufb01ne- tune SE models. Note that this objective function can be applied to any architecture, and even any task that involves speech outputs. In this section we describe the use in SE as a concrete example. However it only requires a waveform as input, and it is end-to-end differentiable, so the PAAP Loss can be applied to any model that produces waveform. The overall learning paradigm is summarized in Algorithm 1. We will present the temporal acoustic parameter estimation in Sub- section 3.1, the phonetic-alignment and weighting in Subsection 3.2, and the overall \ufb01ne-tuning process with the proposed PAAP Loss in Subsection 3.3. 3.1. Temporal Acoustic Parameter Estimation First, we take the pre-trained SE model as our seed model \u03a6, and pass in the noisy audio XN to obtain the enhanced waveform XE (line 3). On top of the seed models, we use a pre-trained estimator network \u03a8 to predict the acoustic parameters given a raw waveform. The acoustic parameters include a set of 25 low-level descriptors, covering prosodic, excitation, vocal tract, and spectral descriptors that are found to be the most expressive of the acoustic characteris- tics as standardized feature set. Unlike prior work which models these acoustic parameters at the utterance level through summary statistics, we incorporate the tem- poral feature of these acoustic parameters in the modeling. We pass the enhanced and clean waveforms to the model to predict temporal acoustic parameter matrices, DE and DC respectively (lines 4-5). The estimator network \ufb01rst performs short-time Fourier Transform (STFT) on the raw waveform, and then passes the spectrogram to a Algorithm 1: Overall work\ufb02ow of applying PAAP Loss in one iteration of our SE paradigm. 1 Input: Noisy waveform XN , clean waveform XC , seed model \u03a6, pre-trained acoustic low-level descriptor estimator \u03a8, estimated acoustic-phonetic weights w. 2 Output: calculated PAAP Loss (cid:96)PAAP 3 XE \u2190 \u03a6(XN ) ; // Enhanced waveform from current model 4 DC \u2190 \u03a8(XC ) ; // Estimated clean acoustic parameters 5 DE \u2190 \u03a8(XE) ; // Estimated enhanced acoustic parameters 6 (cid:96)PAAP \u2190 0 7 N \u2190 len(XC ) ; 8 for i \u2190 1 to N do 9 // the total number of frames j \u2190 Index of phoneme at XC i i )2 \u00b7 wj i \u2212 DC (cid:96)PAAP \u2190 (cid:96)PAAP + (DE N \u00b7 (cid:96)PAAP 10 11 (cid:96)PAAP \u2190 1 12 return (cid:96)PAAP sequential neural network to obtain the predicted temporal acoustic parameters. We note that using the estimated clean acoustic parame- ters in PAAP Loss rather than ground-truth allows much greater ease of use by other researchers, as they do not have to synthesize labels from another toolkit, as in [21]. This bene\ufb01ts us by making our loss more accessible in an arbitrary SE network. 3.2. Phonetic Alignment The next component of the PAAP Loss is the set of acoustic-phonetic weights w, as we would like to weigh the acoustic parameters dif- ferently based on their importance to predict phoneme logits. These acoustic-phonetic weights are estimated using clean speech, through linear regression between the acoustic parameters and their corre- sponding segmented phoneme logits: w = ((DC )(cid:62)DC ))\u22121((DC )(cid:62)PC ) (1) where PC indicates the phoneme logits of the clean waveform. Each column wi is the vector of weights from the 25 acoustic param- eters to phoneme i, plus a bias term. Each weight wij corresponds to how much a unit change in acoustic parameter i changes the log- probability of phoneme j. The weights re\ufb02ect how much information each feature contains about each phoneme, so we can use it to emphasize optimization on differences between clean and enhanced parameter values that are more signi\ufb01cant for the current phoneme. We obtain PC using an unsupervised phonetic aligner with a vo- cabulary of 40 phonemes, and one index for silence. We retain the silence index as we expect the relationship between acoustic param- eters and phonemes will be different over non-speech regions of the utterance, and we would like to include this in the modeling. The unsupervised phonetic aligner allows \ufb02exibility to apply our method on datasets without ground-truth transcriptions. 3.3. Fine-tuning with PAAP Loss During \ufb01ne-tuning, we \ufb01rst predict the phoneme index j for each frame across time, using the argmax of predicted phoneme logits from clean audio. We will then use wj, the acoustic-phonetic weight for phoneme j. We calculate the squared difference between the clean and enhanced acoustic parameters at the current time step, and then perform dot-product with wj (line 8-10). Note that these FullSubNet Demucs Metrics Noisy Baseline PAAP Loss Baseline PAAP Loss PESQ (\u2191) STOI (\u2191) DNSMOS (\u2191) NORESQA (\u2191) WER (\u2193) 1.58 91.52 2.48 2.92 19.0 2.89 96.41 3.21 4.08 12.6 3.00 96.70 3.27 4.13 12.1 2.65 96.54 3.31 3.93 15.0 2.99 97.12 3.34 3.99 13.2 Table 1: Evaluation results of using the PAAP Loss compared with noisy audios"}