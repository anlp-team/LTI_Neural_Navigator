{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the results in Fig. 10 indicate about model size and performance on the zero-shot contextual image retrieval task for Visual Storytelling and Visual Dialog?,answer: The results indicate that performance generally improves with increasing model size.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " Which text-only models are mentioned as potential models that could benefit the framework in the future?,answer: GPT-3, Chinchilla, and PaLM are mentioned as potential models.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What is the main ability of FROMAGe in text generation results?,answer: FROMAGe is able to learn in-context to produce stories rather than caption outputs.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " How did the model perform on the zero-shot VQAv2 evaluation in comparison to prior methods?,answer: The model achieved a score of 28.51, which was better or comparable to prior methods.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What are some limitations and unintended behaviors associated with existing large language models and generative models?,answer: They can make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " How was the FROMAGe model evaluated by human raters?,answer: The model was evaluated by human raters located in the US and Canada through Amazon Mechanical Turk.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What is one core component of the FROMAGe model?,answer: A core component is the frozen LLM backbone used for producing text and visual outputs.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What are some of the problems inherited by the FROMAGe model from text-only LLMs?,answer: Repetitive outputs, not following user instructions, and other common failure modes.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What are some ways to address the limitations and concerns related to text generation for the FROMAGe model according to the text?,answer: Using finetuned language models with human feedback or instruction finetuned models may be one direction.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}, {"question": " What are some potential benefits and risks associated with the image retrieval process used by the FROMAGe model?,answer: Benefits include controlling output results, while risks include existing biases found in training and retrieval datasets.", "ref_chunk": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}], "doc_text": "6.7B). Our results, as shown in Fig. 10, indicate that perfor- mance generally improves with increasing model size on the zero-shot contextual image retrieval task for Visual Sto- rytelling (Huang et al., 2016) and Visual Dialog (Das et al., 2017). This promising trend suggests that our framework is likely to benefit from even larger text-only models such as GPT-3 (175B) (Brown et al., 2020), Chinchilla (70B) (Hoff- 15 Grounding Language Models to Images for Multimodal Inputs and Outputs mann et al., 2022), or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results results show the ability of FROMAGe to learn in-context to produce stories rather than caption outputs. FROMAGe is able to learn in-context and condition on both the input im- ages and text descriptions to produce coherent story outputs which are aligned to a corresponding image (side-by-side qualitative examples in Fig. 13). In addition to the above evaluations, we also ran the zero- shot VQAv2 (Goyal et al., 2017). We apply the same nor- malization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reim- plementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This al- lows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days). We ran evaluations on Amazon Mechanical Turk with hu- man raters located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD / hr. We spent a total of approximately 140 USD to collect our evaluations. D. Current Limitations and Broader Impacts Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). C. Human Evaluation Procedure As detailed in Sec. 5.2 of the main paper, we perform human evaluations of 500 randomly selected examples to determine generated story quality given different input contexts. Users are tasked to evaluate three settings: 1. Generated outputs conditioned on the last image only. 2. Generated outputs conditioned on the preceding text descriptions only. 3. Generated outputs conditioned on the preceding im- ages and text descriptions. The results (described in Sec. 5.2 of the main paper), show that setting #3, which contains the full multimodal context, produces text that is rated as more coherent than both set- tings #1 and #2, which contain less context (which is of a single modality). We present individual ratings in Fig. 12, which summarize the head-to-head comparisons for com- paring one model against another. We observe that #3 produces descriptions that are rated as more relevant to the image than #2, but less relevant than the single image case #1. We attribute this to #1 generating more factual, caption-like outputs (as only a single image is provided as input), while #3 generates more story-like out- puts (and hence are more coherent overall). Overall, these A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as gener- ating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader is- sues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Cap- tions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to pro- duce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images 6https://github.com/GT-Vision-Lab/VQA 16 Grounding Language Models to Images for Multimodal Inputs and Outputs Figure 11. User interface shown to for human raters for performing evaluations. Raters are tasked to compare model outputs with different contexts as inputs, and rate (1) whether they form more coherent stories and (2) are more relevant to the last image. Model outputs are anonymized and shuffled. 4 captions 1 image About the sameGeneration result preference (A"}