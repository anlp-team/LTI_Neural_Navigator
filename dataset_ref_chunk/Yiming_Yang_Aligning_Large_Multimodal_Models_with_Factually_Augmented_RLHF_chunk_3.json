{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Aligning_Large_Multimodal_Models_with_Factually_Augmented_RLHF_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of high-quality instruction tuning data in aligning Large Language Models (LLMs)?", "answer": " High-quality instruction tuning data is essential for aligning LLMs as they traverse vast textual and visual domains, ensuring models produce contextually relevant outputs.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " How did LLaVA synthesize visual instruction data?", "answer": " LLaVA synthesized visual instruction data by representing images as associated captions on bounding boxes to prompt the GPT-4 model.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What was the result of enhancing LLaVA with high-quality instruction-tuning data?", "answer": " Enhancing LLaVA with high-quality instruction-tuning data significantly improved LMM capabilities on benchmark tests, surpassing models trained on datasets larger than theirs.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " Why is it important to differentiate between responses that are less helpful and those that contain multimodal hallucinations?", "answer": " Differentiating between less helpful responses and those containing hallucinations helps in providing clearer guidance for annotation and identification of potential hallucinations in model responses.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " How did the study collect human preferences on LLaVA data?", "answer": " The study collected human preferences on LLaVA data by re-sampling the last response with the SFT model and a temperature of 0.7.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What is the purpose of Factually Augmented RLHF (Fact-RLHF)?", "answer": " Fact-RLHF aims to augment the reward model by providing additional ground-truth information such as image captions to calibrate judgment, preventing policy model hacking.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What information is provided to the reward model in Factually Augmented RLHF?", "answer": " In Fact-RLHF, the reward model is provided with additional factual information about the textual descriptions of the image along with the user prompt and response.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What are Symbolic Rewards in the Fact-RLHF algorithm?", "answer": " Symbolic Rewards in Fact-RLHF penalize selections that diverge from predetermined ground-truth options, such as binary choices and multiple-choice options.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What observation was made about RLHF-trained models producing more verbose outputs?", "answer": " RLHF-trained models often produce more verbose outputs, a phenomenon noted by Dubois et al. (2023), which may be favored by users or automated evaluation systems.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}, {"question": " What is the hyper-parameter beta used for in the full optimization loss equation?", "answer": " The hyper-parameter beta is used to control the scale of the KL penalty in the full optimization loss equation.", "ref_chunk": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}], "doc_text": "derived from the initial policy model (Ouyang et al., 2022) is sometimes applied. Formally, given the set of collected images and user prompts, DRL = {(I, x)}, along with the fixed initial policy model \u03c0INIT and the RL-optimized model \u03c0RL \u03d5 , the full optimization loss is articulated as: (cid:0)\u03c0RL \u03d5 (y|I, x)\u2225\u03c0INIT(y|I, x)(cid:1)(cid:3) , (cid:2)r\u03b8(I, x, y) \u2212 \u03b2 \u00b7 DKL L(\u03c0RL \u03d5 ) = \u2212E(I,x)\u2208DRL,y\u223c\u03c0RL(y|I,x) where \u03b2 is the hyper-parameter to control the scale of the KL penalty. 4 (1) (2) Preprint 2.2 AUGMENTING LLAVA WITH HIGH-QUALITY INSTRUCTION-TUNING Recent studies (Zhou et al., 2023; Touvron et al., 2023b) show that high-quality instruction tuning data is essential for aligning Large Language Models (LLMs). We find this becomes even more salient for LMMs. As these models traverse vast textual and visual domains, clear tuning instructions are crucial. Correctly aligned data ensures models produce contextually relevant outputs, effectively bridging language and visual gaps. For example, LLaVA synthesized 150k visual instruction data using the text-only GPT-4, where an image is represented as the associated captions on bounding boxes to prompt GPT-4. Though careful filtering has been applied to improve the quality, the pipeline can occasionally generate visually misaligned instruction data that can not be easily removed with an automatic filtering script, as highlighted in Table 1. In this work, we consider enhancing LLaVA (98k conversations, after holding out 60k conversa- tions for preference modeling and RL training) with high-quality instruction-tuning data derived from existing human annotations. Specifically, we curated three categories of visual instruction data: \u201cYes\u201d or \u201cNo\u201d queries from VQA-v2 (83k) (Goyal et al., 2017b), multiple-choice questions from A-OKVQA (16k) (Marino et al., 2019), and grounded captions from Flickr30k (23k) (Young et al., 2014a). Our analysis revealed that this amalgamation of datasets significantly improved LMM capabilities on benchmark tests. Impressively, these results surpassed models (Dai et al., 2023; Li et al., 2023a; Laurenc\u00b8on et al., 2023) trained on datasets an order of magnitude larger than ours, as evidenced by Table 7 and 4. For a comprehensive breakdown of each dataset\u2019s influence, refer to Section 3.5. 2.3 HALLUCINATION-AWARE HUMAN PREFERENCE COLLECTION Inspired by the recent RLHF studies that collect helpfulness and harmlessness preferences (Bai et al., 2022b; Touvron et al., 2023b) separately, in this study, we decide to differentiate between responses that are merely less helpful and those that are inconsistent with the images (often characterized by multimodal hallucinations). To achieve this, we provide crowdworkers with the template illustrated in Table 2 to guide their annotations when comparing two given responses. With our current template design, we aim to prompt crowdworkers to identify potential hallucinations in the model\u2019s responses. Nonetheless, our training process integrates a single reward model that emphasizes both multimodal alignment and overall helpfulness2. We collect human preferences on 10k hold-out LLaVA data by re-sampling the last response with our SFT model and a temperature of 0.7. The reward model is initialized from the SFT model to obtain the basic multimodal capabilities. 2.4 FACTUALLY AUGMENTED RLHF (FACT-RLHF) We conduct multimodal RLHF on 50k hold-out LLaVA conversations, with additional 12k multi- choice questions from A-OKVQA and 10k yes/no questions subsampled from VQA-v2. Due to the concerns of existing hallucinations in the synthetic multi-round conversation data of LLaVA, we only use the first question in each conversation for RL training, which avoids the pre-existing hallucinations in the conversational context. Reward Hacking in RLHF In preliminary multimodal RLHF experiments, we observe that due to the intrinsic multimodal misalignment in the SFT model, the reward model is weak and sometimes cannot effectively detect hallucinations in the RL model\u2019s responses. In the text domain, previous work (Bai et al., 2022a; Touvron et al., 2023b) proposed to iteratively collect \u201cfresh\u201d human feed- back. However, this can be quite costly and cannot effectively utilize existing human-annotated data and there is no guarantee that more preference data can significantly improve the discriminative capabilities of the reward model for multimodal problems. Facutual Augmentation To augment the capability of the reward model, we propose Factually Augmented RLHF (Fact-RLHF), where the reward model has access to additional ground-truth 2We are considering the development of a distinct Honest reward model, inspired by the approach in Tou- vron et al. (2023b). This introduces the possibility of constructing a piecewise Honesty-prioritized reward model. We earmark this direction for future exploration. 5 Preprint information such as image captions to calibrate its judgment. In original RLHF (Stiennon et al., 2020; OpenAI, 2022), the reward model needs to judge the quality of the response only based on the user query (i.e., the input image and prompt): Image: [IMAGE] User: [USER PROMPT] Assistant: [RESPONSE] Reward Model: [SCORE] In Factually Augmented RLHF (Fact-RLHF), the reward model has additional information about the textual descriptions of the image: Image: [IMAGE] Factual Information: [5 COCO IMAGE CAPTIONS / 3 A-OKVQA RATIONALS] User: [USER PROMPT] Assistant: [RESPONSE] Augmented Reward Model: [SCORE] This prevents the reward model hacked by the policy model when the policy model generates some hallucinations that are clearly not grounded by the image captions. For general questions with COCO images, we concatenate the five COCO captions as the additional factual information, while for A-OKVQA questions, we use the annotated rationals as the factual information. The factually augmented reward model is trained on the same binary preference data as the vanilla reward model, except that the factual information is provided both during the model fine-tuning and inference. Symbolic Rewards: Correctness Penalty & Length Penalty In some of our RL data, certain questions come with a predetermined ground-truth answer. This includes binary choices (e.g., \u201cYes/No\u201d) in VQA-v2 and multiple-choice options (e.g., \u201cABCD\u201d) in A-OKVQA. These annota- tions can also be regarded as additional factual information. Therefore, in the Fact-RLHF algorithm, we further introduce a symbolic reward mechanism that penalizes selections that diverge from these ground-truth options. Furthermore, we observed that RLHF-trained models often produce more verbose outputs, a phe- nomenon also noted by Dubois et al. (2023). While these verbose outputs might be favored by users or by automated LLM-based evaluation systems"}