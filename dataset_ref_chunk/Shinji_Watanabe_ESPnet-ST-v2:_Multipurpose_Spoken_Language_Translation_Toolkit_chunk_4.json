{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ESPnet-ST-v2:_Multipurpose_Spoken_Language_Translation_Toolkit_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What models show the strongest performances in the text?", "answer": " The CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What model outperforms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation?", "answer": " The large MCA model outperforms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What does Table 6 compare?", "answer": " Table 6 compares Base and large time-sync CTC/attention (TBCA) models to top IWSLT 2022 systems for the medium latency regime.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What does the AL value represent in the context of evaluation?", "answer": " The AL value represent how much the system output lags behind the amount of input read.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What type of SSL representations are ablated in Table 8?", "answer": " Different types of SSL for the frontend and discrete unit portions of S2ST models are ablated in Table 8.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What limitations are mentioned in the text?", "answer": " The limitations mentioned in the text are data-related limitations and evaluation-related limitations.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What future updates may include for ESPnet-ST-v2?", "answer": " Future updates may include more new tasks, such as simultaneous speech-to-speech translation, and cross-toolkit integrations via TorchAudio.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What measurement is used to evaluate SST outputs?", "answer": " AL (Aligned Lag) which measures how much the system outputs lags behind the amount of input read.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " Which models slightly outperform their respective prior works according to the text?", "answer": " All of our models slightly outperform their respective prior works except for Translatotron 2.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}, {"question": " What grants supported Brian Yan and Shinji Watanabe according to the text?", "answer": " Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence.", "ref_chunk": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}], "doc_text": "- - - - 23.2 23.6 24.3 25.1 E MBR (A+B+C+D) \u2713 25.4 Table 4: Base and large CTC/attention (CA) and Multi- decoder CTC/attention (MCA) models compared to top IWSLT 2021 systems for the given segmentation tst2020 En-De test set. KD=Knowledge Distillation, BT=Back- Translation, Ens=Ensemble. \u2020Uses WMT MT data. MODEL BSZ BLEU\u2191/AL\u2193 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 40 40 40 40 22.8 / 3.23 24.4 / 3.23 24.6 / 2.34 22.9 / 2.37 Blockwise Attn Enc-Dec (BAED) Label-Sync Blockwise CTC/Attn (LBCA) Time-Sync Blockwise CTC/Attn (TBCA) Blockwise Transducer (BT) 20 20 20 20 21.0 / 2.77 22.9 / 2.77 22.8 / 1.63 20.9 / 1.71 Table 5: Example SST models \u2013 results on MuST-C-v2 En-De tst-COMMON. BSz=Block Size. 5.2 Results Toolkit Comparison Table 2 summarizes ESPnet-ST-v2 performance, showing one best example model (\u00a74) for each task. ESPnet-ST-v1, Fairseq, and NeurST models are also referenced for comparison. On ST/SST, ESPnet-ST-v2 is 4-7 BLEU higher with 4.5 sec lower AL.3 On S2ST ESPnet-ST-v2 is on par with Fairseq. ST Table 3 shows a variety of approaches, of which the CTC/attention and Multi-decoder CTC/attention (MCA) models show the strongest performances. In Table 4, we scale these two ap- proaches by training on larger corpora and increas- ing model capacity \u2013 our large MCA model outper- forms the best IWSLT 2021 offline track submission on the 2020 test set with given segmentation. SST Table 5 shows a variety of approaches, of which the blockwise Transducer (BT) and time- synchronous blockwise CTC/attention (TBCA) 3This comparison refers to the originally published results from the toolkit description papers. Note that subsequent works using these toolkits have improved the performance. MODEL SSL LLM KD BLEU\u2191 / AL\u2193 IWSLT\u201922 (Top 3 of 5) 1 CUNI-KIT E2E 2 UPV Cascade\u2020 3 FBK E2E\u2020 \u2713 - - \u2713 - - - \u2713 31.5 / 1.93 27.8 / 1.93 25.0 / 1.99 ESPnet-ST-v2 A Base TBCA B Large TBCA - - - 24.7 / 1.93 26.6 / 1.93 Table 6: Base and large time-sync CTC/attention (TBCA) models compared to top IWSLT 2022 sys- tems for the medium latency regime. Evaluated on En- De tst-COMMON-v2. SSL=Speech Self-Supervised Learning, LLM=Large Pre-trained Language Model, KD=Knowledge Distillation. \u2020Uses WMT MT data. MODEL TYPE ASR-BLEU\u2191 Prior Works 1 Translatotron (Jia et al., 2019) Spectral 2 Translatotron2 (Jia et al., 2022a) Spectral 4 Speech-to-Unit (Lee et al., 2022a) Discrete 5 UnitY (Inaguma et al., 2022) Discrete 14.4 30.3 30.8 32.3 ESPnet-ST-v2 A Attn Enc-Dec (Translatotron) B Multi-Decoder (Translatotron2) C Attn Enc-Dec (Speech-to-Unit) D Multi-Decoder (UnitY) Spectral Spectral Discrete Discrete 16.6 24.3 31.3 32.0 Table 7: Example S2ST models \u2013 results on CVSS-C Es-En test set. Prior works shown for comparison. models have the lowest AL. We choose to scale the TBCA to compare with IWSLT submissions due to its superior translation quality, but note that the BT has lower computational overhead due pri- marily to the lack of source-target computation; AL is non-computation aware. In Table 6, we fit the TBCA to the 2 second AL latency regime by selecting a blocksize of 32 and scale it with more data and model capacity \u2013 our large TBCA model would have ranked 3rd out of 6 amongst IWSLT 2022 submissions without using any SSL / LLM representations or knowledge distillation. S2ST Table 7 shows a variety of approaches com- pared to prior works with comparable architectures \u2013 our S2ST models are generally on par with prior works which are considered state-of-the-art. In fact, all of our models slightly outperform their respective prior works except for Translatotron 2. Further, in Table 8 we ablate a range of SSL types for both the frontend and discrete units demonstrat- ing the flexibility of our toolkit. FRONTEND DISCRETE UNIT ASR-BLEU\u2191 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 HuBERT HuBERT HuBERT HuBERT HuBERT 14.8 21.2 21.4 21.5 22.8 FBANK wav2vec2\u2020 HuBERT\u2020 mHuBERT WavLM\u2020 WavLM WavLM WavLM WavLM WavLM 15.0 21.6 22.1 22.0 23.1 Table 8: Ablation on different types of SSL for the frontend and discrete unit portions of S2ST models. \u2020Trained with large settings, others with base settings. 6 Conclusion We presented ESPnet-ST-v2 which now supports offline speech translation, simultaneous speech translation, and offline speech-to-speech transla- tion. ESPnet-ST-v2 will continue to grow to sup- port the community\u2019s interests. Future updates may include more new tasks, such as simultane- ous speech-to-speech translation, and cross-toolkit integrations via TorchAudio. Limitations The first set of limitations to be aware of are data- related. Although prior works have shown the fea- sibility of building E2E systems without source lan- guage transcriptions (Lee et al., 2022b; Chen et al., 2022; Zhang et al., 2021), in this work we only investigate cases where triplet data (source speech, source transcript, target translation) is available for ST/SST and where quadruplet data (source speech, source transcript, target translation, target speech) is available for S2ST. The second set of limitations to be aware of are evaluation-related. For SST, we follow prior works (Ma et al., 2020a; Wang et al., 2020; Anas- tasopoulos et al., 2022) and evaluate AL which is a measure of how much the system outputs lags behind the amount of input read. Notably, this does not consider the actual computation time and only the input-to-output ratio. For S2ST, we follow prior works (Jia et al., 2022a; Inaguma et al., 2022) and evaluate ASR-BLEU. This evaluation is depen- dent on an ASR system, which is not standardized across prior works. And further, our evaluation of S2ST outputs does not include naturalness. Finally, in this work we have not conducted any human evaluation of translation outputs. Acknowledgements Brian Yan and Shinji Watanabe are supported by the Human Language Technology Center of Excellence. This work also used the Extreme Science and Engineering Discovery Environment (XSEDE) (Towns et al., 2014), which is supported by National Science Foundation grant number ACI- 1548562; specifically, the Bridges system (Nys- trom et al., 2015), as part of project cis210027p, which is supported by NSF award number ACI- 1445606, at the Pittsburgh Supercomputing"}