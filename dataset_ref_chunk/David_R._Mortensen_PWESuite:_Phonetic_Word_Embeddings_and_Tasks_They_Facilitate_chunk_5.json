{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_PWESuite:_Phonetic_Word_Embeddings_and_Tasks_They_Facilitate_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the procedure described in the text involve?,answer: Selecting two perturbations of the same phonetic feature and replacing certain phonemes in a word with those perturbations.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " How many times is the above procedure applied to create analogous quadruplets?,answer: 1 or 2 times.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What is the main focus of the analogy task in the text?,answer: Sound analogies at the word level.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What is the overall score based on in the text?,answer: The performance of various embedding models using an evaluation suite.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " According to the text, which model is considered the best overall in the comparison?,answer: The Triplet Margin model.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What is the GPU budget for all included experiments, as mentioned in the text?,answer: 100 hours on GTX 1080 Ti.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What is the purpose of including non-phonetic word embeddings like fastText and BERT in the comparison?,answer: To show that they are different from phonetic word embeddings and are not suited for certain tasks.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " How do the lexical/semantic word embeddings perform compared to explicit phonetic embeddings, as stated in the text?,answer: They perform worse, especially on human similarity and analogies.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What impact do input features have on the performance of the models mentioned in Table 2?,answer: Input features play a major role, with more phonetic features leading to better model performance.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}, {"question": " What feature types are used for phonetic word embedding models as mentioned in the text?,answer: Orthographic characters, IPA characters, and articulatory feature vectors.", "ref_chunk": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}], "doc_text": "word \uebf71 \u2208 W and one of its phonemes on random position \uebe9: p1 = \uebf71,\uebe9. 2. Randomly select two perturbations of the same phonetic feature so that p1 : p2 \u2194 p3 : p4, for example /t/ : /d/ \u2194 /s/ : /z/. 3. Create \uebf72, \uebf73, and \uebf74 by duplicating \uebf71 and replacing \uebf71,\uebe9 with p2, p3, and p4. The new words \uebf72, \uebf73, and \uebf74 do not have to be a real word in the language but we are still interested in analogies in the space of all possible words and their detection. This is possible only for open embeddings. We apply the above procedure 1 or 2 times to create 200 analogous quadruplets with 1 or 2 per- turbations (evenly split). We then measure the Acc@1 to retrieve \uebf74 from W \u222a {\uebf74}. We simply measure how many often the closest neighbour of \uebf72 \u2212 \uebf71 + \uebf73 is \uebf74. Our analogy task is dif- ferent from that of Parrish (2017) who focused on morphological derivation2 and that of Silfverberg et al. (2018), which show that phoneme embed- dings learned via the word2vec objective demon- strate sound analogies at the phoneme level. We consider sound analogies at the word level. 4.3. Overall Score 5. Evaluation We now compare all the aforementioned embed- ding models using our evaluation suite. We show the results in Table 1 with three categories of mod- els. Our models trained using some articulatory features or distance supervision (Section 3) are given first, followed by other phonetic word em- bedding models (Section 2). We also include non- phonetic word embeddings, not as a fair baseline for comparison but to show that these embeddings are different from phonetic word embeddings and are not suited for our tasks: fastText (Grave et al., 2018), BPEmb (Heinzerling and Strube, 2018), BERT (Devlin et al., 2019) and INSTRUCTOR (Su et al., 2022). We chose these embeddings be- cause they are open (i.e., they provide embeddings even to words unseen in the training data). All of these embeddings except for BERT and INSTRUC- TOR are 300-dimensional (see Section 5.5). 5.1. Model Comparison In Table 1 we show the performance of all previ- ously described models. The Triplet Margin model is best overall, outperforming Metric Learner, de- spite its less direct supervision in training. How- ever, it also requires the longest time to train.3 Since all the measured metrics are bounded be- tween 0 and 1, we can define the overall score 2Example decide : decision \u2194 explode : explosion. 3The overall GPU budget for all included experiments is 100 hours on GTX 1080 Ti. We include reproducibility details in the code repository. Human Sim. Art. Dist. Analogies Retrieval Art. Dist. Rhyme Cognate0.010.070.580.540.440.330.620.590.090.180.700.840.750.760.470.570.070.360.790.770.840.820.310.500.650.58-0.030.140.230.47 Rhyme Analogies Retrieval Figure 2: Spearman (upper left) and Pearson (lower right) correlations between performance on suite tasks. All models from Table 1 are used. Surprisingly, the best model for human similarity is a simple count-based model. Semantic word embeddings perform worse than explicit phonetic embeddings, most notably on human similarity and analogies. However, they do perform reasonably on cognate detection. We now examine how much the performance on one task (particularly an intrinsic one) is predic- tive of performance on another task. We measure this across all systems in Table 1 and revisit this topic later for creating variations of the same model. For lexical/semantic word embeddings, Bakarov (2018) notes that the individual tasks do not cor- In Figure 2, we find relate among each other. the contrary for some of the tasks (e.g., retrieval- rhyme or retrieval-analogies). Importantly, there is no strong negative correlation between any tasks, suggesting that performance on one task is not a tradeoff with another. Model Art. IPA Text Metric Learner Triplet Margin Autoencoder Count-based 0.78 0.84 0.50 - 0.64 0.84 0.41 0.56 0.62 0.79 0.41 0.51 Table 2: Overall performance of models with vari- ous input features. Art. = articulatory features. 5.2. Input Features For all of our models, it is possible to choose the input feature type, which has an impact on the performance, as shown in Table 2. Unsurprisingly, the more phonetic the features are, the better the resulting model is. In the Metric Learner and Triplet Margin models we are still using supervision from the articulatory distance, and despite that, the input features play a major role. FR FR DETrain language.80.79.78.79.78.79.79.79.78.76.77.76.76.75.76.76.77.76.78.78.78.78.77.78.77.78.77.74.74.74.74.74.74.74.74.74.73.73.73.73.72.73.73.73.73.76.76.76.76.75.76.76.76.75.77.77.77.77.76.77.77.77.77.79.79.79.78.78.79.78.79.78.80.80.80.79.79.80.79.80.80 PL SW PL SW UZ ES UZ BN BN DEEval language ES EN AM EN AM Figure 3: Suite score of Metric Learner with ar- ticulatory features trained on one language and evaluated on another one. Diagonal shows models trained and evaluated on the same language. 5.3. Transfer Between Languages Recall from Section 3.3 that there are multiple fea- ture types that can be used for our phonetic word embedding model: orthographic characters, IPA characters and articulatory feature vectors. It is not surprising that the characters as features provide little transferability when the model is trained on a different language than it is evaluated on. The transfer between languages for a different model type, shown in Figure 3, demonstrates that not all languages are equally challenging (e.g. Polish is more challenging than German). Furthermore, the articulatory features appear to be very useful for generalizing across languages. This echoes the findings of Li et al. (2021), who also break down phones into articulatory features to share informa- tion across, possibly unseen, phones. 5.4. Embedding Topology Visualization The differences between feature types in Table 2 may not appear very large. Closer inspection of the clusters in the embedding space in Figure 4 reveals, that using the articulatory feature vectors or IPA features yields a vector space which resem- bles one induced by the articulatory distance the most. This is in line with A (articulatory distance, Section 3.3.1) being calculated using articulatory features and is used for the model supervision. 5.5. Dimensionality and Train Data Size So far we used 300-dimensional embeddings. This choice was motivated solely by the comparison to other word embeddings. Now we examine how the"}