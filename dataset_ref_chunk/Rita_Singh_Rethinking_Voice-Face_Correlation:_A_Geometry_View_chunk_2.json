{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Rethinking_Voice-Face_Correlation:_A_Geometry_View_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the phonatory structures responsible for generating voice in human beings?,        answer: The phonatory structures responsible for generating voice in human beings are the vocal tract and vocal cords.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " How are vowels produced in terms of vocal cord vibrations and the vocal tract?,        answer: When producing vowels, the vocal cords vibrate with no obstruction in the vocal tract.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What does linear predictive coding (LPC) model in relation to phonation?,        answer: Linear predictive coding (LPC) models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What is the purpose of introducing the voice-anthropometric measurement (AM)-face paradigm?,        answer: The voice-anthropometric measurement (AM)-face paradigm is introduced to understand the relationship between voice and face geometry by utilizing anthropometric measurements.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " How is voice representation learning for face geometry facilitated?,        answer: Voice representation learning for face geometry is facilitated by utilizing a phonatory module, similar to the auto-regressive impulse-by-filter model used in LPC.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What is the main focus of the paper discussed in the text?,        answer: The main focus of the paper discussed in the text is understanding the relationship between 3D facial shape and voice from a scientific angle.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " How does voice production relate to 3D facial shapes according to the text?,        answer: The text suggests that voice production is highly related to 3D facial shapes.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What is voice-guided face synthesis aiming to achieve?,        answer: Voice-guided face synthesis aims to generate coherent and natural lip movements by using speech inputs.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What are some of the personality traits that can be recognized using human voices?,        answer: Some of the personality traits that can be recognized using human voices include speaker identity, gender, age, and emotion status.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}, {"question": " What sets the work discussed in the text apart from existing works related to voice-face correlation?,        answer: The work discussed in the text investigates the voice-face correlation from a geometry view by studying holistic facial structures rather than focusing on semantic correlations.    ", "ref_chunk": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}], "doc_text": "us go back and understand how voice is generated by human beings. voice is produced by phonatory structures (Fig. 1 (a)), e.g., vocal tract and vocal cords. Specifically, when producing vowels, the vocal cords vibrate with no obstruction in the vocal tract. In contrast, for most consonants, the phonation purely depends on the vocal tract resonance with a pulmonic airflow. The vocal tract can be assumed as a filter that makes the phonemes versatile and personalized. With the phonation mechanism of human beings, as shown in Fig. 1 (b), Markel et al introduces linear predictive coding (LPC) [25] which models phonation as a unit impulse signal modified by a stack of tubes (vocal tract) and encodes personalized voice by vocal tract coefficients. The LPC yields a good physical model of the vocal tract with only voice inputs in an unsupervised manner. As the mouth and nose serve as the most important parts of the vocal tract, we hypothesize that their geometry should be encoded in the voice. With the tight bind of muscles and skeletons, other parts of face geometry may also be represented by voice. Though voice and face geometry should have some correlations, we have no idea about which part of the face voice can represent. Constructing uncorrelated relations will lead to random results and raise the model instability. To tackle this problem, we introduce the voice-anthropometric measurement (AM)-face paradigm. Pre- vious studies have shown that anthropometric measurements like the dimensions of nasal cavities [42] or cranium [48, 49] directly influence the speaker\u2019s voices. In our voice-AM-face paradigm, we first summarize a set of AMs from anthropometry literature [10, 11, 32, 38, 55], then identify predictable AMs and use them to guide the 3D face reconstruction by conducting AM-guided opti- mization. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. In addition, the analysis of AMs also brings a new view to understanding voice-face correlation in a fine-grained fashion. Inspired by LPC which learns the shape of the vocal tract by pro- ducing voice, we utilize a phonatory module to facilitate voice repre- sentation learning for face geometry. Similar to the auto-regressive impulse-by-filter model used in LPC, recently introduced denoising diffusion probabilistic models [18] share a similar structure, which samples a random noise with auto-regressive updating to form the final result. Based on the structure similarity, we choose the diffusion model as our phonatory module. Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 With the predicted AMs, we reconstruct the facial shapes by an optimization-based method, which first projects the 3D facial shapes into a low-dimensional linear space [4]. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. Though this paper mainly focuses on understanding the relationship between the 3D facial shape and voice from a scientific angle, this technique has its potential applications. For example, the identity-fidelity facial shape can be used for criminal profiling scenarios, such as hoax calls and voice-based phishing. In this paper, we try to answer two core questions - (1) Is there a correlation between face geometry and voice? (2) If so, which part of the face can be represented by the voice? To fulfill our target, we collect a large-scale dataset containing ground-truth 3D face scans and corresponding voice recordings from 1026 speaker identities. A voice-AM-face paradigm equipped with a phonatory module is proposed for analyzing the voice-face correlation. Our contributions can be summarized as follows. We propose a voice-AM-face paradigm and a corresponding voice-face dataset for tractable 3D face deduction from voice. \u2022 We investigate voice-face correlation in a fine-grained man- ner by statistically verifying which part of the face can be reflected by the voice. The results can serve as a good refer- ence to support future voice-face research, such as voice-face verification. We leverage voice production as a proxy task to learn face geometry representation and verify that voice production is highly related to 3D facial shapes. 2 RELATED WORKS 2.1 Voice-face Matching and Voice-guided Face Synthesis The human voice contains rich information that can be used to recognize personality traits, such as speaker identity [6, 24, 33], gender [22], age [15, 30, 39], and emotion status [43, 51]. Voices can also be used for monitoring health conditions [1] and other medical applications [17]. Most existing works in this area focus on predicting personality traits that are intuitively related to voice. Such personality traits may have essential correlations between the human voice and their faces [45]. Cross-modal voice-face matching [28, 44, 53] and cross-modal verification [27, 37, 41] are tasks where voices are used as queries to retrieve faces or vice versa, which have received increasing attention in recent years. Voice-guided face synthesis is another related task, which aims to generate coherent and natural lip movements, and includes methods that drive template images [16, 19, 54] or template face meshes [9] to talk by speech inputs, or replace lip movements in a video with movements inferred from another video or speech [8, 46]. Unlike the existing work in related fields that are more focused on semantic correlations between voice and face, our work investigates the voice-face correlation from a geometry view by studying holistic facial structures. There has been recent work that seeks to under- stand the correlations between voice and facial geometry by first recovering 2D faces from voice and then reconstructing 3D faces from the 2D representations [47]. However, during this process, it is still inevitable that the semantic correlations are encoded in the 2D Rethinking Voice-Face Correlation: A Geometry View face and affect the 2D-to-3D face reconstruction. Instead, we aim to model our voice-face correlation from a pure geometry view without the influence of any semantics. 2.2 Phonation and Anthropometry The human voice is generated by phonatory structures, and the phonation of different phonemes may be dependent on different physiological structures. For example, the phonation of consonants includes"}