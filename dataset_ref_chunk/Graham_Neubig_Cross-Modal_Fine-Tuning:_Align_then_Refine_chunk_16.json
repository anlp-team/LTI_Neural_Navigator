{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What data is used in the experiments on Drug Response Prediction?", "answer": " The gene expression data and the drug descriptors.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " How are the gene expression data and drug descriptors normalized in the experiments?", "answer": " They are normalized using min-max normalization.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What is the loss function used during training in the experiments?", "answer": " Mean Squared Error (MSE).", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " How is the prediction performance measured in the experiments?", "answer": " By the coefficient of determination (R2).", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What is the number of features for each treatment sample in the CTRP dataset?", "answer": " 3901.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What is the number of features for each treatment sample in the GDSC dataset?", "answer": " 3739.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What specialized algorithm is compared with ORCA in Table 5?", "answer": " The domain-specific IGTD algorithm.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What type of tasks are DomainNet datasets commonly used to evaluate?", "answer": " Homogeneous domain adaptation (DA) tasks.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What dataset splits are used in the experiments in A.8.2?", "answer": " The splits from Tan et al. (2020) removing mislabeled outliers.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}, {"question": " What is the main difference between fine-tuning and prompting with large-scale pretrained models?", "answer": " Prompting does not update the pretrained weights but modifies the input to query the model.", "ref_chunk": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}], "doc_text": "Integral features Meaningful feature Names 1 symbolic feature 1 symbolic feature 1 symbolic feature - - 1 symbolic feature 1 symbolic feature 1 symbolic feature A.7. Experiments on Drug Response Prediction We adapt the code from the of\ufb01cial GitHub repo of IGTD and download, preprocess, and load the CTRP & GDSC data following the procedures described in the paper\u2019s supplementary material. Notably, both the gene expression data and the drug descriptors are normalized using min-max normalization so that each gene/drug feature has a maximum value of 1 and a minimum value of 0. We then concatenate the features for each gene-drug (treatment) pair. The number of features (cols) for each treatment sample (row) is 3901 for CTRP and 3739 for GDSC. The processed data are stored locally for the ease of data loading. During training, we use the MSE as the loss function since we are in a regression setting. The prediction performance is measured by the coef\ufb01cient of determination (R2). Table 5, we show the results for ORCA and the baselines, which include the domain-speci\ufb01c IGTD algorithm that transforms gene expression pro\ufb01les and drug molecular descriptors into their respective images. We can see that even compared with such highly specialized algorithms, the domain-agnostic ORCA still performs quite well, showing the capacity of cross-modal transfer learning with large-scale pretrained models. A.8. Additional Experiments A.8.1. COMPATIBILITY WITH IN-MODALITY TRANSFER A natural question to ask is whether ORCA can also tackle in-modality tasks. While we design ORCA to enable cross- modal transfer, we hypothesize that it should facilitate same-modality transfer if two domains have large dataset distance. To validate this, we test ORCA on DomainNet datasets, which are commonly used to evaluate homoge- neous DA methods (Peng et al., 2019). From Table 23, we can see that ORCA achieves signi\ufb01cantly better per- formance than the \ufb01ne-tuning baseline, which shows that the feature matching of ORCA can also help in-domain generalization. Table 23: We use the dataset splits in Tan et al. (2020), which removed some mislabeled outliers, and report the prediction errors (\u2193) for ORCA and \ufb01ne-tuning (using Swin-base). Real Painting Sketch Clipart ORCA Fine-tuning 96.71\u00b10.02 93.33\u00b11.33 94.71\u00b10.13 75.79\u00b10.86 94.93\u00b10.24 83.00\u00b10.13 93.61\u00b10.54 86.01\u00b12.62 A.8.2. PROMPTING Apart from \ufb01ne-tuning, a new paradigm of working with large-scale pretrained models is prompting, i.e., we do not update the pretrained weights but only modify the input and query the model for the desired output. Existing language prompting Cross-Modal Fine-Tuning: Align then Re\ufb01ne methods (e.g., Liu et al., 2022) are generally not suitable for cross-modal learning due to the dif\ufb01culty of designing natural prompts for diverse data types. For the 1D tasks we study, there is even no notion of \u201cdiscrete tokens.\u201d Another line of work studies visual prompting by modifying 2D inputs for querying vision transformers. We test two such algorithms, VP (Bahng et al., 2022) and VPT (Jia et al., 2022), on three classi\ufb01cation tasks in our task suite. They are not applicable to the remaining tasks because either the inputs cannot be reshaped to look like images or the outputs are not classi\ufb01cation logits. We test VPT with the pretrained Swin-Base Trans- former (the same model we used for ORCA) and VP with the pretrained ResNet-50 (as the of\ufb01cial imple- mentation does not support vision transformers). The results are shown in Table 24. In general, prompt tun- ing is less effective than \ufb01ne-tuning, and the two base- lines perform signi\ufb01cantly worse than ORCA. This is not surprising given that prompting methods are more intuitively suited to in-modality transfer, where the target and the source data have similar structure or semantic meaning. However, when the target data (e.g., electromyography signals, as in the NinaPro dataset) is drastically different from image data, it is dif\ufb01cult to design prompts or expect good performance by only modifying the inputs without \ufb01ne-tuning the pretrained models. Table 24: Prediction errors (\u2193) of ORCA vs. visual prompting methods. NinaPro Spherical ECG 7.54\u00b10.39 33.18\u00b10.23 31.46\u00b10.83 0.28\u00b10.0059 0.57\u00b10.0044 0.40\u00b10.016 29.85\u00b10.72 98.05\u00b10.13 49.53\u00b11.45 ORCA VP VPT"}