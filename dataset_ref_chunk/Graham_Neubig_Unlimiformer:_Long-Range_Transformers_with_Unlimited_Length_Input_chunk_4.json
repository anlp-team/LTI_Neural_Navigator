{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Unlimiformer:_Long-Range_Transformers_with_Unlimited_Length_Input_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main difference between Unlimiformer and Memorizing Transformers?", "answer": " Unlimiformer is fully non-parametric and can improve performance without fine-tuning, while Memorizing Transformers added additional weights and cannot easily leverage pretrained LMs.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " How does Unlimiformer differ from Memorizing Transformers in terms of applying retrieval attention?", "answer": " While Wu et al. (2022) applies retrieval attention to only a single layer due to computational constraints, Unlimiformer enables the use of retrieval attention in every decoder layer with individualized retrieval per-head.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " What are some benefits of using Unlimiformer without further training?", "answer": " Unlimiformer is the only model that can provide benefits without further training, as it can improve base models without any additional training.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " How does Unlimiformer improve the base model when applied early stop w/ Unlimiformer?", "answer": " Unlimiformer applied at early stop provides significant gains without any special training, improving the base model by, for example, 3.3 ROUGE-1 points on the GovReport dataset.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " In what scenarios does Unlimiformer show the most significant gains when compared to standard finetuning?", "answer": " Injecting Unlimiformer at test time results in the most significant gains, with improvements of 7.2 ROUGE-1 and 3 BERTScore points, while the training remains as computationally cheap as standard finetuning.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " What types of training methods are used with Unlimiformer in the Book Summarization experiments?", "answer": " The Book Summarization experiments with Unlimiformer include Standard finetuning, +test Unlimiformer, +early stop w/ Unlimiformer, Unlimiformer (retrieval training), Unlimiformer (random-encoded training), and Unlimiformer (alternating training).", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " How does Unlimiformer perform in BookSum compared to base models like BARTbase and PRIMERA?", "answer": " Unlimiformer outperforms both base models, BARTbase and PRIMERA, in BookSum. Additionally, the base model BARTbase, even with Standard finetuning, shows competitive ROUGE and BERTScore metrics.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " What does Table 4 show about the performance of Unlimiformer compared to SLED and Memorizing Transformers?", "answer": " Table 4 shows that in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " What are the main findings from the PRIMERA experiments highlighted in Table 4?", "answer": " PRIMERA experiments highlight that Unlimiformer combined with BARTbase performs better than the base PRIMERA across all metrics and datasets. Unlimiformer can also be applied on top of existing long-range transformers and further improve them.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}, {"question": " What is the main advantage of Unlimiformer when applied on top of existing long-range transformers?", "answer": " Unlimiformer can improve the performance of existing long-range transformers like PRIMERA and BARTbase, even when they were pretrained on much more data. Unlimiformer can provide additional improvements over these models across all metrics and datasets.", "ref_chunk": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}], "doc_text": "moderates between the standard cross-attention and attention over retrieved keys from a datastore. Since their public implementation5 is \u201cnot officially supported\u201d and is not fully reproducible, we approximated it by using attention over the index in only a single decoder layer; this is equivalent to their setting with the learned interpolation parameter g set to 1.6 Our work differs from Memorizing Transformers in several key ways: Wu et al. (2022) added additional weights, and thus cannot easily leverage pretrained LMs, while Unlimiformer is fully non-parametric and can improve performance without fine-tuning; further, Wu et al. (2022) applies retrieval attention to only a single layer because of computational constraints, while our attention reformulation enables the use of Unlimiformer in every decoder layer with individualized retrieval per-head, while still being more efficient than Memorizing Transformers, as we detail in Section 2.3. 5 Results 5.1 Long Document Summarization Low-cost training Table 3 shows the results in the long-document summarization datasets. First, we can see that applying Unlimiformer on an existing checkpoint without any training (+test Unlimi- former) improves BARTbase by, for example, 1.8 ROUGE-1 points on both datasets, and improves PRIMERA by 1-1.4 ROUGE-1 points. In contrast, without additional training, SLED decreases performance. Thus, Unlimiformer is the only model that can provide benefits without further training. Early stop w/ Unlimiformer further improves the base model without any special training: it provides, for example, 3.3 ROUGE-1 points gain on GovReport, while the training computational cost is identical to standard finetuning. Train chunked does not provide benefits on its own; however injecting Unlimiformer applied at test time results in the most significant gains: 7.2 ROUGE-1 and 3 BERTScore points improvements, while training is as computationally cheap as standard finetuning. 5https://github.com/google-research/meliad 6Wu et al. (2022) note that in their experiments that most heads learned a value for g such that they attended \u201calmost exclusively\u201d to the external memory. 6 Base model Training method ROUGE 1 / 2 / L / BERTScore GovReport SummScreen BARTbase BARTbase BARTbase BARTbase Standard finetuning SLED (Ivgi et al., 2022) Memorizing transformers Unlimiformer (this work) 48.7 / 19.2 / 22.8 / 64.3 54.7 / 24.4 / 25.4 / 67.0 55.2 / 25.1 / 26.4 / 67.5 56.6 / 26.3 / 27.6 / 68.2 29.7 / 6.2 / 17.7 / 56.3 32.7 / 7.9 / 19.1 / 58.4 32.7 / 7.4 / 19.2 / 57.4 34.7 / 8.5 / 19.9 / 58.5 PRIMERA Standard finetuning PRIMERA Memorizing transformers PRIMERA Unlimiformer (this work) 55.1 / 23.9 / 25.9 / 67.0 57.0 / 25.3 / 26.5 / 67.7 57.4 / 26.2 / 28.0 / 68.1 32.3 / 7.1 / 18.3 / 57.1 33.0 / 7.3 / 18.4 / 57.3 33.3 / 7.6 / 18.9 / 57.7 Table 4: Test results on long-document datasets, when allowing compute-costly, long-range training methods, using different base models. The best metric in every dataset and every training category is marked in bold. The Unlimiformer results in this table are from using the alternating training strategy. Base model Training method ROUGE 1 / 2 / L EntMent BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase BARTbase Hierarchical (Kry\u00b4sci\u00b4nski et al., 2021) Standard finetuning +test Unlimiformer +early stop w/ Unlimiformer Memorizing Transformers Unlimiformer (retrieval training) Unlimiformer (random-encoded training) Unlimiformer (alternating training) 30.0 / 6.0 / 11.0 36.4 / 7.6 / 15.3 35.5 / 7.7 / 15.4 35.5 / 7.7 / 15.4 35.6 / 6.4 / 14.6 36.8 / 8.3 / 15.7 37.3 / 6.7 / 15.2 36.7 / 7.3 / 15.5 10.0 21.9 21.9 10.1 20.3 20.8 20.3 PRIMERA Standard finetuning +test Unlimiformer PRIMERA PRIMERA +early stop w/ Unlimiformer PRIMERA Unlimiformer (retrieval training) PRIMERA Unlimiformer (random-encoded training) PRIMERA Unlimiformer (alternating training) 38.6 / 7.2 / 15.6 38.3 / 7.5 / 15.9 39.5 / 7.3 / 15.8 37.9 / 8.2 / 16.3 39.5 / 7.1 / 15.9 38.2 / 7.1 / 16.0 11.6 18.9 22.2 25.5 19.7 23.4 Table 5: Results on BookSum (average input length \u2248 143k tokens). EntMent is entity recall. Hierar- chical summarization is a baseline reported by Kry\u00b4sci\u00b4nski et al. (2021), where chapter summaries are condensed to form a book summary. The best metric in every dataset is marked in bold. Long-range training Table 4 shows results when allowing computationally expensive training approaches. As shown, in almost all metrics and datasets, Unlimiformer outperforms the SLED and Memorizing Transformers baselines when using the same base model. The PRIMERA experiments in Table 4 highlight two important points: first, Unlimiformer+BARTbase performs better than the base PRIMERA across all metrics and datasets, even though PRIMERA is larger and was pretrained on much more data, using a pretraining objective that was designed for summarization; second, not only can Unlimiformer outperform Longformer-based models such as PRIMERA, Unlimiformer can also be applied on top of existing long-range transformers and further improve them: Unlimiformer+PRIMERA improves over PRIMERA across all metrics and datasets. Additional results on the validation set are provided in Appendix E. 5.2 Book Summarization Table 5 shows the result on BookSum. As shown, Unlimiformer improves both base models BARTbase and PRIMERA, in both low-cost training approaches such as Early stop w/ Unlimiformer, as well as in the long-range training approaches. Random-encoded-, Retrieval-, and Alternating- training show competitive performance, with the best method varying across datasets and models. We found that although Unlimiformer outperforms all base models on BookSum (Table 5), the base BART (Standard finetuning, which truncates the input to the first 1024 tokens) shows competitive ROUGE and BERTScore metrics. This is strongly counterintuitive for book summarization, where 7 l l a c e R n o i t n e M y t i t n E 25 20 15 10 5 Unlimiformer BARTbase e l p m a x e r e p e m i t e v i t a l e R 7 6 5 4 3 2 1 Unlimiformer BARTbase (truncates to 1024) 0 1 2 4 8 16 32 64 100 350 0 0 10 20 30 40 50 60 70 80 90 100 Max input size (in thousands"}