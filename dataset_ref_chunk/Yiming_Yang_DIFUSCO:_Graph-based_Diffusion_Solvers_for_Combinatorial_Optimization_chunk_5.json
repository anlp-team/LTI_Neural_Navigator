{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_DIFUSCO:_Graph-based_Diffusion_Solvers_for_Combinatorial_Optimization_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two diffusion approaches investigated for combinatorial optimization in the text?", "answer": " Continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " Which denoising schedule is found to be superior on discrete diffusion models?", "answer": " Cosine schedule", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " How does discrete diffusion compare to continuous diffusion models when there are more than 5 diffusion steps?", "answer": " Discrete diffusion consistently outperforms continuous diffusion by a large margin", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " What is the trade-off investigated between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers?", "answer": " The trade-off between adapting diffusion-based NPC solvers based on available computation budget by adjusting the total number of diffusion steps", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " Which diffusion schedule is used for fast inference in the experiments?", "answer": " Cosine denoising schedule", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " What diffusion model is found to be more effective than sampling more solutions, even with less computation?", "answer": " Performing more diffusion iterations is generally more effective than sampling more solutions", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " Which diffusion steps and number of samples are considered a good balance between exploration and exploitation for discrete DIFUSCO models?", "answer": " 50 diffusion steps \u00d7 1 sample policy and 10 diffusion steps \u00d7 16 samples policy", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " What are the two main decoding strategies used for discrete DIFUSCO models in the experiments?", "answer": " Greedy and Sampling strategies", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " In which settings does DIFUSCO achieve state-of-the-art performance among other neural NPC solvers?", "answer": " DIFUSCO achieves state-of-the-art performance in both greedy and sampling settings for probabilistic solvers", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}, {"question": " Which model significantly outperforms previous neural solvers on all three settings in the experiments?", "answer": " DIFUSCO", "ref_chunk": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}], "doc_text": "discrete diffusion (b) models on TSP-50 with different diffusion steps and number of samples. Their corresponding per-instance run-time (sec) are shown in (c), where the decomposed analysis (neural network + greedy decoding + 2-opt) can be found in the appendix. (a) Continuous diffusion (c) Runtime 4.2 Design Analysis Discrete Diffusion v.s. Continuous Diffusion We first investigate the suitability of two diffusion approaches for combinatorial optimization, namely continuous diffusion with Gaussian noise and discrete diffusion with Bernoulli noise (Sec. 3.2). Additionally, we explore the effectiveness of different denoising schedules, such as linear and cosine schedules (Sec. 3.3), on CO problems. To efficiently evaluate these model choices, we utilize the TSP-50 benchmark. Note that although all the diffusion models are trained with a T = 1000 noise schedule, the inference schedule can be shorter than T , as described in Sec. 3.3. Specifically, we are interested in diffusion models with 1, 2, 5, 10, 20, 50, 100, and 200 diffusion steps. Fig. 1 demonstrates the performance of two types of diffusion models with two types of inference schedules and various diffusion steps. We can see that discrete diffusion consistently outperforms the continuous diffusion models by a large margin when there are more than 5 diffusion steps2. Besides, the cosine schedule is superior to linear on discrete diffusion and performs similarly on continuous diffusion. Therefore, we use cosine for the rest of the paper. More Diffusion Iterations v.s. More Sampling By utilizing effective denoising schedules, diffu- sion models are able to adaptively infer based on the available computation budget by predetermining the total number of diffusion steps. This is similar to changing the number of samples in previous probabilistic neural NPC solvers [64]. Therefore, we investigate the trade-off between the number of diffusion iterations and the number of samples for diffusion-based NPC solvers. 2We also observe similar patterns on TSP-100, where the results are reported in the appendix. 7 Table 2: Results on large-scale TSP problems. RL, SL, AS, G, S, BS, and MCTS denotes Reinforcement Learning, Supervised Learning, Active Search, Greedy decoding, Sampling decoding, Beam-search, and Monte Carlo Tree Search, respectively. * indicates the baseline for computing the performance gap. Results of baselines are taken from Fu et al. [27] and Qiu et al. [92], so the runtime may not be directly comparable. See Section 4 and appendix for detailed descriptions. ALGORITHM TYPE TSP-1000 LENGTH \u2193 GAP \u2193 TIME \u2193 LENGTH \u2193 GAP \u2193 TSP-500 TSP-10000 TIME \u2193 LENGTH \u2193 GAP \u2193 TIME \u2193 EXACT CONCORDE EXACT GUROBI LKH-3 (DEFAULT) HEURISTICS LKH-3 (LESS TRAILS) HEURISTICS FARTHEST INSERTION HEURISTICS 16.55\u2217 16.55 16.55 16.55 18.30 \u2014 37.66m 23.12\u2217 0.00% 45.63h N/A 0.00% 46.28m 23.12 0.00% 3.03m 23.12 0s 25.72 10.57% 6.65h N/A \u2014 N/A N/A N/A 71.77\u2217 0.00% 2.57h 0.00% 7.73m 71.79 0s 80.59 11.25% N/A N/A \u2014 \u2014 51.27m N/A N/A 8.8h 12.29% 6s AM GCN POMO+EAS-EMB POMO+EAS-TAB DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+G SL+G RL+AS+G RL+AS+G RL+G RL+AS+G SL+G\u2020 SL+G\u2020+2-OPT 20.02 29.72 19.24 24.54 18.93 17.81 18.35 16.80 20.99% 1.51m 31.15 79.61% 6.67m 48.62 16.25% 12.80h N/A 48.22% 11.61h 49.56 14.38% 0.97m 26.58 7.61% 2.10h 24.91 10.85% 3.61m 26.14 1.49% 3.65m 23.56 34.75% 3.18m 141.68 110.29% 28.52m N/A N/A 114.36% 63.45h N/A 14.97% 2.08m 86.44 7.74% 4.49h 80.45 13.06% 11.86m 98.15 1.90% 12.06m 73.99 N/A N/A 97.39% 5.99m N/A N/A N/A 20.44% 4.65m 12.09% 3.07h 36.75% 28.51m 3.10% 35.38m N/A N/A N/A EAN AM GCN DIMES DIMES OURS (DIFUSCO) OURS (DIFUSCO) RL+S+2-OPT RL+BS SL+BS RL+S RL+AS+S SL+S SL+S+2-OPT 23.75 19.53 30.37 18.84 17.80 17.23 16.65 43.57% 57.76m 47.73 18.03% 21.99m 29.90 83.55% 38.02m 51.26 13.84% 1.06m 26.36 7.55% 2.11h 24.89 4.08% 11.02m 25.19 0.57% 11.46m 23.45 106.46% 5.39h N/A 29.23% 1.64h 129.40 121.73% 51.67m N/A 14.01% 2.38m 85.75 7.70% 4.53h 80.42 8.95% 46.08m 95.52 1.43% 48.09m 73.89 N/A 80.28% 1.81h N/A 19.48% 4.80m 12.05% 3.12h 33.09% 6.59h 2.95% 6.72h N/A N/A ATT-GCN DIMES DIMES OURS (DIFUSCO) SL+MCTS RL+MCTS RL+AS+MCTS SL+MCTS 16.97 16.87 16.84 16.63 2.54% 2.20m 23.86 1.93% 2.92m 23.73 1.76% 2.15h 23.69 0.46% 10.13m 23.39 3.22% 4.10m 74.93 2.64% 6.87m 74.63 2.46% 4.62h 74.06 1.17% 24.47m 73.62 4.39% 21.49m 3.98% 29.83m 3.19% 3.57h 2.58% 47.36m Fig. 2 shows the results of continuous diffusion and discrete diffusion with various diffusion steps and number of parallel sampling, as well as their corresponding total run-time. The cosine denoising schedule is used for fast inference. Again, we find that discrete diffusion outperforms continuous diffusion across various settings. Besides, we find performing more diffusion iterations is generally more effective than sampling more solutions, even when the former uses less computation. For exam- ple, 20 (diffusion steps) \u00d7 4 (samples) performs competitive to 1 (diffusion steps) \u00d7 1024 (samples), while the runtime of the former is 18.5\u00d7 less than the latter. In general, we find that 50 (diffusion steps) \u00d7 1 (samples) policy and 10 (diffusion steps) \u00d7 16 (samples) policy make a good balance between exploration and exploitation for discrete DI- FUSCO models and use them as the Greedy and Sampling strategies for the rest of the experiments. 4.3 Main Results Comparison to SOTA Methods We compare discrete DIFUSCO to other state-of-the-art neural NPC solvers on TSP problems across various scales. Due to the space limit, the description of other baseline models can be found in the appendix. Tab. 1 compare discrete DIFUSCO with other models on TSP-50 and TSP-100, where DIFUSCO achieves the state-of-the-art performance in both greedy and sampling settings for probabilistic solvers. Tab. 2 compare discrete DIFUSCO with other models on the larger-scale TSP-500, TSP-1000, and TSP-10000 problems. Most previous probabilistic solvers (except DIMES [92]) becomes untrainable on TSP problems of these scales, so the results of these models are reported with TSP-100 trained models. The results are reported with greedy, sampling, and MCTS decoding strategies, respectively. For fair comparisons [27, 92], MCTS decoding for TSP is always evaluated with only one sampled heatmap. From the table, we can see that DIFUSCO strongly outperforms the previous neural solvers on all three settings. In particular, with MCTS-based decoding, DIFUSCO significantly improving the performance"}