{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_COBRA_Frames:_Contextual_Reasoning_about_Effects_and_Harms_of_Offensive_Statements_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the hypothesis regarding the importance of context in understanding the toxicity of statements?,        answer: The hypothesis is that context is important for understanding the toxicity of statements.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " In the study, how well do models adapt to counterfactual contexts?,        answer: The models, particularly CHARM, are shown to adapt better to counterfactual contexts compared to GPT-3.5.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is the significance of the COBRACORPUS-CF dataset?,        answer: The COBRACORPUS-CF dataset is significant as it pairs toxic comments with counterfactual contexts that significantly alter the toxicity and implications of statements.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " How does the CHARM model perform compared to GPT-3.5 in explaining toxicity?,        answer: CHARM outperforms GPT-3.5 in explaining toxicity, even though it is trained on data generated by GPT-3.5.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is one challenge highlighted in the text regarding the interpretation of statements in counterfactual contexts?,        answer: One challenge is that even the best-performing models find counterfactual contexts challenging.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is COBRA frames and what does it capture?,        answer: COBRA frames is a formalism to distill the context-dependent implications, effects, and harms of toxic language. It captures seven explanation dimensions.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is the potential future application of COBRA?,        answer: Potential future applications of COBRA include automatic categorization of different types of offensiveness, assisting content moderators, and testing linguistic and psychological theories about offensive statements.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is a limitation mentioned in the text regarding the context of toxic statements?,        answer: One limitation is that capturing the full context of statements is challenging, and future work should explore incorporating more contextual variables.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " What is highlighted as a societal consideration of the study?,        answer: One societal consideration is the potential impact of biases latent in distilled contexts for harmful speech and the need for future research to investigate this aspect.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}, {"question": " How does the study address the limitation of machine-generated data?,        answer: The study indirectly manages biases in machine-generated data by generating multiple contexts for every statement.    ", "ref_chunk": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}], "doc_text": "the automatic and human evaluation. This is consis- tent with our hypothesis that context is important for understanding the toxicity of statements. How well do models adapt to counterfactual con- texts? We then investigate how well our model, as well as GPT-3.5 ,10 identifies the offensiveness Table 7: Accuracy, derived from binarizing the \u201cof- fensiveness\u201d explanation, for different models on COBRACORPUS-CF (WoC means Without Context). All Toxic means predicting every statement as toxic. Takeaway: CHARM adapts to counterfactual con- texts better than GPT-3.5 (text-davinci-003 Jan 13th 2022). of statements when the context drastically alters the implications. We then compare different mod- els\u2019 ability to classify whether the statement is of- fensive or not given the counterfactual context in COBRACORPUS-CF. Surprisingly, although our model is only trained on the GPT-3.5-generated COBRACORPUS, it out- performs GPT-3.5 (in a few-shot setting as de- scribed in \u00a73.3) on COBRACORPUS-CF (Table 7). Table 5 shows some example predictions on the counterfactual context pairs. GPT-3.5 tends to \u201cover-interpret\u201d the statement, possibly due to the information in the prompts. For example, for the last statement in Table 5, GPT-3.5 infers the impli- cation as \u201cIt implies that people of color are not typically articulate\u201d, while such statement-context pair contains no information about people of color. In general, counterfactual contexts are still chal- lenging even for our best-performing models. 10text-davinci-003 Jan 13th 2022 6 Conclusion & Discussion We introduce COBRA frames, a formalism to distill the context-dependent implications, effects, and harms of toxic language. COBRA captures seven explanation dimensions, inspired by frame semantics (Fillmore, 1976), social bias frames (Sap et al., 2020), and psychology and sociolinguistics literature on social biases and prejudice (Nieto and Boyer, 2006; Nadal et al., 2014). As a step towards addressing the importance of context in content moderation, we create COBRACORPUS, a novel dataset of toxic comments populated with contex- tual factors as well as explanations. We also build COBRACORPUS-CF, a small-scale, curated dataset of toxic comments paired with counterfactual con- texts that significantly alter the toxicity and impli- cation of statements. We contribute CHARM, a new model trained with COBRACORPUS for producing explanations of toxic statements given the statement and its social context. We show that modeling without contextual factors is insufficient for explaining toxicity. CHARM also outperforms GPT-3.5 in COBRACORPUS-CF, even though it is trained on data generated by GPT-3.5. as a vital step towards addressing the importance of context in content moderation and many other social NLP tasks. Po- tential future applications of COBRA include au- tomatic categorization of different types of offen- siveness, such as hate speech, harassment, and mi- croaggressions, as well as the development of more robust and fair content moderation systems. Fur- thermore, our approach has the potential to assist content moderators by providing free-text expla- nations. These explanations can help moderators better understand the rationale behind models\u2019 pre- dictions, allowing them to make more informed decisions when reviewing flagged content (Zhang et al., 2023). This is particularly important given the growing calls for transparency and accountabil- ity in content moderation processes (Bunde, 2023). Besides content moderation, COBRA also has the potential to test linguistic and psychological the- ories about offensive statements. While we made some preliminary attempts in this direction in \u00a73 and \u00a74, more work is needed to fully realize this potential. For example, future studies could inves- tigate the differences in in-group and out-group interpretations of offensive statements, as well as the role of power dynamics, cultural background, We view COBRA and individual sensitivities in shaping perceptions of offensiveness. Limitations & Ethical and Societal Considerations We consider the following limitations and societal considerations of our work. Machine-generated Data Our analysis is based on GPT-3 generated data. Though not perfectly aligned with real-world scenarios, as demonstrated in Park et al. (2022), such analysis can provide insights into the nature of social interactions. How- ever, this could induce specific biases, such as skewing towards interpretations of words aligned with GPT-3.5\u2019s training domains and potentially overlooking more specialized domains or minor- ity speech (Bender et al., 2021; Bommasani et al., 2021). The pervasive issue of bias in offensive language detection and in LLMs more generally requires exercising extra caution. We deliberately generate multiple contexts for every statement as an indirect means of managing the biases. Neverthe- less, it is a compelling direction for future research to investigate the nature of biases latent in distilled contexts for harmful speech and further investigate their potential impact. For example, it would be valuable to collect human-annotated data on CO- to compare with the machine-generated BRA data. However, we must also recognize that hu- mans are not immune to biases (Sap et al., 2019b, 2022), and therefore, such investigations should be carefully designed. Limited Contextual Variables Although CO- BRACORPUS has rich contexts, capturing the full context of statements is challenging. Future work should explore incorporating more quantitative fea- tures (e.g., the number of followers of the speaker) to supplement contextual variables such as social role and power dynamics. In this work, we fo- cus on the immediate context of a toxic statement. However, we recognize that the context of a toxic statement can be much longer. We have observed significant effects even in relatively brief contexts, indicating the potential for improved performance when more extended contexts are present. We be- lieve that future research could explore the influ- ence of richer contexts by including other modali- ties (e.g., images, videos, etc.). Limited Identity Descriptions Our work fo- cused on distilling the most salient identity charac- teristics that could affect the implications of toxi- city of statements. This often resulted in generic identity labels such as \u201ca white person\u201d or \u201cA Black woman\u201d being generated without social roles. This risks essentialism, i.e., the assumption that all mem- bers of a demographic group have inherent qual- ities and experiences, which can be harmful and perpetuate stereotypical thinking (Chen and Ratliff, 2018; Mandalaywala et al., 2018; Kurzwelly et al., 2020). Future work should explore incorporating more specific identity descriptions that circumvent the risk of"}