{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Regularizing_Self-training_for_Unsupervised_Domain_Adaptation_via_Structural_Constraints_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the goal of unsupervised domain adaptation mentioned in the text?,        answer: The goal is to leverage datasets from the source and target domains to learn a model that performs well in the target domain.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " How is the model defined in the text?,        answer: The model is defined as a composition of an encoder, E\u03c6 : X \u2192 Z, and a classifier, G\u03c8 : Z \u2192 ZP.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " Explain the concept of pseudo-label self-training (PLST) as described in the text.,        answer: PLST leverages a source trained seed model to pseudo-label unlabelled target data by using pixel-wise class probabilities to assign pseudo-labels based on a predefined threshold.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " What is the UDA objective comprised of in the text?,        answer: The UDA objective consists of two components: Ls cls for the source domain and Lt st for the target domain with relative weighting coefficients \u03b1st.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " How does the text suggest alleviating confirmation bias in the self-training scheme?,        answer: The text suggests introducing auxiliary modality information, like depth, to provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " What is the purpose of the depth maps mentioned in the text?,        answer: Depth maps are used to reveal the presence of distinct objects in a scene and to extract object regions by computing a histogram of depth values and clustering them into discrete regions with unique labels.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " How are object regions extracted in the text using the depth maps?,        answer: Object regions are extracted by identifying high density regions in the histogram of depth values above a threshold as centers to cluster the histograms into distinct regions with unique labels.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " What method is adopted for RGB-input clustering in the text?,        answer: SLIC (Simple Linear Iterative Clustering) is adopted as a fast algorithm for partitioning images into multiple segments that respect object boundaries.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " What impact does the number of SLIC segments have according to the text?,        answer: The number of SLIC segments impacts the cluster sizes; small numbers lead to large cluster sizes that are agnostic to the variation in object scales, potentially grouping pixels from distinct object instances together.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}, {"question": " Why is the fusion of depth-based and RGB-based segmentations necessary as per the text?,        answer: Fusing depth-based and RGB-based segmentations yields object regions that are more consistent with the actual objects, helping to correct misclassifications that may occur in individual segmentations.    ", "ref_chunk": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}], "doc_text": "Semantic i )}Ns Segmentation: Consider a dataset Ds = {(xs i=1 of input-label pairs sampled from a source domain dis- tribution, P s X\u00d7Y . The input and labels share the same spatial dimensions, H \u00d7 W , where each pixel of the la- bel is assigned a class c \u2208 {1, . . . , C} and is represented via a C dimensional one-hot encoding. We also have a dataset Dt = {(xt i=1 sampled from a target distri- bution, P t X\u00d7Y where the corresponding labels, {yt i } are un- observed during training. Here, the target domain is sepa- rated from the source domain due to domain shift expressed as P s X\u00d7Y . Under such a shift, the goal of un- supervised domain adaptation is to leverage Ds and Dt to learn a parametric model that performs well in the target domain. The model is de\ufb01ned as a composition of an en- coder, E\u03c6 : X \u2192 Z and a classi\ufb01er, G\u03c8 : Z \u2192 ZP where, Z \u2208 RH\u00d7W \u00d7d represents the space of d-dimensional spa- tial embeddings, ZP \u2208 RH\u00d7W \u00d7C gives the un-normalized distribution over the C classes at each spatial location, and {\u03c6, \u03c8} are the model parameters. To learn a suitable target model, the parameters are optimised using a cross-entropy i , ys i )}Nt i, yt X\u00d7Y (cid:54)= P t Figure 2. Objectness Constraint Formulation: Overall pipeline for computing the objectness constraint using multi-modal object-region estimates derived from RGB-Image and Depth-Map. Depth segmentation is obtained by clustering the histogram of depth values and RGB segmentation is obtained via k-means clustering (SLIC) of raw-pixel intensities. Fusing these two types of segmentation yields object regions that are more consistent with the actual object. For example, a portion of the car in the middle is wrongly clustered with the road in depth segmentation and with the left-wall under RGB segmentation. However, the fused segmentation yields car-regions that completely respect the boundary of the car. objective on the source domain, Ls cls = \u2212 1 Ns Ns(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c=1 imc log ps ys imc(\u03c8, \u03c6) (1) Note that while Eqn. 3 uses a class-agnostic \ufb01xed threshold in practice, this threshold can be made class-speci\ufb01c and dy- namically updated over the course of self-training. Such a threshold ensures that only the highly-con\ufb01dent predictions contribute to successive training. The \ufb01nal self-training ob- jective can be written in terms of pseudo-labels as imc(\u03c8, \u03c6) = \u03c3 (G\u03c8 \u25e6 E\u03c6(xs ps i )) |m,c , where \u03c3 denotes softmax operation and an adaptation ob- jective over the target domain as described next. Pseudo-label self-training (PLST): Following prior works [60, 64], we describe a simple and effective approach to PLST that leverages a source trained seed model to pseudo- label unlabelled target data, via con\ufb01dence thresholding. Speci\ufb01cally, the seed model is \ufb01rst trained on Ds using Eqn. 2 to obtain a good parameter initialisation, {\u03c60, \u03c80}. Then, this model is used to compute pixel-wise class prob- abilities, pt im(\u03c80, \u03c60) using to Eqn. 2 for each target image, xt i \u2208 Dt. These probabilities are used in conjunction with a prede\ufb01ned threshold \u03b4, to obtain one-hot encoded pseudo- labels \u02dcyt imc = (cid:40)1 if c = arg max 0 otherwise c(cid:48) imc(cid:48) and pt pt imc \u2265 \u03b4 (2) (3) Lt st = \u2212 1 Nt Nt(cid:88) i=1 H\u00d7W (cid:88) m=1 C (cid:88) c(cid:48)=1 imc(cid:48) log (cid:0)pt \u02dcyt imc(cid:48) (cid:1) (4) The overall UDA objective is simply, Luda = Ls where \u03b1st is the relative weighting coef\ufb01cients. cls + \u03b1stLt st, 3.1. Supervision For Objectness Constraint An important issue with the self-training scheme de- scribed above is that it is usually prone to con\ufb01rmation bias that can lead to compounding errors in target model pre- dictions when trained on noisy pseudo-labels. To alleviate target performance, we introduce auxiliary modality infor- mation (like, depth) that can provide indirect supervision for semantic labels in the target domain and improve the robustness of self-training. In this section we describe our multimodal objectness constraint that extracts object-region estimates to formulate a contrastive objective. The overview 4 of our objectness constraint formulation is presented in Fig. 2. Supervision via Depth: Segmentation datasets are often accompanied with depth maps registered with the RGB im- ages. In practice, depth maps can be obtained from stereo pairs [12,39] or sequence of images [15]. These depth maps can reveal the presence of distinct objects in a scene. We particularly seek to extract object regions from these depth maps by \ufb01rst computing a histogram of depth values with prede\ufb01ned, b number of bins. We then leverage the prop- erty of objects under \u201dthings\u201d categories [17] whose range of the depth is usually much smaller than the range of en- tire scene depth. Examples of such categories in outdoor scene segmentation include persons, cars, poles etc. This property translates into high density regions (or peaks) in the histogram corresponding to distinct objects at distinct depths. Among these peaks, we use the ones with promi- nence [27] above a threshold, \u03b4peak as centers to cluster the histograms into discrete regions with unique labels. These labels are then assigned to every pixel whose depth values lie in the associated region. An example of the resulting depth-based segmentation for b = 200 and \u03b4peak = 0.0025 is visualised in Fig. 2. Supervision via RGB: Another important form of self- supervision for object region estimates is based on RGB- input clustering. We adopt SLIC [1] as a fast algorithm for partitioning images into multiple segments that respect object boundaries; the SLIC method applies k-means clus- tering in pixel space to group together adjacent pixels that are visually similar. An important design decision is the number of SLIC segments, ks: small ks leads to large clus- ter sizes that is agnostic to the variation in object scales, across different object categories and instances of the scene. Consequently, pixels from distinct object instances may be grouped together regardless of the semantic"}