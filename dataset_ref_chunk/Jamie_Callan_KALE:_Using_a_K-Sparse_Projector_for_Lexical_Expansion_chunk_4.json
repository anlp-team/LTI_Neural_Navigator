{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_KALE:_Using_a_K-Sparse_Projector_for_Lexical_Expansion_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the Teacher LM serve as target for?", "answer": " The learnable dense representations to align to", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What activation function is used within the projector?", "answer": " ReLU activation function", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " How is sparsification achieved in KALE?", "answer": " Through a TopK operation", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What defines the loss function M3SE proposed by Menon et al.?", "answer": " M3SE is defined as a Multi-Margin MSE", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What scoring method is used to compute student scores and teacher scores?", "answer": " Cosine similarity", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What is retrieved from the top 50 passages returned from BM25 for each query?", "answer": " A hard negative", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What aspect is noted regarding the KALE sparse representation?", "answer": " It is beneficial to connect the dense representation output by the projector into the ranking loss", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What does the regularization term promoting equipartitioning take as input?", "answer": " The same vectors that go into the M3SE loss", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What dataset was KALE trained on?", "answer": " MSMARCOv1", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}, {"question": " What evaluation metric is used for TREC DL20?", "answer": " NDCG@10", "ref_chunk": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}], "doc_text": "Teacher LM, which generates a dense vector representation, also through an average-pooling operation. The average-pooled dense vector from the Teacher LM serves as target for the learnable dense rep- resentations to align to. The learnable representation of the input goes through the projector, which creates a new vector of the size of the vocabulary. The activation within the projector is the ReLU activation function, which naturally already enforces a degree of sparsity, by setting all negative values to zero. Also, the ReLU en- sures positivity of the vector values, which is necessary for the TopK operation to function as expected (e.g., negative values with a high absolute value would never be extracted). The representation from the projector then acts as a student in the ranking loss, and is also fed to the equipartitioning component. At inference time, each query and document is augmented with a set of new terms, which requires KALE to output a sparse rep- resentation for each input text. Given the dense vector output by the projector, sparsification is achieved through a TopK operation. The TopK operation grabs a vector, maintains its top \ud835\udc3e values, and zeroes out the remaining dimensions. The sparsified vector is the KALE sparse representation of the input text, ready to be indexed and searched. The resulting sparse dimensions are indexed with a constant term frequency weight. Different weighting schemes did not provide additional benefits. A \ud835\udc3e hyperparameter is set for the Lu\u00eds Borges, Bruno Martins, & Jamie Callan queries, while another is set for the passages. These hyperparame- ters are kept fixed. A dynamic \ud835\udc3e value for every query/passage did not provide significative improvements. The loss function is a weighted sum of two components. The first component aligns the teacher with the student model, and is the Multi-Margin MSE (M3SE) proposed by Menon et al. [25]. Given a set of student scores s and teacher scores t, the M3SE is defined as: M3SE(t, s) = \u2211\ufe01 ((\ud835\udc61\ud835\udc56 \u2212 \ud835\udc61 \ud835\udc57\u2217) \u2212 (\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60 \ud835\udc57\u2217))2 + \u2211\ufe01 [\ud835\udc60 \ud835\udc57 \u2212 \ud835\udc60 \ud835\udc57\u2217]2 +, \ud835\udc56 \u2208\ud835\udc45 \ud835\udc57 \u2208\ud835\udc41 where \ud835\udc45 is the set of relevant passages for a training query, \ud835\udc41 is the set of non-relevant passages, and \ud835\udc57\u2217 is the index of the negative passage with the highest teacher score. This approach enforces a correct margin between the relevant passages and the highest scoring negatives. For the remaining negatives, it suffices to have a lower score than \ud835\udc57\u2217. The student scores and teacher scores are computed with the cosine similarity. For each query, a relevant passage is fetched from the annotated data. A hard negative is retrieved from the top 50 passages returned from BM25 for that query. The relevant passages of the other queries in the batch serve as in-batch negatives. One aspect to note is that although the KALE sparse representation is the vector to be indexed, initial experiments showed that it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |\ud835\udc35 | \u2211\ufe01 B\ud835\udc56, \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 = |\ud835\udc49 | \u2211\ufe01 \ud835\udc51\ud835\udc56\ud835\udc5a\ud835\udc60\ud835\udc63, \ud835\udc56 \ud835\udc63 Equipartition(B) = KL (cid:18) dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 , 1 \ud835\udc37 (cid:19) + KL (cid:18) 1 \ud835\udc37 , dims \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61 (cid:19) . In the previous equations, KL is the Kullback-Leibler divergence, |\ud835\udc35| is the batch size, and |\ud835\udc49 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 \ud835\udc37 denotes a uniform distribution over the \ud835\udc37 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. 4 DATASETS AND EVALUATION METHODS KALE was trained on the MSMARCOv1 [1] dataset. About 800.000 training queries have been released, while approximately 100.000 queries were reserved for model development. The passage collec- tion comprises more than 8 million passages. There is on average (1) (2) (3) (4) KALE: Using a K-Sparse Projector for Lexical Expansion Figure 3: Generating new terms from the KALE sparse rep- resentations, and then expanding the input text with the extracted terms. The augmented passages are then ready to be indexed, while expanded queries can be searched over the resulting index. NT stands for New Term. one relevant passage for each query. Besides MSMARCOv1, evalua- tion is done on the TREC DL19 [4] and TREC DL20 [5] judged sets of queries - composed of 43 and 54 queries, respectively - which are also searched on the MSMARCOv1 passage collection. The retrieval metrics are Recall@10 for both datasets, MRR@10 for MSMARCOv1, and NDCG@10 for TREC DL. The learned vocabulary was further evaluated on out-of-domain data, making use of the BEIR benchmark [36]. BEIR is a collection of 18 datasets from multiple text retrieval tasks over multiple domains. Four of those datasets are not public. The tasks range from bio- medical information retrieval to citation prediction. In this article, the BEIR benchmarks were divided into search tasks and semantic relatedness tasks, and NDCG@10 is the evaluation metric. The experiments mostly relied on BM25 scoring, with the excep- tion of impact indexes. After KALE is trained and a sparse repre- sentation of a query/passage is obtained, KALE creates a series of artificial terms for the dimensions in which the KALE sparse repre- sentation is different than zero, appending each term to the text of the query or passage, with a constant term frequency. The"}