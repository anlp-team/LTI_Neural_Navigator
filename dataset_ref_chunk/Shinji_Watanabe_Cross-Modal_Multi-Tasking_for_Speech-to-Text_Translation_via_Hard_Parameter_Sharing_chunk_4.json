{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Cross-Modal_Multi-Tasking_for_Speech-to-Text_Translation_via_Hard_Parameter_Sharing_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the ConST model?,answer: The ConST model utilizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " How does the M3ST model differ from the ConST model?,answer: The M3ST model utilizes a multi-stage training strategy with textual pre-training and ST fine-tuning stages, while the ConST model focuses on contrastive loss.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What do ConST and M3ST models gain more from according to the text?,answer: ConST and M3ST models appear to gain more from the same external MT data, although their baselines are weaker.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What is the ultimate goal mentioned in the text regarding ST models?,answer: The ultimate goal is to build ST models that leverage both paired and unpaired textual data efficiently.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What is the recent trend mentioned in relation to mBART?,answer: The recent trend is to take the mBART decoder parameters to partially initialize ST encoder-decoder models.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What is highlighted as a form of heterogeneous transfer learning?,answer: Taking the mBART decoder parameters to partially initialize ST encoder-decoder models is highlighted as a form of heterogeneous transfer learning.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What does the ablation study in Table 3 indicate about up-sampling MT inputs?,answer: The ablation study indicates that 4x up-sampling of MT inputs was the best, while no up-sampling was slightly detrimental.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " How does the method in the text exhibit more efficient transfer compared to prior works?,answer: The method in the text exhibits more efficient transfer by yielding +1.8 BLEU in the transfer process.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What key difference is mentioned between the approach of the work and the AudioPalm method?,answer: The key difference is that the work confirms improvement from scratch before adding external MT data and initialization from textual pre-trained models, unlike the AudioPalm method.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}, {"question": " What is the main technical novelty of the approach presented in the text?,answer: The main technical novelty is the method of hard parameter sharing for ST/MT multi-tasking by pre-processing speech into discrete sequences of tokens.", "ref_chunk": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}], "doc_text": "of MT multi-task. Up = Up-sampling factor of MT input text. Ratio = Average length ratio of discrete speech to text. MT Up Ratio BLEU \u2713 \u2713 \u2713 \u2713 - 2x 4x 6x 6.0 3.0 1.5 1.0 28.6 28.5 28.8 29.2 28.7 We take two representative works for comparison in this section. The ConST model [8] is a soft multi-tasking approach which uti- lizes a contrastive loss to encourage matched speech and text inputs to be closer, relative to unmatched speech and text inputs. ConST also uses multiple strategies to create harder examples for the con- trastive loss. The M3ST model [11] is another soft multi-tasking ap- proach which utilizes a multi-stage training strategy. The first stage is a purely textual pre-training stage which incorporates external MT data while the next two stages are ST fine-tuning stages which per- form data mix-up and contrastive learning. ConST and M3ST ap- pear to gain more from the same external MT data, although their baselines are indeed much weaker. Nonetheless, we suspect that modality-specific modules can limit interference from extremely un- balanced MT to ST data ratios, but we leave this for future work. 4.3. Cross-Modal Transfer Learning Ultimately, we\u2019d like to build ST models which efficiently leverage not only paired textual data, but also copious amounts of unpaired textual data. In this section we examine models initialized from mBART [25, 26], an encoder-decoder pre-trained with text denoising objectives and then fine-tuned on large-scale MT data.3 The recent trend is to take the mBART decoder parameters to partially initial- ize ST encoder-decoder models [50, 51, 53, 54]. This is a form of heterogeneous transfer learning [55] \u2013 there is a cost associated with the distributional shift between the textual pre-training domain and the speech-based fine-tuning domain. Cross-modal pre-training [6, 7] has been shown to reduce this cost. We posit that our cross-modal fine-tuning method has a similar effect. 3https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt The results in the final horizontal partition of Table 2 presents models with mBART decoder initialization (see \u00a73). Comparing the single-task model F1 with the multi-task model F2, we see that the latter is +1.8 BLEU better. The single-task model only improves by +0.3 BLEU from mBART initialization (E1 vs. F1); prior works have also noted similarly muted gains [51, 56], indicating deficien- cies in transfer learning across modalities. Our method exhibits a more efficient transfer (E2 vs. F2), yielding +1.8 BLEU. Note that mBART has been fine-tuned on large scale MT data, so we do not find it necessary to include WMT data in our training mixture. 4.4. Ablations on Sequence Lengths Table 3 shows an ablation study on the importance of up-sampling the lengths of MT inputs to match the lengths of discrete speech in- puts (per \u00a72.2.2). We found that 4x up-sampling was best. Note that without any up-sampling, ST/MT multi-tasking was actually slightly detrimental. 6x up-sampling was not the best even though speech and text have equal lengths on average, suggesting that the alignment of each text token to corresponding speech tokens is not uniform. 5. RELATION TO PRIOR WORK Now that we have presented our approach and results, we\u2019ll highlight the technical and empirical novelty of our work. First, our work is closely related to AudioPalm [15]; both of our methods achieve hard ST/MT multi-tasking by discretizing speech. On the surface this makes our methods look quite similar, but Au- dioPalm focuses on initializing speech models from Palm. In fact, their results show that their models are deficient when trained from scratch (see Table 6 in their paper). We take the exact opposite ap- proach: we first confirm that our method improves training from scratch before adding external MT data and initialization from tex- tual pre-trained models. This is a major technical difference in it- self, but it also allows us to investigate several empirical novelties. Namely, we are able to show the individual effects of hard parameter sharing, external MT data, and transfer learning. These three effects are conflated within the experimental setup of the AudioPalm paper which is more focused on demonstrating performance at scale. All told, we view these works as complementary \u2013 this work focuses on a set of sequence-to-sequence models commonly used in ST and other speech processing tasks (CTC, AED, CTC/Attn, RNN-T) which are distinct from AudioPalm\u2019s decoder-only model. Second, our work follows a long line of prior works which in- vestigate ST/MT multi-tasking [5\u201313]. The common theme amongst these approaches is soft parameter sharing, which is a major differ- ence compared to our approach. Further, we examine a larger set of Seq2Seq models to demonstrate general applicability. 6. CONCLUSION We present a method for ST/MT multi-tasking with hard parame- ter sharing, which is not trivially achieved due to the speech-text modality gap. Our approach resolves this by pre-processing speech into discrete sequences of tokens. This allows us to build Seq2Seq models capable of ingesting speech and text via an input vocabulary consisting of discrete speech and text tokens. Given the consistent improvements in ST, we will apply this approach to spoken language understanding and speech summarization in the future. Brian and Shinji are supported by the HLTCOE at JHU. This work used NCSA Delta (project CIS210014) from ACCESS through NSF grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. REFERENCES [1] J. Pino et al., \u201cHarnessing indirect training data for end-to-end auto- matic speech translation: Tricks of the trade,\u201d Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, \u201cSource and target bidi- rectional knowledge distillation for end-to-end speech translation,\u201d Proc. NAACL, 2021. [3] Y. Jia et al., \u201cLeveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,\u201d Interspeech, 2022. [4] S. Ruder, \u201cAn overview of multi-task learning in deep neural net- works,\u201d arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., \u201cSpeecht5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d Proc. ACL, 2021. [6] A. Bapna et al., \u201cMslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., \u201cUnified speech-text pre-training for speech translation"}