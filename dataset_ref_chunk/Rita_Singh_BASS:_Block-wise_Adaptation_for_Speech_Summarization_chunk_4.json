{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_BASS:_Block-wise_Adaptation_for_Speech_Summarization_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does Table 3 shed light on?", "answer": " The improvement in the prediction of different parts of speech in the reference summary using the best BASS-ADAPT model.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " According to Table 4, what modeling strategy yields significant improvements in summarization performance?", "answer": " Concatenating the semantic embedding of the previous block with the current block.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " What are the three updater mechanisms described in Figure 2?", "answer": " Gated attention, concatenation, and hierarchical attention.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " How does gated attention compare to concatenation in terms of performance?", "answer": " Gated attention is able to achieve similar performance while reducing memory footprint compared to concatenation.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " What is the purpose of the proposed BASS method?", "answer": " To help models trained on shorter recordings adapt to longer inputs or to train models from scratch in a block-wise manner.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " What is the main challenge addressed by the BASS algorithm?", "answer": " Training end-to-end speech summarization models over very long inputs.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " What improvement do we see in ROUGE-L when using BASS-Adapt with block inference over the 10-second truncated baseline?", "answer": " A nearly 4-point improvement.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " Who are mentioned in the Acknowledgements section for their helpful feedback?", "answer": " Raphael Olivier, Hira Dhamyal, and Mark Lindsey.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " What is the main focus of the paper by Sharma and Raj referenced in [1]?", "answer": " Xnor-former: Learning accurate approximations in long speech transformers.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}, {"question": " Which benchmark suite of diverse spoken language understanding tasks is mentioned in reference [6]?", "answer": " Slue phase-2.", "ref_chunk": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}], "doc_text": "metrics respectively 0.78 0.81 Finally, Table 3 sheds light on the improvement in the pre- diction of different parts of speech in the reference summary using the best BASS-ADAPT model. We observe that the pro- posed model generally improves the prediction of all parts of speech. Future work may benefit from exploring named entity prediction for summaries. 4.3. Comparison of Block-wise Adaptation Strategies Updater R-1\u2191 R-2\u2191 R-L\u2191 METEOR\u2191 BERTScore\u2191 Concat Gated Attn Hier. Attn 63.99 63.94 59.71 49.00 48.91 43.99 60.17 60.16 55.74 32.17 32.12 29.32 92.51 92.12 91.27 4.2. Block-wise Training versus Truncated Training Table 4 compares the various modeling strategies for the semantic updater. We observe that simply concatenating the semantic embedding of the previous (one) block with the current block yields significant improvements in summarization performance. Of the three updater mechanisms described in Figure 2, gated attention and concatenation appear to yield similar gains in performance, with hierarchical attention performing significantly worse. Gated attention is able to achieve similar performance while reducing having a very small memory footprint compared to concatenation. The proposed BASS method can be used to help models trained on shorter recordings adapt to longer inputs (BASS-Adapt), or to train models from scratch in a block-wise manner (BASS-Train). Inference for blockwise models can be per- formed in the standard manner, i.e., where the entire input is fed in at once to predict the final output. Alternatively, Block inference can be performed, where the input is fed as abutting blocks of input as described in Section 2.1. We train BASS-Adapt initialized from the 10-second truncated baseline to handle 30-second recordings and infer us- ing standard and block mechanisms. BASS-Adapt is compared against BASS-Train by training a model on 30- second recordings from scratch using our BASS algorithm. The latter performs worse - for training from scratch, the challenge is relatively poor initial context. Initially, the learned context is not very helpful which leads to slower convergence and poorer performance. 5. Conclusion In this paper, we address the challenge of training end-to-end speech summarization models over very long inputs. Though certain optimizations can be used to improve the upper limit on input size for summarization models, performance is lim- ited by the truncation of model inputs during training and infer- ence. We propose Block-wise Adaptive for Speech Sequences (BASS) to address this challenge - an algorithm that consumes the input in blocks and passes semantic context across blocks to encourage better learning. The BASS algorithm can be used to adapt pre-trained truncated input models to longer sequences, or train models over long sequences from scratch. We show that the proposed model outperforms truncated baselines and enables the training of speech summarization models with very long inputs. We compare both model adaptation and training strategies in Table 2. We see that the proposed BASS-Adapt approach outperforms BASS-Train on all metrics. We also observe that our proposed BASS algorithm improves over the trun- cated 10-second baseline and the truncated 30-second baseline. BASS-Adapt with block inference results in a nearly 4-point improvement in ROUGE-L over the 10-second truncated base- line, and a 1-point improvement in ROUGE-L over the trun- cated 30-second baseline. This result is comparable to that ob- tained by a truncated input baseline that takes in 60 seconds of audio, showing that our BASS model trained with 30-second 6. Acknowledgements We would like to thank Raphael Olivier, Hira Dhamyal and Mark Lindsey for their helpful feedback. This work used PSC Bridges2 and NCSA Delta through allocations CIS210014 and IRI120015 from the Advanced Cyberinfrastructure Coor- dination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 91.53 93.00 93.80 91.6 92.08 92.38 92.51 89.40 7. References [1] R. Sharma and B. Raj, \u201cXnor-former: Learning accurate approximations in long speech transformers,\u201d arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, \u201cMulti- modal Speech Summarization Through Semantic Concept Learn- ing,\u201d in Proc. Interspeech 2021, 2021, pp. 791\u2013795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, \u201cCombining relevance language modeling and clarity measure for extractive speech summarization,\u201d IEEE/ACM Trans- actions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957\u2013969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, \u201cAttention- based multi-hypothesis fusion for speech summarization,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU), 2021, pp. 487\u2013494. [5] \u2014\u2014, \u201cIntegrating multiple asr systems into nlp backend with at- tention fusion,\u201d in ICASSP 2022 - 2022 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237\u20136241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, \u201cSlue phase-2: A benchmark suite of diverse spoken language understanding tasks,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, \u201cEnd-to-end speech summarization using restricted self-attention,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072\u20138076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, \u201cLeveraging large text corpora for end-to-end speech summarization,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long- document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, \u201cBlockwise Stream- ing Transformer for Spoken Language Understanding and Simul- taneous Speech Translation,\u201d in Proc. Interspeech 2022, 2022, pp. 1746\u20131750. [12] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193\u2013199. [13] N. Moritz, T. Hori, and J. Le, \u201cStreaming automatic speech recog- nition with the"}