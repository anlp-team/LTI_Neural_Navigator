{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Toward_Universal_Speech_Enhancement_For_Diverse_Input_Conditions_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the STFT and iSTFT window and hop sizes?", "answer": " The STFT window size is 32 ms and the iSTFT window size is 16 ms.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What are the embedding dimension D and bottleneck dimension N set to?", "answer": " The embedding dimension D is set to 256 and the bottleneck dimension N is set to 64.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What is the hidden dimension H in each TAC module?", "answer": " The hidden dimension H in each TAC module is 192.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " How many memory tokens are empirically set when processing input signals?", "answer": " The number of memory tokens G is set to 20.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " How is the input signal processed when memory tokens are applied?", "answer": " The input signal is divided into non-overlapping segments of approximately 1 second long (64 frames).", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What optimizer is used for training the models?", "answer": " The models are trained using the Adam optimizer.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What is the STFT window size used for the multi-resolution L1 loss?", "answer": " The STFT window sizes for the multi-resolution L1 loss are {256, 512, 768, 1024}.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What loss function is used for speech separation?", "answer": " The models are trained using the SI-SNR loss [37] for speech separation.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What metrics are used to evaluate the SE models?", "answer": " The SE models are evaluated using wide-band PESQ (PESQ-WB), STOI, SI-SNR, SDR, DNS-MOS (OVRL), and WER.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}, {"question": " What is the SI-SNR improvement reported in Table 2 for the TFPSNet model on WSJ0-2mix test data?", "answer": " The SI-SNR improvement reported is 21.1 for 8 kHz and 21.0 for 16 kHz.", "ref_chunk": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}], "doc_text": "The STFT/iSTFT window and hop sizes are always 32 and 16 ms, respectively. Following TFPSNet [28], the embedding dimension D is set to 256 and the bottleneck dimension N to 64. The transformer layers in the multi-path blocks have the same configuration as in [28]. The hidden dimension H in each TAC module is 192. When processing multi-channel data, we always take the first channel as the reference channel. When the memory tokens in Section 2.4 are applied, we empirically set the number of memory tokens G to 20, and divide the input signal into non-overlapping seg- ments of \u223c1s long (64 frames). The total number of model parame- ters is around 3.1 millon. The pre-trained models and configurations will be released later in ESPnet [27] for reproducibility. Our experiments are done based on the ESPnet toolkit [27]. The models are trained using the Adam optimizer, and the learning rate increases linearly to 4e-4 in the first X steps and then decreases by half when the validation performance does not improve for two con- secutive epochs. We set X to 4000 and 25000 for speech separation and enhancement experiments, respectively. During training, we di- vide the samples into 4-second chunks to reduce memory costs. The batch size of all experiments is 4. We also limit the number of sam- ples for each epoch to 8000. When training on multi-channel data, we shuffle the channel permutation of each sample and randomly select the number of channels (up to 4 channels) to increase diver- sity. We always apply variance normalization to the input signal and revert the variance in the model\u2019s output. For speech separation, all models are trained until convergence (up to 150 epochs) using the SI- SNR loss [37]; and for speech enhancement, we train all models for up to 20 epochs6 using the loss function proposed in [38]. The loss function is a scale-invariant multi-resolution L1 loss in the frequency domain plus a time-domain L1 loss term. We set the STFT window sizes of the multi-resolution L1 loss to {256, 512, 768, 1024} and the time-domain loss weight to 0.5. In each experiment, the model with the best validation performance is selected for evaluation. We evaluate the SE models with the metrics below: wide-band PESQ (PESQ-WB) [39], STOI [40], scale-invariant signal-to-noise ratio (SI-SNR) [37], signal-to-distortion ratio (SDR) [41], DNS- MOS (OVRL)7 [42], and word error rate (WER). Except for WER, a higher value indicates better performance for all metrics. The 6For DNS1 data alone, we only train for up to 5 epochs due to the large amount of data, which are enough for the model to converge. 7https://github.com/microsoft/DNS-Challenge/ blob/master/DNSMOS/DNSMOS/sig_bak_ovr.onnx Table 2: Speech separation performance on WSJ0-2mix and its spa- tialized version (min mode). All models are trained only on 8 kHz data, and tested on 8 kHz and 16 kHz data. Model SI-SNRi ( 8 kHz / 16 kHz ) WSJ0-2mix test data 21.1 21.0 - 20.3 20.9 1ch spatialized test data 18.9 19.9 2ch spatialized test data 24.6 36.1 TFPSNet [28] TFPSNet (reproduced) + SFI STFT/iSTFT USES (1ch) + mem tokens (1ch) / / / / / USES (1-2ch) + mem tokens (1-6ch) / / USES (1-2ch) + mem tokens (1-6ch) 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 12.0 (resampling) 19.7 19.8 19.3 18.4 18.3 24.2 35.0 Whisper Large v2 model8 [43] is used for WER evaluation. 3.3. Evaluation of speech separation performance We first examine the effectiveness of the three components proposed in Section 2 by evaluating the speech separation performance on WSJ0-2mix, which makes the comparison with the top-performing TFPSNet [28] convenient. In addition, the datasets are not large, making it easy to investigate different setups. We train the model on 8 kHz mixture data, and evaluate the performance on both 8 and 16 kHz test data. Table 2 reports the SI-SNR improvement (SI-SNRi) of models trained on a single channel (denoted as 1ch) and on a vari- able number of channels (denoted as 1-2ch and 1-6ch). From the first section in Table 2, we observe that the proposed SFI approach can successfully preserve strong separation performance for the re- produced TFPSNet9 on 16 kHz data. In comparison, first down- sampling 16 kHz mixtures to 8 kHz, then applying TFPSNet trained at 8 kHz for separation, and finally upsampling the separation re- sults to 16 kHz (denoted as \u201cresampling\u201d in the second row) suffer from severe SI-SNR degradation. The proposed USES model also obtains similar performance to TFPSNet on WSJ0-2mix, as our ma- jor modifications focus on the invariance to sampling frequencies, microphone channels, and signal lengths. While applying memory tokens does not change the performance significantly, it enables the model to process variable-length inputs with a constant memory cost during inference. For experiments on the spatialized WSJ0-2mix data, we can see that the model achieves very similar performance on both 8 and 16 kHz data. Although the single-channel SI-SNR performance de- grades \u223c1.4 dB, the multi-channel performance becomes much bet- ter, even with only 2 input channels. Further applying the memory tokens and increasing the input channels during training can improve the performance, especially for the multi-channel case. 3.4. Evaluation of SE performance in a single condition Given the success of the universal properties of USES in a controlled experimental condition in Section 3.3, this subsection compares the SE performance of the proposed model with memory tokens to ex- isting methods on the DNS1 non-blind test data. Our models are respectively trained on the simulated DNS1 v1 and v2 data without reverberation, as described in Table 1. Due to the large amount of training data and limited time, we only train the proposed model for \u223c0.5 epochs on DNS1 v2 data, covering around 45% of the entire 8https://huggingface.co/openai/whisper-large-v2 9Different from [28], our reproduction replaces all T-F path scanning transformers with the time-path scanning transformer. Table 4: Speech enhancement performance of the proposed model in diverse conditions. The models are by default trained only on 8 kHz data, and tested with the original sampling frequencies. \u201cnoisy\u201d"}