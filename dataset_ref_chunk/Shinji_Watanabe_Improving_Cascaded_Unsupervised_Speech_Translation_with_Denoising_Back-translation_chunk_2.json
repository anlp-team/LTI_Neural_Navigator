{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Cascaded_Unsupervised_Speech_Translation_with_Denoising_Back-translation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is one of the main challenges faced by speech-to-text (ST) systems?", "answer": " The lack of parallel data.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " What was the approach used by the first US2TT system to map speech into written words?", "answer": " Learning cross-modal alignment and using cross-lingual embedding.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " How did the system developed by Wang et al. (2022) train an end-to-end US2ST system?", "answer": " They generated pseudo labels from a cascaded system to train the end-to-end US2ST system.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " What is the main input for the UASR system?", "answer": " Audio features or representations.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " How did Baevski et al. (2021) improve UASR performance in multiple languages?", "answer": " They proposed wav2vec-U, building the GAN-based UASR framework on top of the representation from wav2vec 2.0.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " What did the first fully unsupervised neural machine translation (UNMT) model adopt?", "answer": " A seq2seq model.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " How did XLM (Lample and Conneau, 2019) improve UNMT?", "answer": " By adopting masked language modeling pretraining to initialize the encoder and decoder.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " How did researchers attempt to improve the robustness of Neural Machine Translation models?", "answer": " By training or fine-tuning the translation model on the target domain.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " What were some methods proposed to train an unsupervised text-to-speech (UTTS) system?", "answer": " Training on pseudo labels generated from UASR systems.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}, {"question": " What is the purpose of the denormalization module in the US2ST architecture?", "answer": " To transform generated word sequences back into sequences with punctuation marks and capitalized words.", "ref_chunk": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}], "doc_text": "challenge of ST systems is the lack of parallel data. The unsupervised approach for ST is limited, but some progress has been made. The \ufb01rst US2TT system intended to learn the cross- modal alignment to map speech into written words, and used cross-lingual embedding to align words in different languages Chung et al. (2018, 2019b). Ad- ditionally, concurrent work built a US2ST system by combining SOTA UASR, UNMT, and UTTS to build a cascaded US2ST system. Further, they generated pseudo labels from the cascaded system to train an end-to-end US2ST system (Wang et al., 2022). 2.2 UASR UASR takes audio features or representations as input and generates phoneme sequences without supervision. To tackle the challenging problem, Liu et al. (2018) \ufb01rst came out with the idea of applying a Generative Adversarial Network (GAN) (Goodfellow et al., 2020). However, phoneme-level boundaries are required to segment the audio and construct embedding sequences. (Chen et al., 2019) breaks the limit by iteratively re\ufb01ning the audio segments with Hidden Markov Model (HMM) and GAN, achieving complete UASR. Recently, Baevski et al. (2021) proposed wav2vec-U, building the GAN-based UASR frame- work on top of the representation from wav2vec 2.0 (W2V2) (Baevski et al., 2020), a self-supervised speech model. The results outperformed previ- ous SOTA, and are even comparable with some of the best-known supervised methods. More- over, the original paper has shown that with the cross-lingual pre-trained version of W2V2 (Con- neau et al., 2020), UASR in other languages is also available. The follow-up work, wav2vec-U 2.0 (Liu et al., 2022a), enabled the model to be trained end-to-end with the simpli\ufb01ed pipeline and the improved training objective. 2.3 UNMT The \ufb01rst fully unsupervised neural machine trans- lation model adopted a seq2seq model, and the encoder mapped monolingual corpus in two lan- guages to a shared latent space via adversarial train- ing. The decoder learned to reconstruct in both lan- guages from the latent representations by denoising autoencoding loss and online back-translation loss (Lample et al., 2017). Recently, cross-lingual language model pretrain- ing brought large progress to UNMT. XLM (Lam- Figure 1: The framework of our cascade US2ST. ple and Conneau, 2019) \ufb01rst adopted masked lan- guage modeling pretraining to initialize the en- coder and decoder, and then used back-translation loss together with denoising autoencoding loss to \ufb01ne-tune the whole seq2seq model. MASS (Song et al., 2019) further used masked seq2seq pretrain- ing to pretrain encoder and decoder jointly to re- duce the discrepancy between pretraining and \ufb01ne- tuning. Their proposed method can align two lan- guages with only back-translation loss. MASS outperforms XLM and all the other previous SOTA UNMT models in several language pairs. 2.4 Robust NMT Traditional NMT and UNMT models are trained with clean input, thus small perturbations can greatly degrade the performance (Belinkov and Bisk, 2017). To improve the robustness, Di Gangi et al. (2019); Sperber et al. (2017) directly trained or \ufb01ne-tuned the translation model on the target domain. In spite of the improvement they brought, these methods still depend on certain levels of paired data. Directly training a UTTS without any supervision from paired data seems to be extremely hard. How- ever, with recent success in UASR, UTTS might be accomplished in another way\u2014training on the pseudo labels generated from UASR systems (Ni et al., 2022; Liu et al., 2022b). 3 Methods Figure 1 shows the architecture overview of our pro- posed approach to unsupervised speech-to-speech translation (US2ST). We split US2ST into three stages: unsupervised speech recognition (UASR), unsupervised machine translation (UNMT), and un- supervised speech synthesis (UTTS). The modules were trained separately but all in an unsupervised manner. During inference, we form the functional- ity of S2ST by concatenating them. Furthermore, we proposed denoising back-translation, to mitigate the error propagation and the domain mismatch is- sues between UASR and UMT submodules. As for the unsupervised scenario, Sun et al. (2020) intended to improve the robustness of UNMT by applying some perturbation on posi- tional embedding and word embedding to the input. The model learned to reconstruct the original input via denoising autoencoding loss and adversarial training. 2.5 TTS and UTTS Some works intend to improve the performance of TTS through unlabeled data. For instance, pre-training the encoder/decoder (Chung et al., 2019a); utilizing the dual nature of TTS and ASR tasks (Ren et al., 2019); applying variational auto- encoder to learn from speech disentanglement (Lian et al., 2022). 3.1 Base Architecture UASR We conducted our UASR subsystem fol- lowing wav2vec-U (Baevski et al., 2021), and our code is based on their implementation in fairseq3. Besides its breakthrough performance on UASR in multiple languages, the robustness and stabilities across different corpora have also been well ana- lyzed (Lin et al., 2022). Thus, We mainly follow the data preparation procedure, model architecture, and the training objective of wav2vec-U. During inference, the model takes the preprocessed audio features as input and generates phoneme sequences. To further obtain word-level sequences, we adopt 3 https://github.com/facebookresearch/fairseq 3 different decoding strategies, such as lexicon-based kenlm decoder and the weighted \ufb01nite-state trans- ducer (WFST; Mohri et al. (2002)). Self-training techniques on HMM are also applied for better performance. TDN The outputs from the UASR are normalized word sequences. The UNMT model, however, may rely on punctuation marks and capital letters. For better performance, we also learned a text denor- malizer to transform the generated word sequences back into sequences with punctuation marks and capitalized words, namely, denormalization. The module was a transformer-based seq2seq model. We \ufb01rst formed the paired data by normalizing raw text sentences and then trained the model with cross-entropy loss. UNMT UNMT aims to map sentences from source language S to target language T without leveraging any paired data. We conduct UNMT by following the architecture and pretrain process of MASS (Song et al., 2019), which is a transformer- based seq2seq language model. During the \ufb01ne- tuning process, we use denoising back-translation plus denoising autoencoder objective to align two languages and increase robustness. UTTS We conducted UTTS by following the ar- chitecture of Variational"}