{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Making_Scalable_Meta_Learning_Practical_chunk_10.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What configuration was used for the ImageNet-1k data pruning experiment?", "answer": " batch size of 256 and the hyperparameters shown in Table 6", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " How many epochs was CIFAR-10 training run for?", "answer": " 200 epochs", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What baselines were adopted for the ImageNet-1k experiments?", "answer": " EL2N, GraNd, DynaMS", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What GPU was used for the CIFAR-10 experiments?", "answer": " 1 NVIDIA RTX 2080Ti GPU", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What is the uncertainty U measured as?", "answer": " the difference between the predictions of the current model and the exponentially-moving-averaged model", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What adaptation matrix was provided for the Adam optimizer?", "answer": " Adaptation matrix for the gradient in Adam", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What question did the study in section D aim to address?", "answer": " whether scaling up the network size can improve few-shot generalization capability", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What task were the preliminary experiments in section D conducted on?", "answer": " Omniglot 20-way 1-/5-shot tasks", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What did the experiment results in section D show about model size and few-shot classification accuracy?", "answer": " increased model size led to consistent improvements in few-shot classification accuracy", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}, {"question": " What does section E aim to study the empirical effect of?", "answer": " identity approximation for base Jacobian on the meta gradient and the optimal meta solution", "ref_chunk": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}], "doc_text": "batch size of 256 and the configuration shown in Table 6 below. After pruning data based on the meta learning result, we ran ImageNet-1k 4https://github.com/leopard-ai/betty/tree/main/examples 16 training for 120 epochs with the learning rate decayed by 10 at epochs [40, 80] following the set up in DynaMS [63]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-50 SGD 1e-1 None 1e-4 ImageNet-1k train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 ImageNet-1k train set N/A N/A Table 6: Hyperparameters for ImageNet-1k data pruning experiments For the CIFAR-10 data pruning experiment, we ran meta learning for 50 epochs with a batch size of 128, and configuration in Table 7 below. After pruning the data based on the meta learning result, we ran CIFAR-10 training for 200 epochs with the cosine learning rate decay schedule following the setup in DeepCore [22]. model optimizer init_lr lr_scheduler wdecay dataset unroll step SAMA \u03b1 Base ResNet-18 SGD 1e-1 None 5e-4 CIFAR-10 train set 2 1.0 Meta 2-layer MLP Adam 1e-5 None 0 CIFAR-10 train set N/A N/A Table 7: Hyperparameters for CIFAR-10 data pruning experiments Baselines We adopt EL2N [47], GraNd [47], DynaMS [63] as our baselines for the ImageNet- 1k experiments and GraNd [47], forgetting [61], margin [8] for the CIFAR-10 experiments. In detail, EL2N/GraND [47] respectively select samples with large L2-loss/gradient-norm values, forgetting [47] chooses samples that are frequently forgotten during training, and margin [8] chooses samples with least confidence. While these baselines are considered static pruning, DynaMS [63] falls under the category of dynamic pruning where data to be pruned change during training. Dynamic pruning may see the whole training data across different epochs, making a fair comparison difficult. Surprisingly, despite being a static pruning algorithm, SAMA-based data pruning still achieves a better performance than DynaMS. Compute Resources We used 4 NVIDIA Tesla V100 GPUs for Imagenet-1k data pruning meta learning experiments and 1 NVIDIA RTX 2080Ti GPU for CIFAR-10 experiments. Additional Information We measured the uncertainty U via the difference between the predictions of the current model and the exponentially-moving-averaged model. 17 C Algorithmic Adaptation for Adam Optimizer Since the Adam optimizer [32] has been the most popular optimizer to train large models, exemplified by Transformers [68], here we provide the adaptation matrix for Adam. We denote the first and second moments of the gradient in Adam as m and v respectively, and the learning rate as \u03b3. \u2202uadam \u2202g = \u2202u \u2202g (cid:18) \u03b3 \u03b21m + (1 \u2212 \u03b21)g (cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + \u03f5 (cid:19) = \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg + (1 \u2212 \u03b21)\u03f5(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 \u2248 \u03b3 (1 \u2212 \u03b21)\u03b22v \u2212 (1 \u2212 \u03b21)\u03b22mg (cid:112)\u03b21v + (1 \u2212 \u03b21)g2(cid:0)(cid:112)\u03b21v + (1 \u2212 \u03b21)g2 + e(cid:1)2 (because \u03f5 \u226a 1) Adaptation matrices can be similarly derived for other adaptive optimizers. D The Effect of Scaling in Model-Agnostic Meta Learning Since the inception of MAML [14], a myriad of algorithms have been proposed to improve few-shot image classification while assuming a fixed network architecture. In contrast, here we shift our focus from the algorithm to the scale, and propose to study the following question: \u201cLeveraging the compute/memory efficiency of SAMA, can we improve the few-shot generalization capability by scaling up the network size?\u201d. Since SAMA is a variant of implicit differentiation, we closely follow the experiment setup in iMAML [51], where proximity to the initialization weights is explicitly enforced by L2-regularization. The major difference is that iMAML uses a conjugate-gradient-based method, which requires second-order gradient information to compute meta gradients, while we adopt SAMA to achieve improved scaling to larger networks with its superior memory/compute efficiency. We conduct preliminary experiments on the Omniglot 20-way 1-/5-shot tasks with the basic 4-layer CNN architecture, while varying the width (hidden size) of the networks to study the effect of the model size on the few-shot classification accuracy. The experiment results are provided in Figure 4 below. 90 93 92 26 27 96Accuracy 28 Omniglot 20-Way 1-Shot w/ 4-layer CNN 25 91 211Width (Hidden Size) 210 94 29 95 25 97.5 210 97.0 29 96.5 Omniglot 20-Way 5-Shot w/ 4-layer CNN 26 99.5 100.0Accuracy 98.5 99.0 98.0 27 28 211Width (Hidden Size) Figure 4: Few-shot image classification accuracy on Omniglot 20-way 1-/5-shot tasks with varying network sizes. Interestingly, we observe that the increased model size leads to consistent improvements in few-shot classification accuracy. The important question following this observation is \u201ccan we apply scaling laws [30] from other tasks (e.g., language modeling) to general meta learning beyond few-shot image classification?\u201d Since meta learning involves two optimization problems (meta and base) unlike traditional machine learning problems, it is as of now unclear how to define the general concept of \u201cscale\u201d in terms of both model and dataset sizes. We expect that further research in this direction would be critical in systematically studying scalable meta learning. 18 E Justification of the Identity Approximation for Base Jacobian In this section, we aim to study the empirical effect of our identity approximation for base Jacobian on the meta gradient \u2202Lmeta and the optimal meta solution \u03bb\u2217, when the true base Jacobian is not an identity matrix. As obtaining the closed-form solution of the Hessian is impossible in almost all deep learning problems, we study the soundness of the identity approximation of base Jacobian in the simpler \u201cbiased regression\u201d setting [19], for which the bilevel optimization formulation is as follows: \u2202\u03bb \u03bb\u2217 = arg min \u2225X \u2032w\u2217(\u03bb) \u2212 y\u2032\u22252 \u03bb w\u2217(\u03bb) = arg min w \u2225Xw \u2212 y\u22252 + \u03b2\u2225w \u2212 \u03bb\u22252 Given the above formulation, the closed-form solutions for the base Jacobian, the meta-gradient g\u03bb, and the optimal meta solution \u03bb\u2217 are: 1. Base Jacobian = X T X + \u03b2I 2. g\u03bb = \u03b2(X T X + \u03b2I)\u22121(X \u2032T X \u2032w\u2217 \u2212 X \u2032T y\u2032), where w\u2217 = (X T X + \u03b2I)\u22121(X T y + \u03b2\u03bb) 3. \u03bb\u2217 = (AT A)\u22121AT b, where"}