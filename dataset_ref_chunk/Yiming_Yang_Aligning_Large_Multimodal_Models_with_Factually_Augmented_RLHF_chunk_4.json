{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_Aligning_Large_Multimodal_Models_with_Factually_Augmented_RLHF_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the study mentioned in the text?", "answer": " The study focuses on evaluating hallucinations in Language Model Multimodal (LMM) responses.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " Which network architecture is the LLM based on in this study?", "answer": " The LLM is based on Vicuna and utilizes the pre-trained CLIP visual encoder, ViT-L/14.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " What is the purpose of incorporating the response length as an auxiliary penalizing factor?", "answer": " The response length, measured in the number of tokens, is incorporated as a penalizing factor to control hallucinations in LMMs.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " How do the RL models in the study differ from the base LLaVA model?", "answer": " The RL models have similar architectures to the base LLaVA model, but the reward model outputs a scalar value indicating the reward of the response.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " How do the MMHAL-BENCH evaluation metrics differ from previous VLM benchmarks?", "answer": " The MMHAL-BENCH evaluation metrics focus on detecting hallucinations in LMM responses, while previous benchmarks evaluate response quality in a more general sense.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " What types of questions are included in the MMHAL-BENCH to evaluate LMM responses?", "answer": " The MMHAL-BENCH includes questions related to object attributes, adversarial objects, comparisons, counting, spatial relations, environment, holistic descriptions, and others.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " How are the image-question pairs designed in the MMHAL-BENCH to ensure hallucinations in LMM responses?", "answer": " The image-question pairs are designed to cause hallucinations in LMM responses, ensuring that LMMs make false claims about the image contents when answering these questions.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " How do researchers avoid data leakage or evaluation on seen data when creating the MMHAL-BENCH?", "answer": " Researchers select images from the validation and test sets of OpenImages and design brand-new questions to avoid data leakage or evaluation on data that LMMs have observed during training.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " What method is employed to analyze and rate LMM responses on the MMHAL-BENCH?", "answer": " The powerful GPT-4 model is used to analyze and rate LMM responses by comparing them against image contents and human-generated answers provided in the prompt.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}, {"question": " Why is it important to use GPT-4 to aid in evaluating LMM responses on the MMHAL-BENCH?", "answer": " GPT-4 is used to determine whether hallucinations exist in LMM responses by comparing them against image contents and human-generated answers, as the GPT-4 model can only assess text inputs.", "ref_chunk": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}], "doc_text": "(Sun et al., 2023b; Zheng et al., 2023), they tend to introduce more hallucinations for LMMs. In this work, we follow Sun et al. (2023a) and incorporate the response length, measured in the number of tokens, as an auxiliary penalizing factor. 3 EXPERIMENTS 3.1 NEURAL ARCHITECTURES Base Model We adopt the same network architecture as LLaVA (Liu et al., 2023a). Our LLM is based on Vicuna (Touvron et al., 2023a; Chiang et al., 2023), and we utilize the pre-trained CLIP visual encoder, ViT-L/14 (Radford et al., 2021). We use grid features both before and after the final Transformer layer. To project image features to the word embedding space, we employ a linear layer. It\u2019s important to note that we leverage the pre-trained checkpoints of the linear projection matrix from LLaVA, concentrating on the end-to-end fine-tuning phase for multi-modal alignment in our study. For LLaVA-SFT+-7b, we use a Vicuna-V1.5-7b LLM and ViT-L/14 with image resolution 256 \u00d7 256. For LLaVA-SFT+-13b, we use a Vicuna-V1.5-13b LLM and ViT-L/14 with image resolution 336 \u00d7 336. RL Models: Reward, Policy, and Value The architecture of the reward model is the same as the base LLaVA model, except that the embedding output of the last token is linearly projected to a scalar value to indicate the reward of the whole response. Following Dubois et al. (2023), we initialize the value model from the reward model. Therefore, when training an LLaVA-7B-based policy model with an LLavA-13B-based reward model, the value model is also of 13B size. To fit all the models (i.e., police, reward, value, original policy) into one GPU, we adopt LoRA (Hu et al., 2021) for all the fine-tuning processes in RLHF. We use Proximal Policy Optimization (PPO; 6 Preprint Table 3: Automatic evaluation of LLaVA-RLHF on the LLaVA-Bench Evaluation. GPT-4 compares the answers from the VLM model outputs with the answers by GPT-4 (text-only) and gives a rating. We report the relative scores (Liu et al., 2023a) of VLM models compared to GPT-4 (text-only). Subsets Model Conv Detail Complex Full-Set LLaVA7B VIGC7B LLaVA-SFT+ 7B LLaVA-RLHF7B 75.1 83.3 88.8 93.0 75.4 80.6 74.6 79.0 92.3 93.1 95.0 109.5 81.0 85.8 86.3 94.1 LLaVA13BX336 VIGC13BX336 LLaVA-SFT+ 13B\u00d7336 LLaVA-RLHF13B\u00d7336 87.2 88.9 85.8 93.9 74.3 77.4 75.5 82.5 92.9 93.5 93.9 110.1 84.9 86.8 85.2 95.6 Figure 2: Detailed performance of different models on the eight categories in MMHAL-BENCH, where \u201cOverall\u201d indicates the averaged performance across all categories. The questions are col- lected by adversarially filtering on the original LLaVA13BX336 model. Schulman et al. (2017)) with a KL penalty for the RL training. Without further notice, both LLaVA- RLHF-7b and LLaVA-RLHF-13b are trained with a LLaVA-SFT+-13b initialized reward model. More details can be found in Appendix F. 3.2 MMHAL-BENCH DATA COLLECTION To quantify and evaluate the hallucination in LMM responses, we have created a new benchmark MMHAL-BENCH. There are two major differences between MMHAL-BENCH and previous VLM benchmarks: 1) Speciality: In contrast to prevalent LMM benchmarks Liu et al. (2023a;b); Li et al. (2023d) that evaluate the response quality in the general sense (e.g., helpfulness, relevance), we focus on determining whether there hallucination exists in the LMM responses. Our evaluation metrics are directly developed on this main criterion. 2) Practicality: Some previous LMM bench- marks Li et al. (2023d); Rohrbach et al. (2018) also examine hallucination, but they have limited the questions to yes/no questions, which we found the results may sometimes disagree with the de- tailed description generated by LMM. Instead of over-simplifying the questions, we adopt general, realistic, and open-ended questions in our MMHAL-BENCH, which can better reflect the response quality in practical user-LMM interactions. 7 Preprint In MMHAL-BENCH, we have meticulously designed 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics. More specifically, we have observed that LMM often make false claims about the image contents when answering some types of questions, and thus design our questions according to these types: Object attribute: LMMs incorrectly describe the visual attributes of invididual objects, such as color and shape. Adversarial object: LMMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found. Comparison: LMMs incorrectly compare the attributes of multiple objects. \u2022 Counting: LMMs fail to count the number of the named objects. \u2022 Spatial relation: LMMs fail to understand the spatial relations between multiple objects in the response. Environment: LMMs make wrong inference about the environment of the given image. \u2022 Holistic description: LMMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image. Others: LMMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information. We create and filter the questions in an adversarial manner. More specifically, we design the image- question pairs to ensure that the original LLaVA13BX336 model hallucinates when answering these questions. While these questions are initially tailored based on LLaVA13BX336\u2019s behavior, we have observed that they also have a broader applicability, causing other LMMs to hallucinate as well. To avoid data leakage or evaluation on data that LMMs have observed during training, we select im- ages from the validation and test sets of OpenImages (Kuznetsova et al., 2020) and design all brand- new questions. Our image-question pairs cover 12 common object meta-categories from COCO (Lin et al., 2014), including \u201caccessory\u201d, \u201canimal\u201d, \u201cappliance\u201d, \u201celectronic\u201d, \u201cfood\u201d, \u201cfurniture\u201d, \u201cin- door\u201d, \u201ckitchen\u201d, \u201coutdoor\u201d, \u201cperson\u201d, \u201csports\u201d, and \u201cvehicle\u201d. When evaluating LMMs on MMHAL-BENCH, we employ the powerful GPT-4 model (OpenAI, 2023) to analyze and rate the responses. Currently, the publically available GPT-4 API only sup- ports text input, so it cannot judge directly based on the image contents. Therefore, to aid GPT-4\u2019s assessment, we also provide category names of the image content, and a standard human-generated answer in the prompt, in addition to the question and LMM response pair. Consequently, GPT-4 can determine whether hallucination exists in the LMM response by comparing it against the image content and the thorough human-generated answer. When provided"}