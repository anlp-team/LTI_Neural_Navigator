{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yiming_Yang_DIFUSCO:_Graph-based_Diffusion_Solvers_for_Combinatorial_Optimization_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the forward process defined as for models with multinomial noises?,answer: The forward process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt)", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " How is the transition probability matrix denoted in the context of models with multinomial noises?,answer: The transition probability matrix is denoted as Qt.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " How is the posterior at time t \u2212 1 obtained in the context of models with multinomial noises?,answer: The posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " What is the denoising neural network trained to predict in models with multinomial noises?,answer: The denoising neural network is trained to predict the clean data.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " How is the continuous diffusion process defined for discrete data?,answer: The continuous diffusion process is defined by lifting the discrete input into a continuous space.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " What is the denoising neural network trained to predict in continuous diffusion models?,answer: The denoising neural network is trained to predict the unscaled Gaussian noise.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " How is the denoising schedule for fast inference aimed at speeding up the inference process?,answer: The denoising schedule for fast inference aims to reduce the number of steps in the reverse diffusion process.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " What type of network is adopted as the backbone network for both discrete and continuous diffusion models?,answer: An anisotropic graph neural network with edge gating mechanisms is adopted as the backbone network.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " What is the feature propagated using the anisotropic message passing scheme for the next layer in the network?,answer: The features at the next layer are propagated with an anisotropic message passing scheme using node and edge features.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}, {"question": " How are the sinousoidal features utilized in the network?,answer: The sinousoidal features are used for the denoising timestep.", "ref_chunk": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}], "doc_text": "models with multinomial noises [5, 44], the forward (cid:20)(1 \u2212 \u03b2t) \u03b2t (cid:21) \u03b2t (1 \u2212 \u03b2t) process is defined as: q(xt|xt\u22121) = Cat (xt; p = \u02dcxt\u22121Qt) , where Qt = the transition probability matrix; \u02dcx \u2208 {0, 1}N \u00d72 is converted from the original vector x \u2208 {0, 1}N with a one-hot vector per row; and \u02dcxQ computes a row-wise vector-matrix product. Here, \u03b2t denotes the corruption ratio. Also, we want (cid:81)T The t-step marginal can thus be written as: q(xt|x0) = Cat (cid:0)xt; p = \u02dcx0Qt Q1Q2 . . . Qt. And the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c Uniform(\u00b7). (cid:1) , where Qt = q(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0) q(xt|x0) = Cat (cid:32) xt\u22121; p = \u02dcxtQ\u22a4 t \u2299 \u02dcx0Qt\u22121 \u02dcx0Qt \u02dcx\u22a4 t (cid:33) , where \u2299 denotes the element-wise multiplication. According to Austin et al. [5], the denoising neural network is trained to predict the clean data p\u03b8((cid:101)x0|xt), and the reverse process is obtained by substituting the predicted (cid:101)x0 as x0 in Eq. 5: p\u03b8(xt\u22121|xt) = (cid:88) q(xt\u22121|xt, (cid:101)x0)p\u03b8((cid:101)x0|xt) (cid:101)x Continuous Diffusion for Discrete Data The continuous diffusion models [102, 40] can also be directly applied to discrete data by lifting the discrete input into a continuous space [16]. Since the continuous diffusion models usually start from a standard Gaussian distribution \u03f5 \u223c N (0, I), Chen et al. [16] proposed to first rescale the {0, 1}-valued variables x0 to the {\u22121, 1} domain as \u02c6x0, and then treat them as real values. The forward process in continuous diffusion is defined as: q(\u02c6xt|\u02c6xt\u22121) := N (\u02c6xt; Again, \u03b2t denotes the corruption ratio, and we want (cid:81)T t-step marginal can thus be written as: q(\u02c6xt|\u02c6x0) := N (\u02c6xt; and \u00af\u03b1t = (cid:81)t \u221a 1 \u2212 \u03b2t \u02c6xt\u22121, \u03b2tI). t=1(1 \u2212 \u03b2t) \u2248 0 such that xT \u223c N (\u00b7). The \u00af\u03b1t \u02c6x0, (1 \u2212 \u00af\u03b1t)I) where \u03b1t = 1 \u2212 \u03b2t \u03c4 =1 \u03b1\u03c4 . Similar to Eq. 5, the posterior at time t \u2212 1 can be obtained by Bayes\u2019 theorem: \u221a q(\u02c6xt\u22121|\u02c6xt, x0) = q(\u02c6xt|\u02c6xt\u22121, \u02c6x0)q(\u02c6xt\u22121|\u02c6x0) q(\u02c6xt|\u02c6x0) , which is a closed-form Gaussian distribution [40]. In continuous diffusion, the denoising neural network is trained to predict the unscaled Gaussian noise (cid:101)\u03f5t = (\u02c6xt \u2212 1 \u2212 \u00af\u03b1t = f\u03b8(\u02c6xt, t). The reverse process [40] can use a point estimation of \u02c6x0 in the posterior: \u221a \u221a \u00af\u03b1t \u02c6x0)/ p\u03b8(\u02c6xt\u22121|\u02c6xt) = q (cid:18) \u02c6xt\u22121|\u02c6xt, \u02c6xt \u2212 \u221a 1 \u2212 \u00af\u03b1tf\u03b8(\u02c6xt, t) \u221a \u00af\u03b1t (cid:19) For generating discrete data, after the continuous data \u02c6x0 is generated, a thresholding/quantization operation is applied to convert them back to {0, 1}-valued variables x0 as the model\u2019s prediction. 4 (4) is (5) (6) (7) (8) 3.3 Denoising Schedule for Fast Inference One way to speed up the inference of denoising diffusion models is to reduce the number of steps in the reverse diffusion process, which also reduces the number of neural network evaluations. The denoising diffusion implicit models (DDIMs) [101] are a class of models that apply this strategy in the continuous domain, and a similar approach can be used for discrete diffusion models [5]. Formally, when the forward process is defined not on all the latent variables x1:T , but on a subset {x\u03c41 , . . . , x\u03c4M }, where \u03c4 is an increasing sub-sequence of [1, . . . , T ] with length M , x\u03c41 = 1 and x\u03c4M = T , the fast sampling algorithms directly models q(x\u03c4i\u22121|x\u03c4i, x0). Due to the space limit, the detailed algorithms are described in the appendix. We consider two types of denoising scheduled for \u03c4 given the desired card(\u03c4 ) < T : linear and cosine. The former uses timesteps such that \u03c4i = \u230aci\u230b for some c, and the latter uses timesteps such that \u03c4i = \u230acos( (1\u2212ci)\u03c0 ) \u00b7 T \u230b for some c. The intuition for the cosine schedule is that diffusion models can achieve better generation quality when iterating more steps in the low-noise regime [85, 121, 13]. 2 3.4 Graph-based Denoising Network The denoising network takes as input a set of noisy variables xt and the problem instance s and predicts the clean data (cid:101)x0. To balance both scalability and performance considerations, we adopt an anisotropic graph neural network with edge gating mechanisms [9, 54] as the backbone network for both discrete and continuous diffusion models, and the variables in the network output can be the states of either nodes, as in the case of Maximum Independent Set (MIS) problems, or edges, as in the case of Traveling Salesman Problems (TSP). Our choice of network mainly follows previous work [54, 92], as AGNN can produce the embeddings for both nodes and edges, unlike typical GNNs such as GCN [62] or GAT [107], which are designed for node embedding only. This design choice is particularly beneficial for tasks that require the prediction of edge variables. Anisotropic Graph Neural Networks Let h\u2113 ij denote the node and edge features at layer \u2113 associated with node i and edge ij, respectively. t is the sinusoidal features [106] of denoising timestep t. The features at the next layer is propagated with an anisotropic message passing scheme: i and e\u2113 \u02c6e\u2113+1 ij + Q\u2113h\u2113 ij = P \u2113e\u2113 ij + MLPe(BN(\u02c6e\u2113+1 e\u2113+1 ij = e\u2113 i = h\u2113 h\u2113+1 i + R\u2113h\u2113 j, )) + MLPt(t), ij i + \u03b1(BN(U \u2113h\u2113 i + Aj\u2208Ni(\u03c3(\u02c6e\u2113+1 ) \u2299 V \u2113h\u2113 ij j))), where U \u2113, V \u2113, P \u2113, Q\u2113, R\u2113 \u2208 Rd\u00d7d are the learnable parameters of layer \u2113, \u03b1 denotes the ReLU [66] activation, BN denotes the Batch Normalization operator [51], A denotes the aggregation function SUM pooling [116], \u03c3 is the sigmoid function, \u2299 is the Hadamard product, Ni denotes the neighborhoods of node i, and MLP(\u00b7) denotes a 2-layer multi-layer perceptron. ij are initialized as the corresponding values in xt, and h0 For TSP, e0 i are initialized as sinusoidal"}