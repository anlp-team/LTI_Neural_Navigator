{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_I3D:_Transformer_Architectures_with_Input-Dependent_Dynamic_Depth_for_Speech_Recognition_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the input to the first layer of the encoder?", "answer": " X = X(0) \u2208 RT \u00d7d", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " How is the sequence transformed before being mapped to probability distributions for MHA and FFN gates?", "answer": " The sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling.", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What are the two sets of probability distributions that the sequence is mapped to?", "answer": " {p(l) MHA}N l=1, {p(l) FFN}N l=1", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " How many layers does the I3D encoder have in total?", "answer": " The I3D encoder has N layers in total.", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What threshold can be used during inference to generate binary gates?", "answer": " A fixed threshold \u03b2 \u2208 [0, 1]", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What do the gate probability distributions represent at the l-th layer?", "answer": " p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What frameworks are used to train the models?", "answer": " PyTorch and ESPnet", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " How many layers do the I3D encoders have in total?", "answer": " The I3D encoders have 36 layers in total.", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What is the learning rate used for fine-tuning the I3D encoders?", "answer": " 1e \u2212 3", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}, {"question": " What method is used to adjust the computational cost of a trained I3D model at inference time?", "answer": " Changing \u03b2", "ref_chunk": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}], "doc_text": "as shown in Fig. 1c. It pre- dicts the gate distributions for all layers based on the encoder\u2019s input, which is also the input to the \ufb01rst layer: X = X(0) \u2208 RT \u00d7d. In par- ticular, the sequence is transformed to a single vector x = x(0) \u2208 Rd by average pooling. Then, it is mapped to two sets of probability distributions for all N MHA and FFN gates, respectively: {p(l) MHA}N l=1, {p(l) FFN}N l=1 = GGP(x), (12) where p(l) FFN \u2208 R2 are the gate probability distributions at the l-th layer, and the I3D encoder has N layers in total. Here, the deci- sions of executing or skipping modules are made immediately after seeing the encoder\u2019s input, which has lower computational overhead than LocalGP and allows for more \ufb02exible control over the inference architecture. During inference, we can still use a \ufb01xed threshold \u03b2 \u2208 [0, 1] to generate binary gates as in Eqs. (10) and (11). MHA, p(l) 3. EXPERIMENTS 3.1. Experimental setup We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to train all models. We mainly use the CTC framework on LibriSpeech ) \u2193 ( R E W % 13 12 11 10 Transformer I3D-LocalGP I3D-GlobalGP 18 20 22 24 30 Average number of layers 26 28 32 34 36 Fig. 4: Word error rates (%) of CTC-based models vs. average num- ber of layers used for inference on the Tedlium2 test set. Table 1: Word error rates (%) and average number of inference lay- ers of AED-based models on LibriSpeech 100h. Model dev clean test clean Ave #layers WER (\u2193) Ave #layers WER (\u2193) Transformer 36 27 7.8 8.2 36 27 8.0 8.5 I3D-LGP-36 I3D-GGP-36 27.3 27.2 7.9 7.8 27.1 27.1 8.3 8.2 100h [34]. In Sec. 3.5, we also show that I3D can be applied to AED and another corpus, Tedlium2 [35]. Our I3D encoders have 36 layers in total. They are initialized with trained standard Trans- formers and \ufb01ne-tuned with a reduced learning rate (1e \u2212 3) and various \u03bb (usually ranging from 1 to 13) in Eq. (5) to trade off WER and computation. The \ufb01ne-tuning epochs for LibriSpeech 100h and Tedlium2 are 50 and 35, respectively. We compare I3D with two baselines. First, we train standard Transformers with a reduced num- ber of layers. Second, we train a 36-layer Transformer with Lay- erDrop [36, 37] or Intermediate CTC (InterCTC) [38] and perform iterative layer pruning [29] using the validation set to get a variety of models with smaller and \ufb01xed architectures. This baseline is de- noted as \u201cLayerDrop\u201d in Figs. 2 and 3. We can compare I3D, whose layers are dynamically reduced based on the input, against the static pruned models. 3.2. Main results Fig. 2 compares our I3D models with two baselines. We train I3D- CTC models with different \u03bb in Eq. (5) to adjust the operating point. We calculate the number of layers as the average of the number of MHA blocks and the number of FFN blocks. Both I3D-LocalGP and I3D-GlobalGP outperform the standard Transformer and the pruned version using iterative layer pruning [29]. We can reduce the aver- age number of layers to around 20 while still matching the Trans- former trained from scratch. LocalGP achieves similar performance as GlobalGP, but GlobalGP has only one gate predictor, which can be more ef\ufb01cient for inference. The reason why LocalGP is not bet- ter than GlobalGP may be that LocalGP decides whether to execute or skip a block based on the current layer\u2019s input, which depends on decisions at previous layers. This sequential procedure can lead to more severe error propagation. We also show that it is possible to adjust the computational cost of a trained I3D model by changing \u03b2 (see Eqs. (10) (11)) at inference time. Three I3D-GlobalGP models are decoded with different \u03b2. As \u03b2 decreases, more blocks are used, and the WER is usually improved. Fig. 3 shows the results using InterCTC [38]. The WERs are lower than those in Fig. 2, thanks to the auxiliary CTC loss which regularizes training. Again, I3D is consistently better than the Trans- former trained from scratch and the pruned model. 3.3. Analysis of gate distributions Fig. 5 shows the mean and standard deviation (std) of the gate prob- abilities generated by an I3D-GlobalGP model using CTC on Lib- y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (b) FFN gate probabilities. Fig. 5: Predicted gate probabilities (mean and std) at different lay- ers of an I3D-GlobalGP model on LibriSpeech test other. A higher probability means the layer is more likely to be executed. y t i l i b a b o r P 1 0.5 0 1 6 11 16 21 Layer index 26 31 36 (a) MHA gate probabilities (trained with InterCTC). y t i l i b a b o r P 1 0.5 0 1 6 11 21 16 Layer index 26 31 36 (b) FFN gate probabilities (trained with InterCTC). Fig. 6: Predicted gate probabilities (mean and std) at different layers of an I3D-GlobalGP model with InterCTC on LibriSpeech test other. A higher probability means the layer is more likely to be executed. riSpeech test-other. Most layers have a stable probability. Several layers have larger variations depending on the input. For both MHA and FFN, upper layers are executed with high probabilities while lower layers tend to be skipped, which is consistent with [28]. We also show the gate probabilities from an I3D-GlobalGP model trained with InterCTC [38] in Fig. 6. Interestingly, the over- all trend is very different from Fig. 5. Now, the upper layers are almost skipped while the lower layers are executed with very high probabilities, indicating that lower layers of this"}