{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Cross-Modal_Fine-Tuning:_Align_then_Refine_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of Transformers (FPT) work\ufb02ow?", "answer": " To transform inputs to be compatible with pretrained models", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " How does FPT differ from ORCA?", "answer": " FPT does not account for modality difference, while ORCA does", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " What is the problem setup described in the text?", "answer": " A domain consists of a feature space X, a label space Y, and a joint probability distribution P(X, Y)", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " What is the goal in learning a model mt?", "answer": " To correctly map each input xt to its label yt", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " What are the three stages of the cross-modal fine-tuning workflow?", "answer": " Generating task-specific embedder and predictor, pretraining embedder, and fine-tuning to minimize target loss", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " How does ORCA work for different input/output dimensions?", "answer": " By decomposing a transformer-based learner m into three parts: an embedder f, a model body g, and a predictor h", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " What is the purpose of the custom embedding network in ORCA?", "answer": " To transform input x into a sequence of features compatible with the pretrained model", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " What are the input and output feature spaces for transformer-based g in ORCA?", "answer": " Both input and output feature spaces are RS\u00d7D", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " How does the custom prediction head in ORCA handle different types of outputs?", "answer": " It uses task-specific output modules and fine-tunes them for new problems", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}, {"question": " Why is data alignment important in the context of ORCA?", "answer": " To achieve better adaptation by aligning the target data with the pretrained model", "ref_chunk": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}], "doc_text": "Transformers (FPT) (Lu et al., 2022) is a cross-modal \ufb01ne-tuning work\ufb02ow that transforms the inputs to be compatible with the pretrained models. Al- though FPT and ORCA are both general-purpose, FPT does not account for the modality difference (no stage 2 in Fig- ure 1), but we show this step is necessary to obtain effective predictive models and outperform existing baselines. 3. ORCA Work\ufb02ow In this section, we formalize the problem setup and intro- duce the our work\ufb02ow for adapting pretrained transformers. Problem Setup. A domain D consists of a feature space X , a label space Y, and a joint probability distribution P (X , Y). In the cross-modal setting we study, the target (end-task) do- main Dt and source (pretraining) domain Ds differ not only in the feature space but also the label space and by exten- sion have differing probability distributions, i.e., X t (cid:54)= X s, Y t (cid:54)= Y s, and P t(X t, Y t) (cid:54)= P s(X s, Y s). This is in con- trast to the transductive transfer learning setting addressed by domain adaptation, where source and target domains share the label space and end task (Pan & Yang, 2009). i }nt Given target data {xt i=1 sampled from a joint distri- bution P t in domain Dt, our goal is to learn a model mt that correctly maps each input xt to its label yt. We are interested in achieving this using pretrained transformers. Thus, we assume access to a model ms pretrained with data i, yt Cross-Modal Fine-Tuning: Align then Re\ufb01ne i }ns {xs i=1 in the source domain Ds. Then, given a loss function l, we aim to develop mt based on ms such that E(xt,yt)\u223cP t [l(mt(xt), yt)] is minimized. This problem for- mulation does not de\ufb01ne modality explicitly and includes both in-modal and cross-modal transfer. Given the general- ity of the tasks we wish to explore and the dif\ufb01culty of differ- entiating the two settings mathematically, we rely on seman- tics to do so: intuitively, cross-modal data (e.g., natural im- ages vs. PDEs) are more distinct to each other than in-modal data (e.g., photos taken in two geographical locations). i , ys Having de\ufb01ned the learning problem, we now present our three-stage cross-modal \ufb01ne-tuning work\ufb02ow: (1) gener- ating task-speci\ufb01c embedder and predictor to support di- verse input-output dimensions, (2) pretraining embedder to align the source and target feature distributions, and (3) \ufb01ne-tuning to minimize the target loss. 3.1. Architecture Design for Dimensionality Alignment Applying pretrained models to a new problem usually re- quires addressing the problem of dimensionality mismatch. To make ORCA work for different input/output dimensions, we decompose a transformer-based learner m into three parts (Figure 1 stage 1): an embedder f that transforms input x into a sequence of features, a model body g that applies a series of pretrained attention layers to the embed- ded features, and a predictor h that generates the outputs with the desired shape. ORCA uses a pretrained architecture and weights to initialize the model body g but replaces f and h with layers designed to match the target data with the pretrained model\u2019s embedding dimension. In the following, we describe each module in detail. Custom Embedding Network. Denote the feature space compatible with the pretrained model as \u02d9X . For a trans- former with maximum sequence length S and embedding di- \u02d9X = RS\u00d7D. The target embedder f t : X \u2192 \u02d9X mension D, is designed to take in a tensor of arbitrary dimension from X and transform it to \u02d9X . In ORCA, f t is composed of a convolutional layer with input channel cin, output channel cout, kernel size k, and stride k, generalizing the patching operations used in vision transformers to 1D and higher- dimensional cases. We set cin to the input channel of x and cout to the embedding dimension D. We can either treat k as a hyperparameter or set it to the smallest value for which the product of output shape excluding the channel dimension \u2264 S to take full advantage of the representation power of the pretrained model. In the latter case, when we \ufb02atten the non-channel dimensions of the output tensors af- ter the convolution, pad and then transpose it, we can obtain sequence features with shape S \u00d7 D. Finally, we add a layer norm and a positional embedding to obtain \u02d9x. the dot is used to differentiate these intermediate representa- tions from the raw inputs and labels. For transformer-based g, both the input and output feature spaces \u02d9X , \u02d9Y are RS\u00d7D. Custom Prediction Head. Finally, the target model\u2019s pre- diction head ht must take \u02d9y \u2208 \u02d9Y as input and return a task-dependent output tensor. Different tasks often specify different types of outputs, e.g., classi\ufb01cation logits in RK, where K is the number of classes, or dense maps where the spatial dimension is the same as the input and per index logits correspond to K classes. Thus, it is crucial to de\ufb01ne task-speci\ufb01c output modules and \ufb01ne-tune them for new problems. In ORCA, we use the simplest instantiation of the predictors. For classi\ufb01cation, we apply average pooling along the sequence length dimension to obtain 1D tensors with length D and then use a linear layer that maps D to K. For dense prediction, we apply a linear layer to the sequence outputs so the resulting tensor has shape (S, kndim(Y)K), where kndim(Y) is the downsampling factor of the embedder convolution kernel with stride k. This upsamples by the same factor that the embedder downsampled. Then, we can mold the tensor to the desired output dimension2. With an architecture based on the pretrained model but also compatible with the target task, we can now turn our attention to data alignment for better adaptation. 3.2. Embedder Learning for Distribution Alignment Intuitively, transferring knowledge across similar modalities should be easier than across distant ones. Hence, given a tar- get task in a new modality, we aim to manipulate the target"}