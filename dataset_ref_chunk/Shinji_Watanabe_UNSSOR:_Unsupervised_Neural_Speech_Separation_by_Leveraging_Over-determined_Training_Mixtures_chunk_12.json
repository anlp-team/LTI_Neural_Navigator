{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_UNSSOR:_Unsupervised_Neural_Speech_Separation_by_Leveraging_Over-determined_Training_Mixtures_chunk_12.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the analysis window used in the STFT setup described in the text?", "answer": " The square-root Hann window", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " How many dimensional complex STFT spectra are extracted at each frame for 8 kHz sampling rate?", "answer": " 129-dimensional", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What DNN architecture is used in the text?", "answer": " TF-GridNet", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What hyper-parameters are set for TF-GridNet for 8 kHz sampling rate?", "answer": " D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " How is padding handled for mixtures shorter than l seconds in the text?", "answer": " Zeros are padded in the front", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What optimizer is used in the text for training?", "answer": " Adam", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What is the L2 norm set for in the text?", "answer": " Gradient clipping", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " How is the learning rate adjusted in the text?", "answer": " Halved if the validation loss is not improved in two epochs", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What is the batch size set to in the text?", "answer": " Four", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}, {"question": " What GPU is used for training each model in the text?", "answer": " Nvidia A100 40GB", "ref_chunk": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}], "doc_text": "is 8 ms, and the square-root Hann window is used as the analysis window. For 8 kHz sampling rate, a 256-point discrete Fourier transform is applied to extract 129-dimensional complex STFT spectra at each frame. This STFT setup is very common in modern deep learning based separation. Differently, the window and hop sizes of some baselines such as IVA and spatial clustering are considerably larger (see detailed setup in Section 5.1), as we observe that this leads to better separation performance, likely because more reverberation can be covered in each frame and, this way, their model assumptions can be better satisfied. Our DNN architecture is TF-GridNet [23]. Using the symbols defined in Table I of [23], we set its hyper-parameters to D = 48, B = 4, I = 4, J = 1, H = 192, L = 4 and E = 4 for 8 kHz sampling rate. Please do not confuse the symbols in TF-GridNet with the ones in this paper. In each epoch, we sample an l-second mixture segment from each training mixture for model training. If a mixture is shorter than l seconds, we pad zeros in the front rather than in the end, since padding in the end would result in a mixture that has abrupt stop of reverberation, which is not realistic and would be detrimental to FCP-based relative RIR estimation. In comparison, padding zeros in the front can avoid this problem. If a mixture is longer than l seconds, we randomly pick an l-second segment. In default, l is set to four seconds. We normalize the sample variance of each sampled mixture segment to 1.0, before feeding them to DNN for training. Adam (with the default setup in Pytorch v1.9) is used as the optimizer. The 18 Table 5: Supplementary averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 16.0 15.6 14.6 3.44 0.885 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 2.4 2.4 0.2 1.89 0.549 Table 6: Supplementary averaged results of 2-speaker separation on SMS-WSJ (3-channel input and loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 2a UNSSOR 19 0 LMC+ISMS 15.7 15.4 14.4 3.20 0.874 3e iRAS w/ non-causal 1472-tap filters (64 future taps) LiRAS 7.7 7.2 5.4 1.87 0.621 Table 7: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 6-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 13.0 12.5 11.9 3.27 0.832 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.7 1.7 0.0 1.90 0.561 Table 8: Supplementary averaged results of 2-speaker separation on SMS-WSJ (1-ch input and 3-ch loss). Val. set Test set Row Systems I J Loss SDR (dB) SDR (dB) SI-SDR (dB) PESQ eSTOI 1a UNSSOR 19 1 LMC+ISMS 12.5 12.0 11.4 3.18 0.822 2c iRAS w/ non-causal 1536-tap filters (64 future taps) LiRAS 1.3 1.3 \u22120.3 1.87 0.549 L2 norm for gradient clipping is set to 1.0. The learning rate starts from 10\u22123 and is halved if the validation loss is not improved in two epochs. We terminate training once the learning rate is reduced to 6.25 \u00d7 10\u22125. The batch size is set to four, with each segment being 4-second long. For each model, an Nvidia A100 40GB GPU is used for training, and the model converges in three to four days. We sweep \u03b3 in (11) based on the set of {0.02, 0.04, 0.06, 0.1, 0.3, 1.0}. The microphone weight \u03b1p in (4), (9), (10) and (12) is set to 1.0 in default for all microphones (i.e., no weighting), and, in the monaural input case (e.g., in Table 3 and 4), we sweep \u03b11 at the reference microphone 1 based on the set of {1/2, 1/3, 1/4, 1/5, 1/6, 1/8, 1/10} to alleviate overfitting to microphone 1. J Alternative filter length for iRAS and UNSSOR We also provide the results of iRAS that uses the same filter length (in seconds) as that in UNSSOR. Given 8 kHz sampling rate, 32 ms window size, 8 ms hop size, and K = I + 1 + J (defined below (6)) filter taps in FCP, we use M = ((K \u2212 1) \u00d7 8 + 32)/1000 \u00d7 8000 filter taps for each \u02c6hp(c) in (12), and configure \u02c6hp(c) to filter the past M \u2212 8/1000 \u00d7 8000 \u2212 1, the current, and the future 64 (= 8/1000 \u00d7 8000) samples. We filter the future 8 ms of samples, because, in the STFT case, the hop size is 8 ms. We report the results in Table 5-8, each respectively corresponding to the results in Table 1-4. We observe that the performance is not as good as that of UNSSOR. In Fig. 6, we make further comparisons of using different filter taps between UNSSOR and iRAS, following the experimental setup in the previous paragraph. Fig. 6(a) uses six-microphone input, Fig. 6(b) uses monaural input, and both of them use the six-microphone LMC+ISMS (or LiRAS) loss. For UNSSOR, in Fig. 6(a) we set J = 0 and sweep K \u2208 {5, 9, 13, 17, 20, 25} and in Fig. 6(b) we set J = 1 and sweep K \u2208 {5, 9, 13, 17, 21, 25}. For iRAS, we configure the filter to always filter the future 64 samples, and in Fig. 6(a) we sweep the filter taps M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1472} and in Fig. 6(b) we sweep M \u2208 {128, 256, 384, 512, 768, 1024, 1280, 1536}. Notice that in Fig. 6, the filter taps M in iRAS and K in UNSSOR are vertically corresponding to each other. That is, some swepted filter taps M are computed based on the swepted K (e.g., for M = 1024 and K = 13, we have 1024 = ((13 \u2212"}