{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Why_do_Nearest_Neighbor_Language_Models_Work?_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does kNN stand for in the context of the text?,answer: kNN stands for K-nearest neighbors.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What is the purpose of testing kNN in the experiments mentioned in the text?,answer: The purpose is to explore potential solutions for increasing the capacity of softmax and examine if they can achieve a similar effect of kNN-LM.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What is one potential solution suggested in the text for increasing softmax capacity?,answer: One potential solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " Why is mask-to-k(\u00b7) no longer needed when using a smaller weight matrix?,answer: mask-to-k(\u00b7) is no longer needed because the formulation is small enough to fit the entire matrix in the GPU.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What does the comparison in Figure 3 show between the base LM, original kNN-LM, and using either attention layer output or feedforward layer output as hds?,answer: The comparison shows the number of embeddings for each word type versus the interpolated perplexity, indicating that replacing the datastore with a smaller matrix helps, but is only about half as effective as kNN-LM.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " How does over-parameterization affect the perplexity in kNN-LM?,answer: Over-parameterization does not always bring better performance in kNN-LM, as increasing the number of embeddings does not consistently result in better perplexity.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What hypothesis does the text propose regarding the ineffectiveness of simply over-parameterizing Wds?,answer: The hypothesis is that regularization terms may be needed during training to prevent the multiple embeddings for each word type from converging to the same vector, rendering over-parameterization useless.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What is proposed as an alternative to increasing the number of embedding vectors equally for each word type?,answer: An alternative proposal is to adaptively increase the number of embeddings for word types based on word frequency or total training loss for the word.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " How does the use of a Mixture of Softmax attempt to address the softmax bottleneck?,answer: A Mixture of Softmax (MoS) is used to produce more linearly independent probability distributions of words given different contexts, thereby addressing the softmax bottleneck.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}, {"question": " What is the outcome of the alternative methods proposed to increase softmax capacity?,answer: None of the alternative methods, including adaptive embedding increases and Mixture of Softmax, provided additional benefits over the simple multi-embedding approach.", "ref_chunk": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}], "doc_text": "kNN is a sparse approximation of the full softmax over all the embeddings in the datastore Wds. To test this hypothesis, 2Because we previously found little difference between IP and L2 as similarity functions, we use IP in the experiments. For simplicity, we set temperature \u03c4 = 1. 6 we disentangle the effect of the high rank in Wds from the actual saved context embeddings in Wds, by training an embedding matrix of the same desired size to test from scratch. Ratio to Full Datastore SizeInterpolated Perplexity19.00020.00021.00022.0000.000.250.500.751.00 Figure 2: The effect of the size of the datastore used for kNN retrieval on \ufb01nal interpolated perplexity. We explore several potential solutions for increasing the capacity of softmax, and examine if they can achieve a similar effect of kNN-LM. The \ufb01rst and easiest solution is to increase the embedding matrix size by adding more embedding vectors for each word type in the vocabulary. To test this, we replace Wds with a much smaller matrix of size nV \u00d7 D, where we allocate n embedding vectors for each word type. When calculating the probability from this component, we compute the softmax over nV items and sum the probabilities for each vocabulary entry to calculate the \ufb01nal probability. mask-to-k(\u00b7) is no longer needed, as this formulation is small enough to \ufb01t the entire matrix in the GPU. We then \ufb01netune the new Wds on the training data until convergence. Figure 3 compares the base LM and the original kNN-LM versus using either attention layer output (\u201catt\u201d) or feedforward layer output (\u201cffn\u201d) as hds. We plot the number of embeddings for each word type (nV total embeddings in Wds) versus the interpolated perplexity, with full details found in Appendix B. In both cases, comparing with the top horizontal line which represents the perplexity of the base LM, replacing the datastore with a much smaller weight matrix (from Nds to nVds) by assigning only a few more embeddings for each word helps, although only about half as effective as kNN-LM. To give a perspective, the original datastore size is about 5000V . Surprisingly, we \ufb01nd that increasing n does not always bring better performance, even though a larger datastore is better than using a small datastore in kNN-LM. We can see that when hds = ffn, over-parameterization provides very limited improvements, while for hds = att it does not bring consistent improvements at all. Comparing the trend of increasing the embeddings in Wds, with the bottom horizontal line in the plot, which represents the perplexity of the standard kNN-LM using the full datastore (Wds with approx. 5000V embeddings), we can see no clear trend that more trainable embeddings result in better perplexity, and that the gap between using trained embeddings and using full datastore is still signi\ufb01cant. This suggests that simply over-parameterizing Wds is not an effective method of achieving accuracy gains similar to kNN-LM. We hypothesize that this is because by just adding more embeddings, while still using the same training procedure as the original LM, the multiple embeddings for each word type after learning could still be very close to each other, and thus do not increase the softmax capacity much. This suggests that some regularization terms may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different dif\ufb01culties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further \ufb01netuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional bene\ufb01ts over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 7 att ffn Number of Trained Embeddings (nV)Interpolated Perplexity192021222468 Figure 3: The number of embeddings per word type (nV total embeddings in Wds) versus interpolated perplexity. The horizontal line at the top (black) represents the perplexity of the base LM. The horizontal line at the bottom (red) represents the interpolated perplexity using a full datastore with kNN-LM. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkN N of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(\u00b7) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote \u201creal mask\u201d as the accurate nearest neighbors for mask-to-k(\u00b7) selection, and \u201cFAISS mask\u201d as the approximate nearest neighbors returned by the FAISS library.3 Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for ef\ufb01ciency purposes. We denote \u201creal score\u201d as the scores calculated from ground truth distances between the embeddings, and \u201cFAISS score\u201d as the distances returned by FAISS approximate search. The comparison of the"}