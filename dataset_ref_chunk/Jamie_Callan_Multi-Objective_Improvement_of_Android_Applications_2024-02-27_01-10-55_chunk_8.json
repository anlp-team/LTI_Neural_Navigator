{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_Multi-Objective_Improvement_of_Android_Applications_2024-02-27_01-10-55_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What was the threshold set for all experiments in terms of benchmarks?", "answer": " 20 benchmarks", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " Who conducted a study on the changes made by Android developers to improve app performance?", "answer": " Callan et al", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What do Android developers consider as changes within the GI search-space according to Callan et al (2022)?", "answer": " Some changes include moving an operation outside of a for loop if it only needs to be executed once.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What is one example of a change that is not yet achievable according to Callan et al (2022)?", "answer": " Requiring new code to be added that could not be achieved via mutation of the existing code base.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " How many commits were found in previous work that improved runtime, bandwidth, or memory use?", "answer": " 14 commits", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What did the researchers do to ensure the apps would build before testing and measuring?", "answer": " Updated build scripts with newer versions of libraries and build tools.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What tool did the researchers run on the 7 applications to identify performance issues?", "answer": " PMD static analyser", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " How many runs in total did the researchers have for their experiments?", "answer": " 2520 runs", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What type of computer was used for the experiments?", "answer": " High-performance cloud computer with 16GB RAM and 8-core Intel Xenon CPUs.", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}, {"question": " What was used to minimize measurement noise during the experiments?", "answer": " Mann-Whitney U test at the 5% confidence level", "ref_chunk": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}], "doc_text": "placed upon the application of GI. Given the cost associated with manual testing, we set a threshold of 20 benchmarks for all our experiments. To validate our approach, we first run GIDroid on applications with known perfor- mance issues. Callan et al (2022) has recently conducted a study of the changes that Android developers make to improve app performance. They pose that some of those changes are within the GI search-space. For instance, moving an operation outside of a for loop, if only need to be executed once. While others are not yet achievable, e.g., requiring new code to be added that could not be achieved via mutation of the existing code base. We thus use Callan et al (2022)\u2019s criteria to iteratively analyse the 15 Table 2 Parameter settings for the MO algorithms used in our study. Parameter Mutation Rate Crossover Rate No. Generations No. Individuals Selection Crossover Mutation Reference Points Worst Observation (for each prop. and bench.) Value 0.5 0.2 10 40 Binary Tournament Append Lists of Edits Add/Remove an Edit commits from their dataset that improve runtime, bandwidth, or memory use, until we reached our 20 benchmark target. In particular, we found 14 commits in previ- ous work, spread over different versions of 7 applications. Since we also want to find improvements in current software, we stop our selection procedure here and use the current versions of the 7 apps, giving us a total of 21 benchmarks. Once we had our set of versions of apps, we prepared them for GI. Firstly, we had to ensure the apps would build. Over time, a number of changes have been made to the Android build tools, making older versions of code incompatible with modern Android Studio. We require these build tools to function with Android Studio, so we can test and measure the test coverage of applications confirming that they can be safely improved. This meant that we had to update build scripts with newer versions of libraries and build tools. In some cases, there were bugs such as unescaped apostrophes in resource files, which prevented applications from building. These bugs were fixed. In a few cases, the benchmarks also used outdated non-Gradle build systems, so we wrote the necessary build scripts, and modified the project\u2019s directory structure, to be compatible with Gradle and thus with GIDroid. No source code was modified in this process. We ran the PMD static analyser on the 7 applications and ran GIDroid on the classes which showed the most performance issues. This way we could see how our approach compares against human effort for finding performance-improving code transformations of existing code bases, for the 14 previously patched app variants. We could also see whether our approach is able to find yet unknown performance improvements in the current versions of the 7 apps. 6.3 Experimental Setup For each version of code we improve, we run GIDroid 20 times with 400 evaluations. To minimise measurement noise, we use the Mann-Whitney U test at the 5% confidence level to determine whether there is an improvement of a given property (i.e., runtime, memory or bandwidth use). For the evolutionary algorithms, we divide these 400 evaluations into 10 generations with 40 individuals each, as was shown to be effective in previous work, including in the Android domain (Motwani et al (2022); Callan and Petke (2022a)). We set number of evaluations to 400 as, even when using simulation- based testing, the evaluation of an individual is slow, taking up to 2 minutes. We use 16 the Genetic programming parameters in Table 2 as they have been used successfully in the past (Callan and Petke (2022b)). We had 2520 runs in total, taking a mean of 3 hours per run, resulting in roughly 7500 hours of computing time to test our approach. All of our experiments were performed on a high-performance cloud computer, with 16GB RAM and 8-core Intel Xenon CPUs. We ran jobs across 10 nodes, each running separately to avoid interference between fitness measurements. 7 Results and Discussion In this section, we present and analyse the results of our experiments, answering our Research Questions (Section 5). Throughout this section we will refer to the CPU time (s) of the test process as execution time, the size of the occupied Java heap as memory consumption (MB), and the number of bytes sent and received by the test process as network usage (B). Each of these objectives is a fitness function which we aim to minimize. 7.1 RQ1: Known Improvements Figure 5 and 6 show the improvements found in the benchmarks in which we knew improvements were possible. We find improvements to both execution time and mem- ory, but not bandwidth. We believe this is due to the nature of the benchmarks. Although feasible, only one application had bandwidth improvements in its history that would be achievable by GI. This improvement2 required 2 insertions and 2 deletions at once to be achieved and thus was more difficult to evolve over time. We find improvements to execution time of up to 26% and memory of up to 69%. We manually analysed the patches found in order to determine whether GI was capable of finding the same patches that developers made to improve their applications. The result of this analysis can be found in Table 3. In 64% of benchmarks GIDroid is able to find patches containing edits semantically-equivalent to developer patches, providing at least the same % performance improvement. In other words, aside from reproducing improvements, in some cases, we find additional edits, further improving app performance. 7.2 RQ2: Improvements of Current Apps Next, we analyse the results of the experiments on the benchmarks of current versions of applications, to see how well our approach generalizes to code in which there are no known improvements. The performance of each algorithm on versions of software is shown in Figure 5 and Figure 6. We find improvements to the execution time of up to 35% and to memory consumption of up to 32%."}