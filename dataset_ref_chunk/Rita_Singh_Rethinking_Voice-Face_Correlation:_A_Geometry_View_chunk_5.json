{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Rethinking_Voice-Face_Correlation:_A_Geometry_View_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the function of \ud835\udf07 in the context of the text?", "answer": " \ud835\udf07 is the function for computing the mean.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " How is \ud835\udf07 related to computing the standard deviation?", "answer": " \ud835\udf07 is the function for computing the mean, while \ud835\udf0e is the function for computing the standard deviation.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " What is the significance level \ud835\udefc used for?", "answer": " \ud835\udefc is the significance level used to determine whether to reject the null hypothesis.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " What do \ud835\udf08 and N represent in the context of the text?", "answer": " \ud835\udf08 represents the degree of freedom and N represents the number of repeated experiments.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " Why is it necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631?", "answer": " It is necessary because the estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, which results in significantly lower errors on D\ud835\udc632.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " What is the purpose of the phonatory module mentioned in the text?", "answer": " The phonatory module serves as an additional constraint when predicting facial AMs by learning characteristics of voice.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " What is the main difference between the diffusion process and the denoising process described in the text?", "answer": " The diffusion process transforms variables into Gaussian noise through Markov transitions, while the denoising process aims to recover the speech signal from Gaussian noise.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " How is the 3D facial shape reconstructed according to the text?", "answer": " The 3D facial shape is reconstructed by predicting AMs of voice recordings, then generating 3D facial shapes based on the predicted AMs using an optimization-based method.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " What does the computation of AM \ud835\udc44\ud835\udc58 (\u00b7) do?", "answer": " The computation of AM \ud835\udc44\ud835\udc58 (\u00b7) maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}, {"question": " How is the optimization objective defined for reconstructing the 3D facial shape?", "answer": " The optimization objective is defined as minimizing the differences between the AMs of the re-projected 3D facial shape and the predicted AMs, balanced by a loss weight \ud835\udf06.", "ref_chunk": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}], "doc_text": "\ud835\udf0e (\ud835\udf16\ud835\udc58 /\ud835\udf16\ud835\udc36 \ud835\udc58 ) \u221a \ud835\udc41 where \ud835\udf07 (\u00b7) and \ud835\udf0e (\u00b7) are the functions for computing mean and stan- dard deviation respectively. \ud835\udc41 is the number of the repeated experi- ments and we set \ud835\udc41 = 100 here. \ud835\udefc and \ud835\udf08 = \ud835\udc41 \u2212 1 are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5% and then we can read \ud835\udc610.95,\ud835\udc41 \u22121 from t-distribution table. Now we can determine whether to reject \ud835\udc3b0 and accept \ud835\udc3b1,i.e., the AM \ud835\udc5a (\ud835\udc58 ) is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95%,i.e., statistically significant. In contrast, \ud835\udc36\ud835\udc3c\ud835\udc62 \u2265 1 implies that we fail to reject \ud835\udc3b0, for the current experimental results are not statistically significant enough. Note that failing to reject \ud835\udc3b0 does not imply we accept \ud835\udc3b0. We emphasize that it is necessary to compute \ud835\udf16\ud835\udc36 \ud835\udc58 and \ud835\udf16\ud835\udc58 on D\ud835\udc632 rather than D\ud835\udc61 or D\ud835\udc631. This is because our estimators are trained on D\ud835\udc61 and selected by the errors on D\ud835\udc631, we can easily get significantly lower \ud835\udf16\ud835\udc58 and \ud835\udf16\ud835\udc36 \ud835\udc58 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particu- lar, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech \u02dc\ud835\udc63 con- trolled by the voice code \ud835\udc52 extracted from speech \ud835\udc63. During training speech \ud835\udc63 \u2032 which shares speaker identity with \ud835\udc63 is fed to the diffu- sion model as ground-truth. Please note that the phonatory module (4) Conference\u201923, July 2023, Ottawa, Canada only serves as an additional training constraint and is not applied during inference. Let \ud835\udc650, \u00b7 \u00b7 \u00b7 , \ud835\udc65\ud835\udc47 be a sequence of variables with the same dimension where \ud835\udc61 is the index for diffusion time steps. Then the diffusion process transforms \ud835\udc650 into a Gaussian noise \ud835\udc65\ud835\udc47 through a chain of Markov transitions with a set of variance schedule \ud835\udefd1, \u00b7 \u00b7 \u00b7 , \ud835\udefd\ud835\udc47 . Specifically, each transformation is performed accord- ing to the Markov transition probability \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) assumed to be independent of the style code \ud835\udc52 as \ud835\udc5e(\ud835\udc65\ud835\udc61 |\ud835\udc65\ud835\udc61 \u22121, \ud835\udc52) = N (\ud835\udc65\ud835\udc61 ; \u221a\ufe01 1 \u2212 \ud835\udefd\ud835\udc61 \ud835\udc65\ud835\udc61 \u22121, \ud835\udefd\ud835\udc61 \ud835\udc3c ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a condi- tional distribution \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50). Through the reverse transitions \ud835\udc5d\ud835\udf03 (\ud835\udc650:\ud835\udc47 \u22121|\ud835\udc65\ud835\udc47 , \ud835\udc50), the variables are gradually restored to a speech sig- nal with style code condition. The phonatory module actually models a distribution \ud835\udc5e(\ud835\udc650|\ud835\udc50). By applying the parameterization trick [21], we obtain the additional training constraint as \u221a \u221a {E\u2217, \ud835\udf03 \u2217} = arg min = E\ud835\udc650,\ud835\udf16,\ud835\udc61 \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( 1 \u2212 \u00af\ud835\udefc\ud835\udc61 \ud835\udf16, \ud835\udc61, \ud835\udc52)\u22251 (6) \u00af\ud835\udefc\ud835\udc61 \ud835\udc650 + E,\ud835\udf03 where \ud835\udefc\ud835\udc61 = 1 \u2212 \ud835\udefd\ud835\udc61 and \u00af\ud835\udefc\ud835\udc61 = (cid:206)\ud835\udc61 \ud835\udefc\ud835\udc61 \u2032 . As shown in Fig. 2, the \ud835\udf03 is \ud835\udc61 \u2032=1 a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain \u02dc\ud835\udc63 here. 3.4 AM-Guided 3D Facial Shape Reconstruction To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in D\ud835\udc52 first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low- dimensional linear space [5]. By adjusting the coefficients in low- dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix \ud835\udc35 = [\ud835\udc4f1, \ud835\udc4f2, \u00b7 \u00b7 \u00b7 ] \u2208 R3\ud835\udc47 \u00d7 | D\ud835\udc61 | where each column \ud835\udc4f\ud835\udc56 \u2208 R3\ud835\udc47 \u00d71 is a long vector obtained by flattening a 3D facial shape \ud835\udc53\ud835\udc56 \u2208 R\ud835\udc47 \u00d73. \ud835\udc47 is the number of vertices on 3D faces. Since 3\ud835\udc47 \u226b |D\ud835\udc61 |, we compute the project matrix \ud835\udc43 \u2208 R3\ud835\udc47 \u00d7\ud835\udc51 (\ud835\udc51 \u226b 3\ud835\udc47 ) using eigenfaces [5] on \ud835\udc35. Now any flattened 3D facial shape \ud835\udc4f can be approximated by re-projecting a low-dimensional vector \ud835\udefd \u2208 R\ud835\udc4f \u00d71 in the form of \ud835\udc43 \ud835\udefd. We define the computation of AM as \ud835\udc44\ud835\udc58 (\ud835\udc4f) : \ud835\udc4f \u21a6\u2192 R, which maps any flattened 3D facial shape \ud835\udc4f into the \ud835\udc58-th AM of \ud835\udc4f. Since \ud835\udc44\ud835\udc58 (\u00b7) computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below. \ud835\udc3e \u2211\ufe01 (\ud835\udc44\ud835\udc58 (\ud835\udc43 \ud835\udefd) \u2212 \u02c6\ud835\udc5a (\ud835\udc58 ) )2 \u00b7 \ud835\udc67 (\ud835\udc58 ) \ud835\udefd\u2217 = arg min \ud835\udf06\u2225\ud835\udefd \u22252 2 + \ud835\udefd \ud835\udc58=1 where \ud835\udf06 is the loss weight balancing two terms. The reconstructed 3D facial shape is given by \u02c6\ud835\udc4f = \ud835\udc43 \ud835\udefd\u2217. (7) 4 EXPERIMENTS In this section, we elaborate on the dataset setting, implementation details and experimental results. Conference\u201923, July 2023, Ottawa, Canada Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 (a) Male subset(b) Female subset(c) Balanced Female subset Figure 4: The normalized errors and \ud835\udc36\ud835\udc3c s of 24 AMs on (a) male subset, (b) female subset, and (c) a smaller female subset. If 1 \u2212 \ud835\udc36\ud835\udc3c\ud835\udc62 > 0, the AM is predictable else unpredictable. 4.1 Dataset We perform experiments on a private audiovisual dataset D. The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in"}