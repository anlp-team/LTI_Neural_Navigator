{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ML-SUPERB:_Multilingual_Speech_Universal_PERformance_Benchmark_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the ML-SUPERB benchmark?", "answer": " The ML-SUPERB benchmark extends SUPERB to multilingual tasks.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What conclusion was reached about the relevance of SSL model layers in the text?", "answer": " The most relevant layers for ASR are not the last few layers, and different languages show similar behavior in some cases.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What kind of speech data does VoxPopuli provide for training?", "answer": " VoxPopuli provides lecture speech data for training.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " According to the text, which types of languages may not benefit much from multilingual training?", "answer": " Larger language groups may not benefit as much from multilingual training, as shown by models like wav2vec2-large-23 and mHuBERT.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What is noted about the base models in comparison to their corresponding large versions?", "answer": " Base models tend to generalize better to multilingual cases than their large versions, according to the text.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " According to the text, how can the relevance of SSL model layers be related to speech domain?", "answer": " The relevance of SSL model layers may be related to the speech domain, in addition to the speech task, as shown by the behavior observed in different languages.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What type of benchmark is ML-SUPERB?", "answer": " ML-SUPERB is a benchmark designed to guide users in using SSL representations according to their needs, including an analysis of the learned weights for layer importance.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What type of data does Mixtec represent among the sets in the text?", "answer": " Mixtec represents conversational speech data among the sets.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " What is the significance of the learned weights for layer importance according to the text?", "answer": " The learned weights for layer importance can guide users in the use of SSL representations according to their needs.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}, {"question": " Who is invited to participate in the challenge mentioned in the text?", "answer": " The community is invited to participate in the challenge introduced by the paper.", "ref_chunk": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}], "doc_text": "44.4 40.8 42.6 40.5 58.3 45.7 45.8 45.4 43.4 44.3 43.6 40.9 44.5 48.2 45.4 45.8 45.6 11.11 54.4 30.9 50.8 58.7 1.1 6.6 66.9 61.2 46.5 49.3 39.5 52.4 35.9 66.9 54.6 33.1 45.1 21.8 45.6 55.6 71.5 55.4 75.1 66.4 46.6 62.0 40.6 45.5 38.6 37.2 43.4 33.4 28.4 39.2 45.6 37.7 41.9 36.8 58.9 44.2 50.3 44.9 44.3 46.1 43.2 42.1 43.8 49.3 43.5 45.2 44.2 0 755.2 598.3 680.3 735.7 433.8 528.8 947.5 831.9 678.7 779.0 715.4 746.2 Table 4: 1-hour set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 63.7 35.9 35.4 35.7 35.1 34.2 34.9 30.6 35.3 32.2 35.6 33.7 33.0 59.3 35.5 35.7 31.1 32.0 35.3 26.9 22.0 31.4 37.7 43.2 39.6 33.4 57.4 44.3 43.9 42.2 42.2 42.4 40.6 39.3 42.7 43.5 46.6 45.1 43.6 9.3 80.8 8.0 72.1 71.9 64.2 87.1 87.9 86.1 64.1 85.3 57.3 72.5 43.5 83.6 78.2 62.9 66.3 49.7 76.9 85.6 86.0 77.7 86.1 75.6 70.9 58.6 32.1 34.7 33.7 30.9 35.2 28.6 22.9 30.9 35.1 31.8 37.1 29.7 58.1 42.6 42.2 46.0 43.0 43.1 44.6 42.4 41.8 42.2 42.1 44.4 43.1 0 827.2 586.9 768.6 798.0 724.9 894.0 996.0 884.9 783.6 810.2 713.2 812.7 024681012141618202224LayerChineseEnglish1English2English3French1French2German1German2JapaneseMixtecRussianSwahiliSwedishLanguage 4.2 4.1 3.7 3.9 3.8 4.0 4.3 Distibution of Layer Weights By Language 3.3. Layerwise analysis Our benchmark offers tools to guide users in the use of SSL representations according to their needs, including an analysis of the learned weights for layer importance. The results for the XLSR-128 model in monolingual ASR tasks (shown in Fig 1) confirm the conclusions reached by [44] and [45]: the most rel- evant layers for ASR are not the last few layers. We also ob- served that English3, French2, and German2 have very similar behavior. These tasks use VoxPopuli data for training, which is the only dataset with lecture speech in our collection. Addi- tionally, Mixtec is the only conversational speech data among our sets, and we can see a distinct behavior in Fig 1. Therefore, the relevance of SSL model layers may be related to the speech domain (in addition to the speech task) rather than the language. Figure 1: The layerwise weight analysis of XLSR-128 model in the monolingual track. 4. Conclusion to be useful. However, multilingual training that is limited to a few selective languages may not be as beneficial in larger lan- guage groups (e.g., wav2vec2-large-23 and mHUBERT models do not always perform better than their models trained in a sin- gle language). (2) The base models tend to generalize better to multilingual cases than their corresponding large versions, such as wav2vec2-base versus wav2vec2-large and HuBERT- base versus HuBERT-large. This paper introduces ML-SUPERB, a benchmark that ex- tends SUPERB to multilingual the design of the open-source framework and discuss exper- imental More de- tailed policies can be found at https://multilingual. superbbenchmark.org/. We invite the community to par- ticipate in this challenge. tasks. We present results for some example models. 5. References [1] A. Mohamed et al., \u201cSelf-supervised speech representation learning: A review,\u201d JSTSP, 2022. [2] S.-w. Yang et al., \u201cSUPERB: Speech Processing Universal PER- formance Benchmark,\u201d in Proc. Interspeech, 2021, pp. 1194\u2013 1198. [3] A. Baevski et al., \u201cWav2vec 2.0: A framework for self- supervised learning of speech representations,\u201d Proc. NeurIPS, vol. 33, pp. 12 449\u201312 460, 2020. [4] W.-N. Hsu et al., \u201cHuBERT: Self-supervised speech represen- tation learning by masked prediction of hidden units,\u201d TASLP, vol. 29, pp. 3451\u20133460, 2021. [5] H.-S. Tsai et al., \u201cSUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,\u201d in Proc. ACL, 2022, pp. 8479\u20138492. [6] A. Babu et al., \u201cXLS-R: Self-supervised cross-lingual scale,\u201d arXiv preprint speech representation learning at arXiv:2111.09296, 2021. [7] A. Conneau et al., \u201cUnsupervised cross-lingual represen- speech recognition,\u201d arXiv preprint tation learning for arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., \u201cSpeechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,\u201d arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self- supervised models,\u201d JSTSP, vol. 16, no. 6, pp. 1227\u20131241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., \u201cCombining Spectral and Self- Supervised Features for Low Resource Speech Recognition and Translation,\u201d in Proc. Interspeech, 2022, pp. 3533\u20133537. [11] A. Wu et al., \u201cSelf-supervised representations improve end-to- end speech translation,\u201d Proc. Interspeech 2020, pp. 1491\u20131495, 2020. [12] X. Li et al., \u201cASR2K: Speech Recognition for Around 2000 Lan- guages without Audio,\u201d in Proc. Interspeech, 2022, pp. 4885\u2013 4889. [13] S. Evain et al., \u201c LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in Proc. Interspeech, 2021, pp. 1439\u20131443. [14] T. Javed et al., \u201cIndicsuperb: A speech processing universal performance benchmark for indian languages,\u201d arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., \u201cXTREME-S: Evaluating Cross-lingual Speech Representations,\u201d in Proc. Interspeech, 2022, pp. 3248\u2013 3252. [16] V. Pratap et al., \u201cMLS: A large-scale multilingual dataset for speech research,\u201d Proc. Interspeech 2020, pp. 2757\u20132761, 2020. [17] R. Ardila et al., \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. LREC, 2020, pp. 4218\u20134222. [18] K. MacLean, \u201cVoxforge,\u201d Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACL, 2021, pp. 993\u20131003. [20] K. Sodimana et al., \u201cA step-by-step process for building tts voices using open source data and framework for bangla, ja- vanese, khmer, nepali, sinhala, and sundanese,\u201d in Proc. SLTU, 2018, pp. 66\u201370. [21] O. Kjartansson et al., \u201cOpen-source high quality speech datasets for basque, catalan and galician,\u201d in Proc. SLTU, 2020, pp. 21\u2013 27. [22] F. He et al., \u201cOpen-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and tel- ugu speech synthesis systems,\u201d in Proc. LREC, 2020, pp. 6494\u2013 6503. [23] G. Rehm"}