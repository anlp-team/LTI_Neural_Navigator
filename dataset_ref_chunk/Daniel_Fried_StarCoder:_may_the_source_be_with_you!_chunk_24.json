{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_StarCoder:_may_the_source_be_with_you!_chunk_24.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the parameters for the language models code-cushman-001, code-davinci-002, and StarCoderBase?", "answer": " code-cushman-001 (12B), code-davinci-002 (175B), StarCoderBase (15.5B)", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " Which model performs the best according to the text?", "answer": " code-davinci-002", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What evaluation results are mentioned for code-davinci-002 compared to StarCoder and code-cushman-001?", "answer": " Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001.", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What difference in pass counts is mentioned between the 800B and 1000B checkpoints?", "answer": " 100 or higher", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What are the results reported in Table D.4 across different bias domains?", "answer": " Stereotype score for each domain", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What are some examples of pretraining templates mentioned in the text?", "answer": " git commit, GitHub issues, formatted Jupyter notebooks", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What was the impact of using Anthropic\u2019s HHH prompt on the model?", "answer": " Turned the model into a somewhat capable yet brittle technical assistant", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " How did the authors try to improve code generation in the text?", "answer": " By inspecting StarCoder-generated programs and altering prompts", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What type of programming problems did the authors observe in every model evaluated?", "answer": " Cases where the model produces effectively empty solutions", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}, {"question": " What was the pass@1 performance on HumanEval after applying a specific prompt?", "answer": " 40.8%", "ref_chunk": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}], "doc_text": "\"fc00:db20:35b:7399::5\", \"fdf8:f53e:61e4::18\", ], } Listing C.1: Replacements for IP addresses 45 Published in Transactions on Machine Learning Research (12/2023) D Additional Evaluation Results Language Models (Parameters) code-cushman-001 (12B) code-davinci-002 (175B) StarCoderBase (15.5B) cpp c-sharp d go java julia javascript lua php perl python r ruby racket rust scala shell swift typescript 30.59 22.06 6.73 19.68 31.90 1.54 31.27 26.24 28.94 19.29 30.71 10.99 28.63 7.05 25.22 27.62 11.74 22.12 31.26 48.44 27.47 21.71 31.39 40.12 35.74 48.99 40.83 47.40 34.77 46.68 23.13 42.68 17.60 43.40 43.61 23.24 38.02 48.87 Table D.1: Multi-language performance (pass@1) on MultiPL-E HumanEval of StarCoder and two closed- access models only available by API. Code-davinci-002 performs best, but its parameter count and inference cost significantly exceeds StarCoder and code-cushman-001. Format Model Valid (\u2191) Insecure (\u2193) Completion Insertion Completion Insertion StarCoderBase StarCoderBase code-davinci-002 code-davinci-002 855/1000 (85.50%) 987/1000 (98.70%) 984/1000 (98.40%) 986/1000 (98.60%) 340/855 (39.77%) 354/987 (35.87%) 423/984 (42.99%) 421/986 (42.70%) Table D.2: Security evaluation on the Asleep at the Keyboard dataset of StarCoderBase and OpenAI\u2019s code-davinci-002. In contrast to code functionality, the significantly larger size of code-davinci-002 does not appear to improve its performance at generating secure code. 46 30.56 20.56 10.01 21.47 28.53 21.09 31.70 26.61 26.75 16.32 30.35 10.18 17.25 11.77 24.46 28.79 11.02 16.74 32.15 Published in Transactions on Machine Learning Research (12/2023) Problem name Pass count 400B 600B 800B 1000B HumanEval_0_has_close_elements HumanEval_13_greatest_common_divisor HumanEval_152_compare HumanEval_16_count_distinct_characters HumanEval_23_strlen HumanEval_33_sort_third HumanEval_37_sort_even HumanEval_3_below_zero HumanEval_43_pairs_sum_to_zero HumanEval_46_fib4 HumanEval_52_below_threshold HumanEval_86_anti_shuffle HumanEval_97_multiply 20 86 211 0 105 42 90 190 0 197 0 0 1 171 176 185 46 60 0 156 154 34 200 186 0 0 197 153 126 137 200 1 132 0 119 142 170 118 133 5 6 11 0 6 106 0 129 7 6 13 1 21 Table D.3: Pass counts (out of 200 samples) for R on a selection of problems, where the difference in pass counts between the 800B and 1000B checkpoints is 100 or higher. Social Bias LLaMA-13B CodeGen-16B-Multi StarCoder Race/Color Socioeconomic Status Gender Disability Nationality Sexual Orientation Physical Appearance Religion Age Overall 68.99 68.60 59.16 81.67 59.75 73.81 71.43 76.19 72.41 67.84 61.82 68.60 54.96 73.33 47.17 67.86 55.56 54.29 48.28 59.08 63.95 63.37 50.76 81.67 57.23 72.62 57.14 74.29 54.02 61.94 Table D.4: CrowS-Pairs results across different bias domains. We report the stereotype score for each domain. A stereotype score closer to 50% indicates less bias. 47 Published in Transactions on Machine Learning Research (12/2023) E Qualitative Examples E.1 Using Pretraining Templates For the git commit, GitHub issues, and formatted Jupyter notebooks, we use a templated structure with sentinel tokens during pretraining. This template format allows us to easily prompt the model for specific use cases: with the commit format, we can prompt the model to modify code with a natural language instruction, with the GitHub issues format to respond to technical natural language questions, and the Jupyter notebook format to write code based on natural language description. Since we also train on the output of Jupyter code cells, we can use the model to act as a basic interpreter and predict the output of a piece of code. We can force the model to always predict an output by suppressing the empty output token (<empty_output>). Table E.1 illustrates uses of pretraining templates. E.2 Technical Assistant In preliminary explorations, we discovered that using Anthropic\u2019s HHH prompt (Askell et al., 2021) turned the model into a somewhat capable yet brittle technical assistant. We were surprised that, without instruction- tuning, we were able to utilize the 8k context length of StarCoder and let the model answer questions, follow instructions, and help with solving technical problems. We further improved the HHH prompt by including more conversational examples related to the programming domain. We used examples from various sources to create the prompt, including CoT (Wei et al. 2022; Kojima et al. 2022) and Least-to-most prompting (Zhou et al., 2022) for reasoning questions. The prompt\u2019s examples originate from StackExchange, PAL (Gao et al., 2022), Anthropic\u2019s HHH prompt, and our own efforts. We provide example responses in Table E.2 and the prompt itself in Appendix F. Note that the technical assistant has clear limitations: it sometimes proposes wrong solutions, presents wrong facts, and can make offensive comments. E.3 Improving Code Generation with Prompting We inspected StarCoder-generated programs on the HumanEval benchmark and found that there were several cases where the model produces what are effectively empty solutions, e.g., pass or a comment Insert code here. We also observed this kind of failure in every model we evaluated. When this type of problem occurs in practice in an IDE, a programmer addresses them by altering their prompt in some ad hoc way. We tried a few prompt prefixes that could be applied uniformly to all benchmark problems. However, these prefixes are typically model-specific. StarCoder\u2019s input format allows us to prompt it with the name of a file using the <filename> token. We found that the following prefix at temperature 0.1 boosts pass@1 performance on HumanEval to 40.8% (from 33.6% unprompted, Table 12): <filename>solutions/solution_1.py # Here is the correct implementation of the code exercise We also evaluated CodeGen-16B-Mono with the same temperature and prompt (but had to omit the filename since the CodeGen models do not support them). But, we found that this hurts performance, bringing it down to 28.10%. However, some other prefixes may exist that improve its performance. Similarly, we found that this prompt had a negligible impact with StarCoderBase. 48 Published in Transactions on Machine Learning Research (12/2023) Description Examples Jupyter format predicting results for Model input: <jupyter_text>Let\u2019s test our \u2018is_prime\u2018 function:<jupyter_code>print( is_prime(3)) print(is_prime(4)) print(is_prime(29)) print(is_prime(33))<jupyter_output> Model output: True False True False Model input: <jupyter_code>numbers = [1, 9, 8, 3, 27] print([n*2 for n in numbers])<jupyter_output> Model output: [2, 18, 16, 6, 54] Git commits to modify code Model input: <commit_before>def fibonacci(n):<commit_msg>add type hints to function< commit_after>def Model output: def fibonacci(n: int) -> list[int]: GitHub sues Is- Model input: <issue_start><issue_comment>username_0: I am trying to train a text classifier based on BERT using \u2018transformers\u2018 but"}