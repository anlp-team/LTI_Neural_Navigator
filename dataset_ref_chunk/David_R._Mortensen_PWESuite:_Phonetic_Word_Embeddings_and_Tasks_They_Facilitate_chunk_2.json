{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_PWESuite:_Phonetic_Word_Embeddings_and_Tasks_They_Facilitate_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the function P2F used for in Parrish (2017) study?,answer: In Parrish (2017) study, the function P2F is used to map each phoneme to a set of phonetic features.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " How are bi-grams of phonetic features created in Parrish (2017) study?,answer: In Parrish (2017) study, bi-grams of phonetic features are created by taking Cartesian product \u00d7 between sets of phonetic features.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What is the purpose of the function COUNTVEC in Parrish (2017) study?,answer: The function COUNTVEC in Parrish (2017) study outputs bi-gram counts in a vector of constant dimension.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " How are phoneme embeddings learned in Fang et al. (2020) study?,answer: In Fang et al. (2020) study, phoneme embeddings are learned using a deep-learning model without hand-crafted features.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What is the matrix factorization technique used in Sharma et al. (2021) study for learning word embeddings?,answer: In Sharma et al. (2021) study, non-negative matrix factorization is used to learn the embedding matrix for computing phonetic word embeddings.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What is the disadvantage mentioned regarding the approach used in Sharma et al. (2021) study for embedding new words?,answer: The disadvantage mentioned is that the approach used in Sharma et al. (2021) study cannot be used for embedding new words without recomputing the matrix.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What kind of vector representation does the Count-based Vectors approach use?,answer: The Count-based Vectors approach uses a TF-IDF vectorizer of 1-, 2-, and 3-grams for creating vector representations.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " How are articulatory features mapped to vectors in the context of the Articulatory Features and Distance section?,answer: Articulatory features are mapped to vectors by decomposing sounds into constituent properties and assigning values (+1/-1) based on presence/absence in the articulatory vectors.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What does the articulatory distance metric quantify in the context of the Articulatory Features and Distance section?,answer: The articulatory distance metric quantifies the phonetic similarity between a pair of words based on the cost of transformations needed to make their articulatory vectors identical.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}, {"question": " What is the main purpose of the Autoencoder method described in the text?,answer: The main purpose of the Autoencoder method is to create vector representations with a fixed dimension size by using an encoder-decoder architecture and bottleneck vector as the phonetic word embedding.", "ref_chunk": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}], "doc_text": "in phonetic representations of textual input. 2.1. Poetic Sound Similarity Parrish (2017) learns word embeddings captur- ing pronunciation similarity for poetry generation for words in the CMU Pronouncing Dictionary (Group, 2014). First, each phoneme is mapped to a set of phonetic features F using the function P2F : \uef23A \u2192 2F . From the sequence of sets that each sequence of phonemes maps to, bi-grams of phonetic features are created (using Cartesian product \u00d7 between sets \uebe1\uebe9 and \uebe1\uebe9+1) and counted. The function COUNTVEC outputs these bi-gram counts in a vector of constant dimension. The re- sulting vector is then reduced using PCA to the target embedding dimension d. W2F(\uebf8) = \u2329P2F(\uebf8\uebe9)|\uebf8\uebe9 \u2208 \uebf8\u232a F2V(\uebe1) = COUNTVEC.(cid:0) (cid:91) (array) (cid:1) \uebe1\uebe9 \u00d7 \uebe1\uebe9+1 1\u2264\uebe9\u2264|\uebe1|\u22121 \u0192PAR = PCAd({F2V(W2F(\uebf8))|\uebf8 \u2208 W}) (3) The function \u0192PAR can provide embeddings even for words unseen during training. This is because the only component dependent on the training data is the PCA over the vector of bigram counts, which can also be applied to new vectors. 2.2. phoneme2vec Fang et al. (2020) do not use hand-crafted features and learn phoneme embeddings using a more com- plex, deep-learning, model. They start with a gold sequence of phonemes (\uebf8\uebe9) and a noisy sequence of phonemes (y\uebe9). The phonemes are one-hot en- coded in matrices X and Y. The gold sequence is first read by an LSTM model, yielding the ini- tial hidden state h0. From this hidden state, the phonemes ( \u02c6y\uebe9) are decoded using teacher forcing (upon predicting \u02c6y\uebe9, the model receives the correct \uebf8\uebe9 as the input). The phoneme embedding ma- trix V is trained jointly with the model weights and constitutes the embedding function. h0 = LSTM(XV) Lp2v = \u2212(cid:88) (4) log softm\uebe1x(LSTM(Y<\uebe9V)y\uebe9 ) (5) 0<\uebe9\u2264|y| For a fair comparison, we average these vec- tors which are phoneme-level to get word-level embeddings. In addition, in contrast to other em- beddings, these phoneme embeddings are only 50-dimensional. We revisit the question of dimen- sionality in Section 5.5. 2.3. Phonetic Similarity Embeddings Sharma et al. (2021) propose a vowel-weighted phonetic similarity metric to compute similarities between words. They then use it for training pho- netic word embeddings which should share some properties with this similarity function. This is in contrast to the previous approaches, where the (1) (2) embedding training is indirect on an auxiliary task. Given a sound similarity function SPSE, they con- struct a matrix of similarity scores S \u2208 R|W|\u00d7|W| such that S\uebe9,j = SPSE(W\uebe9, Wj). On this matrix, they use non-negative matrix factorization to learn the embedding matrix V \u2208 R|W|\u00d7d such that the following loss is minimized: LPSE = ||S \u2212 V \u00b7 V T ||2 (6) Then, the \uebe9-th row of V contains the embedding for \uebe9-th word from W. A critical disadvantage of this approach is that it cannot be used for embedding new words because the matrix V would need to be recomputed again. We apply the sound similarity function SPSE, defined specifically for English, to all evaluation languages. 3. Our Models We now introduce several embedding baselines. Then, we describe our articulatory distance metric and models trained with supervision therefrom. 3.1. Count-based Vectors Perhaps the most straightforward way of creating a vector representation for a sequence of input char- acters or phonemes \uebf8 \u2208 \uef23\u2217 is simply counting n- grams in this sequence. We use a term frequency- inverse document frequency (TF-IDF) vectorizer of 1-, 2-, and 3-grams (formally denoted [\uebf8] n) across the input sequence of symbols (e.g. characters) with a maximum of 300 features. This vector then becomes our word embedding. For instance, the first dimension may be the TF-IDF score or occur- rence count of the bigram \u2329/dIn/, /a/\u232a. C2V(\uebf8) = [\uebf8] 1 \u222a [\uebf8] 2 \u222a [\uebf8] 3 \u0192count(\uebf8) = TF-IDFfeat ures (features) (7) ({C2V(\uebf8)|\uebf8 \u2208 W}) (8) =d 3.2. Autoencoder Another common approach, though less inter- pretable, for vector representation with fixed di- mension size is an encoder-decoder autoencoder. Specifically, we use this architecture together with forced-teacher decoding and use the bottleneck vector as the phonetic word embedding. In an ideal case, the fixed-size bottleneck contains all the information to reconstruct the whole sequence from \uef23\u2217. \u0192\u03b8(\uebf8) = LSTM(\uebf8|\u03b8) d\u03b8\u2032 (\uebf8) = LSTM(\uebf8|\u03b8\u2032) (encoder) (10) \u2212 log softmax(d\u03b8\u2032 (\u0192\u03b8(\uebf8)|\uebf8<\uebe9)\uebf8\uebe9 ) (11) (decoder) Lauto. = (cid:88) (9) 0<\uebe9\u2264|\uebf8| 3.3. Phonetic Word Embeddings With Articulatory Features 3.3.1. Articulatory Features and Distance Articulatory features (Bloomfield, 1993; Jakobson et al., 1951; Chomsky and Halle, 1968) decom- pose sounds into their constituent properties. Each segment can be mapped to a vector with n dif- ferent features (24 for PanPhon Mortensen et al., 2016) such as whether the phoneme segment is produced with a nasal airflow or if it is produced with raised or lowered tongue tip. A segment is a group of phonetic characters (e.g., as defined by Unicode) that represent a single sound. We de- fine \uebe1: \uef23P \u2192 {\u22121, 0, +1}24 as the function which maps a phoneme segment into a vector of artic- ulatory features. Values +1/-1 mean present/not present and the value 0 is used when the feature is irrelevant. The articulatory distance, also called feature edit distance (Mortensen et al., 2016), is a version of Levenshtein distance with custom costs. Specif- ically, the substitution cost is proportional to the Hamming distance between the source and target when they are represented as articulatory feature vectors. Omitting edge-cases, it is defined as: A\uebe9,j(\uebf8, \uebf8\u2032) = min \uf8f1 \uf8f2 A\uebe9\u22121,j(\uebf8, \uebf8\u2032) + d(\uebf8) A\uebe9,j\u22121(\uebf8, \uebf8\u2032) + \uebe9(\uebf8\u2032) A\uebe9\u22121,j\u22121(\uebf8, \uebf8\u2032) + s(\uebf8\uebe9, \uebf8\u2032 j \uf8f3 A(\uebf8, \uebf8\u2032) = A|\uebf8|,|\uebf8\u2032|(\uebf8, \uebf8\u2032) where d and \uebe9 are deletion and insertion costs, which we set to constant 1. The function s is a sub- stitution cost, defined as the number of elements (normalized) that need to be changed to render the two articulatory vectors identical: s(\uebf8, \uebf8\u2032) = 1 24 24 (cid:88) \uebe9=1 |\uebe1(\uebf8)\uebe9 \u2212 \uebe1(\uebf8\u2032)\uebe9| The articulatory distance A induces a metric space-like structure for words in \uef23\u2217. It quanti- fies the phonetic similarity between a pair of words,"}