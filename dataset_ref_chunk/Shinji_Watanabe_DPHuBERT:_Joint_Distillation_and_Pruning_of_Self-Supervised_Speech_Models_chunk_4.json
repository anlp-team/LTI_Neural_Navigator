{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_DPHuBERT:_Joint_Distillation_and_Pruning_of_Self-Supervised_Speech_Models_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does DPHuBERT outperform in IC, PR, QbE, and SD, even when using only 100h compared to previous distilled models using 960h?", "answer": " Previous distilled models", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " Why does DPWavLM achieve further improvements in 8 tasks compared to DPHuBERT?", "answer": " Unpruned WavLM Base+ is better than the unpruned HuBERT Base", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " What does Figure 2 show about the architecture of DPHuBERT?", "answer": " It is automatically discovered by structured pruning", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " Which layers are pruned the most for the CNN in DPHuBERT?", "answer": " The first and last layers", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " Why are three higher layers entirely removed for MHA in DPHuBERT?", "answer": " Because those layers are more redundant", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " Why are the 4th, 8th, and 12th layers preserved more than their neighbors in FFN for DPHuBERT?", "answer": " Because those layers are explicitly matched between the teacher and student models", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " In which tasks is the model slightly worse than the default setup in 7/10 tasks?", "answer": " Two-step training", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " What does the fourth row of Table 2 show in the ablation studies?", "answer": " Results without pruning CNN (only pruning attention heads and FFN intermediate sizes)", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " According to the conclusion section, what does DPHuBERT outperform in most tasks of SUPERB?", "answer": " Previous distillation methods", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}, {"question": " In the acknowledgements section, which grants supported the PSC Bridges2 and NCSA Delta?", "answer": " National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296", "ref_chunk": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}], "doc_text": "SID ASV Method 70 90 Millions Acc\u2191 Acc\u2191 PER\u2193 WER\u2193 Acc\u2191 MTWV\u2191 F1\u2191 / CER\u2193 Acc\u2191 EER\u2193 DER\u2193 HuBERT Large HuBERT Base 316.60 94.68 95.29 96.30 98.76 98.34 3.53 5.41 3.62 6.42 67.62 64.92 0.0353 0.0736 89.81 / 21.76 88.53 / 25.20 90.33 81.42 5.98 5.11 5.75 5.88 DPHuBERT 94.59 94.51 98.47 4.46 6.23 65.11 0.0246 88.37 / 24.60 83.17 7.05 5.79 Surprisingly, DPHuBERT using 100h already outperforms pre- vious distilled models using 960h in IC, PR, QbE and SD, and has similar results in the other tasks. This shows that DPHu- BERT learns powerful representations even from limited data. We have also compressed WavLM Base+ to obtain DP- WavLM. Compared to DPHuBERT, DPWavLM achieves fur- ther improvements in 8 tasks. This is because the unpruned WavLM Base+ is better than the unpruned HuBERT Base. These results demonstrate that our compression method can be applied to different speech SSL models. Figure 2 shows the architecture of DPHuBERT, which is automatically discovered by structured pruning. For CNN, the first and last layers are pruned the most. For MHA, three higher layers are entirely removed, indicating those layers are more redundant. Our results are consistent with prior studies about pruning [20, 25] or general speech encoders [37\u201340]. For FFN, the 4th, 8th and 12th layers are preserved more than their neigh- bors, because those layers are explicitly matched between the teacher and student models as defined by Eq. (1) in Section 3.2. 4.3. Ablation studies This model is (slightly) worse than the default setup in 7/10 tasks. This verifies that the CNN also has redundant compo- nents which can be pruned, as reported in [25, 41, 42]. 4.4. Results at various sparsities We train DPHuBERT with various target sparsities (t in Eqs. (5) (6)) and show results in Figure 3. For IC and SID, our method can significantly reduce the model size while keep- ing a similar accuracy as the original HuBERT Base. For ASR, the degradation is more severe, probably because the sequence transduction task is more challenging than classification tasks. 4.5. Compressing HuBERT Large Our method can be applied to larger speech SSL models with very limited training cost. In Table 3, HuBERT Large is com- pressed to have a similar size as HuBERT Base, which only takes about 60 GPU hours. The compressed model even out- performs HuBERT Base in several tasks like PR, SF-CER and SID. It is worse than HuBERT Base in KS, QbE and ASV, but the teacher model, HuBERT Large, is also clearly worse than HuBERT Base in those tasks. Table 2 summarizes the results of the following ablation studies. Two-step training. DPHuBERT has two training steps (Sec- tion 3.1 and Figure 1). The pruned model after Step 1 without Step 2 is evaluated in the second row of Table 2. It is worse than DPHuBERT in all tasks, verifying the necessity of Step 2. This is because Step 1 optimizes Eq. (6) where the regulariza- tion term competes with the distillation loss to meet the target sparsity, while Step 2 directly optimizes the distillation loss to improve the student\u2019s learned representations. Distillation methods. As discussed in Section 2.2, DPHuBERT uses layer-to-layer distillation instead of prediction-layer distil- lation in DistilHuBERT [12]. The third row of Table 2 shows that prediction-layer distillation causes severe degradations in all tasks, probably due to the deep student architecture. Directly matching intermediate layers facilitates the training of deep stu- dents as found in [14]. Pruning units. DPHuBERT prunes both CNN and Trans- former because CNN has a high computational cost [25, 41]. The fourth row of Table 2 shows results without pruning CNN (i.e., only pruning attention heads and FFN intermediate sizes). 5. Conclusion This work proposes DPHuBERT, a task-agnostic compres- sion method based on joint distillation and structured pruning. DPHuBERT outperforms previous distillation methods in most tasks of SUPERB. Comprehensive analyses are presented to in- vestigate its performance with less training data or at various sparsity ratios. In addition to HuBERT Base, our method can be directly applied to other speech SSL models such as WavLM and HuBERT Large while still being efficient and effective. In the future, we will exlore more sophisticated distillation objec- tives (e.g., the masking-based distillation loss used in LightHu- BERT [28]) to further improve the performance. 6. Acknowledgements We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. 7. References [1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representa- tions,\u201d in Proc. NeurIPS, 2020. [2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdi- nov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Rep- resentation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451\u20133460, 2021. [3] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Rep- resentation Learning at Scale,\u201d in Proc. Interspeech, 2022. [4] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505\u20131518, 2022. [5] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d in Proc. NeurIPS, 2021. [6] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Inter- speech, 2021. [7] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maal\u00f8e, T. N. Sainath, and S. Watanabe, \u201cSelf-supervised speech representation learning: A"}