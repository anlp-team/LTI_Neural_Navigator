{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_HomeRobot:_Open-Vocabulary_Mobile_Manipulation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the acronym OVMM stand for in the context of the text?", "answer": " Open-Vocabulary Mobile Manipulation", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " Why is Open-Vocabulary Mobile Manipulation considered a foundational challenge for robots in human environments?", "answer": " Because it involves tackling sub-problems from across robotics such as perception, language understanding, navigation, and manipulation.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " What is the goal of the HomeRobot OVMM benchmark?", "answer": " To have an agent navigate household environments, grasp novel objects, and place them on target receptacles.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " What components make up the HomeRobot benchmark?", "answer": " A simulation component and a real-world component.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " What is the success rate achieved by the baselines in the real world in the text?", "answer": " 20%", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " Why is the development of household robotic assistants considered a north star for roboticists?", "answer": " Because it has led to research in various areas like vision, manipulation, navigation, language understanding, and task planning.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " What does the text propose as a common platform and benchmarks to drive the robotics field forward?", "answer": " Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and benchmarks and infrastructure in simulation and the real world.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " How many human-authored interactive 3D scenes are used in the AI Habitat simulator in the simulation component of the benchmark?", "answer": " 200", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " What is the purpose of the Hello Robot Stretch platform used in the real-world component of the benchmark?", "answer": " It is an affordable and compliant platform for household and social robotics.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}, {"question": " Where will the real-world benchmarking be run as a part of the NeurIPS 2023 HomeRobot OVMM competition?", "answer": " As a part of the NeurIPS 2023 HomeRobot OVMM competition.", "ref_chunk": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}], "doc_text": "4 2 0 2 n a J 0 1 ] O R . s c [ 2 v 5 6 5 1 1 . 6 0 3 2 : v i X r a HomeRobot: Open-Vocabulary Mobile Manipulation Sriram Yenamandra\u22171 Mukul Khanna1 Alexander William Clegg2 Angel Chang4 1Georgia Tech Arun Ramachandran\u22171 Theophile Gervet2,3 John Turner2 Devendra Singh Chaplot2 Dhruv Batra1,2 Chris Paxton2 Yonatan Bisk2,3 3Carnegie Mellon 2FAIR, Meta AI Karmesh Yadav\u22171,2 Austin Wang1 Tsung-Yen Yang2 Zsolt Kira1 Vidhi Jain3 Manolis Savva4 Roozbeh Mottaghi2 4Simon Fraser homerobot-info@googlegroups.com (noun): An affordable compliant robot that navigates Abstract: HomeRobot homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer of the nav and place skills. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future work can improve performance. See videos on our website: https://ovmm.github.io/. Keywords: Sim-to-real, benchmarking robot learning, mobile manipulation 1 Introduction The aspiration to develop household robotic assistants has served as a north star for roboticists since the beginning of the field. The pursuit of this vision has spawned multiple areas of research within robotics from vision to manipulation, and has led to increasingly complex tasks and benchmarks. A useful household assistant requires creating a capable mobile manipulator that understands a wide variety of objects, how to interact with the environment, and how to intelligently explore a world with limited sensing. This has separately motivated research in diverse areas like navigation [1, 2], service robotics [3\u20135], language understanding [6, 7] and task and motion planning [8]. We refer to this guiding problem as Open-Vocabulary Mobile Manipulation (OVMM): a useful robot will be able to find and move arbitrary objects from place to place in an arbitrary home. Prior work does not tackle mobile manipulation in large, continuous, real-world environments. Instead, it generally simplifies the setting significantly, e.g. by using discrete action spaces, limited object sets, or small, single-room environments that are easily explored. However, recent developments tying language and vision have enabled robots to generalize beyond specific categories [9\u201313], often through multi-modal models such as CLIP [14]. Further, comparison across methods has remained difficult and reproduction of results across labs impossible, since many aspects of the 7th Conference on Robot Learning (CoRL 2023), Atlanta, USA. Serving Cart Move toy animal from chair to table Drawer TableFind Object on Start ReceptaclePick Object from Start ReceptacleFind Goal ReceptaclePlace Object on Goal Receptacle Move pitcher from drawer to serving cart Pitcher SIM Toy Animal REAL Chair Figure 1: Open-Vocabulary Mobile Manipulation requires agents to search for a previously unseen object at a particular location, and move it to the correct receptacle. settings (environments, and robots) have not been standardized. This is especially important now, as a new wave of research projects have begun to show promising results in complex, open-vocabulary navigation [9, 15, 11, 12, 16] and manipulation [17, 10, 18] \u2013 again on a wide range of robots and settings, and still limited to single-room environments. Clearly, now is the time when we need a common platform and benchmarks to drive the field forward. In this work, we define Open-Vocabulary Mobile Manipulation as a key task for in-home robotics and provide benchmarks and infrastructure, both in simulation and the real world, to build and evaluate full-stack integrated mobile manipulation systems, in a wide variety of human-centric environments, with open object sets. Our benchmark will further reproducible research in this setting, and the fact that we support arbitrary objects will enable the results to be deployed in a variety of real-world environments. OVMM: We propose the first reproducible mobile-manipulation benchmark for the real world, with an associated simulation component. In simulation, we use a dataset of 200 human-authored interactive 3D scenes [19] instantiated in the AI Habitat simulator [20, 21] to create a large number of challenging, multi-room OVMM problems with a wide variety of objects curated from a variety of sources. Some of these objects\u2019 categories have been seen during training; others have not. In the real world, we create an equivalent benchmark, also with a mix of seen and unseen object categories, in a controlled apartment environment. We use the Hello Robot Stretch [22]: an affordable and compliant platform for household and social robotics that is already in use at over 40 universities and industry research labs. Fig. 1 shows instantiations of our OVMM task in both the real-world benchmark and in simulation. We have a controlled real-world test environment, and plan to run the real-world benchmark yearly to assess progress on this challenging problem. Real-world benchmarking will be run as a part of the NeurIPS 2023 HomeRobot OVMM competition [23]. HomeRobot: We also propose HomeRobot,1 a software framework to facilitate extensive bench- marking in both simulated and physical environments. It comprises identical APIs that are imple- mented across both settings, enabling researchers to conduct experiments that can be replicated in both simulated and real-world environments. Table 1 compares HomeRobot OVMM to the literature. Notably, HomeRobot provides a robotics stack for the Hello Robot Stretch which supports a range of capabilities in both simulation and the"}