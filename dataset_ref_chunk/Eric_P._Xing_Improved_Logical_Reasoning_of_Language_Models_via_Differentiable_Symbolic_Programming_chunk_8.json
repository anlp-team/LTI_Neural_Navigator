{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Improved_Logical_Reasoning_of_Language_Models_via_Differentiable_Symbolic_Programming_chunk_8.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many rules are sampled during training time?", "answer": " 200", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What are the two Adam optimizers used in the experiment and what are they used for?", "answer": " ARL and ARE. ARE is used for optimizing models for relation extraction, while ARL is used for the confidence score tensor for the transitive rules.", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What is the learning rate for the Adam optimizer ARL?", "answer": " 0.001", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What are the weights set for the integrity constraints?", "answer": " 0.1 for result integrity violation loss and 0.01 for rule integrity constraint violation loss", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What is the batch size set for training?", "answer": " 16", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What prompt is used to obtain initial rule weights in the CLUTRR experiment?", "answer": " \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What is the prompt used for the transitive rule template?", "answer": " \u201cis R\u2019s R one\u2019s R? <mask>\u201d", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What is the setting for Few(5)-Shot experiments?", "answer": " Randomly selecting 5 examples from the training dataset used for other models", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " How many hand-crafted rules are included in the transitive KB?", "answer": " 92", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}, {"question": " What does the Zero-Shot-CoT prompt suggest GPT-3 to do?", "answer": " Auto-complete while working out a reasoning chain", "ref_chunk": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}], "doc_text": "the reasoning engine, we sample 200 rules according to their weights during the training time and deterministically use the top 200 learned rules during the test time. For the QA-No-Rule setup, the con\ufb01dence score of rules, the MLP classi\ufb01er for relation extraction, and the underlying LM are learned and updated simultaneously during train- ing. To account for their difference, we employ two Adam optimizers ARL and ARE. ARE is used for optimizing models for relation extraction, and thus will take as parameters the MLP classi\ufb01er and the underlying LM. It has a low learning rate 0.00001 since it needs to \ufb01ne-tune LMs. ARL, on the other hand, will take as a parameter the con\ufb01dence score tensor for the transitive rules, and is set to have a higher learning rate of 0.001. For the integrity constraints, we set the result integrity violation loss with the weight 0.1, and set the rule integrity con- straint violation loss with the weight 0.01. We set the batch size to 16 and train for 20 epochs. To obtain the initial rule weights for the compo- sition rule in our CLUTRR experiment, the prompt we use is \u201cMary\u2019s P\u2019s Q is her <mask>.\u201d where P and Q are enumerations of all possible relationships, and the unmasked value is treated as the answer R, producing composite(P, Q, R). For the other rule templates we used, the prompts are 1. transitive: \u201cis R\u2019s R one\u2019s R? <mask>\u201d; the probability of the unmasked word be- ing \u201cyes\u201d is treated the rule weight for transitive(R). 2. symmetric: \u201cdoes A is R of B means B is R of A? <mask>\u201d; the probability of the un- masked word being \u201cyes\u201d is treated the rule weight for symmetric(R). 3. inverse: \u201cA is R of B means B is <mask> of A\u201d; the unmasked value is treated as the answer P, producing inverse(R, P). 4. implies: \u201cdoes R imply P? <mask>\u201d; the probability of unmasked value being \u201cyes\u201d is treated as the rule weight for implies(R, P). GPT-3 Prompt Setups. For Zero-Shot, we use the prompt \u201cSo B is A\u2019s:\u201d for the query pair (A, B) to ask GPT-3 to complete the relationship between A and B. We pick the phrase in the \ufb01rst line or before the \ufb01rst period from the completed text and compare it directly with the ground truth relation. For the Few(5)-Shot setting, we randomly select 5 examples from the training dataset used for other models (k \u2208 [2, 3]) to serve as examples. We use the same prompt for Few-Shot and Fine-Tuned as the Zero-Shot and the automated GPT-3 \ufb01ne- tuning setup for our training dataset, trained for 4 epochs. To add in the transitive KB, we simply in- clude 92 hand-crafted rules in natural language as a part of the prompt, and we performed Zero-shot with KB, and Few(5)-shot with KB experiments. For the Zero-Shot-CoT setting, we use the prompt \u201cWho is B to A? Let\u2019s think step by step\u201d to sug- gest GPT-3 to auto-complete while working out a reasoning chain. Under this setup, it is impossible to compare the answer to the ground truth automat- ically. Therefore, we manually check through the whole test dataset of CLUTRR. B Additional Experimental Results In Table 8, we showcase the failure cases of large LMs for logical inference, where Zero-shot-CoT denotes zero-shot chain-of-thoughts (Kojima et al., 2022b). k Prompt 2 Jeffrey asked his brother William if he would come help him \ufb01x his car next weekend. Gabrielle was delighted to see her grandson, William, at her birthday party. Who is Jeffrey to Gabrielle? Let\u2019s do it step by step: GPT-3 CoT Output Jeffrey is Gabrielle\u2019s grandson. William is Jeffrey\u2019s brother. Ground Truth grandson 3 Elliott felt lonely when his wife Marian was gone to see her mother Molly. Elliott and his daughter Karen took a day off school to go to the zoo. Who is Karen to Molly? Let\u2019s do it step by step: 4 Frank took his daughter Julia to the park. Beverly, Emilie\u00b4s sister went with them. Kimberley was excited because today she was going to the zoo with her uncle Frank. Jenny spent a great day shopping with her daughter, Kimberley. Who is Beverly to Jenny? Let\u2019s do it step by step: Karen is Elliott\u2019s daughter. Molly is Elliott\u2019s wife\u2019s mother. There- fore, Karen is Molly\u2019s grand- daughter. Beverly is Jenny\u2019s sister-in-law. granddaughter niece 5 Melissa and her father, Stephen, went to the marina. Stephen\u2019s daughter, Margaret, had purchased a boat, and they were eager to see it. Annie and her aunt, Evelyn, went to the deli. They got a half a pound of corned beef, and two pounds of salami. Melissa and her sister Annie were arguing over a toy. While they were \ufb01ghting, Melissa\u2019s sister Margaret grabbed the toy, and ran off to play with it by herself. Who is Evelyn to Melissa? Let\u2019s do it step by step: James took his grandson Davis to the baseball game. Sharon and her son Brian went out to lunch together yesterday. Brian and his brother Oliver went to the park to play basketball. Davis played scrabble with his mother Sharon and his brother Oliver. Davis asked his brother Oliver if he would come help him \ufb01x his car next weekend. Who is James to Oliver? Let\u2019s do it step by step: 6 Melissa\u2019s is Stephen. Stephen has a daughter, Margaret. Margaret is Annie\u2019s aunt. There- fore, Evelyn is Annie\u2019s aunt, and Melissa\u2019s great-aunt. father James is Davis\u2019s grandfather, so he is Oliver\u2019s great-grandfather. aunt grandfather 7 George rushed to the hospital to \ufb01nd out that his wife and already given birth to a boy and had named him David. Larry was playing checkers with his brother Craig. Larry\u2019s sister Jean was waiting to play. Judy made mixed drinks for her son Craig. Judy\u2019s daughter Jean was jealous because she was n\u2019t offered one. Jean planned a trip to the zoo for her brother, Craig. They had a great time. George was having trouble with his"}