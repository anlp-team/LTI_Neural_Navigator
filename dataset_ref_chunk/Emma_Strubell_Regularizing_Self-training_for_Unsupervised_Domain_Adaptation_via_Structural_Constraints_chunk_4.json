{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Regularizing_Self-training_for_Unsupervised_Domain_Adaptation_via_Structural_Constraints_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the issue with having a large number of segments (ks) when over-segmenting objects?,answer: A large ks will over-segment each object in the scene, resulting in a trivial objectness constraint.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " How do purely depth-based segments differ in clustering objects compared to purely RGB segments?,answer: Purely depth-based segments may cluster together different object categories at similar depths, while purely RGB segments may assign the same cluster label to objects at distinct depths.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " What is the advantage of fusing region estimates from both depth and RGB modalities?,answer: Fusing region estimates from both depth and RGB modalities can lead to object regions that are more consistent with individual object instances.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " How are region labels assigned to pixels within an object region?,answer: A region label is assigned as the most frequent pseudo-label class within the object region.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " What role does the threshold \u03c4p play in object region labelling?,answer: The threshold \u03c4p selects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above the threshold, minimizing labeling inconsistencies.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " How is the pairwise constraint for objectness computation recast to reduce time complexity?,answer: The pairwise constraint is recast into a prototypical loss that reduces the time complexity to linear by computing a prototypical representation for each region.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " What is the purpose of the contrastive objective in the objectness constraint?,answer: The contrastive objective pulls together pixel representations within an object region and pushes apart those that belong to different object categories.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " How is the similarity score computed for the contrastive objectness constraint?,answer: The similarity score, based on Cosine metric, is computed between each pixel and the prototypical representation for the region.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " Why are objectness constraints only computed for target domain images?,answer: Objectness constraints are only computed for target domain images to improve generality.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}, {"question": " How does the proposed multimodal fusion approach demonstrate the effectiveness of the objectness constraint?,answer: The proposed multimodal fusion approach empirically demonstrates the effectiveness of the objectness constraint derived from fusion of depth and RGB modalities.", "ref_chunk": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}], "doc_text": "class, thus vi- olating the notion of object region. Conversely, a large ks will over-segment each object in the scene, resulting in a trivial objectness constraint. Triviality arises from enforc- ing similarity of pixel-embeddings that share roughly iden- tical pixel neighbourhoods and hence are likely to yield the same class predictions anyway. Thus, to formulate a non-trivial constraint with suf\ufb01- ciently small ks that also respects object boundaries, we propose to fuse region estimates from both depth and RGB modalities.We \ufb01rst obtain ks segments using SLIC over the RGB image followed by further partitioning of each seg- ment into smaller ones based on the depth segmentation. The process, visualised in Fig. 2 highlights the importance of our multimodal approach. Purely depth based segments are agnostic to pixel intensities and may cluster together dis- tinct object categories that lie at similar depths, for instance, the car in the front and the sidewalk. On the other hand, purely RGB segments with suf\ufb01ciently small ks may assign 5 the same cluster label even to objects at distinct depths, for example, the back of the bus and the small car at the back. In contrast, object regions derived from a fusion of these two modalities can lead to object regions that are more consis- tent with individual object instances (for example, the small car at the back as well as the car in the front). We empiri- cally demonstrate the effectiveness of objectness constraint derived from such multimodal fusion in Section 4.3. 3.2. Objectness Constraints through Contrast Our objectness constraint is formulated using a con- trastive objective that pulls together pixel representations within an object region and pushes apart those that belong to different object categories. Formally, we assign a region index and a region label to every pixel associated with an object region of the input scene. Each region index is a unique natural number in {1, . . . , K} where K is the num- ber of object regions. A region label is assigned as the most frequent pseudo-label class within the object region. In practice, noisy pseudo-labels can lead to region labelling that is inconsistent with true semantic labels. To minimise such inconsistencies, we introduce a threshold \u03c4p that se- lects valid object regions for which the proportion of pixels with pseudo-label class same as the region label is above this threshold. This selection excludes the object regions with no dominant pseudo-label class from contributing to the objectness constraint. Since the cost of computing pair- wise constraints is quadratic in the number of pixels, we recast the pairwise constraint into a protoypical loss that re- duces the time complexity to linear. Towards the end, we \ufb01rst compute a prototypical representation for each region using the associated pixel embeddings, \u03bdk = 1 |Uk| (cid:88) p\u2208Uk zp where, Uk is the set of pixel locations with the kth object- region. Then a similarity score (based on Cosine metric) is computed between each pixel and prototypical representa- tion that forms the basis for our contrastive objectness con- straint as Lt obj = 1 S (cid:88) (cid:88) k p\u2208Uk \uf8eb Lt obj(p) \uf8f6 Lt obj(p) = \u2212 log \uf8ec \uf8ed exp(\u02dczp \u00b7 \u02dc\u03bdk) (cid:80) k(cid:48)\u2208\u2126(k) exp(\u02dczp \u00b7 \u02dc\u03bdk(cid:48)) \uf8f7 \uf8f8 where, S is the total number of valid pixels, \u2126(k) is the set of valid object regions that have region labels other than k, and \u02dczp and \u02dc\u03bdk represent L2 normalised embeddings. Note that the objectness constraints are only computed for the target domain images since we are interested in improving (5) (6) (7) Table 1. Test of Generality: We compare the performance of regularised and un-regularised versions of three self-training approaches for two domain settings, namely, GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes. Both per-class IoU and mean IoUs are presented. The numbers in bold indicate higher accuracies in the pairwise comparisons, between a base-method and the base-method+PAC. Source Domain Method d a o r k l a w e d i s g n i d l i u b l l a w e c n e f e l o p t h g i l n g i s . e g e v n i a r r e t y k s n o s r e p r e d i r r a c k c u r t s u b n i a r t r o t o m e k i b CAG [60] CAG + PAC (ours) 87.0 86.3 44.6 45.7 82.9 84.5 32.1 30.5 35.7 35.5 40.6 38.9 38.9 40.3 45.5 49.9 82.6 86.0 23.5 33.5 78.7 81.1 64.0 64.1 27.2 25.5 84.4 84.5 17.5 21.3 34.8 32.9 35.8 36.3 26.7 26.7 32.8 40.0 A T G SAC [2] SAC + PAC (ours) 89.9 93.3 54.0 63.6 86.2 87.2 37.8 42.0 28.9 25.4 45.9 44.9 46.9 49.0 47.7 50.6 88.0 88.1 44.8 45.2 85.5 87.6 66.4 64.0 30.3 28.1 88.6 83.6 50.5 37.5 54.5 43.9 1.5 13.7 17.0 20.1 39.3 46.2 DACS [47] DACS + PAC (ours) 93.4 93.2 54.3 58.8 86.3 87.2 28.6 33.3 33.7 35.1 37.0 38.6 41.1 41.8 50.6 51.4 86.1 87.4 42.6 45.8 87.6 88.3 63.5 64.8 28.9 31.6 88.1 84.3 44.2 51.7 52.7 53.4 1.7 0.6 34.7 31.3 48.1 50.6 A H T N Y S I CAG CAG + PAC (ours) SAC [2] SAC + PAC (ours) 87.0 87.0 91.7 83.2 41.0 42.0 52.7 40.5 79.0 80.0 85.1 85.4 9.0 12.0 22.6 30.0 1.0 3.0 1.5 2.0 34.0 30.0 42.2 43.0 15.0 17.0 44.1 42.2 11.0 17.0 30.9 33.8 81.0 80.0 82.5 86.3 - - 81.0 88.0 73.8 89.8 55.0 57.0 63.0 65.3 16.0 5.0 20.9 33.5 77.0 75.0 84.9 85.1 - - 17.0 20.0 29.5 35.2 - - 2.0 1.0 26.9 29.9 47.0 52.0 52.2 55.3 DACS [47] DACS + PAC (ours) 84.9 90.6 23.0 46.7 83.7 83.3 16.0 18.7 1.0 1.3 36.3 35.1 35.0 34.5 42.8 32.0 81.7 85.1 - 89.5 88.5 63.5 66.0 34.5 35.0 85.3 83.8 - 41.5 43.1"}