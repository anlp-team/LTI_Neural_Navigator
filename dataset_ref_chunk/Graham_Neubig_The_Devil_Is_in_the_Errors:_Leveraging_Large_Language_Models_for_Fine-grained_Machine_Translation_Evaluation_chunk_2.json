{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_The_Devil_Is_in_the_Errors:_Leveraging_Large_Language_Models_for_Fine-grained_Machine_Translation_Evaluation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the main findings regarding the performance of PaLM-2 models when using AUTOMQM?", "answer": " PaLM-2 models can be prompted to generate rich MQM-like annotations, outperforming their score prediction counterparts at the segment-level.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What percentage of words that are part of major errors can be correctly identified by annotations predicted by PaLM-2 models?", "answer": " Over 50% of words that are part of major errors.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " How do annotations predicted by PaLM-2 models compare to the ones produced by state-of-the-art supervised word-level evaluators?", "answer": " Annotations predicted by PaLM-2 models are comparable to the ones produced by state-of-the-art supervised word-level evaluators.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What is the difficulty in evaluating machine translation?", "answer": " The difficulty arises because the set of correct translations for a given source sentence is often very large and not entirely known in advance.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What is reference-based evaluation in machine translation?", "answer": " Reference-based evaluation includes a reference translation created by a professional human translator, which is used as additional information when assessing the candidate translation.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " Why is framing machine translation evaluation as a score prediction task considered problematic?", "answer": " Any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What is the role of Multidimensional Quality Metrics (MQM) in evaluating machine translation?", "answer": " MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimensions, without providing a quality score for each translation.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What is the significance of leveraging transfer learning in machine translation metrics?", "answer": " Transfer learning is important to achieve metrics with better correlation with human judgments.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " How do generative LLMs contribute to natural language understanding and evaluation tasks?", "answer": " Generative LLMs consistently demonstrate impressive results in natural language understanding and zero- and few-shot transfer tasks, sparking interest in using these models for evaluation.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}, {"question": " What approach did Lu et al. take to improve ChatGPT-based evaluators?", "answer": " Lu et al. leveraged error analysis, a prompting technique similar to AUTOMQM, to enhance ChatGPT-based evaluators.", "ref_chunk": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}], "doc_text": "state-of-the-art learned met- rics. We find that, with AUTOMQM, PaLM-2 mod- els can be prompted to generate rich MQM- like annotations, outperforming their score prediction counterparts at the segment-level. Furthermore, annotations predicted by PaLM- 2 models correctly identify over 50% of words that are part of major errors, and are compa- rable to the ones produced by state-of-the-art supervised word-level evaluators. Our findings might have significant implica- tions for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023). 2 Background: MT Evaluation Machine translation evaluation is one of the most well-studied evaluation problems in NLP (Callison- Burch et al., 2008; Freitag et al., 2022). In this task, given 1. a source sentence in a (source) language 2. a candidate translation in a (target) language an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natu- ral language generation evaluation problems, this task is difficult because the set of correct transla- tions for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, of- ten (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candi- date translation. This sub-problem is known as reference-based evaluation (as opposed reference- less evaluation or quality estimation). Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a can- didate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high cor- relation with human-provided scores (Freitag et al., 2022). However, framing machine translation evalu- ation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a). This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) frame- work (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine transla- tion. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimen- sions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Impor- tantly, the MQM framework does not ask anno- tators to provide a quality score for each transla- tion, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic met- rics that leverage MQM data only use the final qual- ity score produced by the framework and discard the error span information and classification. 3 Related Work The success of learned machine translation met- rics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neu- ral network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More re- cently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zero- and few-shot transfer and, naturally, interest in em- ploying these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since ex- tended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b). Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim- ilarly to AUTOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoder- decoder language models, required supervised data to work, and overall underperformed other top met- rics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that do- ing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 ex- amples per language pair. Concurrently with our work, Xu et al. (2023) proposed INSTRUCTSCORE, a LLaMA-based evaluator that asks models to iden- tify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don\u2019t leverage zero- and few- shot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM. Additionally, WMT Word-Level Quality Esti- mation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of ma- jor severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level anno- tations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation mod- els. 4 Using LLMs to Predict Quality Scores Recent works have shown that large language mod- els are versatile, general-purpose models that can be used to tackle many problems in NLP, includ- ing evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by explor- ing how LLMs can be used for machine translation evaluation through score prediction. 4.1 Prompting We start by measuring how far we can push the performance of LLMs"}