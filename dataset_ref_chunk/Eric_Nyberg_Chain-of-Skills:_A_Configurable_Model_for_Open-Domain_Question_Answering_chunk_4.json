{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_Nyberg_Chain-of-Skills:_A_Configurable_Model_for_Open-Domain_Question_Answering_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of evaluations are conducted using the retrieval skill from the pretrained model?,        answer: Zero-shot evaluations    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " In the oracle second-hop retrieval setting based on HotpotQA, what is used as evidence?,        answer: Gold first-hop evidence    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " Which model is reported to achieve the highest accuracy in the Zero-shot top-k evaluations on NQ, WebQ, and EntityQuestions?,        answer: co-Condenser (Gao and Callan, 2022)    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " What is the percentage of questions for which the answer string is found in the top-k passages known as?,        answer: Passage EM (Exact Match)    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " What method is used to fine-tune the pretrained model with extra skills on NQ, HotpotQA, and OTT-QA?,        answer: Multi-task fashion    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " In the supervised in-domain evaluation, which model outperforms DPR-PAQ and other baselines for the NQ dataset?,        answer: COS    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " What do the cross-data evaluations on SQuAD and EntityQuestions aim to test?,        answer: Model robustness towards domain shift    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " What is used to evaluate the model in the Zero-shot results?,        answer: Self-supervised dense retrievers and BM25 as baselines    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " In the Supervised In-domain results for NQ, what models are compared with COS?,        answer: DPR-multi, ANCE-multi, DPR-PAQ, co-Condenser, SPAR-wiki    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}, {"question": " What is the main advantage of COS over DPR-PAQ in the NQ dataset evaluation?,        answer: COS can combine multiple skills for inference    ", "ref_chunk": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}], "doc_text": "et al. (2020). More detailed (pretraining/fine-tuning) data statis- tics and experimental settings are in Appendix B. 4.2 Evaluation Settings We evaluate our model in three scenarios. Zero-shot Evaluation Similar to recent self- supervised dense retrievers on Wikipedia, we con- duct zero-shot evaluations using the retrieval skill from our pretrained model on NQ, WebQ, Enti- tyQuestions and HotpotQA. To assess the model\u2019s ability to handle expanded query retrieval, we de- sign an oracle second-hop retrieval setting (gold first-hop evidence is used) based on HotpotQA. Fol- lowing Izacard et al. (2021) and Ram et al. (2022), we report top-k retrieval accuracy (answer recall), EntityQuestions Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 NQ WebQ HotpotQA Avg BM25 Contriever (Izacard et al., 2021) Spider (Ram et al., 2022) 62.9 67.8 68.3 78.3 82.1 81.2 62.4 65.4 65.9 75.5 79.8 79.7 70.8 61.8 65.1 79.2 74.2 76.4 37.5 48.7 35.3 50.5 64.5 48.6 58.4 60.9 58.7 70.9 75.2 71.5 COS (pretrain-only) 68.0 81.8 66.7 80.3 70.7 79.1 77.9 87.9 70.8 82.3 Table 1: Zero-shot top-k accuracy on test sets for NQ, WebQ and EntityQuestions, and dev set for HotpotQA. Top-20 Top-100 Top-20 Top-50 Top-100 DPR-multi (Karpukhin et al., 2020) ANCE-multi (Xiong et al., 2021a) DPR-PAQ (Oguz et al., 2022) co-Condenser (Gao and Callan, 2022) SPAR-wiki (Chen et al., 2021b) 79.5 82.1 84.7 84.3 83.0 86.1 87.9 89.2 89.0 88.8 CORE (Ma et al., 2022a) 74.5 82.9 87.1 COS COS w/ CORE configuration 79.9 80.5 88.9 88.6 92.2 91.8 Table 3: Supervised top-k accuracy on OTT-QA dev. COS 85.6 90.2 Table 2: Supervised top-k accuracy on NQ test. Passage EM MDR (Xiong et al., 2021b) Baleen (Khattab et al., 2021) 81.20 86.10 i.e., the percentage of questions for which the an- swer string is found in the top-k passages. IRRR (Qi et al., 2021) TPRR (Zhang et al., 2021a) HopRetriever-plus (Li et al., 2021) AISO (Zhu et al., 2021) 84.10 86.19 86.94 88.17 Supervised In-domain Evaluation We further fine-tune our pretrained model with two extra skills (entity span proposal and reranking) on NQ, Hot- potQA and OTT-QA, again in a multi-task fash- ion. Unlike multi-hop data with supervision for all skills, only single retrieval and reranking data is available for NQ. During training, all datasets are treated equally without any loss balancing. Differ- ent from previous retrieval-only work, we explore Chain-of-Skills retrieval by using different skill configurations. Specifically, we use skill con- figuration for task A, B and C shown in Figure 1 for NQ, OTT-QA and HotpotQA, respectively. We again report top-k retrieval accuracy for NQ and OTT-QA following previous work. For HotpotQA, we follow the literature using the top-1 pair of evi- dence accuracy (passage EM). Cross-data Evaluation To test the model robust- ness towards domain shift, we conduct cross-data evaluations on SQuAD and EntityQuestions. Al- though considerable success has been achieved for supervised dense retrievers using in-domain eval- uations, those models have a hard time general- izing to query distribution shift (e.g., questions about rare entities; Sciavolino et al., 2021) com- pared with BM25. In particular, we are interested to see whether Chain-of-Skills retrieval is more robust. Again, top-k retrieval accuracy is used. COS 88.89 Table 4: Supervised passage EM on HotpotQA dev. 4.3 Results Zero-shot Results For zero-shot evaluations, we use two recent self-supervised dense retrievers, Contriever (Izacard et al., 2021) and Spider (Ram et al., 2022), and BM25 as baselines. The results are presented in Table 1. As we can see, BM25 is a strong baseline matching the average retrieval per- formance of Spider and Contriever over considered datasets. COS achieves similar results on NQ and WebQ compared with self-supervised dense meth- ods. On the other hand, we observe significant gains on HotpotQA and EntityQuestions, where both dense retrievers are lacking. In summary, our model shows superior zero-shot performance in terms of average answer recall across the board, surpassing BM25 with the largest gains, which in- dicates the benefit of our multi-task pretraining. Supervised In-domain Results As various cus- tomized retrievers are developed for NQ, OTT- QA and HotpotQA, we compare COS with differ- ent dataset-specific baselines separately. For NQ, we report two types of baselines, 1) bi-encoders with multi-dataset training and 2) models with aug- mented pretraining. For the first type, we have DPR-multi (Karpukhin et al., 2020) and ANCE- multi (Xiong et al., 2021a), where the DPR model is initialized from BERT-based and ANCE is ini- tialized from DPR. For the second type, DPR-PAQ (Oguz et al., 2022) is initialized from the RoBERTa- large model (Liu et al., 2019b) with pretraining us- ing synthetic queries (the PAQ corpus (Lewis et al., 2021)), co-Condenser (Gao and Callan, 2022) in- corporated retrieval-oriented modeling during lan- guage model pretraining on Wikipedia; SPAR-wiki (Chen et al., 2021b) combine a pretrained lexical model on Wikipedia with a dataset-specific dense retriever. Both co-Condenser and SPAR-wiki are initialized from BERT-base. As shown by results for NQ (Table 2), COS outperforms all baselines with or without pretraining. It is particularly en- couraging that despite being a smaller model, COS achieves superior performance than DPR-PAQ. The reasons are two-fold: Oguz et al. (2022) has shown that scaling up the retriever from base to large size only provides limited gains after pretraining. More- over, DPR-PAQ only learns a single retrieval skill, whereas COS can combine multiple skills for in- ference. We defer the analysis of the advantage of chain-of-skills inference later (\u00a75.2). For OTT-QA, we only compare with the SOTA model CORE (Ma et al., 2022a), because other OTT-QA specific retrievers are not directly compa- rable where extra customized knowledge source is used. As CORE also uses multiple skills to find evi- dence chains, we include a baseline where the infer- ence follows the CORE skill configuration but uses modules from COS. For HotpotQA, we compare against three types of baselines, dense retrievers focused on expanded query retrieval MDR (Xiong et al., 2021b) and Baleen (Khattab et al., 2021), sparse retrieval combined with query reformulation IRRR (Qi et al., 2021) and TPRR (Zhang et al., 2021a)"}