{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Deep_Speech_Synthesis_from_MRI-Based_Articulatory_Representations_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What approach is adopted to center each frame in the model described in the text?,answer: A centering approach that centers each frame around a relatively fixed point is adopted.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " Where is the center point located in the model?,answer: The center point is located on the hard palate.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What preprocessing step was found useful in the model?,answer: Denoising was found useful.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What are the two types of intermediate representations compared in the text?,answer: The two types of intermediate representations compared are spectrums and deep representations.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What model architecture is used as the baseline method in the text?,answer: The CNN-BiLSTM architecture is used as the baseline method.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " Which neural vocoder is used to reconstruct the waveform signal in the text?,answer: HiFi-CAR is used as the neural vocoder.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What method does the HiFi-CAR model use to directly synthesize waveforms from articulatory features?,answer: The HiFi-CAR model uses an autoregressive version of the HiFi-GAN convolutional neural network.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What challenge does the currently available dataset suffer from?,answer: The dataset suffers from poor quality due to significant reverberation and noise.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What off-the-shelf toolkit is employed to enhance the quality of speech recordings in the text?,answer: The Adobe Podcast toolkit is employed.", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}, {"question": " What weighted sum is used as the target waveform in the MRI-to-speech task in the text?,answer: 0.9 * ye + 0.1 * yo", "ref_chunk": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}], "doc_text": "ability of such models to generalize to unseen positions. Thus, we adopt a centering ap- proach that centers each frame around a relatively fixed point to improve generalizability. Specifically, we calculate the stan- dard deviation (\u03c3x, \u03c3y) of each of the 170 points across the training set and center every frame at the point with the lowest (cid:112)\u03c32 y. This center point is located on the hard palate, cir- cled in green in Figure 3. We note that the standard deviation results reflect human speech production behavior, as the hard palate is relatively still across utterances whereas the tongue varies noticeably, highlighting the interpretability of our MRI- based articulatory features. Another preprocessing step that we found useful was denoising, which we detail in Section 3.4. x + \u03c32 epiglottis tongue nose velum arytenoid neck hard palate upper lip pharynx trachea lower teeth chin lower lip back nasal cavity Figure 3: Standard deviation of each MRI feature (Sec. 2). 3. Models 3.1. Intermediate-Representation Baselines Currently, a popular speech synthesis approach is to first syn- thesize an intermediate representation from the input and then map the intermediate representation to the waveform domain [26, 27, 14]. Wu et al. [13] showed that directly synthesizing speech from EMA outperformed a spectrum-intermediate ap- proach in terms of computational efficiency and yielded com- parable synthesis quality. Intuitively, omitting the spectrum in- termediate reflects how the human speech production process does not perform this intermediate mapping [13]. In this work, we also compare using intermediate representations with di- rectly synthesizing from inputs. We observe two popular types of intermediate representations in the literature: (1) spectrums [26, 27, 14], and (2) deep representations [4]. To compare our proposed direct modelling approach in Section 3.3 with both in- termediate modelling methods, we experiment with Mel spec- trogram and HuBERT [28] intermediates. For the Mel spectro- gram calculation, we use size-240 hops, size-1024 FFTs, Hann windows, and 80 Mels. With HuBERT, we use the output of the model\u2019s last hidden layer, linearly interpolated to match the MRI input sampling rate. We denote spectrum-intermediate models with \u201cSpe.\u201d and HuBERT ones with \u201cHub.\u201d in our re- sults below for readability. In our MRI-to-speech task, di- rect modeling is both more computationally efficient and more high-fidelity than the intermediate approaches, as discussed in Sections 4 and 5. We detail the model architectures of our intermediate-representation baselines in Section 3.2. 3.2. CNN-BiLSTM Baseline As per Yu et al. [14], we employ the CNN-BiLSTM architec- ture as the baseline method. This method involves processing each MRI frame through a sequence of four CNN layers, with two max-pooling layers incorporated in the middle. The ex- tracted features are then aggregated along the time axis and fed to a BiLSTM layer to generate the mel-spectrogram. Since the inputs in our MRI-to-speech task are sequences of vectors rather than the MRI video inputs used in Yu et al. [14], we use 1D convolutions instead of 2D and 3D. Finally, a neural vocoder is used to reconstruct the waveform signal. For this vocoder, we use HiFi-CAR [13], which outperforms the WaveGlow architec- ture [29] used by Yu et al. [14]. HiFi-CAR is an autoregressive version of the HiFi-GAN convolutional network [30], detailed in Section 3.3. It is worth noting that they used the original speech data, without any denoising, resulting in unsatisfactory performance. For a fair comparison, we also train this model using enhanced speech. In our experiments in Sections 4 and 5 below, we refer to this model as CBL for readability. 3.3. HiFi-CAR Model Similar to the method used by Wu et al. [13], our model di- rectly synthesizes waveforms from articulatory features with- out the need for an intermediate representation. Specifically, we build on their HiFi-CAR model, which is a HiFi-GAN convolu- tional neural network [30] modified to be autoregressive using the CAR-GAN audio encoder [31]. To our knowledge, training models to directly synthesize waveforms from MRI data has not yielded successful results previously. However, we observe that this model outperforms our baselines in terms of both compu- tational efficiency and fidelity, as discussed in Sections 4 and 5. We also use the HiFi-CAR vocoder to map intermediate features to waveforms for our intermediate-representation baselines. For all HiFi-CAR models, we initialize their weights with those of a HiFi-GAN spectrum-to-waveform vocoder pre-trained on Lib- riTTS.2 We note that this initialization approach noticeably im- proves performance compared to Wu et al. [13]. Further mod- eling details can be found in the accompanying codebase. 3.4. Speech Enhancement Model The currently available dataset [19] suffers from poor quality due to significant reverberation and noise, which poses a sig- nificant challenge for accurate modeling of the relationship be- tween MRI and speech. To circumvent this issue, we employed an off-the-shelf Adobe Podcast toolkit3, which processes speech recordings to enhance their quality and makes them sound as if they were recorded in a professional studio. Therefore the re- sulting speech is better suited for our purposes. Unfortunately, due to its proprietary nature, we do not have access to its tech- nical details. Through our observation, however, we conjec- ture that it may contain a pipeline of bandwidth extension [32] and speech enhancement [33]. Specifically, we hypothesize that the toolkit up-samples the speech to 48kHz and leverages HiFi- GAN [34] to generate high-quality speech. We downsample the enhanced speech to the waveform sampling rate of our MRI In our MRI- dataset to keep model output lengths the same. to-speech task, we use 0.9 \u2217 ye + 0.1 \u2217 yo as our target wave- form, where ye is the enhanced waveform and yo is the original one. Using this weighted sum yields more intelligible MRI-to- speech models than using just ye, which may be due to how deep speech enhancers add irregular noise that can be smoothed to more learnable targets by adding the original, more natural waveforms. 4. Computational Efficiency Given the importance of computational efficiency for real-time, on-device speech synthesizers we compare the number of pa- rameters and inference times between our"}