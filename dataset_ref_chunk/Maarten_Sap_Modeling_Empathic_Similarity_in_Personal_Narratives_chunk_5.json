{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Modeling_Empathic_Similarity_in_Personal_Narratives_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the user study described in the text?", "answer": " To see how the model performs in retrieving a story that is empathically similar to a story that the users told themselves.", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " How many participants were recruited for the study?", "answer": " 150 participants", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What percentage of the participants were women?", "answer": " 58%", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What type of study design was used in the research?", "answer": " Within-subject study design", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What tool was used to measure empathy towards each story?", "answer": " A shortened version of the State Empathy Survey", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What were the three dimensions of empathy covered in the survey?", "answer": " Affective, cognitive, and associative empathy", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What was the main finding of the study regarding empathy towards stories retrieved by different models?", "answer": " Users significantly empathized more with stories retrieved by the model finetuned on EMPATHICSTORIES than off-the-shelf SBERT.", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What was the effect of the model on empathy across all three dimensions?", "answer": " The effect was present across all three dimensions of empathy: affective, cognitive, and associative empathy.", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " What were some common reasons why participants empathized with the retrieved stories?", "answer": " Related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions.", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}, {"question": " Why did participants sometimes not empathize with the retrieved stories, especially in the case of the finetuned model?", "answer": " Due to stark differences in the main events of their own story and the model\u2019s selected story.", "ref_chunk": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}], "doc_text": "Bao et al., 2021; Sharma et al., 2020) are humans verifying or ranking model out- puts based on inputs from test data. This provides a valuable signal of model quality, but isn\u2019t represen- tative of how a model could be used in real-world applications due to input distribution mismatch and lack of personal investment in the task. Our hu- Figure 3: Total empathy for the story retrieved by our model vs. SBERT. Error bars show standard error. man evaluation is a full user study to see how the model performs in retrieving a story that is empath- ically similar to a story that the users told them- selves. Through our user study, we demonstrate the applicability of the task to improve empathy towards retrieval of human stories, as well as how our dataset was used to develop the empathic sim- ilarity retrieval task and why the task matters in the real-world. Our hypothesis is: Users will em- pathize more with stories retrieved by our model (BART finetuned on EMPATHICSTORIES) than sto- ries retrieved by SBERT. 7.1 Participants and Recruitment We recruited a pool of 150 participants from Pro- lific. Participants were primarily women (58%, 38% men, 3% non-binary, 1% undisclosed) and white (73%, 8% Black, 9% other or undisclosed, 4% Indian, 3% Asian, 2 % Hispanic, 1% Native American). The mean age for participants was 37 (s.d. 11.6), and participants on average said they would consider themselves empathetic people (mean 4.3, s.d. 0.81 for Likert scale from 1-5). 7.2 Study Protocol Participants rated their mood, wrote a personal story, then rated their empathy towards the sto- ries retrieved by the baseline and proposed models. They additionally answered questions about the story they wrote (main event, emotion, and moral of the story) and their demographic information (age, ethnicity, and gender). User Interface. We designed a web interface simi- lar to a guided journaling app and distributed the link to the interface during the study. The inter- face connects to a server run on a GPU machine Figure 4: Breakdown of empathy dimensions for the story retrieved by our model vs. SBERT (4x Nvidia A40s, 256GB of RAM, and 64 cores), which retrieves story responses in real time. Writing Prompts and Stories Retrieved. We carefully designed writing prompts to present to the participants to elicit highly personal stories, in- spired by questions from the Life Story Interview (McAdams, 2007), an approach from social science to gather key moments from a person\u2019s life. Conditions. We used a within-subject study de- sign, where each participant was exposed to 2 con- ditions presented in random order. In Condition 1, participants read a story retrieved by our best performing model on the empathic similarity task (BART + finetuning). In Condition 2, participants read a story retrieved by SBERT. For both models, we select the best response that minimizes cosine distance. Measures. To measure empathy towards each story, we used a shortened version of the State Empathy Survey (Shen, 2010), which contains 7 questions covering affective (sharing of others\u2019 feel- ings), cognitive (adopting another\u2019s point of view), and associative (identification with others) aspects of situational empathy. We also ask users to provide a free-text explanation of whether and why they found the retrieved story empathically resonant, to gain qualitative insights into their experience. 7.3 Effects on Empathy With our results shown in Figure 3, we found through a paired t-test (N = 150) that users signif- icantly empathized more with stories retrieved by our model finetuned on EMPATHICSTORIES than off-the-shelf SBERT (t(149) = 2.43, p < 0.01, Cohen\u2019s d = 0.26), validating our hypothesis. In addition, this effect was present across all three dimensions of empathy: affective (t(149) = 1.87, p = 0.03, Cohen\u2019s d = 0.21), cognitive (t(149) = Figure 5: Reasons why participants did or did not empathize with the retrieved story. 2.05, p = 0.02, Cohen\u2019s d = 0.21), and associa- tive empathy (t(149) = 2.61, p = 0.005, Cohen\u2019s d = 0.27), as shown in Figure 4 (empathy values are the summed scores from the empathy survey). Interestingly, the difference in empathic response across conditions is strongest for associative empa- thy, which measures how much the user can iden- tify with the narrator of the story. We examine reasons why users empathized with retrieved stories across conditions (Figure 5). Across both conditions, empathy towards a story was often related to how well-read, genuine, and consistent the story was, and if the user could empathize with the narrator\u2019s emotional reactions. When participants did not empathize with a re- trieved story, this was more often than not due to stark differences in the main events of their own story and the model\u2019s selected story. This effect was strongest for our finetuned model, as it was trained on data with a more open definition of em- pathy than just sharing the same situation. In cer- tain cases, this could result in the events being too different for the user to empathize with. Interestingly, we see that our model chose stories that aligned better on events and emotions with respect to the story they wrote, and participants thought the stories were more original compared to SBERT-retrieved stories. In cases where the participant did not empathize with the retrieved story, SBERT-retrieved stories were considered less consistent, less genuine, less, original, did not read as well, and did not match on emotions as well compared to our model. narrator felt, and the takeaway of the story. For example, one participant shared that \u201cI found no moment where I didn\u2019t fully understand the author, and I share a very similar story about my father...its absolutely amazing...I enjoyed this study very much.\u201d Other participants wrote, \u201cI empathize heavily with this story because it has many similarities to my own. Kind of a \u2018started from the bottom, now we\u2019re here\u2019 vibe, which I love to see\u201d and \u201cI can relate to the feelings of abandonment and regret expressed.\u201d 8 Future Directions for Empathic Similarity In summary, few prior works on text-based empa-"}