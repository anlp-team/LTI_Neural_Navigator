{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_The_Hidden_Dance_of_Phonemes_and_Visage:_Unveiling_the_Enigmatic_Link_between_Phonemes_and_Facial_Features_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What type of model is used for phoneme segmentation in the text?,answer: Wav2Vec2-Large-XLSR-53 model", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " How many hours of unlabeled speech data was used to train the model?,answer: More than 50,000 hours", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " What is the first step in training the model?,answer: Contrastive task on masked latent speech representations", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " Why is the model fine-tuned on multi-lingual labeled common voice data?,answer: To quantize the latent shared across languages", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " What normalization technique is used for the mel-frequency bins?,answer: By mean and variance of each mel-frequency bin", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " What does AM stand for in the text?,answer: Acoustic Models", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " How many experiments are conducted for each phoneme-AM pair?,answer: 10 repeated experiments", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " Which phoneme had the highest 1 - CIu value according to Figure 3?,answer: /i:/", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " What is the explanation given for why vowel phonemes achieve good performance in the test set?,answer: No constriction of airflow in the vocal tract", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}, {"question": " Which region of the face exhibits more predictable AMs according to the text?,answer: Nose and mouth", "ref_chunk": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}], "doc_text": "phoneme segmentation approaches. Specifically, we use the Wav2Vec2-Large-XLSR-53 model [23] developed by FAIR, which learns powerful speech representa- tions from more than 50.000 hours of unlabeled speech. This Figure 2: (a) The selected landmarks. (b) The visualization of the 6 most predictable AMs. They are arranged in descending order from left to right and top to bottom. Numbers in the face denote the index of landmarks. model is trained using a contrastive task on masked latent speech representations and can learn a quantization of the latent shared across languages. It is then fine-tuned on multi-lingual labeled common voice data. Since our data primarily contains standard English pronunciations, this model can provide rela- tively high segmentation accuracy. We adopt the wav2vec2-xlsr-53-espeak-cv-ft in huggingface 1 in our experiments. After splitting, we choose the most frequently used phonemes which have number of sam- ples \u2265 5000. The detailed list is provided in the label of Fig. 3. For each phoneme recording, we follow [11] and perform 64- dimensional log mel-spectrograms using an analysis window of 25ms, with a hop of 10ms between frames. We perform nor- malization by mean and variance of each mel-frequency bin. AM summarization. We summarize the most commonly used AMs [24, 25, 26, 27, 28], including distances, proportions, and angles in Table 1. The selected landmark is shown in Fig. 2 (a). These AMs are more robust than 3D coordinate representa- tions. This is attributed to the complete elimination of variations induced by spatial misalignment, thus rendering them more re- liable and resistant to perturbations. The ground truth AMs are normalized to have a mean of zero and a variance of one. Table 1: The summarized AMs. distance 31-37 39-43 2-7 55-63 32-36 33-35 30-53 54-61 40-42 50-53 59-53 proportion 31-37 / 27-30 32-36 / 59-53 32-36 / 27-30 54-64 / 31-37 31-37 / 59-53 56-62 / 31-37 angle 31-30-37 31-29-37 29-30-34 Training details. For each phoneme-AM pair, we conduct In 10 repeated experiments to ensure statistical significance. 1https://huggingface.co/facebook/wav2vec2-xlsr-53-espeak-cv-ft each experiment, we randomly sample 5000 data samples and randomly split them into the Dt/Dv1 /Dv2 set in the ratio of 70%/10%/20%. We follow the typical settings of Adam [29] for optimization of the estimator. The loss function we use is the mean squared error loss. The size of the mini-batch and learning rate is set to 128 and 0.0001, respectively. 4.3. Results 4.3.1. Analysis of phonemes For each phoneme, we calculate the average 1 \u2212 CIu result with every AMs. As can be seen from Fig. 3, /i:/ got the high- est avg. 1 \u2212 CIu value 0.199, and /b/ got the lowest value -0.06. When 1 \u2212 CIu is lower than 0, AMs are averagely un- predictable from the phoneme. The three phonemes with the lowest and negative values are /t/, /b/ and /d/, which are all plosive consonants. During the pronunciation of plosive con- sonants, we complete stoppage of airflow followed by a sud- den release of air through trivial mouse open and close, and there is minimal movement of the facial muscles and structures. Consequently, the prediction of any acoustic model based solely on such phonemes is challenging. On the contrary, most vow- els achieve good performance in the test set, and all the top 6 phonemes belong to vowels with 1 \u2212 CIu > 0.10. Com- pared with consonants, there is no constriction of airflow in the vocal tract when pronouncing vowels. In order to produce spe- cific vowels, the facial muscles have relatively greater move- ment during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus vowel phonemes may carry more information about facial features. This, therefore, can make the model better capture the hidden correlation when predicting AMs. 1\u2212\ud835\udc36\ud835\udc3c! Figure 3: Phonemes in descending order by avg. 1 \u2212 CIu. 4.3.2. Analysis of AMs Similarly, for each AM, we also calculated the average 1 \u2212 CIu results with all phonemes. To intuitively locate the most pre- dictable AMs (with the highest avg. 1 \u2212 CIu) on the 3D face, we visualize them in Fig. 2 (b). Most of the predictable AMs are around the nose and mouth. On the contrary, AMs around the eyes are less predictable. This is consistent with the fact that the nose and mouth shapes (distances, proportions, and an- gles) affect the pronunciation of phonemes. Other than the nose and mouth, the jaw is another region frequently occurring in the most predictable AMs. Since the jaw is another region that exhibits frequent movement during pronunciation, we hypothe- size that for a specific AM, if it is more frequently moved during the pronunciation of phonemes, the AM is generally more pre- dictable. We further verify this hypothesis in the next section. 4.3.3. Relationship between phonemes and AMs Table 2: Detailed results of phoneme-AM pairs. AMs /E/ /D/ /f/ /i:/ /v/ /w/ /\u00e6/ 0.10 -0.04 0.08 0.23 0.11 0.18 0.18 31-30-37 0.10 0.04 0.19 0.11 0.10 0.21 -0.09 0.05 0.09 -0.07 0.21 0.06 0.11 0.21 0.02 0.08 0.05 0.04 -0.03 0.08 0.09 39-43 50-53 2-7 We investigate the detailed relationship between phoneme and AM pairs to verify our hypothesis. As shown in Table 2, we list 4 typical AMs paired with 7 phonemes, where 39-43 is an oblique distance of the lip, 31-30-37 is an angle in the nose, 50-53 is the distance between the lip and jaw, and 2-7 is the distance between eyebrow. In the case of AM 2-7, no matter pairing with any phoneme, the value is relatively low (all 1 \u2212 CIu values close to 0). During the pronunciation pro- cess of any phonemes, the movement of this particular region is very limited. Therefore, phonemes can barely carry information about AMs in this part. However, for 39-43, it shows that /i:/, /w/, and /\u00e6/ have the highest value. When pronouncing these three phonemes, the mouth usually grins in order to control the output airflow. And the distance between facial landmarks 39 and 43 could slightly influence the airflow from"}