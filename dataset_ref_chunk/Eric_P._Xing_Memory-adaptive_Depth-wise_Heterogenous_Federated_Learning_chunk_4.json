{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Memory-adaptive_Depth-wise_Heterogenous_Federated_Learning_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What could be buffered to compute once the memory budget permits?,answer: the activation {zt+1 i=1", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " What is the rationale behind saving the redundant forward-pass from the first block to the jth block?,answer: to save the computation required for approximately solving Jk subproblems", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " Explain the concept of depth-wise sequential learning illustrated in Figure 4.,answer: It involves training blocks sequentially, freezing each updated block and using its activation to train the next block", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " How is memory-efficient inference achieved in the depth-wise training approach?,answer: By storing the activation zj in the hard drive and discarding the predecessor activation zj\u22121", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " How does the concept of handle extreme memory constraints with partial training work?,answer: It skips the first few blocks that are too large to fit, without significant performance degradation", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " What is the purpose of exploiting sufficient memory with mutual knowledge distillation?,answer: To improve the performance of federated learning by utilizing rich computing resources and reducing communication burden", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " Explain the memory cost table provided in Table 1 with respect to the depth and width of PreResNet-20.,answer: It shows the memory cost in MB for different blocks within the neural network based on depth and width", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " What is the main focus of the experimental setups in the text?,answer: The experiments are mainly conducted on CIFAR-10 and CIFAR-100 datasets", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " How are class distributions simulated in the experiments mentioned?,answer: Using the Dirichlet distribution with specific parameters to create non-IID settings with class imbalance", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}, {"question": " What is the significance of adopting pathological non-IID data partition in the experiments?,answer: It is used to study the impact of extreme class distributions on the performance of the models in federated learning", "ref_chunk": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}], "doc_text": "training for the jth block. We re- mark that when the memory budget permits, the activation {zt+1 i=1, which no longer requires the gradient, could be buffered to compute {zt+1 is ob- k,\u2113 }j\u22121 k,\u2113 }j\u22121 j, \u03d5t+1 j\u22121,i}nk j,i }nk i=1 once the \u03b8t+1 k,j (1) tained, hence saving the redundant forward-pass from the first block to the jth block. Also, the number computation required for approximately solving Jk subproblems in the form of (1) should be similar to approximately solving the problem minW Fk(W) if we perform the same number of local updates, and ignore the negligible computation over- head in updating in the head \u03d5. This is because the amount of computation required by one gradient evaluation \u2207W Fk is equivalent to that of the summation of gradient evaluation of {\u2207\u03b8j L}Jk j=1. \ud835\udf19\ud835\udf03!,$\ud835\udf03!,% \ud835\udc66\" \ud835\udf03!,# \ud835\udc58-thclient decomposition\ud835\udc58\u2032-thclient decomposition\ud835\udc65 \ud835\udf03!\u2018,#\ud835\udf03!\u2018,$ Figure 3: Memory-adaptive neural network decomposition. The second row represents the full neural network with the block size indicating the memory consumption. The first and third rows explain the neural network decomposition based on different clients\u2019 memory budgets. \ud835\udf19 \ud835\udc66# \ud835\udc65 \ud835\udc65 \ud835\udc66# \ud835\udc66# \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% InactiveActive\ud835\udc65 Frozen Block Training Block \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf03!,#\ud835\udf03!,$\ud835\udf03!,% \ud835\udf19 \ud835\udf19 Figure 4: An example of depth-wise sequential learning. There are three training steps: 1) training the first block and the classifier with the skip connection (He et al. 2016a); 2) freezing the updated first block and using its activation to train the second block and the classifier with the skip con- nection; 3) freezing the updated first two blocks and using the activation of the second block to train the third block and the classifier. Memory-efficient Inference. Depth-wise inference fol- lows the similar logic of the frozen-then-pass forward in depth-wise training. Specifically, for each input x, we store the activation zj in the hard drive and discard the predeces- sor activation zj\u22121. Then we can reload zj into memory as the input and get the activation zj+1. The procedure is re- peated until the prediction \u02c6y is obtained. We end this section by giving the detailed algorithmic de- scription in Algorihtm 1. Handle Extrem Memory Constraints with Partial Tranining According to the memory consumption analysis in the Em- pirical Study, the memory bottleneck of training a neural network is related to the block with the largest activations, which some devices may still not afford. These \u201clarge\u201d blocks are usually the layers close to the input side. To tackle this issue, we borrow the idea of partial training in FEDEPTH, where we skip the first few blocks that are too large to fit even after the finnest blocks decomposition. This mechanism would not incur significant performance degra- dation because the input-side layers learn similar represen- tations on different datasets (Kornblith et al. 2019), and clients with sufficient memory will provide model param- eters of these input-side layers in the FL aggregation phase. Figure 5 emeprically validates such a strategy. We train a customized 14-layer ResNet (13 convolution layers and one classifier) on MNIST with 20 clients under non-IID distribu- tion, respectively and measure the similarity of neural net- work representations, using both canonical correlation anal- ysis (CCA) and centered kernel alignment (CKA) (Kornblith et al. 2019). Figure 5: Correspondences between layers of different local neural networks trained from private datasets in FL under non-IID distribution. We observe that early layers, but not later layers, learn similar representations. Exploit Sufficient Memory with Mutual Knowledge Distillation Previous works on heterogeneous FL ignore the situation where some clients with rich computing resources may par- ticipate in the federated training on the fly. Their sufficient memory budget could be potentially utilized to improve the performance of FL via regularizing local model training (Mendieta et al. 2022; Li et al. 2020a). Ensembling multiple models is an effective way to improve generalization and reduce variance (Shi et al. 2021; Nam et al. 2021). How- ever, considering each model is independently trained in en- semble learning methods, we have to upload/download all of these models in FL settings leading to a significant com- munication burden. Therefore, we design a new training and aggregation method based on mutual knowledge distillation (MKD) (Hinton et al. 2015; Zhang et al. 2018), where all student neural networks learn collaboratively and teach each other. Therefore, clients with sufficient memory only need to upload one of the local models to the server for aggregation because the knowledge consensus achieved among all mod- els through distillation. Formally, assume the kth client has Depth Memory Width Memory B1\u223c3 B4 B5\u223c6 B7 B8\u223c9 20.02 14.05 10.07 7.21 5.28 \u00d7 1 8 \u00d7 1 6 \u00d7 1 3 \u00d7 1 2 \u00d71 14.51 19.34 38.68 58.02 116.04 Table 1: Memory cost (in MB) with respect to depth and width of PreResNet-20. Each block consists of 2 convolu- tion layers. B1\u223c3 indicates Block 1, 2 and 3 in PreResNet- 20 have the same memory cost of 20.02 MB. The values are estimated by pytorch-summary1. a rich memory budget to train M > 1 models. Then locally it solves min k ,\u00b7\u00b7\u00b7 ,WM {W 1 k } 1 M M (cid:88) m=1 Fk(W m k ) + 1 M \u2212 1 M (cid:88) m\u2032\u0338=m KL (cid:16) hm\u2032 \u2225hm(cid:17) where hm are logits calculated from the model Wm over the local training set and KL is the Kullback Leibler Divergence. More con- (cid:80)nk = 1 cretely, KL is nk the logits of the ith sample computed over model Wm. (cid:16) \u2225hm(cid:17) hm\u2032 i=1 KL(hm\u2032 i \u2225hm i ), where hm i Experiments Experimental Setups Datasets and data partition. Our experiments are mainly conducted on CIFAR-10 and CIFAR100 (LeCun et al. 1998; Krizhevsky and Hinton 2009). Extensive results are attached in the appendix. To simulate the non-IID setting with class imbalance, we follow (Yurochkin et al. 2019; Acar et al. 2021; Gao et al. 2022) to distribute each class to clients using the Dirichlet distribution with \u03b1(\u03bb), where \u03bb = {0.3, 1.0}. Besides, we adopt pathological non-IID data partition \u03b2(\u039b) that is used by the"}