{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Inference-Time_Policy_Adapters_(IPA):_Tailoring_Extreme-Scale_LMs_without_Fine-tuning_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the IPA model mentioned in the text?", "answer": " Improving reliability and trustworthiness in various downstream applications.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " How does IPA improve the generated dialogue response compared to Blenderbot?", "answer": " IPA significantly improves faithfulness while preserving dialogue quality, outperforming all other baselines.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " What is controlled decoding and why is it important?", "answer": " Controlled decoding involves designing new decoding algorithms to explore controlled generation at inference time, which helps in imposing constraints on text generation.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " How has reinforcement learning historically been used in NLG tasks?", "answer": " Reinforcement learning has been used in tasks like machine translation, summarization, dialogue systems, and text games to optimize for non-differentiable rewards.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " What is a potential ethical concern associated with the use of IPA?", "answer": " One potential concern is that IPA could be used for unintended malicious purposes like producing hateful or toxic content.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " What is a limitation of the IPA model mentioned in the text?", "answer": " One limitation is that it requires access to the output logits of the base LM, which hinders compatibility with certain models.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " How does IPA combine the generalizability of RL with the flexibility of inference-time techniques?", "answer": " IPA combines RL with plug-and-play flexibility, allowing customization of large language models without costly fine-tuning.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " What type of decoding algorithm is GBS and what does it do?", "answer": " GBS is a lexically constrained decoding algorithm that generalizes beam search by constraining the decoding space with keyword-related penalties.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " Why is reinforcement learning susceptible to reward hacking?", "answer": " Reinforcement learning methods like REINFORCE can lead to issues with reward hacking when optimizing for arbitrary non-differentiable rewards.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}, {"question": " What role does the Perspective API play in the reward function of IPA for the toxicity reduction task?", "answer": " The Perspective API calls are used as the reward function for the toxicity reduction task in IPA.", "ref_chunk": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}], "doc_text": "2022a). Moreover, pre-trained dialogue models like Blenderbot demonstrate even worse performance at generating faithful response, de- spite being trained on WoW and other knowledge- grounded dialogue datasets in their pre-training stage. IPA significantly improves the faithfulness of the generated dialogue response over its base policy Blenderbot while preserving the dialogue quality (i.e., coherence and engagingness), outper- forming all other baselines. Our results showcases the potential of IPA to improve reliability and trust- worthiness in various downstream applications. 5 Related Work Controlled Decoding Recent studies have ex- plored controlled generation at inference time by designing new decoding algorithms (Keskar et al., 2019; Mireshghallah et al., 2022; Li et al., 2022a; Chen et al., 2022; Zhang et al., 2022). For example, Neurologic decoding (Lu et al., 2020), and GBS (Hokamp and Liu, 2017) generalize beam search for lexically constrained decoding, by constrain- ing decoding space with keyword-related penalties. DExperts (Liu et al., 2021b) modifies output dis- tribution during decoding with attribute-specific expert models. Another line of research develops gradient-based decoding for more general control (Qin et al., 2020, 2022; Sha, 2020; Dathathri et al., 2020b; Kumar et al., 2021). For example, COLD Decoding (Qin et al., 2022) introduces energy- based modeling to impose arbitrary constraints on text and samples with Langevin dynamics. Despite their progress, these approaches either are designed for particular control types or rely on computation- ally expensive gradient computations. Reinforcement Learning for NLG RL has his- torically been used in multiple NLG tasks such as machine translation (Wu et al., 2016; Nguyen et al., 2017), summarization (Paulus et al., 2017), dialogue (Li et al., 2016; Zhou et al., 2017), text games (Narasimhan et al., 2015; Hausknecht et al., 2020), etc to optimize for an arbitrary non- differentiable reward. This was often done us- ing online policy gradient methods such as RE- INFORCE (Sutton and Barto, 2018), leading to documented issues with reward hacking (Choshen et al., 2020; Kiegeland and Kreutzer, 2021). Re- cent advances introduce a KL reward penalty which significantly increases the naturalness of generated text (Ouyang et al., 2022; Korbak et al., 2022). This method has been used extensively to tune a base LM via online on-policy (Ramamurthy* et al., 2023), off-policy (Guo et al., 2022; Lu et al., 2022b), and offline (Snell et al., 2023; Korbak et al., 2023) RL. Such methods quickly become compu- tationally infeasible for extreme-scale LMs. 6 Conclusion we present IPA, a lightweight inference-time policy adapter that tailor a frozen large language model towards desirable properties (e.g., safety, coher- ence) in an efficient, generalizable, and flexible way. Specifically, IPA combines the generaliz- ability of RL with the plug-and-play flexibility of inference-time techniques, permitting customiza- tion of large language models without the need for costly fine-tuning. Extensive experiments across five challenging text generation tasks show that IPA brings consistent improvements over LLMs, outperforming competitive baselines \u2014 sometimes even surpassing expensive fine-tuning. We hope our work sheds light on creative and efficient algo- rithmic innovations to complement the pursuit of model scales with academic-level resources. 7 Limitations and Ethical Consideration While the versatility of the IPA is a crucial feature that enables aligning large language models with arbitrary user-given objectives, it may also pose potential dual-use concerns, especially when com- bined with the power of large language models. First, as with any controllable text generation technique, IPA could be potentially used for unin- tended malicious purposes, such as manipulating models to produce hateful, toxic content or misin- formation. As malicious users can already exploit any existing techniques for harmful purposes the- oretically, we foresee minimal risk introduced by IPA specifically. Nevertheless, we highly recom- mend avoiding such negative applications of IPA. Moreover, similar to any RL-based method that depends on the reward function for learning sig- nals, IPA is susceptible to the innate shortcomings from the reward model. For instance, we use the Perspective API calls as the reward function for the toxicity reduction task; any limitations or potential biases from these public API calls will propagate into the learning of IPA. Nonetheless, as more ac- curate, transparent, and inclusive classifiers are de- veloped, we anticipate that IPA would inherit those improvements as well. Beyond these two primary concerns, another in- herent limitation of IPA is its requirement to access the output logits of the base LM. This constraint hinders IPA\u2019s compatibility with certain models, such as GPT-4, which permit access only to the output, not the logits. Finally, like general RL frameworks, IPA relies on the assumption that user objectives are quantifiable through a reward func- tion. However, this premise may not always hold, particularly when user objectives are inherently challenging to measure, thus limiting IPA\u2019s appli- cability. References Armen Aghajanyan, Sonal Gupta, and Luke Zettle- moyer. 2021. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Pro- ceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 7319\u2013 7328. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Michael Chang, Abhishek Gupta, Sergey Levine, and Thomas L. Griffiths. 2019. Automatically compos- ing representation transformations as a means for generalization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Howard Chen, Huihan Li, Danqi Chen, and Karthik Controllable text genera- arXiv preprint Narasimhan. 2022. tion with language constraints. arXiv:2212.10466. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In Interna- tional Conference on Learning Representations. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane"}