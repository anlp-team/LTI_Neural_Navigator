{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Deep_Speech_Synthesis_from_MRI-Based_Articulatory_Representations_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the focus of the study mentioned in the text?,        answer: The focus of the study is articulatory synthesis using human vocal tract information.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What are some limitations of using electromagnetic articulography (EMA) for articulatory synthesis?,        answer: Limitations of EMA include lack of critical articulatory information like excitation and nasality, which limits generalization capabilities.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What alternative feature set is proposed in the text to bridge the gap in articulatory synthesis?,        answer: An alternative MRI-based feature set is proposed in the text.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What procedures are introduced in the text to enhance the generalizability of deep learning methods trained on MRI data?,        answer: Normalization and denoising procedures are introduced to enhance generalizability.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What is the proposed method in the text to enhance speech fidelity?,        answer: The proposed method involves utilizing a generative adversarial network (GAN) based method that directly synthesizes waveform from articulatory features.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What is the significance of the MRI dataset used in the study?,        answer: The MRI dataset captures dynamic information about vocal tract movements and shaping during human speech production.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What method is used to track the contours of vocal tract air-tissue boundaries in the MRI frames?,        answer: A semi-automatic method is used to track the contours.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " How is the MRI feature set pruned to mitigate the problem of overfitting?,        answer: The MRI feature set is pruned by discarding segments that do not contribute much to understanding speech production variation across utterances.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " What model is employed to estimate sentence boundaries in the study?,        answer: A pre-trained BERT-based model is employed to estimate sentence boundaries.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}, {"question": " How are the audio recordings aligned in the study?,        answer: The audio recordings are aligned using the Montreal force aligner.    ", "ref_chunk": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}], "doc_text": "3 2 0 2 l u J 5 ] S A . s s e e [ 1 v 1 7 4 2 0 . 7 0 3 2 : v i X r a Deep Speech Synthesis from MRI-Based Articulatory Representations Peter Wu1, Tingle Li1, Yijing Lu3, Yubin Zhang3, Jiachen Lian1, Alan W Black2, Louis Goldstein3, Shinji Watanabe2, Gopala K. Anumanchipalli1 1University of California, Berkeley, United States 2Carnegie Mellon University, United States 3University of Southern California, United States peterw1@berkeley.edu Abstract In this paper, we study articulatory synthesis, a speech syn- thesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable syn- thesizers. While recent advances have enabled intelligible artic- ulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excita- tion and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to en- hance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Fi- nally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and iden- tify the most suitable MRI feature subset for articulatory synthe- sis. Index Terms: speech synthesis, articulatory synthesis this problem, we enhance the utterances and propose a genera- tive adversarial network (GAN) based method that directly syn- thesizes waveform from articulatory features, which produces noticeably more intelligible speech than the baselines. We summarize our contributions as follows: \u2022 We propose a novel MRI-based representation for ar- ticulatory synthesis, along with effective preprocessing strategies for such data. We demonstrate that our proposed model outperforms baselines across several evaluation metrics. We quantitatively and qualitatively identify the advan- tages of MRI over EMA and the most important MRI features for articulatory synthesis. Code and additional related information will be available at https://github.com/articulatory/articulatory. 2. MRI Dataset 1. Introduction Deep speech synthesis technology has made significant ad- vancements in recent years, leading to high-performing models for tasks such as text-to-speech [1, 2, 3], voice conversion [4, 5], and speech translation [6, 7]. However, the development of brain-to-speech devices [8, 9] still poses significant challenges, requiring faster and more data-efficient models. Articulatory synthesis [10, 11, 12, 13, 14, 15] offers a potential solution by synthesizing speech from a compact, smooth, and interpretable articulatory space [16, 17, 18, 19, 20, 21]. Electromagnetic articulography (EMA) is a commonly used articulatory representation [13], but it only contains 6 x- y points, making it challenging to comprehensively capture ar- ticulatory movements. Real-time magnetic resonance imaging (MRI) is a state-of-the-art tool that captures dynamic informa- tion about vocal tract movements and shaping during human speech production, offering a feature-rich alternative to EMA. It contains hundreds of x-y points, including positional informa- tion for the hard palate, pharynx, epiglottis, velum, and larynx, all of which are important for speech production but not directly described in raw EMA data. Moreover, recent advances in im- age acquisition and reconstruction techniques have enabled suf- ficient temporal and spatial resolutions (e.g., 12ms and 2.4 \u00d7 2.4 mm2) that allow researchers to study the intricate and dynamic interactions during speech production [22, 23]. However, since MRI dataset participants speak from inside a tube-shaped MRI machine, there is a noticeable amount of re- verberation in the collected utterances, resulting in an unsatis- factory performance on MRI-to-speech synthesis. To overcome We utilize the real-time MRI and its corresponding audio recordings of one native American English speaker (female, 25- year-old), with a total speech duration of approximately 17 min- utes and a sampling rate of 20 kHz, which are acquired from a publicly available multispeaker MRI dataset [19]. This dataset includes midsagittal real-time MRI videos with a spatial reso- lution of 2.4 \u00d7 2.4 mm2 (84 \u00d7 84 pixels) and a temporal res- olution of 12-ms (83 frames per second), capturing the vocal tract movements during the production of a comprehensive set of scripted and spontaneous speech materials. To prepare the MRI data for our model, we use a semi- automatic method [24] to track the contours of vocal tract air- tissue boundaries in each raw MRI frame (Figure 1) and seg- mented the contours into anatomical components, as shown in Figure 2 and Figure 3. To mitigate the problem of overfitting, we pruned the MRI feature set by discarding segments that did not contribute much to understanding how speech production varies across utterances. Figure 3 presents the full set of seg- ments, while Figure 2 shows the reduced set. The original set comprises 170 x-y coordinates, whereas the reduced set con- tains only 115. We then concatenate and flatten the 115 x-y coordinates into a 230-dimensional vector, which we used as input for our MRI-to-speech synthesis task. Since the raw MRI data is composed of long utterances with lots of silences, we first segment the utterances into sentence- long pieces. Then we employ a pre-trained BERT-based model1 to estimate sentence boundaries, and align the audio recordings as well as the orthographic and phonological transcriptions us- ing Montreal force aligner [25]. The resulting alignments are 1https://huggingface.co/felflare/ bert-restore-punctuation Figure 1: One MRI frame during the utterance \u201capa\u201d. lower lip lower lip upper lip lower teeth chin tongue tip tongue dorsum lower incisor tongue hard palate upper lip epiglottis velum tongue blade pharynx arytenoid Figure 2: Extracted MRI features for the utterance \u201capa\u201d. Lighter is earlier in time. The labeled points are the estimated EMA features (Sec. 2). manually calibrated by professional phoneticians. By utiliz- ing the estimated sentence boundaries and word alignments, we split the audio recordings and MRI data into 236 utterances, to- taling 11 minutes. Finally, these utterances are randomly split into a 0.85-0.05-0.10 train-val-test split, resulting in 200, 11, and 25 utterances in the train, val, and test sets, respectively. Furthermore, the head location is not fixed within the MRI data, which can negatively impact the"}