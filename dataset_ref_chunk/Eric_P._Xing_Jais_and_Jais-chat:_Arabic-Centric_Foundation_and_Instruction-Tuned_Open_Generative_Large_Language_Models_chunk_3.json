{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the ratio of the Arabic:English:code mix in the pretraining data?", "answer": " 1:2:0.4", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " Where do they collect their Arabic training data from?", "answer": " Multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " How is the English content translated to Arabic in the dataset augmentation process?", "answer": " Using an in-house machine translation system", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " What dataset is used for upscaling the Arabic tokens?", "answer": " 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " What is the translation performance of the in-house translation system for English to Arabic?", "answer": " 31 and 40 BLEU points on Flores-101 and a held-out test dataset, respectively", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " What is the token count for English data sourced from The Pile?", "answer": " 232B tokens", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " Which dataset in The Pile includes a mix of fiction and non-fiction books?", "answer": " Books3", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " From where is the DeepMind Mathematics dataset sourced?", "answer": " Various topics formatted as natural language prompts", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " What is the dataset derived from the CourtListener platform?", "answer": " FreeLaw", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}, {"question": " What is the total number of tokens for English datasets other than The Pile?", "answer": " 46B tokens", "ref_chunk": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}], "doc_text": "3,036,944,104 8,242,639,393 5,920,544,065 3,331,705,832 2,426,671,361 2,180,480,535 210,506,141 31,757,468 Total 55,178,798,942 Table 1: Composition and breakdown of our Arabic pretraining dataset (without translation). Moreover, the extent of knowledge of Arabic world embedded in these models is limited, as they only include relatively small amounts of native Arabic text. To tackle this challenge, we pretrain our model with the largest Arabic dataset in the world, while further extending it with English data and some programming code, to improve the logical reasoning abilities of the model. Our pretraining data mix is 1:2:0.4 for Arabic:English:code. We arrived at this ratio through extensive experiments on smaller models, which we describe in Section 3. We base this mix on all of the available Arabic data, as this is the smallest of the three data sources. We collect our Arabic training data from multiple sources including web pages, Wikipedia articles, news articles, Arabic books, and social network content. To augment the dataset, we also translate English content to Arabic using an in-house machine translation system.4 We restrict this to high-quality English resources such as the English Wikipedia and English books. We apply checks to avoid translating English sources with embedded code, or text that is not well structured. A breakdown of the Arabic dataset (except the translated content) is detailed in Table 1. Specifically, we use text from the following sources: Abu El-Khair: a collection of more than five million news articles, collected from ten major news sources of Arabic countries over a period of fourteen years [AEK16]. Aranews: Arabic news corpus from multiple sources ranging from year 2005-2022 [GEQ12] ArabicText 2022: an open-source Arabic collection5 prepared by the Beijing Academy of Artificial Intelligence (BAAI), that includes Arabic text corpora such as ArabicWeb22-A, ArabicWeb16 [SKF+16], OSCAR6, ArabicWeb22-B, CC100-AR [CKG+20], and Arabic Tweets. Arabic subset of C4: a cleaned version of the Common Crawl using the cleaning and the filtering de- scribed in [RSR+20]. We use the Arabic subset of this corpus. Arabic Wikipedia: Wikipedia written in Arabic7 ArabicNews 2020: an in-house news crawl at Inception of various Arabic news channels. 4Our in-house translation system is a standard transformer sequence-to-sequence model implemented in the FairSeq library [OEB+19] and trained on public datasets available in OPUS [Tie12]. The English to Arabic translation performance is 31 and 40 BLEU points [PRWZ02] on Flores-101 and a held-out test dataset, respectively. 5https://data.baai.ac.cn/details/ArabicText-2022 6https://oscar-project.org/ 7https://dumps.wikimedia.org/ 5 Maktabah: a corpus of approximately 6,500 Arabic books.8 UN Meeting transcripts: the United Nations Parallel Corpus,9 v1.0 [ZJDP16] which is available in the six official languages of the United Nations, of which we use the Arabic documents. Other Sources: a combined dataset of multiple smaller corpora including poetry, news, entertainment, sports, and management documents.10 We further augment the Arabic data by translating 3B tokens from English Wikipedia and 15B tokens from the Books3 corpus. As a result, we increase the Arabic data from 55B to 72B tokens. Subsequently, we upsample this Arabic data 1.6 times, obtaining 116B Arabic tokens. For English, we use The Pile [GBB+20], a collection of 22 high-quality datasets, from which we randomly sample 232B English tokens and 46B tokens from its GitHub subset. Table 2 shows details about the English data we use. Specifically, we use text from the following sources, part of The Pile: Pile-CC: A subset of The Pile dataset, derived from the Common Crawl, a collection of website crawls from 2008 onwards. The dataset includes raw web pages, metadata, and text extractions from diverse domains. Due to the varying quality of the data in Common Crawl, Pile-CC is created using jusText [EN13] on Web Archive files for extraction, yielding higher quality output than directly using the WET files [GBB+20]. Books3: Derived from the contents of the Bibliotik private tracker made available by Shawn Presser [Pre20]. It is a mix of fiction and non-fiction books, significantly larger than the next largest dataset, BookCorpus2, and was included for its value in long-range context modeling and coherent storytelling. ArXiv: A subset of the ArXiv preprint repository for research papers, which has been in operation since 1991.11 PubMed Central: A subset of the PubMed online repository for biomedical articles, managed by the United States\u2019 National Center for Biotechnology Information (NCBI).12 OpenWebText2: A web scrape dataset produced by EleutherAI, inspired by WebText [RWC+19] and OpenWebTextCorpus [GC19]. Wikipedia (en): The dataset, sourced from the TensorFlow Datasets13, includes articles from the English Wikipedia as a standard source of high-quality text for language modeling. FreeLaw: This dataset is derived from the CourtListener platform14, part of the Free Law Project, which provides access to legal opinions from federal and state courts in the United States. PubMed Abstracts: This dataset15 includes abstracts from 30 million publications in PubMed, managed by the National Library of Medicine. It encompasses the significantly limited coverage of full texts in PubMed Central (PMC) and includes MEDLINE abstracts from 1946 to the present day. DeepMind Mathematics: A collection of mathematical problems from various topics formatted as nat- ural language prompts [SGHK19]. It is included in The Pile to enhance the mathematical ability of the language models [BMR+20]. Project Gutenberg (PG-19): This dataset consists of classic Western literature from Project Gutenberg, specifically books published before 1919 [RPJ+20]. It represents distinct styles compared to the more modern Books3 and BookCorpus datasets and is already used for long-distance context modeling. BookCorpus2: An expanded version of the original BookCorpus [ZKZ+15], comprising books by un- published authors, minimizing overlap with Project Gutenberg and Books3, which include published books. It is commonly used for language model training [RNSS18]. 8https://www.kaggle.com/datasets/mahmoudqaddoumi/arabic-library 9https://conferences.unite.un.org/uncorpus 10https://master.dl.sourceforge.net, https://github.com/ceefour/hadith-islamware, //alt.qcri.org/resources1/qedcorpus/QEDCorpusv1.4_MT.tgz 11https://arxiv.org/ 12https://www.ncbi.nlm.nih.gov/pmc 13https://www.tensorflow.org/datasets/catalog/wikipedia#wikipedia20200301en 14https://www.courtlistener.com/ 15https://github.com/thoppe/The-Pile-PubMed 6 https: Language English English English English English English English English English English English English English English English English Dataset Pile-CC [GBB+20] Books3 [Pre20] ArXiv11 PubMed Central12 OpenWebText2 [RWC+19] Wikipedia13 FreeLaw14 PubMed Abstracts15 DM Mathematics [SGHK19] Gutenberg (PG-19) [RPJ+20] BookCorpus2 [ZKZ+15] EuroParl [Koe05] PhilPapers16 YoutubeSubtitles17 NIH ExPorter18 Enron Emails [KY04] Tokens (Billions) 25.1 25.1 25.1 25.1 12.5 25.1 10.4 10.4 16.7 18.8 18.8 4.2 4.2 3.3 3.3 3.8 English Total 232 Other GitHub19 46"}