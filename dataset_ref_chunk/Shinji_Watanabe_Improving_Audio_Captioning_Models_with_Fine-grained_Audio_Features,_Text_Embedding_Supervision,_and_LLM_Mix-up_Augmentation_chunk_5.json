{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Audio_Captioning_Models_with_Fine-grained_Audio_Features,_Text_Embedding_Supervision,_and_LLM_Mix-up_Augmentation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the paper written by Kim, H. Lee and G. Kim in 2019?", "answer": " AudioCaps: Generating captions for audios in the wild", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " What challenge did W. Yuan, Q. Han, D. Liu et al. participate in, and what was the title of their system in 2021?", "answer": " DCASE 2021 challenge task 6; Automated audio captioning with weakly supervised pre-training and word selection methods", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " In which year did X. Xu, Z. Xie, M. Wu and K. Yu present their system for the DCASE2022 challenge task 6?", "answer": " 2022", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " What approach did Z. Ye, Y. Zou, F. Cui and Y. Wang use for automated audio captioning?", "answer": " Multi-task learning", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " Who developed the HYU submission for the DCASE 2023 task 6a?", "answer": " J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " What organization developed the IRIT-UPS DCASE 2023 audio captioning and retrieval system?", "answer": " IRIT-UPS", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " Which techniques did C. P. Narisetty, T. Hayashi, R. Ishizaki et al. leverage for audio captioning?", "answer": " State-of-the-art ASR techniques", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " What do PANNs stand for and what is their main application?", "answer": " Large-scale pre-trained audio neural networks; Audio pattern recognition", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " What is the purpose of HTS-AT, developed by K. Chen, X. Du, B. Zhu et al.?", "answer": " Sound classification and detection", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}, {"question": " In which year did S. Chen, Y. Wu, C. Wang et al. propose the BEATs approach?", "answer": " 2022", "ref_chunk": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}], "doc_text": "Kim, H. Lee and G. Kim, \u201cAudioCaps: Gener- ating captions for audios in the wild,\u201d in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., \u201cThe DCASE 2021 challenge task 6 system: Automated audio captioning with weakly super- vised pre-training and word selection methods,\u201d Tech. Rep., DCASE Challenge, 2021. [5] X. Xu, Z. Xie, M. Wu and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio- text retrieval pre-training,\u201d Tech. Rep., DCASE Challenge, 2022. [6] Z. Ye, Y. Zou, F. Cui and Y. Wang, \u201cAutomated audio caption- ing with multi-task learning,\u201d Tech. Rep., DCASE Challenge, 2022. [7] J.-H. Cho, Y.-A. Park, J. Kim and J.-H. Chang, \u201cHYU submis- sion for the DCASE 2023 task 6a: Automated audio captioning model using AL-MixGen and synonyms substitution,\u201d Tech. Rep., DCASE Challenge, 2023. [8] E. Labb\u00b4e, T. Pellegrini and J. Pinquier, \u201cIRIT-UPS DCASE 2023 audio captioning and retrieval system,\u201d Tech. Rep., DCASE Challenge, 2023. [9] C. P. Narisetty, T. Hayashi, R. Ishizaki et al., \u201cLeveraging state-of-the-art ASR techniques to audio captioning.,\u201d in Proc. DCASE, 2021. [10] Q. Kong, Y. Cao, T. Iqbal et al., \u201cPANNs: Large-scale pre- trained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. [11] K. Chen, X. Du, B. Zhu et al., \u201cHTS-AT: A hierarchical token- semantic audio transformer for sound classification and detec- tion,\u201d in Proc. ICASSP, 2022. [12] P.-Y. Huang, H. Xu, J. Li et al., \u201cMasked autoencoders that listen,\u201d in Proc. NeurIPS, 2022. [13] S. Chen, Y. Wu, C. Wang et al., \u201cBEATs: Audio pre-training with acoustic tokenizers,\u201d arXiv preprint arXiv:2212.09058, 2022. [14] J. F. Gemmeke, D. P. Ellis, D. Freedman et al., \u201cAudio Set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017. [15] L. Ouyang, J. Wu, X. Jiang et al., \u201cTraining language models to follow instructions with human feedback,\u201d in Proc. NeurIPS, 2022. [16] H. Touvron, T. Lavril, G. Izacard et al., and efficient foundation language models,\u201d arXiv:2302.13971, 2023. \u201cLLaMA: Open arXiv preprint [17] H. Su, J. Kasai, Y. Wang et al., \u201cOne embedder, any task: Findings of ACL, Instruction-finetuned text embeddings,\u201d 2023. [18] J. Schulman, B. Zoph, C. Kim et al., \u201cIntroducing ChatGPT,\u201d OpenAI Blog, 2022. [19] A. van den Oord, Y. Li and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv preprint arXiv:1807.03748, 2018. [20] A. Gulati, J. Qin, C.-C. Chiu et al., \u201cConformer: Convolution- augmented transformer for speech recognition,\u201d in Proc. Inter- speech, 2020. [21] K. Miyazaki, T. Komatsu, T. Hayashi et al., \u201cConvolution- augmented transformer for semi-supervised sound event detec- tion,\u201d in Proc. DCASE, 2020. [22] D. S. Park, W. Chan, Y. Zhang et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. Interspeech, 2019. [23] H. Zhang, M. Cisse, Y. N. Dauphin and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, \u201cEfficient audio captioning transformer with patchout and text guidance,\u201d Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., \u201cWavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,\u201d arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., \u201cThe curious case of neural text degeneration,\u201d in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., \u201cBART: Denoising sequence-to-sequence pre-training for natural language gener- ation, translation, and comprehension,\u201d in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., \u201cExploring the lim- its of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, \u201cMTEB: Massive text embedding benchmark,\u201d arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, \u201cPSLA: Improving au- dio tagging with pretraining, sampling, labeling, and aggrega- tion,\u201d IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., \u201cCan audio captions be eval- uated with image caption metrics?,\u201d in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-scale self- supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., \u201cSelf-critical sequence training for image captioning,\u201d in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., \u201cAn encoder-decoder based audio captioning system with transfer and reinforcement learn- ing,\u201d in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, \u201cCLAP: learning audio concepts from natural language supervision,\u201d in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., \u201cImageBind: holistic AI learning across six modalities,\u201d Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, \u201cCIDEr-R: Ro- bust consensus-based image description evaluation,\u201d in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., \u201cCLIPScore: A in reference-free evaluation metric for image captioning,\u201d Proc. EMNLP, 2021."}