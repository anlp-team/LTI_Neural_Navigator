{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Reproducing_Whisper-Style_Training_Using_An_Open-Source_Toolkit_And_Publicly_Available_Data_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the task token mentioned in the text?", "answer": " The task token is used to distinguish between ASR (Automatic Speech Recognition) and ST (Speech Translation).", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " How does the use of a separate ST token for each target language benefit translation?", "answer": " Having a separate ST token for each target language enables translation to any language.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " How is the text transcription modified at the end?", "answer": " The text transcription is appended either with or without utterance-level timestamps.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " What is the total number of versions of the Whisper-style training model developed?", "answer": " Three versions have been developed: OWSM v1, v2, and v3.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " What data source was Whisper pre-trained on?", "answer": " Whisper was pre-trained on 680k hours of labeled audio data sourced from the Internet.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " How long is each long-form utterance limited to during data preparation for OWSM?", "answer": " Each long-form utterance is limited to a maximum duration of 30 seconds.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " How is the time resolution affected by downsampling in OWSM v3?", "answer": " OWSM v3 performs 4 times downsampling, resulting in a time resolution of 20ms.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " What kind of loss function does OWSM use for ASR targets?", "answer": " OWSM uses a joint CTC loss for ASR targets.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " What kind of decoding algorithms are supported by OWSM for inference?", "answer": " OWSM supports decoding algorithms such as greedy search, beam search, and joint CTC/attention decoding (for ASR only).", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}, {"question": " How does the batch size and number of total updates in training OWSM compare to Whisper?", "answer": " OWSM uses the same batch size as Whisper but has a smaller number of total updates.", "ref_chunk": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}], "doc_text": "Then, it uses a task token to distinguish between ASR and ST. There is a sepa- rate ST token for each target language, which enables translation to any language. Finally, it appends the text transcription either with or without utterance-level timestamps. All timestamps are quantized and represented as special tokens. 2.2. Data preparation Whisper is pre-trained on 680k hours of labeled audio data sourced from the Internet, which is not publicly accessible. To construct a speech dataset for large-scale supervised learning, we combine train- ing sets from various publicly available ASR and ST corpora. These diverse corpora encompass a wide range of speaking styles, record- ing environments, and languages. Our datasets are prepared using an open-source toolkit, ESPnet [22]. However, OWSM is trained on long-form audio data, which deviates from previous recipes in ESP- net. Consequently, we have developed new data preparation scripts tailored specifically for Whisper-style training. We concatenate con- secutive utterances within the same long talk based on their original timestamps. Each long-form utterance is limited to a maximum du- ration of 30 seconds. During training, all utterances are padded to precisely 30 seconds, optimizing the utilization of computational re- sources. To date, we have developed three versions at different scales, denoted as OWSM v1, v2, and v3 in Table 1. Our largest dataset, v3, comprises 180k hours of labeled audio data. This constitutes approximately one quarter of the total data employed by OpenAI Whisper in its training process [15]. The individual datasets utilized by our models are listed below: OWSM v3: all data in v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], Vox- Populi [46], and WSJ [47]. 2.3. Model architectures OWSM follows Whisper to utilize a Transformer encoder-decoder architecture [1], where the encoder and decoder have the same num- ber of layers. However, OWSM additionally employs a joint CTC loss for ASR targets [48], which was empirically shown to stabi- lize our training process. The input waveforms are converted to 80- dimensional log Mel filterbanks with a window length of 25ms and a hop length of 10ms. The extracted features are augmented using SpecAugment [49] and normalized by their global mean and vari- ance. The features are then processed by a two-dimensional convo- lution module to reduce the sequence length. OpenAI Whisper [15] always downsamples the sequence by 2, resulting in a time resolu- tion of 20ms. Our OWSM v2 and v3 perform 4 times downsampling, which further improves efficiency. The detailed configurations of Transformer encoder and decoder layers are summarized in Table 1. OWSM v1 and v3 use the same configurations as Whisper small and medium, respectively, while OWSM v2 is slightly smaller than v3. 4 For inference, OpenAI Whisper implements both greedy decod- ing and beam search with temperature fallback. The latter is a com- plicated procedure relying on many heuristics and hyperparameters such as beam sizes, temperatures, log probability threshold, and gzip compression rate threshold. Our OWSM utilizes the ESPnet frame- work [22], thereby ensuring compatibility with various decoding al- gorithms originally supported by ESPnet, including greedy search, beam search, and joint CTC/attention decoding (for ASR only) [50]. OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], and TEDLIUM3 [29]. OWSM v2: all data in v1, GigaST [30], Multilingual Lib- riSpeech [31], and WenetSpeech [32]. 3Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org. 4OWSM has slightly more parameters than Whisper under the same con- figuration, because the ESPnet model has a larger convolution downsampling module and does not share the input embedding and output projection in its decoder. Table 1: Details of data, model architectures, and training config- urations. We gradually increase data and model sizes from v1 to v3. The model configurations of OWSM v1 and v3 match those of Whisper small and medium, respectively. Although OWSM v3 cov- ers more languages than Whisper, our data size remains significantly smaller, making our task much more challenging. \u2217Our v3 model is initialized with the pre-trained v2 to reduce training time (see Sec- tion 2.5). OpenAI Whisper small medium large OWSM (ours) v3\u2217 v1 v2 Data Total hours (k) English ASR - Multilingual ASR - Translation Languages BPE vocabulary size 680 438 117 125 99 51,865 38 22 1 15 22 20k 129 67 22 40 23 50k 180 73 67 40 151 50k Model architectures Parameters (M) Hidden size Layers Attention heads Time resolution (ms) 244 768 12 12 20 769 1024 24 16 20 1550 1280 32 20 20 889 712 272 768 1024 1024 18 12 16 12 40 20 24 16 40 Training configurations Batch size Total updates Warmup updates Learning rate Optimizer Joint CTC weight 5e-4 256 1,048,576 2048 2.5e-4 AdamW NA 256 300k 500k 470k 10k 20k 10k 1.75e-4 1e-3 5e-4 2.5e-4 AdamW 0.3 2.4. Training details OWSM is implemented in ESPnet [22] based on PyTorch [51]. Ta- ble 1 compares the training hyperparameters of different models. OWSM uses the same batch size as Whisper, but the number of to- tal updates is smaller. OWSM is trained on NVIDIA A100 (40GB) GPUs. Each GPU takes two samples, and gradient accumulation is applied whenever necessary to ensure the total batch size is 256. Specifically, OWSM v1 is trained for around 7 days on 32 A100 GPUs and OWSM v2 and v3 are trained for around 10 days on 64 A100 GPUs. After training, five checkpoints with the highest vali- dation accuracies are averaged to generate the final checkpoint. 2.5. Challenges and training tips Large-scale distributed training presents significant challenges, par- ticularly when the computation budget is limited. As we scale up from a few thousand hours of data to nearly 200 thousand hours, we have encountered a range of issues. Here, we discuss some"}