{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_User-Centric_Evaluation_of_OCR_Systems_for_Kwak\u2019wala_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many participants were divided into three groups and how many users were in each group?", "answer": " Nine participants were divided into three groups of three users each.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " What was the purpose of randomly assigning task setups within the Latin Square constraints?", "answer": " The purpose was to spread out the effect of undetectable or unexpected characteristics.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " Why were all factors (user, page, task setup) randomized in the experiment?", "answer": " To control sources of variability and improve the generalization of the statistical analysis.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " What method was used to introduce randomness across factors and reduce variance in the study?", "answer": " The Latin Square Design was used.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " How many pages were selected for transcription tasks in the user study?", "answer": " Nine pages were selected.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " What type of feedback was obtained from participants after completing the transcriptions?", "answer": " Qualitative feedback was obtained through a short survey.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " How long were the nine transcription tasks designed to take approximately?", "answer": " Approximately 7 hours.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " How many measurements of transcription speed were collected for quantitative analysis?", "answer": " There were 81 measurements of transcription speed collected.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " What statistical technique was used to analyze the transcription time data?", "answer": " Linear Mixed Effects (LME) modeling was used.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}, {"question": " What were the two random effects in the LME modeling for transcription time?", "answer": " Transcriber identity and page number.", "ref_chunk": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}], "doc_text": "the page being transcribed. We randomly divide the nine participants into three groups of three users each (to \ufb01t the 3x3 square) and choose a \ufb01xed set of nine pages from the documents that all participants will transcribe in their tasks. For each group of three users, we form three squares (since we have nine pages). The task setups \u2013 i.e., baseline, Ocular, post-correction \u2013 are randomly assigned within the Latin Square constraints. Adding randomization for all factors (user, page, task setup) is aimed at spreading out the effect of undetectable or unsus- pected characteristics. An example of task setup assignments for one group of three users for the nine pages is in Figure 5.8 Therefore, each user has nine transcription tasks with the task setups evenly distributed so all users are suf\ufb01ciently timed on each setup. The user does not transcribe the same page more than once, but all users transcribe the same set of nine pages (with 8We follow https://online.stat.psu.edu/ stat503/lesson/4/4.4 and randomize Latin Squares separately for each group of users and each set of pages, so task setup assignments may not look identical across groups. user1 user2 user3 page1 ocu post base page2 base ocu post page3 post base ocu page4 ocu base post page5 post ocu base page6 base post ocu page7 post base ocu page8 ocu post base page9 base ocu post Figure 5: Task setup assignments for a group of three users using the Latin Square design. We use 3x3 Latin Squares because we have three task setups: Baseline (base), Ocular (ocu), and post-correction (post). We need three squares for each group of users because we have nine pages for transcription. All users transcribe the same set of pages, but with the Latin Square framework, they have different task setups for each page which helps control sources of variability. All user identi\ufb01ers and page identi\ufb01ers are randomized before applying the Latin Square design. varied task setups). The Latin Square Design, thus, introduces randomness across the factors to reduce variance and improve the generalization of the sta- tistical analysis. Dataset selection We selected nine pages from the Hunt-Boas volumes for the user study ex- periments, which were randomly chosen from a larger subset of 50 pages that community-based researchers deemed representative of the volumes and important to transcribe. We also obtained qualitative feedback through a short survey that the participants \ufb01lled out after completing the transcriptions. The survey asked several questions about the experience with the user study, including if the transcribers found spe- ci\ufb01c tasks more dif\ufb01cult than others, whether they preferred typing from scratch or correcting OCR outputs (and which they thought was faster) as well as general feedback on the task and interface. 3.6 Quantitative Analysis 3.5 Evaluation Procedure The nine transcription tasks were designed to take approximately 7 hours to complete. The partici- pants accessed the Label Studio interface remotely through any web browser and \ufb01rst completed the keyboard practice tasks described above. Then, the participants began the transcription tasks and the interface recorded all timestamps for when tran- scriptions were edited and submitted. After the participants completed all tasks, we collected the timestamp information and computed how long it took to complete each task \u2013 with nine users tran- scribing nine pages each, we have 81 measurements of transcription speed to be used for quantitatively evaluating the utility of the OCR systems. We also calculated the character error rate (CER) of each transcription with respect to the transcription for the same page by our most experienced participant (a Kwak\u2019wala heritage language learner who is very familiar with the orthography and had tran- scribed parts of the Hunt/Boas volumes before the user study), and discarded time measurements for transcriptions with CER \u2265 1%. Across all 81 tran- scriptions, only one had an error rate higher than this threshold, and thus, the quantitative analysis below is conducted with 80 time measurements.9 To quantify the effect of introducing OCR into the transcription process, we analyze the measure- ments of transcription speed that were collected from the user study tasks. As stated previously, we cannot use the time values directly to make a gener- alized conclusion because transcription time is not independent of the sources of variability. Instead, we use the statistical technique of Linear Mixed Effects (LME) modeling (Bates, 2007) to describe the relationship between the response variable (the transcription time) and the factors that contribute to variance. The term \u201cmixed effects\u201d refers to a combination of random effects and \ufb01xed effects. We have two random effects: 1. transcriber identity, which can take values from user1 to user9; 2. page number, which can take values from page1 to page9. We also have two \ufb01xed effects: 1. transcriber group, which can either be yes or no indicating prior familiarity with the Kwak\u2019wala language or not; 9There was no statistical difference between character er- ror rates of from-scratch and corrected transcriptions as well as between participants with and without prior knowledge of Kwak\u2019wala. The participants chosen for the user study had ex- perience in transcription tasks, and all except one transcription were highly accurate (CER < 1%). Task Setup Time Est. (min.) p-value Baseline With OCR 61.65 28.21 3.04e-07 * 4.80e-08 * Table 1: Per-page transcription time estimates in min- utes from the LME model comparing the baseline, which does not use any OCR, with the task setups that use some form of OCR (either Ocular or post- correction). The time estimate for producing an accu- rate transcription of a page is reduced by 33.44 minutes when OCR technologies are used beforehand. The p- value is < 0.05, indicating statistical signi\ufb01cance (*). 2. task setup, which can be one of the three se- tups described above \u2013 baseline, Ocular, or post-correction. The LME estimation models the transcription time as a function of the above random and \ufb01xed effects. Using the estimations, our primary analysis attempts to identify whether the task setup affects transcription time in a statistically signi\ufb01cant man- ner. We additionally look at whether"}