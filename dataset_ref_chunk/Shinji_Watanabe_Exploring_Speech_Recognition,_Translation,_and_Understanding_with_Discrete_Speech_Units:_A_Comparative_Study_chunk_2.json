{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Exploring_Speech_Recognition,_Translation,_and_Understanding_with_Discrete_Speech_Units:_A_Comparative_Study_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are some techniques that have been used for discretization in previous studies?,answer: Vector quantization (VQ) and clustering.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What are some examples of models that employ vector quantization (VQ) for discretization?,answer: VQ-VAE, Wav2Vec models, and recent popular neural codec models.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " Why does the text express favoritism towards clustering-based methods?,answer: Due to their inherent versatility, catering to diverse tasks.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " How does clustering benefit feature extraction methods?,answer: It enables a wide choice of feature extraction methods, including spectral features or intermediate representations.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What are two methods used to remove redundancies from discrete units?,answer: De-duplication and subword modeling.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What modeling techniques can be applied to discrete speech units due to their similarity to tokenized text in NLP tasks?,answer: Similar processing and modeling techniques used for text tokenization.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What technique is used to reduce the input sequence length of discrete tokens in Seq2Seq modeling?,answer: Convolutional subsampling (Conv-Sub) layer.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What is the purpose of introducing a Conv-Sub layer in E2E speech processing models?,answer: To reduce the sequence length without performance degradation.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What open-source toolkit is used for conducting the experiments in the text?,answer: ESPnet.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}, {"question": " What is the hidden dimension set to in each layer of the encoder architecture in the experiments?,answer: 256.", "ref_chunk": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}], "doc_text": "Previous studies have employed techniques such as vector quantization (VQ) and cluster- ing for discretization. The former typically necessitates the learn- ing of a specialized vector quantization module during the training phase. Prominent examples include VQ-VAE [29], Wav2Vec mod- els [10\u201312], and recent popular neural codec models [27, 30] with multi-level residual VQ component. The alternative applies the clus- tering algorithms to the features extracted from the signals, where the cluster indices are directly used as the discrete representations. K-Means clustering is often used in HuBERT-like models [13, 14]. We are in favor of the clustering-based methods due to their in- herent versatility, thereby catering to diverse tasks. The benefit of this approach comes in threefolds: 1. It enables a wide choice of feature extraction methods, includ- ing spectral features or intermediate representations from SSL or supervised learning-based models. 2. Distinct layers retain different information [14], and we can choose an optimal feature for different purposes. 3. The vocabulary size can be easily tuned for the balance of in- formation distinctions and efficiency without modifying the pre- trained models. 2.2. Data Manipulations The discrete units derived from the preceding stages are temporally aligned with the original speech features. However, the discrete units still contain trivial redundancies of speech, such as repeated or commonly co-existing units. Note that after the speech signal is converted into discrete units, it can be regarded as a type of spo- ken language similar to tokenized text in conventional NLP tasks. Hence, similar processing and modeling techniques can be applied in a trivial way. To remove the redundancies introduced by repeated or commonly co-existing units, two methods have been employed to significantly reduce the input sequence length [17]: 1. De-duplication: This approach involves condensing consecutive subsequences featuring identical tokens into a single token to re- duce redundancy. 2. Subword Modeling: This technique combines frequent patterns of discrete unit subsequences and reassigns them to metatokens, to enhance the input token representation. Besides these two methods, simple data augmentation, time masking, is also adopted in [17], serving as a regularization tech- nique during training. 2.3. Seq2Seq Modeling with Convolutional Subsampling We use Seq2Seq models to map the discrete speech units to other target outputs with E2E training. We take the advantage of well- established Seq2Seq models, including AED [5], CTC [3], and RNN-Transducer [4]. The detailed definitions of these models can be found in their references. Convolutional Subsampling. The input sequence length of discrete tokens is generally longer than the target sequence length. As we employ the widely used recent self-attention model [9] in our exper- iments, the sequence length poses a problem. The computation cost is quadratically dependent on the input sequence length. Hence, to reduce the computation overhead, we introduce a convolutional sub- sampling (Conv-Sub) layer between the input embedding layer and the self-attention networks (SAN), a common technique in conven- tional E2E speech processing models based on continuous speech features. This Conv-Sub layer reduces the sequence length to 1 2 or even 1 3 without performance degradation, as mentioned in [17]. By default, a 2-layer 1D-convolutional (CONV1D) block is adopted. 3. EXPERIMENTS 3.1. General Setup Our experiments are conducted using the open-source E2E speech processing toolkit, ESPnet [28], aligning with [17]. To thoroughly evaluate the performance of speech processing tasks using discrete speech representations, we adopted various tasks, including ASR, ST, and SLU. For speech data processing and transcription normal- ization, we follow the existing recipes in ESPnet to make it compa- rable with prior works. Unless stated otherwise, the joint CTC/AED framework is pre- dominantly employed for both training and inference. We use a 12- layer EBranchformer [9] as the encoder architecture in all the ex- periments. In each layer of the encoder, the hidden dimension d is set to 256. The number of heads in the self-attention is 4. For the decoder, we employ a 6-layer Transformer decoder with 4 attention heads. During inference, no external language models are used, and the beam size is set to 10 across all experiments, which may lead to slightly worse performance than other studies that primarily focus on achieving state-of-the-art performance. The process of speech discretization hinges heavily on the In the prior study [17], the final layer of the choice of features. Table 1: CER or WER (%) on ASR benchmarks using the joint CTC/AED framework. Dataset Language Metric Evaluation Sets Discrete Tokenization K-means clusters BPE size FBank Results Discrete Units SSL (top line) AISHELL CHiME4 CommonVoice Gigaspeech How2-2000 LibriSpeech-100 LibriSpeech ML-SUPERB (1h) Must-C SPGIspeech SWBD TEDLIUM3 CH EN FR EN EN EN EN 143 EN EN EN EN CER WER WER WER WER WER WER CER WER WER WER WER dev/test {dt05,et05} real 5mics dev / test dev / test test {dev,test}-{clean,other} {dev,test}-{clean,other} normal / few-shot tst-COMMON {dev,val} unnorm eval2000 (callhm / swbd) dev / test 2000 1000 1000 1000 2000 2000 2000 2000 2000 1000 1000 1000 6000 2000 1500 3000 6000 6000 6000 6000 4000 5000 1500 2000 4.2 / 4.5 12.0 / 17.4 12.8 / 14.6 11.8 / 11.6 10.9 6.1 / 16.7 / 6.3 / 17.0 2.5 / 6.3 / 2.6 / 6.2 59.6 / 58.6 8.3 6.0 / 6.0 13.5 / 7.5 9.4 / 8.8 4.6 / 4.9 8.4 / 7.1 20.9 / 23.5 11.2/11.6 10.9 3.8 / 6.7 / 3.9 / 7.0 2.2 / 4.5 / 2.4 / 4.6 44.8 / 46.4 6.1 5.9 / 5.9 11.3 / 7.6 9.0 / 8.9 3.8 / 4.0 5.5 / 4.7 12.9 / 15.0 10.2 / 10.3 10.8 3.2 / 5.3 / 3.1 / 5.5 1.9 / 3.7 / 1.9 / 3.7 21.8 / 38.9 5.7 5.5 / 5.5 10.1 / 6.5 8.9 / 8.8 Table 2: Average sequence length (seq len) on training set of discrete speech unit (Disc Unit) and output asr transcription (ASR Trans). Lengths of discrete unit after manipulations are provided, including de-duplication \u2192 subword modeling \u2192 convolutional subsampling. Dataset Disc unit +De-dup Manipulations +BPE length reduction ratio Conv-Sub Type length BPE-ASR Trans AISHELL CHiME4 CommonVoice (FR)"}