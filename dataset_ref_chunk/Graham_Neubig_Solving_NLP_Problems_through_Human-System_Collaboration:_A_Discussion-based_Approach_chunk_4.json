{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_Solving_NLP_Problems_through_Human-System_Collaboration:_A_Discussion-based_Approach_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " According to the text, what is the purpose of few-shot-discussion?,answer: The purpose of few-shot-discussion is to generate discussion utterances with higher accuracy compared to zero-shot and few-shot systems.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " What is the main finding regarding the performance of zero-shot and few-shot systems in generating discussion utterances?,answer: The main finding is that the performance of zero-shot and few-shot systems is almost the same, indicating that just showing examples does not improve the discussion ability.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " How does the accuracy of the label determined by discussion compare to the accuracy without discussion?,answer: The accuracy of the label determined by discussion is higher compared to the accuracy without discussion.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " In terms of handling objections, how does the few-shot-discussion system compare to the zero-shot system?,answer: The few-shot-discussion system handles objections better compared to the zero-shot system.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " What is noted about the objection ability of the few-shot system and the zero-shot system?,answer: The few-shot system has a similar objection ability as the zero-shot system.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " According to the text, why is it necessary to be careful of people who manipulate predictions with malice arguments?,answer: It is necessary to be careful as the system tends to be weak at objecting to humans.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " What is the advantage of using pre-trained models to annotate unlabeled data?,answer: Using pre-trained models to annotate unlabeled data has been shown to improve performance.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " How do GPT-3.5 and ChatGPT contribute to generating discussion data in the text?,answer: GPT-3.5 and ChatGPT are used to generate discussion data for investigation purposes.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " How do human discussions contribute to generating pseudo-discussion data?,answer: Human discussions are used to provide prompts for generating pseudo-discussion data.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}, {"question": " What is the potential benefit of using pseudo-discussion data in fine-tuning systems?,answer: The potential benefit is that pseudo-discussion data can be used to produce enough data for fine-tuning at a low cost.", "ref_chunk": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}], "doc_text": "few-shot few-shot-dis. 51.83 70.31 70.15 48.63 55.08 57.24\u2020 41.70 52.31 55.63\u2020 40.52 52.18 55.19\u2020 Table 4: The accuracy on SNLI and ANLI (R1, R2, R3) evaluation data. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. \u2020 indicates statistically significant scores according to McNemar\u2019s test (p < 0.01). that few-shot-discussion can generate discussion utterances with higher accuracy than zero-shot and few-shot, which do not use discussion examples data. The performance of zero-shot and few-shot is almost the same, suggesting that just showing examples does not improve the discussion ability. Also, the difference between supportive and unsup- portive utterance accuracies is greater in few-shot- discussion than in zero-shot and few-shot systems. Therefore, because the few-shot-discussion can generate more supportive utterances, it is thought that such discussions can result in more appropriate labels. Table 2 shows the accuracy of the label deter- mined by discussion in the settings for evaluating the acceptance ability and objection ability, respec- tively. In terms of the objection, it can be seen that the few-shot-discussion system handled objec- tions well in comparison to the zero-shot system. In addition, Table 3 shows the accuracy8 of the predicted label without discussion, and the accu- racy of the final label reached as a result of the discussion between humans and systems. Further- more, the few-shot system has a similar objection ability as the zero-shot system, and there is a pos- 8To facilitate discussion, this evaluation is limited to in- stances where three of the five cloudworkers have the same label in SNLI data. This makes it more challenging than using the entire SNLI data. SNLI R1 R2 R3 GPT-3.5 dis. GPT-3.5 pseudo 66.14 65.67 53.90 54.00 50.40 49.60 50.42 50.50 ChatGPT dis. ChatGPT pseudo 68.51 68.66 53.90 54.00 52.82 52.51 52.33 52.10 Table 5: The accuracy on SNLI and ANLI (R1, R2, R3) test data for few-shot systems using manually created discussion examples and pseudo-discussion examples. Upper scores are by GPT-3.5, and lower scores are by ChatGPT. sibility that the performance of label prediction by these systems is not necessarily directly related to the ability to discuss. In comparison with accep- tance, it is necessary to be careful of people who manipulate predictions with malice arguments, as the system tends to be weak at objecting to hu- mans. Furthermore, from the fact that the accuracy of the few-shot-discussion system has improved the most, it is clear that the proposed data can be used to have discussions with humans that lead to improved performance. Table 4 shows the accuracy of each system for the evaluation data of SNLI and ANLI. In SNLI, the few-shot-discussion system performs worse than the few-shot system, but in the three datasets of ANLI, we find that the performance is the best. This is because ANLI is more difficult data com- pared to SNLI, and we hypothesize that through discussion, systems get a more detailed understand- ing of problems, which in turn contributes to per- formance improvement. From the results of previous experiments, we found that discussion between humans and systems is beneficial for improving performance.9 There- fore, the few-shot-discussion system, in which a discussion example is also given as a prompt, is expected to achieve a deeper understanding of NLI problems and improve performance through the discussion example in the prompt. 6 Analysis 6.1 Pseudo-Discussion Data One drawback of using discussion data is that it can be costly to create compared to datasets that only have gold labels. Using pre-trained models to an- notate unlabeled data and use this data for training has been shown to improve performance (Wang 9We show examples of human-system discussion in Ap- pendix A. SNLI R1 R2 R3 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 85.2 87.7\u2020 86.2\u2020 90.3\u2020 85.4 85.1 84.6 85.3 67.4\u2020 68.2\u2020 67.6 71.7\u2020 65.2 64.0 67.9 66.2 55.2\u2020 56.1\u2020 55.5\u2020 58.4\u2020 53.9 51.1 54.7 53.1 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 53.0 w/ dis. w/o dis. MPT MPT-inst. Falcon Falcon-inst. MPT MPT-inst. Falcon Falcon-inst. 86.7\u2020 86.9 88.1 90.7\u2020 85.4 86.0 88.5 89.7 68.3\u2020 68.8\u2020 68.1 71.9\u2020 65.2 64.0 67.9 67.8 55.2\u2020 56.1\u2020 55.5 58.4\u2020 53.9 51.1 54.7 55.5 55.0\u2020 55.3\u2020 54.9 57.6\u2020 52.4 50.7 54.2 56.4 Table 6: Accuracy on SNLI and ANLI (R1, R2, R3) test data for fine-tuned systems with and without pseudo- discussion data. Additional fine-tuning with pseudo discussion data for instruction tuned and non-instruction tuned models for MPT and Falcon. The upper and lower scores are the results using pseudo discussion data generated by GPT-3.5 and ChatGPT, respectively. \u2020 indicates statistically significant scores for w/ dis. and w/o dis. according to McNemar\u2019s test (p < 0.01). et al., 2021; Honovich et al., 2022; Wang et al., 2022b). Therefore, we propose to use GPT-3.5 and ChatGPT to generate discussion data in a zero-shot and use them as discussion examples for a few-shot to investigate if it is possible to achieve the same level of improvement as from using manually cre- ated data. If a system can automatically produce high-quality data, it can produce enough data for fine-tuning at a low cost. Therefore, we also inves- tigate the effectiveness of pseudo-discussion data in fine-tuning. In generating human discussions, the system is given prompts in the form of the premise, hypoth- esis, gold label, and the labels from each human. The human labels are randomly chosen to be the gold label or the other incorrect label. For exam- ple, given the premise \u201cA nun is taking a picture outside.\u201d and hypothesis \u201cA nun is taking a selfie.\u201d with the gold label of neutral, the prompt would be \u201cReproduce a multi-turn interactive discussion in which the following premise and hypothesis are entailment, contradiction, or neutral, with the hu- mans agreeing with each other on the final label. Human1\u2019s label is neutral, and Human2\u2019s label is a contradiction. In the end, they agree on the label of neutral. Premise: A nun is taking a picture outside. Hypothesis: A nun is taking a selfie.\u201d. The GPT-3.5 and ChatGPT generate human dis-"}