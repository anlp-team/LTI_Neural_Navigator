{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_The_Devil_Is_in_the_Errors:_Leveraging_Large_Language_Models_for_Fine-grained_Machine_Translation_Evaluation_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of metric meta-evaluation?", "answer": " The quality of an automatic evaluation metric is estimated by comparing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " How is system-level accuracy calculated in metric meta-evaluation?", "answer": " System-level accuracy calculates the percent of system pairs that are ranked the same by the metric and ground-truth scores, averaged over a set of language pairs.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " What is the standard correlation reported at the segment level by WMT?", "answer": " Kendall\u2019s \u03c4", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " Why is there concern about using Kendall\u2019s \u03c4 with respect to ties?", "answer": " Different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " What is the pairwise accuracy score recommended by Deutsch et al. (2023)?", "answer": " A pairwise accuracy score, denoted acc\u2217, which ranges between 0 and 1, and a random metric would achieve 33% accuracy.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " How is Span Precision (SP) defined?", "answer": " SP measures the overlap of predicted spans and gold (annotated) spans.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " What is Major recall (MR) in evaluating predicted spans?", "answer": " It captures the percentage of gold major errors that were predicted as errors (either minor or major).", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " What is the purpose of reporting Matthews Correlation Coefficient (MCC)?", "answer": " One of the official metrics in the word-level quality estimation tasks.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " What did the results show about zero-shot LLM evaluators and learned metrics at the system level?", "answer": " Zero-shot LLM evaluators generally had higher system-level performance than learned metrics.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}, {"question": " How do PaLM-2 models compare with GPT-based GEMBA evaluator in terms of system-level performance?", "answer": " PaLM-2 models exhibited higher system-level performance than GPT-based GEMBA.", "ref_chunk": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}], "doc_text": "To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call S and M , comparing their finetuned and zero-shot performance.6 Metric Meta-Evaluation The quality of an au- tomatic evaluation metric is estimated by compar- ing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work re- ports three different agreement scores, as follows. The first is system-level accuracy, which calcu- lates the percent of system pairs that are ranked the same by the metric and ground-truth scores, micro- 5We consider a span as any maximal consecutive sequence of words marked as BAD, assigning every span the major severity. 6We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning. averaged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the aver- age score across all segments. At the segment-level, the standard correlation that is reported by WMT is Kendall\u2019s \u03c4 . However, recent work pointed out problems with Kendall\u2019s \u03c4 with respect to ties (Deutsch et al., 2023). In short, different variants of \u03c4 are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc\u2217, ranges between 0 and 1, and a random metric would achieve 33% accuracy. We report the \u201cgroup-by-item\u201d variant of the pairwise accuracy score from Deutsch et al. (2023) in ad- dition to Pearson\u2019s \u03c1, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT). Span Meta-Evaluation Since AUTOMQM pro- vides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annota- tions. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the over- lap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the per- centage of gold major errors that were predicted as errors (either minor or major). More formally, consider the set of ground truth spans S\u22c6, where each span consists of a sequence of words, i.e., si = (w(a), w(a+1), \u00b7 \u00b7 \u00b7 ). Let S\u22c6 maj \u2286 S\u22c6 be the subset containing only the major errors. Given a span set S, we define its positional set P (S) as the set containing the positions of all the words in every span in S. For example, assuming a span si = (w(n), w(n+1), \u00b7 \u00b7 \u00b7 ) in S starts at the nth position in the text, its corresponding positional set will include the positions {n, n+1, ..., n+len(si)\u2212 1}. Then for a set of predicted spans \u02c6S, SP and MR are defined as: SP( \u02c6S) = MR( \u02c6S) = |P ( \u02c6S) \u2229 P (S\u22c6)| |P ( \u02c6S)| |P ( \u02c6S) \u2229 P (S\u22c6 |P (S\u22c6 maj)| maj)| (1) (2) Intuitively, we care for overall precision (regard- less of severity) since we want to make sure pre- dicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors, as these have a larger impact on translation qual- ity and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coeffi- cient (MCC), one of the official metrics in the word- level quality estimation tasks (Zerva et al., 2022). 6.2 Results 6.2.1 Score Prediction Table 2 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings. Prompting A first observation is almost all zero- shot LLM evaluators have higher system-level per- formance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UNI- CORN achieving the best performance. At the seg- ment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently out- perform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor perfor- mance at both the system and segment levels.7 Nevertheless, PaLM-2 models achieve high cor- relations with human judgments, and the reference- less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative trans- lations of the same sentence (acc\u2217). When com- paring PaLM-2 models with Kocmi et al. (2022)\u2019s GPT-based GEMBA evaluator (Table 3), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher system- level performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting. Figure 4 shows the distribution of scores pro- duced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the 0-100 range, these models almost always out- put one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)\u2019s 7Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well. System-Level Segment-Level All (3 LPs) EN-DE ZH-EN EN-RU Model Ref? Accuracy \u03c1 acc\u22c6 \u03c1 acc\u22c6 \u03c1 acc\u22c6 Baselines MetricX-XXL COMET-22 COMET-QE \u2713 \u2713 \u2717 85.0% 83.9% 78.1% 0.549 0.512 0.419 61.1% 0.581 60.2% 0.585 56.3% 0.505 54.6% 0.495 54.1% 0.469 48.8% 0.439 60.6% 57.7% 53.4% Prompting PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN PaLM 540B PaLM-2 BISON PaLM-2 UNICORN FLAN-PaLM-2 UNICORN \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 90.1% 88.7% 90.1%"}