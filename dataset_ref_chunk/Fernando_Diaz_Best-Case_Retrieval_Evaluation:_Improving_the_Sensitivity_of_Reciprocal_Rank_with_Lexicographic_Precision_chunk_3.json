{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Fernando_Diaz_Best-Case_Retrieval_Evaluation:_Improving_the_Sensitivity_of_Reciprocal_Rank_with_Lexicographic_Precision_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does RL1 represent in the context of distribution of utility among possible users?", "answer": " RL1 represents the best-case performance over possible users.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " How is the best-case utility over a representation of users determined?", "answer": " The best-case utility is determined as RR1 (equivalently ESL1) where RR\ud835\udc56 monotonically degrades in rank.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " In terms of rankings, what does RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) imply?", "answer": " It implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " What is the focus of best-case preference-based evaluation?", "answer": " The focus is on measuring differences in performance instead of absolute performance.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " How does preference-based evaluation differ from metric-based evaluation?", "answer": " Preference-based evaluation explicitly models the preference between two rankings, while metric-based evaluation computes some evaluation metric for each ranking.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " What is the goal of designing a preference-based evaluation in this context?", "answer": " To preserve the best-case properties of RL1 metrics with much higher sensitivity.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " What criteria does the concept of psychological relevance suggest are necessary but not sufficient for judging an item relevant?", "answer": " Judging any item relevant in general is necessary but not sufficient criteria.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " What does lexicographic sorting in social choice literature involve?", "answer": " It involves looking at utilities in the best-off positions and iteratively inspecting lower utility positions until finding an inequality.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " How is a tie avoided in lexicographic sorting?", "answer": " A tie can only happen if two rankings have all relevant items in exactly the same positions.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}, {"question": " What ordering does lexicographic sorting generate over all positions of relevant items?", "answer": " Lexicographic sorting generates a total ordering over all positions of relevant items.", "ref_chunk": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}], "doc_text": "relevant item. From this perspective, we can, for a specific ranking, look at how utility is distributed amongst possible users, as represented by their recall levels. For example, we can ask how utility for users with high and low recall requirements compares; or what the average utility across these populations is. While previous work has looked at the average-case utility [8] and worst-case utility [9], in this work we suggest that RL1 represents the best-case performance over these possible users. The proof is relatively simple. Because RR\ud835\udc56 monoton- ically degrades in rank, the best-case utility over this representation of users is RR1 (equivalently ESL1). The next-best-case is RR2 and so forth until we reach RR\ud835\udc5a, which we refer to as the worst-case. So, given two rankings \ud835\udf0b and \ud835\udf0b \u2032, observing RR1 (\ud835\udf0b) > RR1 (\ud835\udf0b \u2032) implies that the best-case performance over possible user recall requirements is higher in \ud835\udf0b compared to \ud835\udf0b \u2032. Both uncertainty over recall levels and over psychological rele- vance focus on possible populations of users. Because the utility to the user implies utility to the system designer (e.g., for objec- tives like retention), understanding the best-case performance is valuable in decision-making. From the perspective of social choice theory, best-case retrieval evaluation is inherently optimistic and represents risk-seeking decision-making. 3.2 Lexicographic Precision The problem with evaluating for best-case retrieval (as shown in Section 2) is the tendency for multiple rankings to be tied, especially as (i) we increase the number of relevant items and (ii) systems optimize for retrieval metrics. We can address these ceiling effects by developing a best-case preference-based evaluation that focuses on measuring differences in performance instead of absolute per- formance [8]. While metric-based evaluation models the preference between rankings by first computing some evaluation metric for each ranking, preference-based evaluation explicitly models the preference between two rankings. Prior research has demonstrated that preference-based evaluation can be much more sensitive than metric-based evaluation [8], making it well-suited for addressing the ceiling effects described in Section 2. Under best-case preference-based retrieval, we are interested in answering the question, \u2018under the best possible scenario, which ranking would the user prefer?\u2019 In this respect, it is a user-based evaluation method, but one based on preferences and measurement over a population of users. More formally, given an information need and two rankings \ud835\udf0b and \ud835\udf0b \u2032 associated with two systems, metric-based evaluation uses an evaluation metric \ud835\udf07 : \ud835\udc46\ud835\udc5b \u2192 \u211c (e.g. reciprocal rank or average precision) to compute a preference, \ud835\udf07 (\ud835\udf0b) > \ud835\udf07 (\ud835\udf0b \u2032) =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 where \ud835\udf0b \u227b \ud835\udf0b \u2032 indicates that we prefer \ud835\udf0b to \ud835\udf0b \u2032. Notice that, if \ud835\udf07 (\ud835\udf0b) = \ud835\udf07 (\ud835\udf0b \u2032), then we cannot infer a preference between \ud835\udf0b and \ud835\udf0b \u2032. We contrast this with preference-based evaluation, which directly models this relationship \u0394 : \ud835\udc46\ud835\udc5b \u00d7 \ud835\udc46\ud835\udc5b \u2192 \u211c, \u0394(\ud835\udf0b, \ud835\udf0b \u2032) > 0 =\u21d2 \ud835\udf0b \u227b \ud835\udf0b \u2032 Next, we consider uncertainty over psychologically relevant items. When evaluating a retrieval system, we often use relevance labels derived from human assessors or statistical models. But what if a specific user does not find the top-ranked item labeled relevant actually relevant to them? For example, a user may have already seen a specific item or they may desire an item with a specific (miss- ing) attribute. A judged relevant item might be inappropriate for any number of reasons not expressed in the request. The concept of psychological relevance [11] suggests that judging any item relevant in general (as is the case in many retrieval benchmarks, including those used in TREC) is a necessary but not sufficient criteria to Our goal is to design a preference-based evaluation that preserves the best-case properties of RL1 metrics with much higher sensitivity. Consider the two position vectors \ud835\udc5d and \ud835\udc5d\u2032 in Figure 4 associated with the two rankings \ud835\udf0b and \ud835\udf0b \u2032. These two vectors are tied in the best case (i.e., \ud835\udc5d1 = \ud835\udc5d\u2032 1). However, we can break this tie by looking at the next-best case (i.e. \ud835\udc5d2) where, because \ud835\udc5d2 < \ud835\udc5d\u2032 2, we say that \ud835\udf0b \u227b \ud835\udf0b \u2032. If we had observed a tie between the next-best case, we could compare \ud835\udc5d3, and so forth. This is known as lexicographic sorting in the social choice literature [17] and reflects a generalization of best-case sorting. Given two sorted vectors of utilities, here reflected by the rank position, the 9 500 8 = n n n-1 <latexit sha1_base64=\"0Z4JlA8iYg6awHPgf7vVaWYiDbg=\">AAAB7HicbZA9SwNBEIbn4leMX1FLm8UgWIU7EbUzaGMZwUsCyRH2NnvJkr29Y3dOCCG/wcZCEVt/kJ0/xc7NJYUmvrDw8L4z7MyEqRQGXffLKaysrq1vFDdLW9s7u3vl/YOGSTLNuM8SmehWSA2XQnEfBUreSjWncSh5MxzeTvPmI9dGJOoBRykPYtpXIhKMorX8jskY65YrbtXNRZbBm0Pl+jvKVe+WPzu9hGUxV8gkNabtuSkGY6pRMMknpU5meErZkPZ526KiMTfBOB92Qk6s0yNRou1TSHL3d8eYxsaM4tBWxhQHZjGbmv9l7Qyjq2AsVJohV2z2UZRJggmZbk56QnOGcmSBMi3srIQNqKYM7X1K9gje4srL0DirehfV83u3UruBmYpwBMdwCh5cQg3uoA4+MBDwBC/w6ijn2Xlz3melBWfecwh/5Hz8AL2uklc=</latexit> 2 2 4 10 \u0000 <latexit sha1_base64=\"8Q9MIF5tmntsfpBcQKFy38+B7+U=\">AAAB6HicbVDJSgNBEK2JW4xb1KOXxiDkFGYEl2PQi8cEzAKZIfR0apI2PQvdPUIY8gUe9KCIVz/FT/Dmh3i3sxw08UHB470qqur5ieBK2/aXlVtZXVvfyG8WtrZ3dveK+wdNFaeSYYPFIpZtnyoUPMKG5lpgO5FIQ19gyx9eT/zWPUrF4+hWjxL0QtqPeMAZ1UaqJ91iya7YU5Bl4sxJqVp2y98fj26tW/x0ezFLQ4w0E1SpjmMn2suo1JwJHBfcVGFC2ZD2sWNoRENUXjY9dExOjNIjQSxNRZpM1d8TGQ2VGoW+6QypHqhFbyL+53VSHVx6GY+SVGPEZouCVBAdk8nXpMclMi1GhlAmubmVsAGVlGmTTcGE4Cy+vEyapxXnvHJWN2lcwQx5OIJjKIMDF1CFG6hBAxggPMAzvFh31pP1ar3NWnPWfOYQ/sB6/wGimJCI</latexit>p<latexit sha1_base64=\"fXfk0Lq5jyRW4bqrFK2t5Ax8G70=\">AAAB6XicbVDJSgNBEK2JW4xb1KOXxiDmFGYEl2PQi8coZoHMEHo6PUmTnu6hu0cIQ/5AEA+KePVP/ARvfoh3O8tBEx8UPN6roqpemHCmjet+Obml5ZXVtfx6YWNza3unuLvX0DJVhNaJ5FK1QqwpZ4LWDTOcthJFcRxy2gwHV2O/eU+VZlLcmWFCgxj3BIsYwcZKt8lxp1hyK+4EaJF4M1Kqlv3y98ejX+sUP/2uJGlMhSEca9323MQEGVaGEU5HBT/VNMFkgHu0banAMdVBNrl0hI6s0kWRVLaEQRP190SGY62HcWg7Y2z6et4bi/957dREF0HGRJIaKsh0UZRyZCQav426TFFi+NASTBSztyLSxwoTY8Mp2BC8+ZcXSeOk4p1VTm9sGpcwRR4O4BDK4ME5VOEaalAHAhE8wDO8OAPnyXl13qatOWc2sw9/4Lz/AAMXkLk=</latexit>p0 \ud835\udeffRR1 (\ud835\udf0b, \ud835\udf0b \u2032) \ud835\udeffESL1 (\ud835\udf0b, \ud835\udf0b \u2032) sgnLP(\ud835\udf0b, \ud835\udf0b \u2032) rrLP(\ud835\udf0b, \ud835\udf0b \u2032) \u0394(\ud835\udf0b, \ud835\udf0b \u2032) 0 0 \u22121 \u2212 1 8 (a) Lexicographic Precision (b) Preference Magnitude Figure 4: Lexicographic precision between two rankings \ud835\udf0b and \ud835\udf0b \u2032 with \ud835\udc5a = 5 relevant items in corpus of size \ud835\udc5b. (a) Using the sorted positions of relevant items, lexiprecision returns a preference based on the highest-ranked difference in positions. (b) The magnitude of preference between \ud835\udf0b and \ud835\udf0b \u2032 under different schemes. lexicographic maximum begins by looking at utilities in the best- off positions (i.e. \ud835\udc5d1 and \ud835\udc5d\u2032 1) and iteratively inspects lower utility positions until we find an inequality. If we exhaust all \ud835\udc5a relevance levels, we indicate that there is not preference between the rankings. Note that a tie can only happen if two rankings have all relevant items in exactly the same positions. Lexicographic sorting generates a total ordering over all positions of relevant items, in contrast with just inspecting \ud835\udc5d1, which com- presses all arrangements onto \ud835\udc5b possible values. Because of its basis in lexicographic ordering, we refer to this lexicographic precision or lexiprecision. 3.3 Number of Ties Under Lexicographic Precision We can contrast the number of ties as \ud835\udc5a increases in RL1 metrics with the number of ties as \ud835\udc5a increases in lexiprecision. In the latter, we only observe ties when the positions of the relevant items for two rankings are the same and, therefore, we have (cid:0) \ud835\udc5b (cid:1) possible \ud835\udc5a"}