{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_CodeBERTScore:_Evaluating_Code_Generation_with_Pretrained_Models_of_Code_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of CodeBERTScore?", "answer": " CodeBERTScore is an evaluation metric for code generation that encodes both the generated code tokens and the natural language input preceding the generated code, modeling the consistency between the generated code and its given natural language context.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " How does CodeBERTScore differ from BERTScore in evaluating code generation?", "answer": " CodeBERTScore encodes not only the generated tokens but also the natural language input preceding the generated code, while BERTScore only encodes the generated tokens.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " What was the outcome of the evaluation of CodeBERTScore across four programming languages?", "answer": " The evaluation found that CodeBERTScore achieves a higher correlation with human preference and functional correctness than all existing metrics.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " How many language-specific pretrained models were released along with CodeBERTScore?", "answer": " Five language-specific pretrained models were released to be used with CodeBERTScore.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " What is the main advantage of large language models (LLMs) in natural-language-to-code generation (NL\u2192Code)?", "answer": " LLMs have the ability to accurately generate long completions that span multiple tokens and lines, rather than just a single next-token.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " What problem arises when evaluating different code generation models?", "answer": " Evaluating different code generation models remains a challenging problem that requires an accurate and reliable evaluation metric due to the diversity in implementation and coding style.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " What is one limitation of token-matching metrics like BLEU and CrystalBLEU in evaluating generated code?", "answer": " Token-matching metrics like BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does not account for diversity in implementation, variable names, and code conventions.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " How does CodeBLEU attempt to address the limitation of requiring a lexical exact match?", "answer": " CodeBLEU attempts to lower the requirement for a lexical exact match by relying on data-flow and Abstract Syntax Tree (AST) matching in addition to token matching.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " What is one drawback of execution-based evaluation in verifying the functional correctness of generated code?", "answer": " Execution-based evaluation requires datasets with hand-written test cases for each example, which is costly and labor-intensive to create.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}, {"question": " Why might accepting partial predictions be problematic in evaluation by CodeBLEU?", "answer": " Partial predictions may lead to code that does not parse fully, and thus cannot be fully evaluated by CodeBLEU.", "ref_chunk": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}], "doc_text": "3 2 0 2 t c O 1 3 ] E S . s c [ 2 v 7 2 5 5 0 . 2 0 3 2 : v i X r a CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Shuyan Zhou\u2217 Uri Alon\u2217\u2020 Sumit Agarwal Graham Neubig Language Technologies Institute, Carnegie Mellon University {shuyanzh,ualon,sumita,gneubig}@cs.cmu.edu Abstract Since the rise of neural natural-language-to- code models (NL\u2192Code) that can generate long expressions and statements rather than a single next-token, one of the major prob- lems has been reliably evaluating their gen- In this paper, we propose erated output. CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural lan- guage input preceding the generated code, thus modeling the consistency between the gener- ated code and its given natural language con- text as well. We perform an extensive eval- uation of CodeBERTScore across four pro- gramming languages. We find that Code- BERTScore achieves a higher correlation with human preference and with functional correct- ness than all existing metrics. That is, gener- ated code that receives a higher score by Code- BERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub.1 1 Introduction Natural-language-to-code generation (NL\u2192Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural lan- guage and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Al- lal et al., 2023). LLMs have reached such a high NL\u2192Code accuracy that they are now useful for the broad programming audience and actually save \u2217Equal contribution \u2020 Now at Google DeepMind 1The code and data are available at https://github.com/ developers\u2019 time when implemented in tools such as GitHub\u2019s Copilot. This sharp rise in LLMs\u2019 us- ability was achieved thanks to their ability to ac- curately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allama- nis and Sutton, 2013; Movshovitz-Attias and Co- hen, 2013). Nevertheless, evaluating and compar- ing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models\u2019 generated outputs, and existing met- rics are sub-optimal. Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping n- grams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most oc- curring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does for diversity in implementation, not account variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c). CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by rely- ing on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program- neulab/code-bert-score Reference: int f(Object target) { int i = 0; for (Object elem: this.elements) { if (elem.equals(target)) { return i; } i++; } return -1; } (a) The ground truth reference \u2013 find the index of target in this.elements. Non-equivalent candidate: Equivalent candidate: boolean f(Object target) { int f(Object target) { for (Object elem: this.elements) { for (int i=0; i<this.elements.size(); i++) { if (elem.equals(target)) { return true; Object elem = this.elements.get(i); if (elem.equals(target)) { } return i; } } return false; } return -1; } } (b) Preferred by BLEU & CrystalBLEU \u2013 find whether or not target is in this.elements. (c) Preferred by CodeBERTScore \u2013 find the index of target in this.elements. Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a). mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully eval- uated by CodeBLEU (for example, predicting the first line of a for loop, without the loop\u2019s body). Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to di- versity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Addi- tionally, executing model-generated code is sus- ceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively. Our approach In this work, we introduce Code- BERTScore, an evaluation metric for code genera- tion, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang First, CodeBERTScore encodes"}