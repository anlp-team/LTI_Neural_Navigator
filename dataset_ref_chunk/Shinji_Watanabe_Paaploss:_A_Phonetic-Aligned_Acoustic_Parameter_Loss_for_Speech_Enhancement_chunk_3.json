{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Paaploss:_A_Phonetic-Aligned_Acoustic_Parameter_Loss_for_Speech_Enhancement_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of incorporating weights in the acoustic parameter differences?", "answer": " Weights are used to incorporate phonetic information in the acoustic parameter differences.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " How is the PAAP Loss calculated?", "answer": " The PAAP Loss calculates the weighted difference between acoustic parameters for each time step.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " What STFT settings are used in the implementation?", "answer": " STFT with a hop length of 160 and window length of 512 is used in the implementation.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " How is the PAAP Loss used in the network?", "answer": " The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to fine-tune the network.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " What type of data was used in the experiments?", "answer": " Data from the Deep Noise Suppression Challenge from InterSpeech 2020 was used to synthesize audio pairs for training and validation sets.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " How are the baseline models pre-processing their input data before training?", "answer": " The baseline models pre-process their input data slightly before training.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " What synthetic test set is used for the final evaluation of the models?", "answer": " The DNS 2020 synthetic test set with no reverberation is used for the final evaluation.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " What models are selected for the experimental results?", "answer": " State-of-the-art Demucs and FullSubNet models representing time domain and time-frequency domain models are selected.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " What factor is the PAAP Loss weighed by before adding it to the original loss during fine-tuning?", "answer": " The PAAP Loss is weighed by a factor of 0.1 before adding it to the original loss for fine-tuning.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}, {"question": " How is the acoustic improvement measured in the context of the PAAP Loss?", "answer": " The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech.", "ref_chunk": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}], "doc_text": "and baseline models on the synthetic test set. weights are used to incorporate phonetic information in the acous- tic parameter differences, not to directly predict phoneme logits. In this way, the PAAP Loss calculates the weighted difference between acoustic parameters for each time step. In our implementation, we use STFT with hop length of 160 and window length of 512 to determine the total number of frames N . Both the phoneme logits and acoustic parameters have N vectors of values. We iterate the above process over all frames in the utterance, and average the PAAP Loss by the total number of frames. The PAAP Loss is used as an auxiliary loss alongside the original loss of the SE model to \ufb01ne-tune the network. We follow the optimal setting of [21] by keeping all weights frozen except the speech enhancement model. In our work, this applies to both acoustic-phonetic weights w and the weights of the temporal acoustic estimator network \u03a8. 4. EXPERIMENTS 4.1. Data We used data and scripts from the Deep Noise Suppression (DNS) Challenge from InterSpeech 2020 [6] to synthesize 50,000 pairs of 30-second (s) noisy and clean audio for training. We further synthe- sized another 10,000 audio pairs for validation set. The synthesis is performed under the default setting, where the Signal to Noise Ratio (SNR) is sampled uniformly between 0 and 40 decibels (dB). Then, noise audios from DNS noise set are selected with suf\ufb01cient duration to span the selected clean utterance from Librivox, and added to the clean [28]. Our baseline models pre-process their input data slightly before training, and we follow each model\u2019s respective con\ufb01guration during its \ufb01ne-tuning. Demucs splits 30s audios into 10s segments with a 2s stride, and FullSubNet randomly samples a 3.072s segment from the 30s audio during each iteration. For the \ufb01nal evaluation of the models, we use the DNS 2020 synthetic test set with no reverberation. This set consists of 150 ut- terances from Graz University\u2019s clean speech dataset [29], combined with noise categories randomly sampled from more than 100 noise classes. The SNR levels of the test set were uniformly sampled be- tween 0 and 25 dB. 4.2. Experimental Results To demonstrate that our proposed method is robust at improving var- ious architectures, we select state-of-the-art Demucs [30] and Full- SubNet [31] representing time domain and time-frequency domain models, respectively. These models are also open-sourced, so we use their pre-trained checkpoints to allow the reproducibility of the results of our work. For our unsupervised phonetic aligner, we use a wav2vec2-based method [32]. In our experiments, we weigh the PAAP Loss by a factor of 0.1 before adding to the original loss to \ufb01ne-tune the seed SE model. In Fig. 1: Acoustic improvement (in %) for FullSubNet (upper) and Demucs (lower) by using the proposed PAAP Loss, where acoustic improvement is reduction in MAE as de\ufb01ned in Section 4.3.1. the \ufb01ne-tuning process, the pre-trained temporal acoustic parameter estimator is a 3-layer bi-directional long short-term memory (LSTM) [33] with 512 hidden units. Table 1 shows the evaluation results eval- uation by \ufb01ne-tuning FullSubNet and Demucs with the additional PAAP Loss. We \ufb01rst look at Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) as they are canonical evaluations for speech enhancement. We see signi\ufb01cant improve- ments in these metrics using our PAAP loss. Note that these are strong state-of-the-art models and hence improvements are hard to achieve. PESQ in particular improves by almost 4% and 13% for FullSubNet and Demucs respectively. Since our goal is to improve perceptual quality, the gold stan- dard evaluation is mean opinion score from humans. This is calcu- lated as the average of ratings on a 1-5 scale. Conducting a Mean Opinion Score (MOS) study is costly so we include two of the cur- rent state-of-the-art estimation approaches to estimate MOS, DNS- MOS [34], and NORESQA-MOS (Non-matching Reference based Speech Quality Assessment) [35]. We observe that our PAAP loss once again shows improvements in these metrics for both models. Finally, we also calculate word error rate (WER) to evaluate whether our enhancement reduces distortions that affect downstream speech processing applications. Since we do not have ground-truth transcriptions, we use WavLM [36] base model from HuggingFace on clean speech to get the transcriptions as reference. We then apply the same recognizer to baseline and our enhanced speech to com- pare. We see improvements in WER as well, demonstrating that our method bene\ufb01ts both human perceptual quality and the ability to in- terface with speech technologies. 4.3. Analysis 4.3.1. Acoustic improvement Fig. 1 provides a visualization of the percentage improvement of the 25 acoustic parameters after using the PAAP Loss to \ufb01ne-tune the model. The acoustic improvement is measured by the reduction in mean absolute error (MAE) between the acoustic parameters of the enhanced and clean speech. Formally, if DE, DC \u2208 RN \u00d725 are the enhanced and clean estimated acoustics, for each acoustic parameter j we compute MAE(DE j , DC j ) = 1 N N (cid:88) i=1 |DE ij \u2212 DC ij| and then average over all acoustic parameters to get MAE(DE, DC ). Formally, the acoustic improvement as reduction in MAE is MAE(DE, DC ) \u2212 MAE(DB, DC ) MAE(DB, DC ) 100% where DB stands for the acoustic parameters from the baseline enhancement model. For FullSubNet, we can observe that the PAAP Loss has the most improvement on MFCC features and loudness. On the other hand, for Demucs, most of the acoustic improvement of features are at the similar level with FullSubNet, except that the loudness and the F0 on a semitone frequency scale have a larger boost of 30%. Among all the acoustic features, the acoustic im- provements are relatively small for formant frequencies and formant bandwidths for both models, but we conclude that we are getting a consistent improvement on all of the acoustic low-level descriptors across different categories of SE models. 4.3.2. Phoneme-dependent acoustic improvement In the previous section, we looked at overall improvements for each"}