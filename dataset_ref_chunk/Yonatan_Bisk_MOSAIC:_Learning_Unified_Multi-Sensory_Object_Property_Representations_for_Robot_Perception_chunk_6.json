{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_MOSAIC:_Learning_Unified_Multi-Sensory_Object_Property_Representations_for_Robot_Perception_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " In which scenario did the interactive behaviors condition perform comparably to the non-interactive condition?", "answer": " The interactive behaviors condition performed comparably to the non-interactive condition for visual properties like transparency and size.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " Why did the interactive behaviors condition significantly outperform the non-interactive one for the shape property category?", "answer": " Interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What did the MOSAIC framework demonstrate in the fetch object task?", "answer": " The full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What do the results underscore about the adaptability and applicability of unified representations?", "answer": " The results underscore the adaptability and applicability of unified representations across diverse tasks, including those involving natural language instructions.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What is one of the exciting directions for future research mentioned in the text?", "answer": " Considering the transfer of unified representations across different robot morphologies is mentioned as an exciting direction for future research.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What is one limitation mentioned in the current study regarding the fetch object task?", "answer": " One limitation mentioned is that for the fetch object task, the evaluation was done using a zero-shot transfer condition rather than a learning-based approach to find the target object.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What do the authors plan to explore in future work with regard to the fetch object task?", "answer": " In future work, the authors plan to explore learning-based policies for solving the fetch object task, potentially increasing the versatility and adaptability of their framework.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What task did the authors showcase the effectiveness of unified representations in, using a simple linear probe setup?", "answer": " The authors showcased the effectiveness of unified representations in tasks such as category recognition, using a simple linear probe setup.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What type of models are being considered towards building general-purpose robots?", "answer": " Foundation models are being considered towards building general-purpose robots.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}, {"question": " What was one of the intentions behind introducing the MOSAIC framework?", "answer": " One of the intentions behind introducing the MOSAIC framework was to enable robots to generate versatile, multimodal representations and leverage these across various downstream robot tasks.", "ref_chunk": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}], "doc_text": "Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, sug- gesting that interaction with objects may not yield signifi- cantly more information in these scenarios. Shape: Intrigu- ingly, for the shape property category, the interactive behav- iors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and ap- plicability of unified representations across diverse tasks, including those involving natural language instructions. VI. CONCLUSION AND FUTURE WORK We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified learning representations across various downstream robot tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot condi- tions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where inter- active behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learning- based policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework. REFERENCES [1] T. Thesen, J. F. Vibell, G. A. Calvert, and R. A. \u00a8Osterbauer, \u201cNeu- roimaging of multisensory processing in vision, audition, touch, and olfaction,\u201d Cognitive Processing, vol. 5, pp. 84\u201393, 2004. [2] D. Alais, F. Newell, and P. Mamassian, \u201cMultisensory processing in review: from physiology to behaviour,\u201d Seeing and perceiving, vol. 23, no. 1, pp. 3\u201338, 2010. [3] D. A. Bulkin and J. M. Groh, \u201cSeeing sounds: visual and auditory interactions in the brain,\u201d Current opinion in neurobiology, vol. 16, no. 4, pp. 415\u2013419, 2006. [4] S.-C. Kim and S. Ryu, \u201cRobotic kinesthesia: Estimating object geom- etry and material with robot\u2019s haptic senses,\u201d IEEE Transactions on Haptics, 2023. [5] X. Zhang, S. Amiri, J. Sinapov, J. Thomason, P. Stone, and S. Zhang, \u201cMultimodal embodied attribute learning by robots for object-centric action policies,\u201d Autonomous Robots, pp. 1\u201324, 2023. [6] Q. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, \u201cA review of tactile information: Perception and action through touch,\u201d IEEE Transactions on Robotics, vol. 36, no. 6, pp. 1619\u20131634, 2020. [7] G. Tatiya, J. Francis, and J. Sinapov, \u201cTransferring implicit knowl- edge of non-visual object properties across heterogeneous robot mor- phologies,\u201d in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 315\u201311 321. [8] J. K. Bizley, G. P. Jones, and S. M. Town, \u201cWhere are multisensory signals combined for perceptual decision-making?\u201d Current opinion in neurobiology, vol. 40, pp. 31\u201337, 2016. [9] C. V. Parise, C. Spence, and M. O. Ernst, \u201cWhen correlation implies causation in multisensory integration,\u201d Current Biology, vol. 22, no. 1, pp. 46\u201349, 2012. [10] J. Francis, N. Kitamura, F. Labelle, X. Lu, I. Navarro, and J. Oh, \u201cCore challenges in embodied vision-language planning,\u201d Journal of Artificial Intelligence Research, vol. 74, pp. 459\u2013515, 2022. [11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y. Xie, T. Zhang, Z. Zhao et al., \u201cToward general-purpose robots via foundation models: A survey and meta-analysis,\u201d arXiv preprint arXiv:2312.08782, 2023. [12] C. Chen, K. Z. Zhang, Z. Chu, and M. Lee, \u201cAugmented reality in the metaverse market: the role of multimodal sensory interaction,\u201d Internet Research, 2023. [13] S. Cai, K. Zhu, Y. Ban, and T. Narumi, \u201cVisual-tactile cross-modal data generation using residue-fusion gan with feature-matching and perceptual losses,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 7525\u20137532, 2021. [14] J. Sinapov, C. Schenck, K. Staley, V. Sukhoy, and A. Stoytchev, \u201cGrounding interactions: categories Experiments with 100 objects,\u201d Robotics and Autonomous Systems, vol. 62, no. 5, pp. 632\u2013645, may 2014. [Online]. Available: https: //www.sciencedirect.com/science/article/pii/S092188901200190X [15] G. Tatiya and J. Sinapov, \u201cDeep multi-sensory object category recognition using interactive behavioral exploration,\u201d in International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, May 20-24, 2019. IEEE, 2019, pp. 7872\u20137878. [Online]. Available: https://doi.org/10.1109/ICRA.2019.8794095 semantic in behavioral [16] P. Xiong, J. Liao, M. Zhou, A. Song, and P. X. Liu, \u201cDeeply supervised subspace learning for cross-modal material perception of known and unknown objects,\u201d IEEE Transactions on Industrial Informatics, vol. 19, no. 2, pp. 2259\u20132268, 2022. [17] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. H. Adelson, L. Fei-Fei, R. Gao, and J. Wu, \u201cSee, hear, and feel: Smart sensory fusion for robotic manipulation,\u201d in Conference on Robot Learning (CoRL), vol. 205. Auckland, New Zealand: Proceedings of Machine Learning Research (PMLR), dec 2022, pp. 1368\u20131378. [Online]. Available: https://proceedings.mlr.press/v205/li23c.html [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), vol. 139. Proceedings of Machine Learning Research (PMLR), 2021, pp. 8748\u20138763. http://proceedings.mlr.press/v139/radford21a.html [Online]. Available: [19] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, \u201cContrastive learning of medical visual representations from paired images and text,\u201d in Machine Learning for Healthcare Conference. PMLR, 2022, pp. 2\u201325. [20] J.-B. Alayrac, J. Donahue, P. Luc,"}