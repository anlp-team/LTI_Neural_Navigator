{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ML-SUPERB:_Multilingual_Speech_Universal_PERformance_Benchmark_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the definition of SUPERBs according to the text?", "answer": " SUPERBs aggregates all task-specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model on the task t.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What does st,i(u) represent in the SUPERBs formula?", "answer": " st,i(u) is the ith metrics for task t and SSL model u.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What analysis tools are provided to facilitate a more comprehensive analysis of the benchmark?", "answer": " Various analysis tools are provided including character error rate (CER), aggregated scores for different language groups, and visualizations of the learnable layer weights.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What approach does the wav2vec2 model use in its pre-training?", "answer": " The wav2vec2 model uses a contrastive learning approach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What is one feature of the robust-wav2vec2-large model mentioned in the text?", "answer": " The robust-wav2vec2-large model considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " Which model obtains the best performance in the 1-hour set for the monolingual ASR task?", "answer": " XLSR-128 achieves the best performance in the 1-hour set for the monolingual ASR task.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " In the multilingual ASR task, what type of models have shown superior performance compared to the baseline model?", "answer": " Models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What is one noteworthy finding mentioned regarding the performance of large models in the multilingual ASR task?", "answer": " Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " What model has been the dominant model for both 10-minute and 1-hour datasets in the LID task?", "answer": " XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets in the LID task.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}, {"question": " According to the text, which model is considered the best overall in terms of performance?", "answer": " XLSR-128 is considered the best model overall in terms of performance.", "ref_chunk": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}], "doc_text": "and the few-shot training set. For overall performance, we use the SUPERBs metric from the SUPERB benchmark [38]. We denote st,i(u) as the ith met- rics for task t and SSL model u. T is the set of four tasks and It is the set of metrics for the task t. SUPERBs aggregates all task- specific scores st(u) with respect to baseline (i.e., FBANK) and state-of-the-art (SOTA) model4 on the task t. The SUPERBs is defined as: st,i(u)\u2212st,i(FBANK) st,i(SOTA)\u2212st,i(FBANK) (1) We expect SUPERBs can provide a comprehensive view of the model performance on the benchmark and take the difficulty of tasks into consideration. Analysis support: To facilitate a more comprehensive analy- sis of the benchmark, we provide various analysis tools. For the multilingual ASR evaluation, we present the character error rate (CER) for each language as well as aggregated scores for dif- ferent language groups, in addition to the average CER for both normal and few-shot cases. In line with previous studies [39, 40], we also offer visualizations of the learnable layer weights and their learning curve during training. (cid:80)T t (cid:80)It i SUPERBs(u) = 1000 |T | 1 |It| 3. Experiments 3.1. Candidate models ML-SUPERB welcomes all speech SSL models trained on ei- ther monolingual or multilingual data. We believe the analysis of multilingual scenarios for monolingual speech SSLs is also valuable according to previous works [9\u201311]. In this paper, we show the experimental results of some example model candi- dates as shown in Table 2. wav2vec2: wav2vec2 is a popular speech SSL model for speech recognition [3]. Its pre-training uses a contrastive learning ap- proach that prioritizes identifying true quantized latent speech representations over masked time steps from distractors. The wav2vec2 model has also been extended to many other ver- sions for specialized use cases. For example, robust-wav2vec2- large [41] considers the diversity of speech types, such as read speech, conversational speech, and noisy speech, by including additional corpora in the pre-training stage. Wav2vec2-base- 23 and wav2vec2-large-23 are pre-trained on Voxpopuli [19], with a focus on European languages. Additionally, XLSR scales up the multilingual training in wav2vec2 by incorporating more languages and data [6, 7]. 4The SOTA models for each setting are discussed in Sec. 3.2. HuBERT: HuBERT uses an iterative offline clustering step to generate pseudo labels for each frame. During training, it pre- dicts the pseudo labels of the masked frame, which helps to im- prove the quality of the learned features. Similar to wav2vec2, HuBERT also has different versions, such as a multilingual Hu- BERT [43] trained in three European languages (fra, spa, eng) and HuBERT trained on Mandarin [42]. 3.2. Experimental Results The experimental results are shown in Table 3 for 10-minute set and Table 4 for 1-hour set. Monolingual ASR: In the monolingual ASR task, all speech SSL models outperform the FBANK baseline. XLSR-128 achieves the best performance in the 1-hour set, while HuBERT- large obtains the best performance in the 10-minute set. Sev- eral findings are noteworthy: (1) HuBERT-based models out- perform wav2vec2-based models when the training data and model size are similar. (2) Large models usually obtain bet- ter results than their base versions. (3) While the XLSR series of models deliver impressive performances in the 1-hour set, we have observed their instability in the 10-minute set, particularly on Asian languages such as cmn. Multilingual ASR: In the multilingual ASR task, all models trained using self-supervised learning (SSL) techniques have shown superior performance compared to the baseline model using FBANK features. Among the SSL models, XLSR-128 achieves the best results across all conditions. Our experiments also reveal some interesting findings: (1) Models trained with more languages generally outperform those trained on mono- lingual datasets, although this may not always be the case. For example, mHuBERT-base performs worse than HuBERT-based models trained on English only. (2) Large models trained on monolingual data do not necessarily have better representations for multilingual scenarios. For instance, HuBERT-large per- forms worse than HuBERT-base, and wav2vec2-large is less ef- fective than wav2vec2-base. One possible explanation for the lack of performance improvement with larger models is their limited ability to generalize, despite having similar training losses as base models. (3) The robust-wav2vec2-large model achieves decent scores on multilingual ASR, suggesting that our benchmark corpus may need to consider different acoustic en- vironments, as it includes multiple source datasets. LID: In the LID task, we notice similarities with multilingual ASR, but there are also notable differences. (1) XLSR-128 has been the dominant model for both 10-minute and 1-hour datasets. (2) While most SSL models have improvements over FBANK, some do not, particularly those based on wav2vec2 (e.g., wav2vec2-large-23 for the 10-minute set and wav2vec2- large for the 1-hour set). (3) Larger models with more param- eters and pre-trained data do not necessarily lead to better per- formance compared to base models. Joint Multilingual ASR + LID: In the joint multilingual ASR+LID task, the results generally align with the other two tasks in the multilingual track. (1) SSL models outperform FBANK on ASR, but some models perform worse on LID. (2) Base models exhibit better generalization ability and often per- form better on test sets. (3) There is no single best model that dominates the task, particularly in few-shot cases and LID tasks. Overall: In terms of overall performance as measured by SUPERBs in Sec. 2.4, XLSR-128 is the best model for both the 10-minute and 1-hour sets. Major findings include: (1) mul- tilingual training with a broad coverage of languages, as seen in XLSR models that include more than 50 languages, has proven Table 3: 10-minute set ML-SUPERB benchmark. SSL Monolingual ASR CER/PER Multilingual ASR Normal CER LID Few-shot Normal CER ACC Multilingual ASR + LID Few-shot CER Normal ACC CER SUPERBs FBANK wav2vec2-base [3] wav2vec2-large [3] robust-wav2vec2-large [41] wav2vec2-base-23 [19] wav2vec2-large-23 [19] XLSR-53 [7] XLSR-128 [6] HuBERT-base [4] HuBERT-large [4] HuBERT-base-cmn [42] HuBERT-large-cmn [42] mHuBERT-base [43] 72.1 44.2 42.0 44.4 49.2 42.0 49.5 39.7 42.8 38.2 43.1 39.4 41.0 62.4 43.0 42.6 40.1 37.7 42.1 33.9 29.2 39.8"}