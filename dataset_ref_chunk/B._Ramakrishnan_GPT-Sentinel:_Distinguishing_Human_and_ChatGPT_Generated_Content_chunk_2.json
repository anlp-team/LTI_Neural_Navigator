{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/B._Ramakrishnan_GPT-Sentinel:_Distinguishing_Human_and_ChatGPT_Generated_Content_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What method did Solaiman et al. use to achieve state-of-the-art performance in detecting text generated by GPT-2?", "answer": " Solaiman et al. fine-tuned a pre-trained RoBERTa model on a labeled dataset.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " How much labeled training data was required for the fine-tuned model by Solaiman et al. on RoBERTa?", "answer": " 200k labeled training data.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " What is the name of the language model based on GPT-3.5 architecture?", "answer": " ChatGPT.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " Who introduced ChatGPT on November 30, 2022?", "answer": " OpenAI.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " What is the unique identifier (UID) used in the OpenGPTText data set?", "answer": " Unique identi\ufb01er.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " How many textual samples are there in the OpenGPTText data set?", "answer": " 29,395 textual samples.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " What is the source of the OpenWebText data set?", "answer": " Web content sourced from URLs shared on Reddit.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " What cleaning procedure was implemented on the OpenGPTText data set?", "answer": " Removing excessive new-line characters and mapping Unicode characters onto the ASCII character set.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " When are OpenGPTText and OpenGPTText-Final planned to be released on Kaggle?", "answer": " May 2023.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}, {"question": " How was the input text truncated to improve training efficiency?", "answer": " Truncated to a maximum of 512 tokens.", "ref_chunk": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}], "doc_text": "Also, Solaiman et al. [1, 4] \ufb01ne-tuned a pre-trained RoBERTa model on a labeled dataset to create a content detector that achieved state-of-the-art 2 Table 1: Detailed statistics for OpenGPTText data set as of Apr 24, 2023. The subsets not listed in the table were not paraphrased in OpenGPTText. The category \u201cFailed to Rephrase\u201d corresponds to one of the following situations: 1. the content length exceeds the API limit, 2. the content is blocked by OpenAI content \ufb01lter. Subset OpenGPTText OpenWebText Failed to Rephrase Percentage urlsf_00 urlsf_01 urlsf_02 urlsf_03 urlsf_04 urlsf_05 urlsf_06 urlsf_09 3, 888 3, 923 3, 260 3, 891 3, 684 3, 602 3, 494 3, 653 391, 590 392, 347 391, 274 390, 161 390, 250 389, 874 390, 339 389, 634 27 0 652 10 218 296 409 243 0.99% 1.00% 0.83% 1.00% 0.94% 0.92% 0.90% 0.94% Total 29, 395 3, 125, 469 1, 885 0.94% performance of 90% accuracy in detecting text generated by GPT-2. However, the supervised learning method requires a large amount of labeled data, in contrast to previously discussed methods. The \ufb01ne-tuned model on RoBERTa by Solaiman et al. required 200k labeled training data. 3 Data Set Collection ChatGPT is a language model based on the GPT-3.5 architecture. It succeeded InstructGPT, which was previously published by Ouyang et al. [2]. As it was introduced by OpenAI on November 30, 2022, there is currently no publicly available data set that systematically collects the outputs generated by ChatGPT as far as we know. Consequently, we undertook the task of creating our own data set for ChatGPT outputs. Building upon the work of Gokaslan et al. [8] and their OpenWebText corpus. We named the data set OpenGPTText. 3.1 OpenGPTText Overview The OpenGPTText data set consists of paraphrased textual samples that were generated by the gpt-3.5-turbo language model using the OpenWebText corpus as its source. The data set contains 29,395 textual samples, each corresponding to a piece human-written text from the OpenWebText corpus that shares a same unique identi\ufb01er (UID). Up to April 24, 2023, the OpenGPTText only contains approximately 1% of paraphrased samples of the OpenWebText data set in some speci\ufb01c subsets. The number of samples in each subset is listed in table 1. 3.2 Data Source The OpenWebText data set [8] is a publicly available resource that comprises web content sourced from URLs shared on Reddit with a minimum of three votes. This data set is a reconstitution of the original WebText corpus, which was initially described by Radford et al. [9]. Since the data set was compiled in 2019, it is improbable that the textual content it contains was algorithmically generated. 3.3 Data Collection Method The rephrasing procedure used OpenAI\u2019s API on gpt-3.5-turbo model, with the prompted instruc- tion: \u201cRephrase the following paragraph by paragraph\u201d. However, it should be noted that the samples with length larger than 2,000 words were \ufb01ltered out as the gpt-3.5-turbo can only take in at most 3,000 tokens. Some text samples blocked by OpenAI content \ufb01lter was also excluded from OpenGPTText. The number of texts that were not successfully paraphrased due to either of the two reasons is reported in the \u201cFailed to Rephrase\u201d column in table 1. 3 3 4 3 3 OpenWebText-Original 2 2 1 1 0 0 2 OpenWebText-Final 1 1 5 PCA projection of hidden state ofRoBERTa-Sentinel on OpenWebText before and after sanitize PCA projection of hidden state ofRoBERTa-Sentinel on OpenGPTText before and after sanitize 4 3 0 1 1 0.0 1.5 OpenGPTText-Final OpenGPTText-Original 1.0 0.5 0.5 2 2 2.0 Figure 1: PCA of hidden state distribution of RoBERTa-Sentinel model on OpenWebText (Left) and OpenGPTText (Right) before and after cleaning. Note that the cleaning process affected the distribution of OpenWebText signi\ufb01cantly. 3.4 Data Set Cleaning Upon inspecting the OpenGPTText data set, we observed certain stylistic disparities between Chat- GPT\u2019s output and the corpus in OpenWebText. Speci\ufb01cally, our analysis revealed that ChatGPT\u2019s output tend to include the Unicode character \u201cright double quotation mark\u201d (U+201D) in place of the ASCII character \u201cquotation mark\u201d (U+0022) used in the OpenWebText corpus. Furthermore, ChatGPT also tends to incorporate two consecutive new-line characters between paragraphs, whereas the OpenWebText corpus utilizes two to six new-line characters consecutively. In an effort to enhance the resilience of our classi\ufb01er and eliminate the potential in\ufb02uence of these susceptible features, we undertook measures to sanitize both the OpenWebText and OpenGPTText data sets. To achieve this, we implemented a cleaning procedure that involved removing excessive new-line characters and mapping Unicode characters onto the ASCII character set. These steps were taken to mitigate any possible confounding effects of these variables on the performance of our classi\ufb01er. Pincipal Component Analysis (PCA) of hidden state distribution in \ufb01gure 1 shows that the cleaning process has signi\ufb01cantly changed the distribution of dataset. The resulting, clean data set are called OpenWebText-Final and OpenGPTText-Final in the discussion below. 3.5 Data Set Release Our plan entails the release of both OpenGPTText and OpenGPTText-Final on Kaggle in May 2023. 4 Method The following models were trained using the OpenWebText-Final and OpenGPTText-Final data set, partitioning 80% of the data set for training, 10% for validation, and the remaining 10% for testing. Given that the texts in the data set have varying lengths, we truncated input text to a maximum of 512 tokens to improve training ef\ufb01ciency, while also padding any text with less than 512 tokens with additional <PAD> tokens. To address memory constraints while using a relatively large batch size during \ufb01ne-tuning, we performed gradient accumulation, whereby the optimizer was updated after a certain number of forward passes. 4.1 RoBERTa-Sentinel Model The \ufb01rst method we proposed is to leverage the pretrained RoBERTa model [10] to extract relevant features from the input text, followed by an MLP with gaussian error linear units (GELU, [11]) and two fully connected layers for classi\ufb01cation. To preserve the general linguistic knowledge of the 4 12 \ud835\udc471(1) \ud835\udc381 \ud835\udc471 \ud835\udc471 \ud835\udc471(2) \ud835\udc43(Human)\ud835\udc43(ChatGPT) MLP \ud835\udc472 \ud835\udc472 RoBERTa-BaseThisisGPT \ud835\udc47 \ud835\udc47 \u2026768768 \ud835\udc38[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc36\ud835\udc3f\ud835\udc46(1) 12 1 1 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47\ud835\udc41 \ud835\udc47[\ud835\udc36\ud835\udc3f\ud835\udc46] \ud835\udc472"}