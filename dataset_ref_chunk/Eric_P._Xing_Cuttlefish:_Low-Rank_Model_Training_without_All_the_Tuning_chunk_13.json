{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Cuttlefish:_Low-Rank_Model_Training_without_All_the_Tuning_chunk_13.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What datasets were used in the experiments described in the text?", "answer": " CIFAR-10, CIFAR-100, SVHN, ImageNet, and GLUE datasets", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What licensing information is provided for the datasets used in the experiments?", "answer": " The datasets used have their own licenses, but all are publicly available", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What framework was used to implement the methods and baselines in the experiments?", "answer": " PyTorch", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " How can users access the code necessary to replicate the experiments presented in the text?", "answer": " Through a public GitHub repository with comprehensive documentation", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What types of instances were used for the experiments in the text?", "answer": " p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " How were the experimental environments configured in the text?", "answer": " Through Docker containers from NVIDIA GPU Cloud (NGC)", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What is the purpose of the provided installation scripts in the text?", "answer": " To facilitate the installation of necessary components not included in the Docker containers", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What data processing techniques were employed for CIFAR-10 and CIFAR-100 datasets?", "answer": " Standard augmentation techniques: channel-wise normalization, random horizontal flipping, and random cropping", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " What are the mean and standard deviation values used for color channel normalization in CIFAR-10 and CIFAR-100 datasets?", "answer": " \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}, {"question": " How many classes are in the SVHN dataset described in the text?", "answer": " 10 classes", "ref_chunk": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}], "doc_text": "of US West (Oregon)), which can be used to launch large-scale experiments. Code licenses (if publicly available)?: N/A Data licenses (if publicly available)?: We use CIFAR-10, CIFAR-100, SVHN, Imagenet (ILSVRC 2012) datasets and the GLUE benchmark which come with their own licenses. All datasets are publicly available. Compilation: All methods and baselines are implemented in PyTorch, therefore requiring no compilation. Work\ufb02ow framework used?: N/A Transformations: N/A Binary: N/A Archived (provide DOI)?: We use Zenedo to cre- ate a publicly accessible archival repository for our GitHub repository, i.e., https://doi.org/10.5281/ zenodo.7884872. Data set: For our main experiments, we employ CIFAR- 10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. As preparing the ImageNet dataset can be time-consuming, we provide a ready-to-use public AMI - ami-05c0b3732203032b3 in the US West (Oregon) region for convenience. A.3 Description We have made available the code necessary to replicate all the experiments presented in this paper through a public GitHub repos- itory that contains comprehensive documentation, allowing users to seamlessly execute the experiments. Run-time environment: N/A A.3.1 How delivered Hardware: Our experiments were conducted using Ama- zon EC2 instances, speci\ufb01cally p3.2xlarge, g4dn.metal, and p4d.24xlarge. Run-time state: N/A Our entire codebase is available on the GitHub repository: https://github.com/hwang595/Cuttlefish. To fa- cilitate easy setup on AWS, we offer a public AMI - identi\ufb01ed by ami-05c0b3732203032b3 (in the region of US West (Oregon)) - which can be utilized to launch large-scale experiments. CUTTLEFISH: Low-rank Model Training without All The Tuning A.3.2 Hardware dependencies run. For all our experiments we used p3.2xlarge, g4dn.metal, and p4d.24xlarge Amazon EC2 instances. To reproduce our results, one instance of each type is required. A.3.3 Software dependencies We established our experimental environments using Docker, con\ufb01guring them through PyTorch Docker containers from NVIDIA GPU Cloud (NGC). The experiments involving the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets were based on the NGC container nvcr.io/nvidia/pytorch:20.07-py3, while those focused on the GLUE benchmark utilized the nvcr.io/nvidia/pytorch:22.01-py3 container. Since there are addi- tional software dependencies not included in the Docker containers, we have supplied installation scripts, accompanied by instructions in the README \ufb01le of our GitHub repository, to facilitate the installation of these necessary components. A.3.4 Data sets For all our experiments we used CIFAR-10, CIFAR-100, SVHN, ImageNet (ILSVRC 2012), and GLUE datasets. For CIFAR- 10, CIFAR-100, SVHN, and GLUE datasets, our code will automatically download them. For the ImageNet dataset, we have it ready and provided via the public AMI - with ID ami- 05c0b3732203032b3 (in the region of US West (Oregon)). A.4 Installation In the GitHub README, we offer comprehensive instructions for installing dependencies and con\ufb01guring the Docker environments. A.5 Experiment work\ufb02ow We have provided a detailed README along with our GitHub repository which provides bash scripts to execute and launch the experiments. A.6 Evaluation and expected result During the experiment, logs containing details such as accuracy and running time will be displayed directly. However, given the inherent variability in machine learning tasks and the diversity of hardware and system con\ufb01gurations, it is important to note that the exact accuracy and running time \ufb01gures reported in the paper may not be replicated. Nevertheless, by using the artifacts provided, one can expect to achieve comparable accuracy and running time outcomes. A.7 Experiment customization The experiment can be customized by trying on different hardware setups. One example of this will be to run these experiments on slower GPUs (or other hardware, e.g., CPUs). Another option would be to try to support more model architectures using the heuristics of CUTTLEFISH (an interesting example will be adopting CUTTLEFISH for some recently designed large language models). A.8 Notes If the evaluator utilizes our provided AMI, the initial disk ini- tialization will take an extended period of time during the \ufb01rst CUTTLEFISH: Low-rank Model Training without All The Tuning B EXPERIMENTAL SETUP In this section, we delve into the speci\ufb01cs of the datasets B.1 and model architectures B.2 employed in our experiments. Ad- ditionally, we elaborate on the software environment B.3 and the implementation details of all methods included in our experi- ments B.4. Our code can be accessed at https://github. com/hwang595/Cuttlefish. Hugging Face 2. In accordance with prior work (Devlin et al., 2018; Jiao et al., 2020; Hu et al., 2021), we exclude the problematic WNLI downstream task. B.2 Model architectures In this section, we provide a summary of the network architectures utilized in our experiments. B.1 Dataset We carried out experiments across multiple computer vision and NLP tasks to evaluate the performance of CUTTLEFISH and the other considered baselines. In this section, we discuss the speci\ufb01cs of each task in greater detail. CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR- 100 comprise 60,000 color images with a resolution of 32\u00d732 pixels, where 50,000 images are used for training and 10,000 for validation (since there is no provided test set for CIFAR- 10 and CIFAR-100, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets) (Krizhevsky et al., 2009). CIFAR-10 and CIFAR-100 involve 10-class and 100-class clas- si\ufb01cation tasks, respectively. For data processing, we employ standard augmentation techniques: channel-wise normalization, random horizontal \ufb02ipping, and random cropping. Each color channel is normalized with the following mean and standard deviation values: \u00b5r = 0.485, \u00b5g = 0.456, \u00b5b = 0.406; \u03c3r = 0.229, \u03c3g = 0.224, \u03c3b = 0.225. The normalization of each channel pixel is achieved by subtracting the corresponding channel\u2019s mean value and dividing by the color channel\u2019s standard deviation. SVHN. The SVHN dataset comprises 73,257 training images and 26,032 validation images, all of which are colored with a resolution of 32\u00d732 pixels (Netzer et al., 2011). This classi\ufb01- cation dataset consists of 10 classes. As there is no clear test- validation split for the SVHN dataset, we follow the convention of other papers by conducting experiments and reporting the highest achievable accuracy on the validation datasets. There are 531,131 additional images for SVHN, but we do not include them in our experiments for this paper. For data processing, we employ the same data augmentation"}