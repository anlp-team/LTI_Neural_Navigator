{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_PWESuite:_Phonetic_Word_Embeddings_and_Tasks_They_Facilitate_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the importance of including analyses with A, rather than SH alone?,answer: The limitations of judgements produced from a small English-only corpus highlight the importance of including analyses with A, rather than SH alone.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What is the correlation between A and SH?,answer: A and SH do not correlate positively, with a Pearson coefficient of -0.74.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What is the purpose of retrieval in word embeddings?,answer: An important usage of word embeddings is the retrieval of associated words, which is also utilized in extrinsic evaluations and other applications.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " How is the immediate neighbor of a word denoted and defined based on SP?,answer: The immediate neighbor of a word based on SP is denoted as N, and it is defined as the word closest to the given word based on SP.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " Why is the retrieval neighborhood capped at n = 1000 samples?,answer: The retrieval neighborhood is capped at n = 1000 samples to compute n\u2212r percentile rank, which ensures that the metric will be bounded between 0 (worst) and 1 (best), for overall evaluation.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What are the two types of errors identified in the retrieval task for the Metric Learner model with articulatory features?,answer: The two types of errors identified are incorrect neighbors with low sound similarity and plausible neighbors that are not considered as correct.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What is the focus of the rhyme detection task?,answer: The focus of the rhyme detection task is on perfect rhymes, where the sounds from the last stressed syllables of two words are identical.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " How is cognate detection framed and evaluated?,answer: Cognate detection is framed as a binary classification task, with the evaluation based on the accuracy of identifying potential cognate pairs.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What is the purpose of sound analogies in phonetic word embeddings?,answer: Sound analogies help in assessing the ability of well-trained phonetic word embeddings to capture relationships between words based on sound similarities.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}, {"question": " What is the higher accuracy in the rhyme detection task indicative of?,answer: The higher the accuracy in the rhyme detection task, the more useful information for the task is encoded in the embeddings.", "ref_chunk": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}], "doc_text": "For the rule-based similarity metric SP, we use articulatory distance (Mortensen et al., 2016), as described in Section 3.3.1. 4.1.2. Human Judgement Vitz and Winkler (1973) asked people to judge the sound similarity of English words. For selected word pairs, we denote the collected judgements (scaled from 0\u2013least similar to 1\u2013identical) with the function SH. For example, SH(slant, plant) = 0.9 and SH(plots, plant) = 0.4. Like the previous task, we find correlations between S\u25e6\u0192 and SH. We note SH judgments were produced from a small English-only corpus. These limitations highlight the importance of including analyses with A, rather than SH alone. In fact, A and SH do not correlate positively, with Pearson coefficient \u22120.74. 4.1.3. Retrieval An important usage of word embeddings is the re- trieval of associated words, which is also utilized in the analogies extrinsic evaluation and other ap- plications. Success in this task means that the new embedding space has the same local neigh- bourhood as the original space induced by some non-vector-based metric. Given a word dataset W and one word \uebf7 \u2208 W, we sort W \\ {\uebf7} based on both S\u25e6\u0192 and SP distance from \uebf7. Based on this ordering, we define the immediate neighbour of \uebf7 based on SP, denoted \uebf7N and ask the question What is the average rank of \uebf7N in the ordering by S\u25e6\u0192 ? If the similarity given by S\u25e6\u0192 is copying SP perfectly, then the rank will be 0 because \uebf7N will be the closest to \uebf7 in S\u25e6\u0192 . Again, for SP we use the articulatory distance A (Section 3.3.1). Even though there are a variety of possible metrics to evaluate retrieval, we focus on the average rank. We further cap the retrieval neighborhood at n = 1000 samples and compute n\u2212r percentile rank as n . This choice is done so that the metric will be bounded between 0 (worst) and 1 (best), which will become important for overall evaluation later (Section 4.3). Error analysis. We identify two types of errors in the retrieval task for the Metric Learner model with articulatory features. The first one are sim- ply incorrect neighbours with low sound similarity, such as the word carcass, whose correct neigh- bour is cardiss but for which krutick is chosen. The next group are plausible ones, such as for the word counterrevolutionary, its neighbour in ar- ticulatory distance space counterinsurgency and the retrieved word cardiopulmonary. In this case we might even say that the retrieved word is closer. 4.2. Extrinsic Evaluation 4.2.1. Rhyme Detection There are multiple types of word rhymes, most of which are based around two words sounding similarly. We focus on perfect rhymes: when the sounds from the last stressed syllables are identi- cal. An example is grown and loan, even though the surface character form does not suggest it. Clearly, this task can be deterministically solved if one has access to the articulatory and stress information of the concerned words. Nevertheless, we wish to evaluate whether this information can be encoded in a fixed-length vector produced by \u0192 . We create a balanced binary prediction task for rhyme detection in English and train a small multi- layer perceptron classifier on top of pairs of word embeddings. The linking hypothesis is that the higher the accuracy, the more useful information for the task there is in the embeddings. 4.2.2. Cognate Detection Cognates are words in different languages that share a common origin. We include loanwords alongside genetic cognates. Similarly to rhyme detection, we frame cognate detection as a binary classification task where the input is a potential cognate pair. CogNet (Batsuren et al., 2019) is a large cognate dataset of many languages, making it ideal to evaluate the usefulness of phonetic em- beddings. We add non-cognate, distractor pairs in the dataset by finding the orthographically closest word that is not a known cognate. For example, plant EN and planteFR are cognates, while plant EN and planeEN are not. Although cognates also pre- serve some of the similarities in the meaning, we detect them using phonetic characteristics only. 4.2.3. Sound Analogies Just as distributional semantic vectors can com- plete word-level analogies such as man : woman \u2194 king : queen (Mikolov et al., 2013b), so too should well-trained phonetic word embeddings cap- ture sound analogies. For example of a sound analogy, consider /dIn/ : /tIn/ \u2194 /zIn/ : /sIn/. The Model INTRINSIC Human Sim. Art. Dist. (Pearson) (Pearson) Retrieval (rank perc.) Analogies (Acc@1) EXTRINSIC Rhyme (accuracy) Cognate (accuracy) OVERALL s r u O Metric Learner Triplet Margin Count-based Autoencoder 0.46 0.65 0.82 0.49 0.94 0.96 0.10 0.16 0.98 1.00 0.84 0.73 84% 100% 13% 50% 83% 77% 79% 61% 64% 66% 68% 50% 0.78 0.84 \u22c6 0.56 0.50 \u2019 Poetic Sound Sim. s r e phoneme2vec h t O Phon. Sim. Embd. 0.74 0.77 0.16 0.12 0.09 0.05 0.78 0.80 0.50 35% 17% 0% 60% 88% 51% 57% 64% 52% 0.53 0.56 0.29 c BPEmb i t fastText n a m BERT e S INSTRUCTOR 0.23 0.25 0.10 0.60 0.08 0.12 0.34 0.12 0.60 0.64 0.69 0.73 5% 2% 4% 7% 54% 58% 58% 54% 66% 68% 63% 66% 0.36 0.38 0.40 0.45 Table 1: Embedding method performance in our evaluation suite. Higher number is always better. difference within the pairs is [\u00b1voice] in the first phoneme segment of each word. With this intuition in mind, we define a perturba- tion as a pair of phonemes (p, q) differing in one ar- ticulatory feature. We then create a sound analogy corpus of 200 quadruplets \uebf71 : \uebf72 \u2194 \uebf73 : \uebf74 for each language, with the following procedure: for our evaluation suite as the arithmetic average of results from each task. We mainly consider the results of all available languages averaged but later in Section 5.3 discuss results per language as well. To allow for future extensions in terms of languages and tasks, this evaluation suite is versioned, with the version described in this paper being v1.0. 1. Choose a random"}