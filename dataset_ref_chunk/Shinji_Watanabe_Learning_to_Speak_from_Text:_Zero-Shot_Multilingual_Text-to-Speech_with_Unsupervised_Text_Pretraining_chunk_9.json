{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_9.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Which type of text outperformed in all the metrics and languages, Spoken Text or Written Text?", "answer": " Spoken Text", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " What was the average difference in MCD between Spoken Text and Spoken+Written Text?", "answer": " 0.11 lower MCD for Spoken+Written Text", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " For which scenario did Spoken+Written Text outperform Spoken Text in MCD and CER?", "answer": " unseen language scenario", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " Why were several languages deliberately excluded from the paired data used for supervised learning?", "answer": " to study their impact on the training", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " Which languages were excluded in the comparison case for supervised learning?", "answer": " de and nl", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " What was the alternative architecture explored for the bottleneck layer?", "answer": " single-layer Transformer", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " Which languages represented the seen languages in both cases of Original and Excluded scenarios?", "answer": " ru, hu, fi", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " What do the results suggest about including a wider variety of languages in multilingual TTS training?", "answer": " can potentially improve performance", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " What is the significance of the attention map in the proposed model compared to the baseline model?", "answer": " more continuous attention map in the proposed model", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}, {"question": " What do the results of the cross-attention maps indicate about the effectiveness of massively multilingual TTS training?", "answer": " improvement in cross-attention without paired speech-text data", "ref_chunk": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}], "doc_text": "languages from paired data. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average dif- ference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pre- training. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text. How- ever, for the unseen language, Spoken+Written Text outper- formed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. In this section, we have deliberately excluded several lan- guages from the paired data used for the supervised learning described in \u00a7 2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first ex- cluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr\u00a8om et al., 2021]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learn- ing was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Ta- ble 1, while Excluded denotes the case where only fi, ru, hu, and el were used. B Architecture of bottleneck layer As described in \u00a7 2.4, we used the residual layer for our bot- tleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen lan- guages in the evaluation presented in \u00a7 3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformer The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. In- terestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indi- cate that in the context of multilingual TTS training, perfor- mance can potentially be improved by including a wider va- riety of languages rather than restricting to similar languages. Method ru hu fi de fr MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER es Original (de, fr, nl, fi, hu, ru, el) Excluded (fi, hu, ru, el) 7.38 7.00 10.62 11.11 5.01 5.32 6.05 6.92 4.99 4.98 5.28 5.46 5.65 10.39 3.79 34.11 6.48 10.90 7.15 49.65 9.05 10.00 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instabil- ity of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results sug- gest that our unsupervised text pretraining can improve cross- attention in the absence of paired speech-text data. The re- sults are also reflected in the intelligibility difference between the baseline and the proposed methods presented in \u00a7 3.3. Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al., 2022b]. The two languages (de, fr) were included in the paired data for Original, but were excluded from the paired data for Ex- cluded. We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in \u00a7 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first at- tention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for 18.27 24.80"}