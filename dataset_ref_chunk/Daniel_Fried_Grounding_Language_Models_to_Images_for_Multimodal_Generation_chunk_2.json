{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Who proposed the Transformer model for text-to-image generation?", "answer": " DALL-E (Ramesh et al., 2021)", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What improvements were made to the framework proposed by DALL-E for text-to-image generation?", "answer": " Improvements were made through model scaling, pretraining, and improved image quantization models.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What is Flamingo proposed for?", "answer": " Flamingo proposed a visual language model for text generation.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " How does FROMAGe differ from Flamingo in terms of multimodal outputs?", "answer": " FROMAGe can generate coherent multimodal outputs, unlike Flamingo.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What approach was proposed by Houlsby et al. for transferring pretrained LLMs to new language tasks?", "answer": " Houlsby et al. proposed adapters for transferring pretrained LLMs to new language tasks.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What does Prefix and prompt tuning enable in pretrained LLMs?", "answer": " Prefix and prompt tuning enables adaptation of a pretrained LLM to new settings by finetuning a small set of parameters while keeping the rest frozen.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " How does MAGMA improve upon the approach proposed by Frozen?", "answer": " MAGMA improves upon Frozen by training adapter modules for improved performance on downstream tasks.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What does ESPER use reinforcement learning for?", "answer": " ESPER uses reinforcement learning to improve zero-shot transfer and caption style transfer.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " According to Lu et al., how well can language-pretrained transformers transfer to non-language tasks?", "answer": " Lu et al. show that language-pretrained transformers can transfer well to non-language tasks.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}, {"question": " What is the training objective of FROMAGe?", "answer": " FROMAGe is trained with a multi-task objective of image captioning and image-text retrieval.", "ref_chunk": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}], "doc_text": "Trans- former model (Vaswani et al., 2017), with the best models achieving gains from scaling model size (Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022), increasing pre- training data (Hoffmann et al., 2022), improving finetuning objectives (Wei et al., 2021; Tay et al., 2022), and more. LLMs for vision-and-language. The strong performance of LLMs has also inspired work in vision-and-language research. DALL-E (Ramesh et al., 2021) proposed a Trans- former based model for text-to-image generation (Reed et al., 2016) by treating images as sequences of discrete tokens. This framework was improved upon by other meth- ods (Yu et al., 2022a; Ding et al., 2022) through model scaling, pretraining, and improved image quantization mod- els (Esser et al., 2021; Yu et al., 2021). Flamingo (Alayrac et al., 2022) proposed a visual language model for text gen- eration, with an impressive ability to adapt to and achieve state-of-the-art on a variety of vision-and-language tasks. Several other approaches (Wang et al., 2022; Li et al., 2022a) also propose multi-task vision-and-language pretraining ap- proaches to improve model performance. CM3 (Aghajanyan et al., 2022) trained a causally masked model on a large HTML corpus, and showed that the model is capable of gen- erating images and text. We differ from previous work in that our model is capable of generating coherent multimodal outputs: Flamingo is incapable of producing visual outputs, while CM3 generally produces poor visual outputs (further comparison in the appendix). In addition, FROMAGe is efficient and requires significantly less compute: it is trained in 1 GPU day (Flamingo uses 1535 TPUs for 15 days, and CM3 uses 384 GPUs for 24 days), and does not require web-scale interleaved image-text data. Efficient adaptation of pretrained models. Lastly, our work builds upon approaches for parameter and resource efficient adaptation of pretrained models. Prefix and prompt tuning (Lester et al., 2021; Li & Liang, 2021) enable adap- tation of a pretrained LLM to new settings by finetuning a small set of parameters to act as an input prefix, while keep- ing the rest of the model parameters frozen. Houlsby et al. (2019) proposed adapters for transferring pretrained LLMs to new language tasks. Frozen (Tsimpoukelli et al., 2021) proposed training a visual encoder to enable few-shot learn- ing for multimodal tasks. MAGMA (Eichenberg et al., 2022) improved upon Frozen by training adapter modules for im- 2 Grounding Language Models to Images for Multimodal Inputs and Outputs proved performance on downstream tasks. ESPER (Yu et al., 2022b) uses reinforcement learning to improve zero-shot transfer and caption style transfer. Lu et al. (2022) show that language-pretrained transformers can transfer well to non-language tasks. LIMBeR (Merullo et al., 2022) ana- lyzes pretrained vision and language models, and finds that learnt representations are functionally equivalent up to a linear transform. Our work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. 3. Method Our approach integrates a language model and visual model while keeping their parameters frozen. We learn translation parameters (parameterized as linear layers) to cast images into text space, and text embeddings into visual space. Our motivation for keeping the models frozen is to leverage the capabilities of the LLM learnt from large scale pretraining. We find that this enables better generalization to zero-shot and few-shot settings (further analysis in Sec. 5.1). 3.1. Model Architecture Language model. FROMAGe takes an autoregressive large language model p\u03b8, originally trained with the max- likelihood objective on text-only data, and keeps its parame- ters \u03b8 frozen. Given text x (e.g., an image caption), the mod- els we use extract a sequence of input tokens (s1, . . . , sT ) using a byte-level BPE tokenizer (Sennrich et al., 2015; Radford et al., 2019; Brown et al., 2020). The models were trained to maximize the log likelihood of the token sequence, factorized as a sum of conditional log probabilities: log p\u03b8(x) = t (cid:88) log p\u03b8(st|s1, . . . , st\u22121) t=1 Visual model. To extract visual information from an input image y corresponding to a caption x, we use a pretrained visual backbone model which produces visual embeddings v\u03d5(y) \u2208 Rm. The weights \u03d5 are kept frozen as well. k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. Mapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirec- tional attention typically used in previous models (Radford et al., 2021; Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embed- dings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seam- lessly interleaving image retrieval within generated text. Finally, to map the model\u2019s output representations to visual space, we train a linear mapping Wt \u2208 Rp\u00d7q. This maps the hidden representation of [RET] from the last hidden layer of the LLM, h\u03b8(xi) \u2208 Rp, into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping Wi \u2208 Rm\u00d7q to map the visual embeddings v\u03d5(yi) into the same retrieval space. 3.3. Training Setup We train FROMAGe with a multi-task objective of image captioning and image-text retrieval (summarized in Fig. 2): Image captioning. Similar to previous work (Tsim- poukelli et al., 2021; Eichenberg et al., 2022), we formulate image"}