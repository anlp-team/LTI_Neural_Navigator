{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/David_R._Mortensen_Transformed_Protoform_Reconstruction_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the research article by Joe H Ward Jr. from 1963?", "answer": " Hierarchical grouping to optimize an objective function", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What is the purpose of splitting the dataset into train, validation, and test sets?", "answer": " To conduct hyperparameter searches and use early stopping for model training", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " How many GPUs and CPUs were used in the experiments?", "answer": " 4 GPUs and 20 CPUs", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What is the difference in the number of parameters between the RNN and Transformer models for the Romance orthographic dataset?", "answer": " RNN has 304,151 parameters while the Transformer has 812,986 parameters", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " How many GPU hours does it take to run the full Romance orthographic dataset for the RNN model?", "answer": " Less than 7 GPU hours", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What dataset was used to compute the dendrograms for the Chinese phylogenetic trees?", "answer": " The Romance orthographic dataset", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What evaluation metric is used for Rom-orth since it is not in IPA?", "answer": " Character edit distance is used instead of PED", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What is the accuracy increase percentage for the Transformer model on Rom-phon compared to the baseline?", "answer": " 41.38%", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " Which model has the highest FER value on the Rom-orth dataset?", "answer": " CorPaR + PosStrIni (List et al., 2022)", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}, {"question": " What does the GQD value represent for the dendrograms derived from the RNN and Transformer models?", "answer": " 0.4 for both models", "ref_chunk": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}], "doc_text": "Machine Learning Research, 12:2825\u20132830. Simone Pompei, Vittorio Loreto, and Francesca Tria. 2011. On the accuracy of language trees. PloS one, 6(6):e20109. Taraka Rama, Johann-Mattis List, Johannes Wahle, and Gerhard J\u00a8ager. 2018. Are automatic methods for cognate detection good enough for phylogenetic re- construction in historical linguistics? arXiv preprint arXiv:1804.05416. Andreas Sand, Morten K. Holt, Jens Johansen, Gerth St\u00f8lting Brodal, Thomas Mailund, and Chris- tian N. S. Pedersen. 2014. tqDist: a library for com- puting the quartet and triplet distances between bi- nary or general trees. Bioinformatics, 30(14):2079\u2013 2080. Andreas Sand, Morten K Holt, Jens Johansen, Rolf Fagerberg, Gerth St\u00f8lting Brodal, Christian NS Ped- ersen, and Thomas Mailund. 2013. Algorithms for computing the triplet and quartet distances for binary general trees. Biology, 2(4):1189\u20131209. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Joe H Ward Jr. 1963. Hierarchical grouping to opti- mize an objective function. Journal of the American statistical association, 58(301):236\u2013244. A Training We split 70%, 10%, and 20% of our dataset into train, validation, and test sets, respectively. We conduct hyperparameter searches using WandB (Biewald, 2020) and use early stopping, picking the epoch with lowest edit distance on validation data. All experiments are performed on a Ubuntu server with 4 GPUs and 20 CPUs. For both the RNN and the Transformer, Meloni et al. (2021)\u2019s dataset takes less than 7 GPU hours to run, while H\u00b4ou (2004) takes less than 1 GPU hour. For the full Romance orthographic dataset, the RNN model has 304,151 parameters, while the Transformer has 812,986 parameters. For the Romance phonetic dataset, the RNN has around 661,803 parameters, and the Transformer has around 818,640 parame- ters. For the Chinese dataset, the RNN has around 216,819 parameters, while the Transformer has around 2,010,967 parameters. B Hyper-parameters Refer to Table 5 and Table 6 for the best hyperpa- rameters we found during hyperparameter search via WandB. C Supplementary Results In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)\u2019s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also ex- tracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5. Figure 3: Consensus tree of the dendrograms from the 10 runs of the Transformer for the Chinese dataset. Figure 4: Consensus tree of the dendrograms from the 10 runs of the RNN for the Chinese dataset Figure 5: A gold phylogeny of Romance (left) compared with those derived by probing the RNN model (middle) and the Transformer model (right) on Rom-orth. GQD is 0.4 for both models. Dataset Sinitic Model Random daughter Majority constituent CorPaR SVM +Pos- Str RNN PED \u2193 3.7702 3.5031 3.2795 1.6894 NPED \u2193 0.8405 0.7806 0.7278 0.3692 Acc % \u2191 0% 0% 0% 15.52% FER \u2193 0.2893 0.2013 0.3972 0.1669 BCFS \u2191 0.2748 0.3695 0.3332 0.5418 Transformer (present work) 1.0720 \u00b1 0.0536 0.9814 \u00b1 0.0437 0.2432 \u00b1 0.0121 0.2204 \u00b1 0.0093 35.47% \u00b1 1.40% 39.50% \u00b1 3.02% 0.0896 \u00b1 0.0042 0.0857 \u00b1 0.0057 0.6747 \u00b1 0.0166 0.6971 \u00b1 0.0102 Rom-phon Random daughter CorPaR +PosIni SVM +Pos- StrIni RNN Rom-orth Transformer (present work) Random daughter CorPaR +Ini SVM +Pos- Str RNN 6.1534 1.6847 1.5787 0.9670 \u00b1 0.0163 0.9027 \u00b1 0.0194 4.2567 0.9531 0.8988 0.6914 0.1978 0.1861 0.1229 \u00b1 0.0020 0.1146 \u00b1 0.0021 0.4854 0.1160 0.1105 0.06% 22.18% 24.69% 52.09% \u00b1 0.59% 53.16% \u00b1 0.66% 2.97% 47.23% 50.43% 0.6264 0.0728 0.0713 0.0385 \u00b1 0.0011 0.0378 \u00b1 0.0011 \u2212 \u2212 \u2212 0.4016 0.7403 0.7610 0.8293 \u00b1 0.0024 0.8421 \u00b1 0.0029 0.5147 0.8400 0.8501 Transformer (present work) 0.5958 \u00b1 0.0083 0.5568 \u00b1 0.0086 0.0772 \u00b1 0.0013 0.0724 \u00b1 0.0013 69.74 % \u00b1 0.23% 71.15% \u00b1 0.38 % \u2212 \u2212 0.8913 \u00b1 0.0016 0.8994 \u00b1 0.0015 Table 4: Evaluation of models and baselines using various metrics, averaged across 10 runs (same hyperparameters, different seeds), with standard deviations. Because Rom-orth is not in IPA, character edit distance is used instead of PED, and we cannot accurately calculate FER. See Section 6.1 for an explanation of each evaluation metric. learning rate num encoder layers num decoder layers embedding size n head dim feedforward dropout training epochs warmup epochs weight decay batch size Romance (phon & orth) Sinitic 0.00013 3 3 128 8 128 0.202 200 50 0 1 0.0007487 2 5 128 8 647 0.1708861 200 32 0.0000001 32 Table 5: Hyper-parameters used in training the Trans- former learning rate num encoder layers num decoder layers embedding size hidden size dim feedforward dropout training epochs warmup epochs batch size Romance-phon Romance-orth Sinitic 0.00055739 1 1 107 185 147 0.1808 181 15 8 0.000964 1 1 51 130 111 0.323794 193 15 8 0.000864 1 1 78 73 136 0.321639 237 15 4 Table 6: Hyper-parameters used in training the RNN Dataset Model PED \u2193 NPED \u2193 Acc % \u2191 FER \u2193 BCFS \u2191 Rom-phon Random daughter (Chang et al., 7.1880 0.8201 0% 1.1396 0.3406 2022) CorPaR + Ini (List et al., 2022) 2.0885 0.2491 14.29% 0.0874 0.6799 SVM + PosStrIni (List et al., 2022) 1.9005 0.2276 17.05% 0.0883 0.7039 RNN (Meloni et al., 2021) 1.4581 0.1815 36.68 % 0.0592 0.7435 Transformer (present work) 1.2516 0.1573 41.38% 0.0550 0.7790 Rom-orth Random daughter (Chang et al., 2022) 6.3272 0.6542 0.55% \u2212 0.4023 CorPaR + PosStrIni (List et al., 2022) 1.8313 0.2001 18.89% \u2212 0.7227 SVM + PosStr (List et al., 2022) 1.6995 0.1867 21.66% \u2212 0.7454 RNN (Meloni et al., 2021) 1.3189 0.1505 38.89% \u2212 0.7742 Transformer (present work) 1.1622 0.1343 45.53% \u2212 0.7989 Table 7: Evaluation of models and baselines with various metrics on Meloni et al. (2021)\u2019s Romance datasets,"}