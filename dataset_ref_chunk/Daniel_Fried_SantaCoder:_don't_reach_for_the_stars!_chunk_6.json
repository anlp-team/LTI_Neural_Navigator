{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_SantaCoder:_don't_reach_for_the_stars!_chunk_6.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Where can you find the full results tables of the text2code experiments?", "answer": " Appendix A", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What is the observed performance drop for the Multi Query Attention (MQA) model compared to the Multi Head Attention (MHA) model?", "answer": " 1-4% on HumanEval and 1-3% on MBPP", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What is the difference in parameters between the MHA model and the MQA model?", "answer": " MHA has more parameters (1.3B) than MQA (1.1B)", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What is the impact of the FIM model compared to the No-FIM model?", "answer": " Minor drop in performance, 2-4% lower on HumanEval and 1% lower on MBPP", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " Which filters showed noticeable improvements on the text2code benchmarks?", "answer": " Near-deduplication and comment-to-code filters", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " How does the GitHub stars filter affect performance?", "answer": " Performance consistently drops by 3-6% on HumanEval and MBPP, and by 5-11% on the fill-in-the-middle benchmark", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What improvements were observed by the fertility filter on fill-in-the-middle benchmarks?", "answer": " The fertility filter improved performance by 2-4% across the three languages", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What data filters were used for the final model, SantaCoder?", "answer": " Near-deduplication and comments-to-code filters", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " What was the training iteration count for the final model, SantaCoder?", "answer": " 600K iterations (236B tokens)", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}, {"question": " How did doubling the training iterations impact the text2code performance?", "answer": " Led to much stronger performance on MultiPL-E, boosting performance across all benchmarks and programming languages", "ref_chunk": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}], "doc_text": "show the text2code results in Figure 4 and report the \ufb01ll-in-the middle evaluations in Table 6. We show the HumanEval performance throughout all training runs in Figure 3. You can \ufb01nd the full results tables of the text2code experiments are Appendix A. Slight drop in performance for MQA We see a small drop in performance for Multi Query Attention (MQA) compared to Multi Head Attention (MHA). As shown in Table 5, the MHA model improves pass@100 with 1-4% on HumanEval and with 1-3% on MBPP. We speci\ufb01cally observe 9 Preprint Figure 3: HumanEval pass@100 performance throughout training for all models. Note that evalua- tion shown here is based on OpenAI Python prompts and might differ (slightly) from the MultiPL-E prompts used in the rest of this paper. Model Size Left-to-right pass@100 JavaScript Java Python Fill-in-the-middle ex. match Python Java JavaScript InCoder CodeGen-multi CodeGen-mono Codex12 6.7B 0.36 2.7B 0.42 2.7B 2.5B (cid:55) (cid:55) 0.38 0.39 (cid:55) (cid:55) 0.47 0.39 0.57 0.60 0.49 (cid:55) (cid:55) (cid:55) 0.51 (cid:55) (cid:55) (cid:55) 0.31 (cid:55) (cid:55) (cid:55) SantaCoder 1.1B 0.41 0.47 0.49 0.62 0.60 0.44 Table 7: Comparing the performance of the \ufb01nal version of SantaCoder with InCoder (Fried et al., 2022), CodeGen (Nijkamp et al., 2022), and Codex (Chen et al., 2021) on left-to-right (HumanEval pass@100) and \ufb01ll-in-the-middle benchmarks (HumanEval line \ufb01lling, exact match). noticeable improvements for the JavaScript versions of the text2code benchmarks. However, it should be noted that the MHA model has more parameters (1.3B) than the MQA model (1.1B), and a head-to-head comparison might, therefore, not be entirely fair. We think that the inference speed-ups of MQA might outweigh the small drop in performance. FIM for cheap We observe a minor drop in performance of the FIM model compared to the No-FIM model. Speci\ufb01cally, we see that the pass@100 performance of the FIM model is 2-4% lower on HumanEval and 1% lower on MBPP. While Bavarian et al. (2022) presented evidence for the existence of a FIM-for-free property (i.e., arguing that autoregressive models can be trained with FIM without harming left-to-right capabilities), we do \ufb01nd a small but consistent drop of FIM models on left-to-right text2code benchmarks. Modest impact of near-deduplication, comments, and fertility \ufb01lter On text2code benchmarks, we observe small gains for the near-deduplication and comment-to-code \ufb01lters and a neutral effect of the tokenizer \ufb01lter. The near-deduplication \ufb01lter improves HumanEval performance by 1-3% and MBPP by 1-4% across the three programming languages. The comment-to-code \ufb01lter improves HumanEval performance by 0-2% but decreases MBPP performance in certain cases (Java). See Appendix A for the full results table. On \ufb01ll-in-the-middle benchmarks, we see that the tokenizer 12This is the performance of a Codex model reported by Chen et al. (2021). It is not clear if this model is available via the OpenAI API. 10 Preprint Multi\u2212MBPP Pass@10 0.00.20.40.60.8 JavaJavaScriptPython Multi\u2212MBPP Pass@100 BaselineCommentsDedup AltFertilityStarsFinal Multi\u2212MBPP Pass@1 Model LanguageEstimate Multi\u2212HumanEval Pass@1 0.00.20.40.60.8 Multi\u2212HumanEval Pass@10 JavaJavaScriptPython0.00.20.40.60.8 Multi\u2212HumanEval Pass@100 Figure 4: Pass@k rates on Multi-HumanEval and Multi-MBPP by model and language fertility \ufb01lter performs well, improving performance by 2-4% across the three languages. The near- duplication and comments \ufb01lters have a mixed effect, improving \ufb01ll-in-the-middle performance for Python but deteriorating performance for JavaScript. GitHub stars deteriorate performance Surprisingly, we \ufb01nd that the GitHub stars \ufb01lter performs poorly. On HumanEval and MBPP, the pass@100 performance consistently drops by 3-6% across the three languages. On the \ufb01ll-in-the-middle benchmark, the performance drops by 5-11% (Table 6). Note that the stars \ufb01lter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality. 6.2 FINAL MODEL Based on the insights from the architecture and dataset ablations, we train a \ufb01nal model, which we call SantaCoder, with MQA and FIM and the two data \ufb01lters that yielded the best results: more near- deduplication and comments-to-code \ufb01lter. We train this model for 600K iterations (236B tokens) and keep all other hyper-parameters the same. Improved text2code performance Doubling the training iterations leads to much stronger text2code performance on MultiPL-E, signi\ufb01cantly boosting performance across all benchmarks and programming languages (see Figure 4). Looking at the performance throughout training (Figure 11 Preprint 3), it is likely that longer training can further increase performance. Surprisingly, we \ufb01nd that the \ufb01nal training run did not improve the \ufb01ll-in-the-middle evaluations (see Table 6), at least on these single line in\ufb01lling tasks. Comparison to InCoder, CodeGen, and Codex Table 7 compares our SantaCoder model to comparably-sized code generation models from previous work on the MultiPL-E benchmark, using the methodology described in Section 5.4. We \ufb01nd that our model generally outperforms previ- ous open-source multi-language code generation models despite being smaller, outperforming the InCoder 6.7B (Fried et al., 2022) model on both left-to-right generation and single line \ufb01ll-in-the- middle in\ufb01lling across languages, and obtaining comparable or stronger performance to CodeGen- multi 2.7B (Nijkamp et al., 2022). 7 CONCLUSION We described the progress of the BigCode project until December 2022. The community took its \ufb01rst steps towards redacting PII and demonstrated that regular expressions are reasonably effective at detecting emails and IP addresses. Future work should focus on increasing the precision and recall of secret keys, as well as detecting other sensitive information such as names, usernames, and pass- word. Using the PII-redacted version of The Stack, we conducted a series of architectural and data \ufb01ltering ablations. One of our main \ufb01ndings was that \ufb01ltering for Github stars consistently decreased performance across all benchmarks and programming languages. Using the \ufb01ndings of these abla- tion studies, we trained a \ufb01nal 1.1B model\u2014dubbed SantaCoder\u2014for 236B tokens and showed it is able to outperform previous multi-lingual code models (InCoder-6.7B and CodeGen-Multi-2.7B) on both left-to-right generation and in\ufb01lling tasks. We anticipate that larger architectures and more training data will be able to produce stronger multilingual, in\ufb01lling-capable models, and plan to continue to scale"}