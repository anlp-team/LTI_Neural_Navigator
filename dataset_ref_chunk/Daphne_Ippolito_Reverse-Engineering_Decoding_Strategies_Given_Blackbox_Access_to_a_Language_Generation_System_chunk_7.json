{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Reverse-Engineering_Decoding_Strategies_Given_Blackbox_Access_to_a_Language_Generation_System_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " Why did the researchers vary the random seed for each system being evaluated in the experiments?", "answer": " To avoid any systematic biases resulting from always using the same choice of exemplars.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " What was the expectation for non-exemplar-based prompts in the experiments?", "answer": " To rely on the expectation that letters, digits, and common words are present in most model vocabularies.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " Why were different prompts needed to attack ChatGPT compared to experiments on open-source models?", "answer": " Because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " What is the goal of the BLOOM 3B model released by BigScience?", "answer": " To enable public research on large language models.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " Under which license was the Pythia 2.7B model released by EleutherAI?", "answer": " MIT License.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " Where can the Pythia 2.7B model be downloaded from?", "answer": " https://github.com/EleutherAI/pythia.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " What is the goal of the GPT-2 base and XL models released by OpenAI?", "answer": " To foster language model research.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " How were the preliminary experiments conducted in terms of computational resources?", "answer": " They were initially run in Google Colab using a Pro membership with access to one Tesla T4.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " What kind of machine was used for subsequent experiments in terms of computational resources?", "answer": " A Google Cloud machine with 8 Tesla V100s.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}, {"question": " What is the approximate cost per hour for using the Google Cloud machine for experiments?", "answer": " $17 per hour.", "ref_chunk": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}], "doc_text": "completely randomly: negativity diarrhea problems eloqu money aspect vertex fraternity stone breast skies pushes probabilities ink north creditor\u201d. In our experiments estimating top-k, for each system being evaluated, we varied the random seed, resulting in a slightly different prompt. We did this to avoid any systematic biases resulting from always using the same choice of exemplars. For the non-exemplar-based prompts, we did not assume vocabulary access and instead relied on the expectation that letters, digits, and common words are present in most model vocabularies. As mentioned in the main paper, different prompts were needed to attack ChatGPT than for the experiments on open-source models because ChatGPT expects its inputs to be in a conversation format and it does not offer control over the number of words generated (without careful prompt design, it tends to return tens to hundreds of words). Table 5 gives the prompts used to attack ChatGPT. E Scientific Artifacts We use the following language models in our research: BLOOM 3B: This model was released by BigScience under the RAIL License v1.0 with the goal to \u201cto enable public research on large language models\u201d (Scao et al., 2022). It can be downloaded at https://huggingface.co/bigscience/bloom. Pythia 2.7B: This model was released by EleutherAI under the MIT License with the goal of enabling research on \u201cinterpretability analysis and scaling laws\u201d (Biderman et al., 2023). It can be downloaded at https://github.com/EleutherAI/pythia. GPT-2 base and XL: These models were released by OpenAI under the MIT license with the goal of fostering language model research (Radford et al., 2019). They can be downloaded at https://huggingface.co/gpt2. ChatGPT and GPT-3 model family: These models are only available via OpenAI\u2019s API or through OpenAI\u2019s web interface. Our experiments with them fall under OpenAI\u2019s research policy, found at https://openai.com/api/policies/sharing-publication/#research-policy. We chose these models evaluate on because (1) we wanted to evaluate our method on a wide range of independently trained models using different paradigms and training dataset choices. For example, though we conduct all our experiments using English prompts, we can observe the impact of BLOOM being trained on multilingual data, in that for the MONTHS prompt, BLOOM puts significant probability-mass on non-English month names, which could affect our p estimates for BLOOM models. F Computational Resources Preliminary experiments were run in Google Colab using a Pro membership, which gave access to one Tesla T4. Subsequent experiments were running on a Google Cloud machine with 8 Tesla V100s. No more than 100 hours were spent running computation on this machine, which has a cost of $17 per hour. G Additional Results Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values. Table 6: Performance at p estimation across 100 estimations with p values random from 0 to 1. On the left, GPT-2 Base was used to compute known distributions, and on the right GPT-3 was used to compute the known distributions. Model GPT-3 Davinci v1 Acc\u00b1.05 RMSE Acc\u00b1.05 RMSE GPT-2 Base GPT-2 Base GPT-2 XL Davinci v1 BLOOM-3B pythia-2.7b 0.93 0.82 0.23 0.77 0.88 0.03 0.04 0.14 0.04 0.03 0.08 0.07 0.51 0.07 0.07 0.19 0.21 0.06 0.22 0.20"}