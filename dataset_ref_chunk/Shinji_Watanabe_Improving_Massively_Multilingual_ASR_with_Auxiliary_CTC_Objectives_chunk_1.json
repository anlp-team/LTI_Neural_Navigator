{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Massively_Multilingual_ASR_with_Auxiliary_CTC_Objectives_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the work described in the text?", "answer": " The main focus is on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID).", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " How many languages are included in the FLEURS ASR benchmark?", "answer": " FLEURS contains 102 languages.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the abbreviation CTC stands for in the context of the text?", "answer": " CTC stands for Connectionist Temporal Classification.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the function of CTC in the Hybrid CTC/Attention architecture?", "answer": " CTC optimizes a model to predict the alignment between input and output sequences.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " How many hours of training data does each language in FLEURS have?", "answer": " Each language in FLEURS has only 7-10 hours of training data.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the assumed requirement to determine the latent state in CTC?", "answer": " The assumption is that only the observation X is required to determine the latent state of CTC at any given frame.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the key challenge in building a single system for recognizing multiple languages?", "answer": " The key challenge is the vast variability of phonology, grammar, and written scripts across languages.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What do the experimental results demonstrate about the proposed technique in the text?", "answer": " The experimental results demonstrate the effectiveness of the proposed technique over standard CTC/Attention-based hybrid models.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the benefit of the proposed method for encoding-decoding models?", "answer": " The proposed method allows early encoder layers to focus on language identity while subsequent layers focus on Automatic Speech Recognition (ASR).", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}, {"question": " What is the relative reduction in Character Error Rate (CER) achieved by the state-of-the-art systems on FLEURS over prior work?", "answer": " The state-of-the-art systems achieved a relative reduction of 28.4% in CER over prior work on FLEURS.", "ref_chunk": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}], "doc_text": "3 2 0 2 b e F 7 2 ] L C . s c [ 2 v 9 2 8 2 1 . 2 0 3 2 : v i X r a IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Language Technologies Institute, Carnegie Mellon University, USA ABSTRACT Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classi\ufb01cation (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effective- ness of our technique over standard CTC/Attention-based hybrid mod- els. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are available at https://github.com/ espnet/espnet/tree/master/egs2/fleurs/asr1. globe, the typological diversity of which makes language identi\ufb01ca- tion a relevant component of transcription. Languages in FLEURS are also individually low-resourced: each language has only 7-10 hours of training data. This makes FLEURS a unique challenge that can help ASR progress to the long-tail of the world\u2019s \u223c7000 languages. We apply self-conditioned CTC [24\u201332], which uses Connectionist Temporal Classi\ufb01cation (CTC) models in intermediate encoder layers to condition subsequent layers on intermediate pre- dictions, to Hybrid CTC/Attention architectures [23] as a basis for our LID conditioning approach. We then design intermediate LID targets of varying granularity and use these to examine the effect of conditioning our encoder-decoder models on LID predictions starting from early layers of the encoder. Our proposed method, which allows early encoder layers to focus on LID while subsequent encoder and decoder layers focus on ASR, is bene\ufb01cial compared to standard self-conditioning. Together with self-supervised models and Conformer architectures, our state-of-the-art (SOTA) systems obtained 10.1 CER on FLEURS, a relative 28.4% reduction over prior work. Index Terms\u2014 multilingual ASR, low-resource ASR, CTC 2. BACKGROUND 1. INTRODUCTION Recent advancements in multilingual speech processing have shown great promise towards building speech systems for all, expanding language coverage beyond the high-resources [1\u201316]. In particular, practitioners have demonstrated that neural techniques are capable of large-scale cross-lingual representation learning by training multilin- gual automatic speech recognition (ASR) systems on massive private [8, 10, 14, 16] or public speech corpora [2, 17\u201321]; however, these works demonstrate that performance still varies across languages. One of the inherent challenges in building a single system which can recognize many languages is the vast variability of phonology, grammar, and written scripts [22]. Therefore, a key to understanding why multilingual ASR systems exhibit certain errors is to examine whether the underlying model actually knows which language it should be transcribing \u2013 in other words, if there was a correct language identi\ufb01cation (LID). To this end, systems that jointly model LID and ASR via multi-tasking [6, 16, 23] offer one view to the inner-workings of the multilingual decision process. However, we are interested in frameworks which more explicitly model LID as a dependency for ASR under the presumption that knowing the correct language of an utterance makes it easier to be transcribed. Therefore, in this work, we seek to build massively multilingual models which 1) condition transcription predictions on language identity likelihoods and 2) contribute our reproducible models and recipes which use publicly available data, with the broader objective of improving explain-ability and use-ability. To achieve this, our work focuses on the FLEURS ASR dataset [21]. FLEURS contains 102 languages from across the In this section, we discuss the CTC studies that we build upon to cre- ate our fully LID-conditioned model. These studies propose different multi-task training methods [23, 24] and condition the encoder on intermediate predictions [27, 29]. 2.1. Hybrid CTC/Attention We use a Hybrid CTC/Attention architecture [23] as our model foundation. Let X = (xt \u2208 RD|t = 1, ..., T ) be a T -length input sequence based on D-dimensional acoustic feature xt and Y = (ys \u2208 V|s = 1, ..., S) be an S-length output sequence with vocabulary V. CTC [33] optimizes a model to predict the monotonic alignment between X and Y . It models the conditional probability of Pctc(Y |X) as a series of latent sequences at each input frame. This latent sequence is obtained by introducing a blank token \u2205 into Y , t \u2208 V \u222a {\u2205}|t = 1, ..., T ). such that Z ctc = (zctc PCTC(Y |X) \u2248 (cid:88) T (cid:89) PCTC(z CTC t |X,(cid:24)(cid:24)(cid:24)z1:t\u22121) Z CTC \u2208F \u22121(Y ) t=1 Where F \u22121 is the function of all latent sequences Z ctc given Y . CTC operates with the assumption that only the observation X is required to determine the latent state zctc at any given frame. The Hybrid CTC/Attention encoder \ufb01rst converts input X into the hidden vector h in Equation (2). The hidden vector is then used to obtain the frame- wise CTC posterior (Equation 3) and token-wise attention posterior distributions of X (Equation 4). t h = ENC(X) PCTC(Z|X) = CTC(h) PAttn(yl|X, y1:l\u22121) = DEC(h, y1:l\u22121) (1) (2) (3) (4) Combining Equations (1, 3, 4) obtains the logarithmic linear combi- nation of these posterior distributions over all frames and decoded tokens used to optimize the encoder-decoder network: L = \u2212(1 \u2212 \u03bb) log PAttn(Y |X) \u2212 \u03bb log PCTC(Y |X) Where \u03bb is the CTC weighting term. Hybrid CTC/Attention thus jointly optimizes a shared encoder with both CTC and attention losses, while the decoder is trained purely on the attention loss. 2.2. Intermediate CTC InterCTC [24\u201326] was proposed to regularize the training"}