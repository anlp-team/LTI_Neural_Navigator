{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Toward_Universal_Speech_Enhancement_For_Diverse_Input_Conditions_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the shape of the complex spectrum generated by the short-time Fourier transform (STFT) module?", "answer": " The shape of the complex spectrum generated by the STFT module is 2 \u00d7 F \u00d7 T, where 2 denotes the real and imaginary parts, F is the number of frequencies, and T is the number of frames.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What is the purpose of the TAC module in the proposed model?", "answer": " The TAC module in the proposed model is used for channel modeling.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What is the bottleneck dimension N projected by the point-wise convolutional layer in the proposed model?", "answer": " The bottleneck dimension N is projected by the point-wise convolutional layer in the proposed model.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What is the purpose of the parametric rectification linear unit (PReLU) activation in the proposed model?", "answer": " The PReLU activation is applied to the output in the proposed model.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What is the result of processing the bottleneck features by K stacked multi-path blocks in the proposed model?", "answer": " The output is a single-channel representation of the same shape after processing by K stacked multi-path blocks.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What is the name given to the proposed method in the context of speech enhancement and separation?", "answer": " The proposed method is called unconstrained speech enhancement and separation (USES).", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What property of the STFT filterbanks allows for consistent T-F representations across different sampling frequencies?", "answer": " The linear shift of the frequency response of STFT filterbanks for all center frequencies allows for consistent T-F representations across different sampling frequencies.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " How do the spectra generated by using fixed-duration STFT window and hop sizes for different sampling frequencies differ?", "answer": " The spectra have the same number of frames and different numbers of frequency bins, while the resolution is consistent across different sampling frequencies.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " What modifications were made to the original TFPSNet model in the proposed USES model?", "answer": " The original TFPSNet model was modified by adopting the complex spectral mapping method, replacing convolutional layers for mask estimation with a single 2D convolutional layer, and replacing projection layers with 2D convolutional and transposed convolutional layers.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}, {"question": " How does the proposed USES model handle inputs with two variable dimensions, time and frequency?", "answer": " The proposed USES model can build a sampling-frequency-independent (SFI) model by leveraging the property of consistent T-F representations across different sampling frequencies.", "ref_chunk": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}], "doc_text": "Reshape to \ud835\udc41\u00d7\ud835\udc36\u00d7 \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Microphone channel modeling\u24d0\u24d1\u24d2 iSTFT Linear2\ud835\udc3b\u2192\ud835\udc41 LayerNorm STFT Transformer Layer TAC Module \ud835\udc36\u00d7\ud835\udc37\u00d7\ud835\udc39\u00d7\ud835\udc47 Concat Multi-path Block\u00d7\ud835\udc3e (1) Only take the reference microphoneor(2) Merge the microphone dimensioninto the batch dimension Linear\ud835\udc41\u2192\ud835\udc3b FTN 2D Conv\ud835\udc41,\ud835\udc37,(1,1) (c) TAC module.It is applied to both single-channel (\ud835\udc36=1) and multi-channel (\ud835\udc36>1) inputs. (a) Proposed USES model. ConcatenateCF \u00b7TN \ud835\udc36\u00d7\ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47 Average LayerNormCF \u00b7 TN Channel Average Linear\ud835\udc3b\u2192\ud835\udc3b 2D Conv\ud835\udc37,\ud835\udc41,(1,1) Transformer LayerFrequency sequence modelingTemporal sequence modeling 2\u00d7\ud835\udc39\u00d7\ud835\udc47 2D Conv2,\ud835\udc37,(3,3) CF \u00b7TN Reshape to \ud835\udc41\u00d7\ud835\udc39\u00d7\ud835\udc47Split the microphone dimensionfrom the batch dimension \ud835\udc36\u00d72\u00d7\ud835\udc39\u00d7\ud835\udc47 CF \u00b7TN Transform (a) 16kHz(b) 8kHz Fig. 1: Overview of the proposed versatile SE model. The kernel size and feature maps of convolutional layers are annotated in gray. 2.1. Overview The overall architecture of the proposed model is illustrated in Fig. 1. We base our proposed approach on a recently proposed dual-path network called time-frequency domain path scanning network (TF- PSNet) [28]. It is one of the top-performing speech separation mod- els in the time-frequency (T-F) domain, and we believe that it can achieve strong performance in speech enhancement as well. As will be shown in Section 2.2, this model is a natural fit for handling dif- ferent sampling frequencies. Without loss of generality, we assume that the input signal contains C microphone channels, where C can be 1 or more. The encoder consists of a short-time Fourier transform (STFT) module and a subsequent 2D convolutional layer 1 . The former converts each input channel into a complex spectrum with shape 2 \u00d7 F \u00d7 T , where 2 denotes the real and imaginary parts, F is the number of frequencies, and T the number of frames. The lat- ter processes each microphone channel independently and projects each T-F bin into a D-dimensional embedding for multi-path mod- eling. The encoded representations are then processed by channel- wise layer normalization 2 and projected to a bottleneck dimension N by a point-wise convolutional layer 3 . The bottleneck features are processed by K stacked multi-path blocks 4 , which outputs a single-channel representation of the same shape. The parametric rec- tification linear unit (PReLU) activation 5 is applied to the output, which is later projected back to D-dimensional by a point-wise con- volutional layer 6 . Finally, the output is converted to the complex- valued spectrum via 2D transposed convolution (TrConv, 7 ) and then to waveform via inverse STFT (iSTFT). We call the proposed method unconstrained speech enhancement and separation (USES) as it can be used in diverse input conditions.3 Fig. 2: STFT with fixed-duration window and hop sizes (e.g., 32 ms and 16 ms) will generate spectra with the same frequency and temporal resolution for different sampling frequencies. sequence modeling and another for temporal sequence modeling, as shown in Fig. 3 (b). The transformer layers are the same as those in [28, 30]. The main differences include 1) we do not include any T-F path modeling (along the anti-diagonal direction) as we found it not so helpful in the preliminary experiments; 2) we additionally insert a TAC module for channel modeling (Sec 2.3). 2.2. Sampling-Frequency-Independent design We follow the basic idea in [20] for sampling-frequency-independent (SFI) model design. Namely, we rely on the STFT/iSTFT to obtain consistent T-F representations across different sampling frequencies (SFs). Since the frequency response of STFT filterbanks shifts lin- early for all center frequencies [31], it can be easily extended to han- dle different SFs. As shown in Fig. 2, if we use fixed-duration STFT window and hop sizes (e.g., 32 and 16 ms) for different SFs, the resultant spectra will have constant T-F resolution. As a result, the STFT spectra of the same signal sampled at different SFs will have the same number of frames and different numbers of frequency bins, while the resolution is always consistent. We can leverage this prop- erty to build an SFI model easily as long as the model is capable of handling inputs with two variable dimensions, time and frequency. Compared to TFPSNet, we make modifications to the encoder and decoder, following the observations in a recent paper [29]. Specifically, we adopt the complex spectral mapping method instead of complex-valued masking in TFPSNet, as it is shown to produce better performance [29]. Therefore, the original convolutional lay- ers for mask estimation are replaced with a single 2D convolutional layer 6 . The projection layers in the encoder and decoder are also replaced with 2D convolutional 1 and transposed convolutional 7 layers, respectively. The multi-path block 4 is mostly the same as that in TFPSNet, containing a transformer layer for frequency Interestingly, the time-frequency domain dual-path models such as TFPSNet4 and the proposed USES model inherently satisfy this requirement and can be directly used for SFI modeling without any 3While we mainly focus on speech enhancement in this paper, we also show in Section 3.3 that this model works well for speech separation. 4However, this property is not noticed in the original paper [28]. Table 1: Detailed information of the corpora used in our SE experiments. \u201c#Ch\u201d denotes the number of microphone channels in the data. \u201cT60\u201d denotes the reverberation time. \u201cTrain. SNR\u201d represents signal-to-noise ratio in the training data. \u201c(Simu)\u201d and \u201c(Real)\u201d denote the synthetic and recorded data, while \u201cA\u201d and \u201cR\u201d in parentheses represent anechoic and reverberant, respectively. Dataset Train (hr) Dev (hr) Test (hr) VoiceBank+DEMAND [22] DNS1 (v1) [23] DNS1 (v2) [23] CHiME-4 [24] REVERB [25] WHAMR! [26] 8.8 (A) 90 (A) 2700 (Simu) 14.7 (Simu) 15.5 (A) 58.0 (R) 58.0 0.6 (A) 10 (A) 300 (Simu) 2.9 (Simu) 3.2 (A) 14.7 (R) 14.7 0.6 (A) 0.42 (R) 0.42 Same as above (Simu) 2.3 (Real) 2.2 (Simu) 4.8 (Real) 0.7 (A) 9.0 (R) 9.0 modification. This is because these models treat the SE process as decoupled frequency sequence modeling a and temporal sequence modeling b , as illustrated in Fig. 1 (b), and the transformer lay- ers can naturally process variable-length frequency sequences when different SFs are processed. In summary, the proposed model is in- herently capable of SFI modeling, and"}