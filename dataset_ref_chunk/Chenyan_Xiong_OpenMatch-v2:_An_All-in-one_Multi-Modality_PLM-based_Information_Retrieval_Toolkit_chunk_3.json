{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Chenyan_Xiong_OpenMatch-v2:_An_All-in-one_Multi-Modality_PLM-based_Information_Retrieval_Toolkit_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does OpenMatch-v2 do during the dense retrieval index-building phase?", "answer": " OpenMatch-v2 directly operates on the raw TSV data to fetch the title and text and assemble them into the final input to the model.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " How does the distillation trainer in OpenMatch-v2 fine-tune dense retrieval models?", "answer": " The distillation trainer fine-tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " What is the purpose of template-based data processing in OpenMatch-v2?", "answer": " Template-based data processing in OpenMatch-v2 simplifies the process of converting data into a compatible format for training and reduces manual effort.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " How does OpenMatch-v2 improve efficiency in conducting large-scale experiments?", "answer": " OpenMatch-v2 improves efficiency by introducing template-based data processing, which reduces the need for manual data-processing scripts and frees up researchers to focus on developing their core algorithms.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " What approach does OpenMatch-v2 use for efficient data accessing during training and inference?", "answer": " OpenMatch-v2 integrates with HF Datasets to provide efficient data accessing by maintaining an on-disk cache in Arrow format and allowing direct iteration over the raw dataset from disk during training.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " How does OpenMatch-v2 handle searching on large IR datasets with limited resources?", "answer": " OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, allowing for searching on large IR datasets with limited resources.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " What are the two types of losses that the distillation trainer in OpenMatch-v2 can use for training models?", "answer": " The distillation trainer in OpenMatch-v2 can train models using either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " How does OpenMatch-v2 reduce the need for manual data-processing scripts?", "answer": " OpenMatch-v2 reduces the need for manual data-processing scripts by introducing template-based data processing.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " What dataset format is widely used for storing IR data according to OpenMatch-v2?", "answer": " TSV and JSONLINES formats are widely used for storing IR data according to OpenMatch-v2.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}, {"question": " What does the template in OpenMatch-v2 define?", "answer": " The template in OpenMatch-v2 defines how the input should be built with the data in each field by replacing the key enclosed in angle brackets with its corresponding value during processing.", "ref_chunk": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}], "doc_text": "on the NQ dataset during the dense retrieval index-building phase. Instead of asking users to convert the data into a required form, OpenMatch-v2 directly operates on the raw TSV data, fetch- ing the title and text to assemble them into the final input to the model according to the given template. (\ud835\udc5e, \ud835\udc51) pairs with re-ranker scores, and a distillation trainer that fine- tunes dense retrieval models with the numeric, continuous training signals obtained from the re-ranker. The distillation trainer can train models in either pairwise (using MarginMSE loss) or listwise (using KLDivLoss) fashion. Note that this method can be used in combination with query synthesis, i.e. to first generate queries and then label the (\ud835\udc5e, \ud835\udc51) pairs using a re-ranker [35]. 4.3 Infrastructure In this section, we present the infrastructure of OpenMatch-v2, which improves efficiency and eventually makes conducting large- scale experiments in resource-limited environments possible. Template-based Data Processing. IR datasets come in various formats, and previous IR toolkits such as OpenMatch (v1) require users to convert their data into a compatible format for training and inference. To simplify this process and reduce manual effort, we introduce template-based data processing. We observe that TSV and JSONLINES formats are widely used for storing IR data, which can be considered as a collection of key-value objects. For instance, the NQ corpus data from DPR [13, 14] is a TSV file consisting of 20 million lines, with each line containing an identifier, document title, and document text specified by the TSV header (as shown in Figure 1). To prepare the raw data for inference, users only need to specify a template that defines how the input should be built with the data in each field. In the template, the key enclosed in angle brackets \u201c<>\u201d will be replaced with its corresponding value during processing. So the template defined in Figure 1 generates processed inputs that concatenate the document title and text, separated by the BERT sequence separator [SEP]. Other datasets like BEIR [34] in JSONLINES format can be processed in a similar template-based approach. Template-based data processing reduces the need for manual data-processing scripts, freeing up researchers to focus on developing their core algorithms. SIGIR \u201923, July 23\u201327, 2023, Taipei, Taiwan, China. Table 3: RAM and disk usages of different loading approaches of an 18GB dense retrieval pre-training dataset. Mode Map Stream In-memory # Training Processes 4 4 4 RAM On-disk Cache 22GB 7GB 0 7GB 0 79GB Table 4: Search time and resources used in different sharding scenarios. Experiments are conducted on A100 GPUs. # GPUs 8 4 2 1 # Big Shards 1 2 4 8 Retrieval Time 37s 42s 52s 72s Peak RAM Usage 198GB 107GB 66GB 39GB Efficient Data Accessing. OpenMatch-v2 integrates with HF Datasets [15] to provide efficient data accessing during training and inference. In contrast to OpenMatch (v1), which loads all the data into memory, OpenMatch-v2 keeps data memory-mapped by main- taining an on-disk cache in Arrow format2. This feature enables OpenMatch-v2 to randomly access large datasets with minimal memory overhead. However, the cache often requires more than 1x the disk space of the original dataset. As presented in Table 3, an 18GB pre-training dataset needs an 22GB additional disk space for the cache. To reduce disk usage, we leverage the stream feature of HF Datasets to allow users to directly iterate over the raw dataset from disk during training. We\u2019ve wrapped the above-mentioned memory mapping and streaming features in the dataset classes. class CustomTrainDataset ( TrainDatasetBase ): def get_process_fn ( self , epoch : int , hashed_seed : int ): \"\"\" Write code here to return a processing function that takes a raw example as input and outputs the processed result \"\"\" ... # Define this class to expose interfaces for streaming class StreamCustomTrainDataset ( StreamTrainDatasetMixin , CustomTrainDataset ): pass # No additional code is required ! # Define this class to expose interfaces for random access ( via memory - mapping ) class MappingCustomTrainDataset ( MappingTrainDatasetMixin , CustomTrainDataset ): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or random- accessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Shi Yu, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu Figure 2: The process of the two-stage sharded search. OpenMatch-v2 iteratively loads and unloads the first-stage big shards (A-D), and searches on the second-stage small shards (B0-B3) in parallel on multiple GPUs. The figure presents the search on shard B. Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device\u2019s memory. Take NQ as an example, its 20M documents can consume 20M \u00d7 768 dimensions \u00d7 4 bytes per dimension \u2248 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4. 5 CONCLUSION In this paper, we introduce OpenMatch-v2, an all-in-one Python toolkit for experimenting with PLM-based IR. We make a couple of improvements over the previous"}