{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/S._Welleck_Self-Refine:_Iterative_Refinement_with_Self-Feedback_chunk_13.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main task of the SELF-REFINE system mentioned in the text?", "answer": " To generate human-like responses for open-domain dialogue.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What is the purpose of using iterative refinement in the SELF-REFINE system?", "answer": " To improve the quality of the responses over multiple iterations.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What does the INIT module in the SELF-REFINE system do?", "answer": " Generates a response following the conversation based on dialogue context.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " How does the FEEDBACK module in the SELF-REFINE system contribute to response generation?", "answer": " It provides feedback on 10 qualitative aspects to improve response quality.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What are some of the qualitative aspects judged by the feedback in the SELF-REFINE system?", "answer": " Relevant, Informative, Interesting, Consistent, Helpful, Engaging, Specific, Safe, User understanding, Fluent.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What is the purpose of the ITERATE module in the SELF-REFINE system?", "answer": " To refine the output based on feedback to match the quality aspects better.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " How is the baseline model INIT different from the SELF-REFINE approach?", "answer": " INIT does not use feedback for refinement, while SELF-REFINE employs iterative feedback to improve responses.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " How many iterations are performed in the SELF-REFINE system?", "answer": " A maximum of k = 3 iterations.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What examples are used for the FEEDBACK module in the SELF-REFINE system?", "answer": " In-context examples with responses and feedback on qualitative aspects, including low-scoring variations.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}, {"question": " What is the overall goal of SELF-REFINE in dialogue response generation?", "answer": " To generate coherent, engaging, and high-quality responses through iterative refinement.", "ref_chunk": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}], "doc_text": "Comment per line ratio across different SELF-REFINE iterations. (c) Number of function units across different SELF-REFINE iterations. Figure 11: Evaluation on code readability task with SELF-REFINE across multiple metrics Example M Dialogue Response Generation Open-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is 30 Starting Code: v0 print((int((int(eval(input()))+1)/2))) Code v1 print( (int( (int(eval(input())) + 1) / 2 ) ) Code v2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations. challenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses. M.1 Modules We follow the high-level description of the framework from Section 2, and instantiate our framework as follows. Init This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation. Feedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13. Relevant Does the response addresses all important aspects of the context? \u2022 Informative - Does the response provide some information relevant to the context? \u2022 Interesting - Doe the response beyond providing a simple and predictable answer to a question or statement? Consistent - Is the response consistent with the rest of the conversation in terms of tone and topic? Helpful - Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging - Is the response engaging and encourage further conversation? \u2022 Specific - The response contains specific content related to a topic or question, \u2022 Safe - Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? User understanding - Does the response demonstrate an understanding of the user\u2019s input and state of mind? Fluent Is the response fluent and easy to understand? 31 Context: Speaker A - What are your hobbies? Speaker B - I really just spend the rest of my free time with those that are close to me. Speaker A - I m very much interested in playing table tennis. Response: That sounds like a lot of fun! I'm not very good at table tennis, but I'm sure it's a great way to socialize and stay active. Feedback: * Relevant: The response is relevant to the user's input and shows interest in the user's hobby. 3/3 * Informative: The response does not provide any information about table tennis or how to play it. 1/3 * Interesting: The response is not very interesting or intriguing. 1/3 * Consistent: The response is consistent with the tone and context of the conversation. 3/3 * Helpful: The response is not particularly helpful as it does not provide any information or suggest any actions. 1/3 * Engaging: The response is somewhat engaging. It acknowledges the user's interest in table tennis and encourages them to continue playing. 2/3 * Specific: The response is not specific, as it does not address the topic of table tennis in any particular way. 1/3 * Safe: The response is safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information. 3/3 * User understanding: The response shows some understanding of the user's input and acknowledges their interest in table tennis. 2/3 * Fluent: The response is fluent in terms of grammar and flow of words. 3/3 * Total score: 20/30 New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics. Figure 13: SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better. Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13. M.2 Setup and Experiments Model and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. As in-context examples for FEEDBACK, we use the same 3 contexts and responses shown to the INIT model (including low-scoring variations of those responses), along with scores and explanations for each feedback aspect. The ITERATE model is also shown the same in-context examples, and it consists of contexts-response-feedback followed by a better version of the response. For SELF-REFINE,"}