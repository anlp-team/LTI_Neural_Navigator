{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Joint_Prediction_and_Denoising_for_Large-Scale_Multilingual_Self-Supervised_Learning_chunk_3.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What model is used for the multilingual k-means extraction in the text?", "answer": " AR-HuBERT Base", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " Why is continual learning important in adapting models to a multilingual setting?", "answer": " Continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training.", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " How is data imbalance between languages typically addressed in training multilingual models?", "answer": " Data imbalance between languages is typically addressed through temperature-based upsampling.", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " What is the first step in the two-stage training approach mentioned in the text?", "answer": " The first step is to pre-train on a large unbalanced portion of the data.", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " Which two datasets are used in the subset for continual pre-training in the text?", "answer": " FLEURS and BABEL", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " What architecture is used by WavLabLM in the pre-training settings?", "answer": " HuBERT Large architecture", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " How is the Common Voice dataset utilized in the text?", "answer": " The Common Voice dataset is used for pre-training data in the CV-HuBERT approach.", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " What computational resources were used for pre-training the HuBERT models in the text?", "answer": " 4 A100 GPUs", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " Which toolkit was used for conducting all experiments in the text?", "answer": " The ESPnet toolkit", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}, {"question": " What are the three variants of WavLabLM evaluated in the experimental setup?", "answer": " WavLabLM EK, WavLabLM MK, and WavLabLM MS", "ref_chunk": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}], "doc_text": "com- pute, we adopt AR-HuBERT Base, an English HuBERT Base model replicated through academically-resourced SSL [39] 3. We can then use AR-HuBERT Base to extract features from our multilingual pre-training dataset and train a multilingual k-means. Since we are using AR-HuBERT Base, we can also keep following the HuBERT pipeline and recycle the k-means model used for AR-HuBERT Large, another academic repro- duction [39]. As such, we can perform continual learning on AR-HuBERT Large to adapt it to a multilingual setting (Figure 1, left), which we denote as WavLabLM EK. While the multilingual k-means is likely more optimal since the dis- crete representations will be more expressive of multilingual speech, continual learning can significantly reduce compute costs while leveraging knowledge from English pre-training. 3.3. Multi-stage Pre-training A persistent challenge in training multilingual models is the data imbalance between languages. For example, over half of our the data consists of English speech (Table 2), which is not uncommon in these large-scale settings. A common way to address this issue is through temperature-based upsampling, where speech from lower-resource languages has an increased chance of being sampled into a batch [24, 25]. While effective, this is difficult to scale to large collections of multiple corpora: different datasets use different language labelling standards, if labels are provided at all. Furthermore, it also suffers from increased computational overhead and relies on additional tuning of the temperature parameter. To reduce the experimental mass while obtaining balanced performance by languages, we instead adopt a simple two- stage training approach (Figure 1, right). We first pre-train on a large unbalanced portion of the data, before performing continual learning on a smaller balanced subset. We first pre-train only on the full 40k hours described in Section 3.1, which is split unevenly across 136 languages (Table 2), to obtain WavLabLM MK. We then continue pre-training on a subset consisting of FLEURS and BABEL (total 2000 hours), yielding WavLabLM MS. We chose FLEURS due to its equal language spread across 102 languages, and BABEL to retain a diverse selection of conversational speech. 3.4. Pre-training Settings WavLabLM uses the HuBERT Large architecture [12]. We [39] in follow the same hyperparameters as Chen. et al. their reproduction. The model consists of 24 Transformer [38] encoder layers, each with a hidden size of 1024, a feed-forward dimension of 4096, and an output feature size of 768. The model uses the Adam optimizer [40] with 32,000 warmup steps 3Improved performance can be likely achieved by using existing multilin- gual SSL models, such as XLS-R [24], for feature extraction. For HuBERT SSL, feature quality is a key indicator of downstream performance [12, 39] Fig. 1. Diagram of our proposed approaches. WavLabLM EK is initialized with HuBERT Large and uses English-based k-means (left). WavLabLM MK is trained from scratch with multilingual k-means, and used to initialize WavLabLM MS (right). and a peak learning rate of 0.0005. Models were pre-trained on 32 40GB Nvidia A100 GPUs 4, and each GPU handled at most 30 seconds of audio per batch. As discussed in Section 2.2, we use an augmentation probability pnof 0.2 and a utterance mixing probability pu of 0.1. The energy ratio of a DNS noise is sampled from a continuous uniform distribution of -5 to 5, while the energy ratio of a mixed utterance is sampled from -5 to 20. For the multi-stage model described in Section 3.3, we perform pre-training on the FLEURS+BABEL subset for an additional 10k steps. 4. MULTI-RESOLUTION MULTILINGUAL SSL While we obtained promising results with our WavLabLM models, the computational resources required to pre-train them were still beyond the typical amounts available to academic groups. As such, we sought to further slim down our approach. We first propose using a smaller model, the 95M parameter HuBERT Base [12] architecture, which has been found to occasionally outperform larger variants in monolingual SSL [28, 29]. For pre-training data, we use the Common Voice [31] subset of the Openli110 dataset discussed in Section 3.1, which contains 13k hours of data split across 92 languages. We choose Common Voice due to its diverse selection of languages and relatively large size, while still being manageable with fewer resources. We designate this approach as CV-HuBERT. To our knowledge, CV-HuBERT is the first attempt in multilin- gual SSL to match the performance of large-scale multilingual SSL models with smaller ones that use less data. For further potential efficiency gains, we investigate the effectiveness of SSL at different resolutions, which has shown to be beneficial in monolingual SSL [41] and other speech pro- cessing tasks [42\u201344]. As such, we extend the multi-resolution (MR) HuBERT to the multilingual setting. This is first ac- complished by training HuBERT models at lower resolutions (40ms and 80ms) than the original architecture (20ms). Af- ter pre-training, the models are then fused by stacking the representations from the various resolutions. As such, we pre- train four variations of CV-HuBERT at 20ms, 40ms, and 80ms resolutions. All the models are based on HuBERT-base config- uration, except for the convolution feature extractor used for modulating the modeling resolution. For the multi-resolution implementation, we utilize the same parallel architecture dis- cussed in [41], which repeats the features of low resolution to match the temporal dimension of high-resolution features. All models are trained from scratch, and use the English k-means from AR-HuBERT Base discussed in Section 3.2. All models were trained with only 4 A100 GPUs. Detailed configurations for each model are shown in Table 4. 5. EXPERIMENTAL SETUP We conducted all experiments using the ESPnet toolkit [32]. We evaluate three variants of WavLabLM, which was proposed in Section 3. We also conduct separate ablations on HuBERT SSL at four different resolutions, as noted in Section 4. Table 4 presents details for the data and parameters of each model. The three variants of WavLabLM are all pre-trained on the 40k hours of multilingual data described in Section 3.1. WavLabLM EK is initialized from a reproduced HuBERT Large model [39], and uses an English-based k-means for clustering. WavLabLM MK is trained"}