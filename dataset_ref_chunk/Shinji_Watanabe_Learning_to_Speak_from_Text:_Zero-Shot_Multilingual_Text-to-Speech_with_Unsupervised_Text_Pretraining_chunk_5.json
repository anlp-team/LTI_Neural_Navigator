{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Learning_to_Speak_from_Text:_Zero-Shot_Multilingual_Text-to-Speech_with_Unsupervised_Text_Pretraining_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are ablation studies and why are they conducted?,        answer: Ablation studies are experiments where certain components or features of a system are removed to evaluate their impact on performance. They are conducted to understand the importance of different parts of a system.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What was the effect of excluding the bottleneck layer in the study?,        answer: Excluding the bottleneck layer led to a performance drop in all languages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " How did the removal of the bottleneck layer affect the unseen language?,        answer: The largest increase in performance metrics (1.21 in MCD) was observed in the unseen language when the bottleneck layer was removed.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What is the purpose of the bottleneck layer in the study?,        answer: The bottleneck layer projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction to improve generalization for zero-shot TTS.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What was the impact of including language IDs in the proposed method?,        answer: The inclusion of language IDs led to an average improvement of 0.5 in MCD and 4.48% in CER, indicating the effectiveness of using language IDs in the approach.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " In the study, what was the difference in performance when initializing the encoder was excluded?,        answer: Excluding the initialization of the encoder resulted in an improvement of 0.04 in MCD and a 2.27% increase in CER on average, indicating that the method benefits more from the pretraining of the language-aware embedding layer.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What was the effect of updating the language-aware embedding layer during supervised learning?,        answer: Freezing the language-aware embedding layer led to improved performance for most languages and metrics, with an average difference of 0.29 in MCD and 1.04% in CER.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What were the two language families selected for evaluation of zero-shot TTS on unseen languages?,        answer: The two language families selected were Indo-European and Uralic, with de and hu chosen from each family.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " What were the observed improvements in CER and MCD for the language de in the zero-shot TTS evaluation?,        answer: The pretraining improved the CER by around 10% and MCD by around 0.3 for the language de.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}, {"question": " How did the results suggest the performance of zero-shot TTS is language-dependent?,        answer: The results suggested that the performance of zero-shot TTS is language-dependent based on the limited improvement in CER for the language hu compared to de, indicating language-specific impacts.    ", "ref_chunk": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}], "doc_text": "further evaluate our method, we conducted several abla- tion studies. Table 4 lists the results. Bytes multilingual rep- resents the byte-based proposed method in the evaluation of \u00a7 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. In W/o bottleneck layer, we excluded the bottleneck layer and simply added the token and language embedding to ob- tain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the lan- guages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This sug- gests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in im- proving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that ex- cluded language IDs, referred to as W/o language ID. It cor- responds to a simple multilingual BERT pretraining [Wu and Dredze, 2019] that uses only text tokens across different lan- guages. We observed that the use of language IDs led to an Method de fr Seen ru fi Unseen es Avg. MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer W/o language ID W/o initializing encoder Updating language-aware embedding layer 6.06 6.07 5.59 6.05 5.01 5.09 3.75 6.22 7.15 7.09 6.52 6.75 9.09 9.99 9.31 6.93 7.71 7.77 7.12 7.46 28.52 22.58 16.47 11.42 5.33 5.23 4.86 5.16 6.47 6.99 5.03 8.00 10.26 10.45 9.02 9.48 24.01 32.70 21.91 17.21 6.99 6.96 6.42 6.75 13.74 14.06 11.85 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Proposed (IPA multilingual) Baseline (IPA monolingual) ru (AMOS) fi (AMOS) Baseline (IPA multilingual w/ LIDs) Baseline (IPA multilingual w/o LIDs) Natural nl (AMOS) fr (AMOS) 11.522.533.544.5defr Avg (AMOS) Baseline (Bytes multilingual w/o LIDs) hu (AMOS) el (AMOS) Proposed (Bytes multilingual) 11.522.533.544.5de (MOS)fr (MOS) de (AMOS) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Method de hu MCD CER MCD CER Natural 2.75 2.12 Oracle IPA monolingual IPA multilingual 7.38 6.16 4.07 9.76 7.59 5.28 24.62 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Proposed (Bytes multilingual) Proposed (IPA multilingual)es (MOS)es (AMOS) 11.522.533.544.5 Oracle (IPA monolingual) 0.556 Baseline (IPA multilingual) Natural 0.444es (AB test)p-value: 0.011 Oracle (IPA multilingual) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. average improvement of 0.5 MCD and 4.48% CER, indicat- ing the effectiveness of our approach in using language IDs. In W/o initializing encoder, we did not initialize the en- coder \u03b8E before the supervised leaning described in \u00a7 2.2. Instead, we only initialized the parameters \u03b8T, \u03b8L, and \u03b8B with the parameters pretrained in \u00a7 2.1. Through this eval- uation, we investigated whether the performance gain with our method resulted from the initialization of the language- aware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, sug- gesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. In Updating language-aware embedding layer, we updated the language-aware embedding layer during supervised learn- ing, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average dif- ference of 0.29 in MCD and 1.04% in CER. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr\u00a8om et al., 2021]. In this evaluation, we selected de and hu from each of the families. During supervised learning in \u00a7 2.2, we ex- cluded the paired data for each of de and hu and instead in- cluded the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown bet- ter results in \u00a7 3.3. We observed that the pretraining im- proved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on cross- lingual transfer for NLP tasks [Wu and Dredze, 2019]. Fig. 3 visualize the token embedding Z and encoder in- puts Hin averaged on each utterance. We used a t-distributed stochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008]. We observed overlaps in the token embed- ding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization sug- gest that the cross-lingual transfer works better when simi- lar languages sharing the token embedding space are present during supervised learning. However, for languages with dis- tinct token and language embeddings, the cross-lingual trans- ferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in \u00a7 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr."}