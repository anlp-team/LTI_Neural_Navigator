{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Improving_Massively_Multilingual_ASR_with_Auxiliary_CTC_Objectives_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the objective of using the CTC loss of intermediate layers in deep encoder networks?", "answer": " The objective is to condition the encoder layers on intermediate predictions and improve model performance.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " How is the hidden vector hint of an intermediate encoder layer used?", "answer": " The hidden vector hint is summed with a linear projection of the intermediate posterior distribution and input into the next encoder layer.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " What is the purpose of Self-conditioned CTC (SC-CTC) in the context of intermediate outputs?", "answer": " SC-CTC passes intermediate outputs to the next layer to condition the encoder.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " How is the CTC posterior distribution of the entire encoder network conditioned on the intermediate predictions?", "answer": " Equation (7) is recursively applied for each intermediate layer, allowing conditioning on the intermediate predictions.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " What is the main focus of the proposed method described in the text?", "answer": " The proposed method focuses on using an auxiliary CTC objective to improve language identification and transcription predictions.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " Why is conditioning encoder layers on noisy transcription predictions considered a drawback in the auto-regressive setting?", "answer": " Conditioning later encoder layers on noisy transcription predictions of earlier layers can hinder model performance.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " How is the latent variable I introduced to modify Equation (1) in the proposed method?", "answer": " The latent variable I represents the intermediate language identification (LID) predictions.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " What is the purpose of defining utterance-level LIDs and token-level LIDs in the proposed method?", "answer": " Utterance-level LIDs and token-level LIDs help in aligning languages with spoken words and predicting language at different levels of granularity.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " What is the hierarchical setup of intermediate layers used in the hierarchical conditioning described in the text?", "answer": " A hierarchical setup of K intermediate layers is constructed where each layer makes increasingly complex predictions.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}, {"question": " How is the complete loss function used to train the encoder-decoder network derived?", "answer": " The complete loss function is a sum of the encoder-decoder attention loss, CTC loss, and the LID-conditioned intermediate loss with appropriate weights.", "ref_chunk": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}], "doc_text": "of deep encoder networks by using the CTC loss of intermediate layers as part of a multi-task objective. The intermediate posterior distribution can be obtained in a manner similar to Equation (3), with the hidden vector hint of an intermediate encoder layer. CTC(Z int|X) = CTCint(hint) P int The log-likelihood of Equation (6) can then be used as the objective function of the intermediate layer. Self-conditioned CTC (SC-CTC) [27] also uses this intermediate output to condition the encoder by passing intermediate outputs to the next layer. The normalized hidden representation of the intermediate layer hint is summed with a linear projection of the intermediate posterior distribution Z int to the hidden dimension, and input into the next encoder layer (Equation 7). h = ENC \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z int) = CTC(h) Equation (7) is recursively applied for each intermediate layer, until the output of the \ufb01nal layer is passed into Equation (8). This allows the CTC posterior distribution of the entire encoder network to be conditioned on the intermediate predictions Z int. 3. PROPOSED METHOD In this section, we propose an auxiliary CTC objective such that early encoder layers can focus on language identi\ufb01cation, and the transcription predictions of later layers can be conditioned on the predicted language. We also de\ufb01ne a hierarchy of CTC objectives to take advantage of both LID and self-conditioning, and frame it within a Hybrid CTC/Attention setup. 3.1. Explicit multilingual conditioning While SC-CTC\u2019s conditioning on early predictions bene\ufb01ts non- autoregressive models [27, 30], it could be a drawback in the auto- regressive setting by conditioning later encoder layers on the noisy transcription predictions of earlier layers. We want to condition en- coder layers on the LID without these noisy early predictions. To accomplish this, we propose the following method: train intermediate layers to only classify the spoken language and propagate their pre- dictions to future layers via self-conditioning. We \ufb01rst introduce the latent variable I to Equation (1), where I represents the intermediate LID predictions, such that Equation (1) is modi\ufb01ed as follows: PCTC(Y |X) = (cid:88) PCTC(Y |I, X)PCTC(I|X) I\u2208I The formulation of Equation (9) can be realized by modifying the intermediate CTC network (Equations 7 and 8) to predict the LID instead of transcriptions as follows: h = ENC \ufb01n(NRM(hlid) + LIN(Z lid))) (5) (6) (7) (8) (9) (10) Table 1. Comparison of different labels in the multi-task framework (Sec. 3.1). In LIDtok, all tokens are replaced with LIDs, while LIDutt only retains a single LID label. Task ASR LIDtok LIDutt Label [EN US] ALL [EN US] [EN US] [EN US] [EN US] [EN US] YOU NEED Fig. 1. Proposed hierarchical architecture. The LID predictions of the intermediate layer are used to train the next layer. PCTC(Z|X, Z lid) = CTC(h) This allows the encoder to condition its predictions on the LID. We then de\ufb01ne an auxiliary CTC task with Equations (1) and (9), where the model\u2019s intermediate layers attempt to predict the language, I. Llid = \u2212 log PCTC(I|X) We then create two different sets of labels that can represent I: utterance-level LIDs (LIDutt) and token-level LIDs (LIDtok). An utterance-level LID is a single label from the set of all possible lan- guages iutt \u2208 B. In other words, only a single LID token is used as the ground truth. Alternatively, we can de\ufb01ne a S-length token-level LID sequence corresponding to each S-length label sequence as fol- lows: I tok = {itok s \u2208 B|s = 1, ..., S}. LIDtok thus explicitly aligns the language with each spoken word. This approach, inspired by code-switching [34], forces the model to predict both the frame-level alignment and segmentation between tokens. The task effectively becomes one of identifying the language of each token rather than each utterance. We hypothesize this will aid the model in mapping the audio to the large multilingual text space, even without any code- switched utterances. Example labels can be found in Table (1). 3.2. Hierarchical conditioning Explicitly training all intermediate layers on LID allows the model to condition on language information, but perhaps early layers may be suf\ufb01cient to predict LID, allowing later layers to predict inter- mediate transcripts instead. This progression can be realized using (11) (12) hierarchical conditioning [26, 29, 35], where layers perform incre- mentally more complex predictions. We construct a hierarchical setup of K intermediate layers, such that the k = 1 intermediate layer is trained using Equation (12) to predict the spoken language (Figure 1). The auxiliary LID task is given to an earlier intermediate layer, such that following encoder layers can be conditioned on its predic- tion. Later intermediate layers are trained with SC-CTC to keep the regularization bene\ufb01ts. hint = ENC h = ENC int(NRM(hlid) + LIN(Z lid))) \ufb01n(NRM(hint) + LIN(Z int))) PCTC(Z|X, Z lid) = CTC(h) The encoder output h is therefore both self-conditioned and LID- conditioned. Equation 12 can be summed with the loss of all SC-CTC layers, which is then averaged to produce the objective function used to train the intermediate layers: Lhier = 1 K (Llid + K (cid:88) Linter k ) k=2 Where Linter is the negative log-likelihood of Equation (6), the poste- rior CTC distribution of an intermediate layer. The overall CTC loss can then be obtained in Equation (17) with a weighted sum of the hierarchical loss (Equation 16) with the CTC loss of the full encoder network, where w is the weight of the intermediate losses. LCTC = (1 \u2212 w)LCTC enc + wLhier Substituting Equation (17) into Equation (5) yields the complete loss function used to train our encoder-decoder network with both LID conditioning and SC-CTC: L = (1 \u2212 \u03bb)Latt + \u03bb((1 \u2212 w)LCTC enc + wLhier) Speci\ufb01cally, our model jointly optimizes the encoder with the LID- conditioned intermediate loss, the CTC loss, and the encoder-decoder attention loss. The decoder is conditioned on the prepended LID token and optimized with the attention loss (Figure 1). 4. EXPERIMENTS 4.1. Datasets As discussed in Sec. 1, the experiments were conducted on FLEURS [21],"}