{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Multi-Channel_Target_Speaker_Extraction_with_Refinement:_The_WavLab_Submission_to_the_Second_Clarity_Enhancement_Challenge_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What optimizer is used during training?,answer: The Adam optimizer", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What is the learning rate used during training?,answer: 0.001", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " How are the MISO DNNs trained on segments of the input signal?,answer: The MISO DNNs are trained on random 4 s segments, ensuring that the target and interferer-only beginning part appear both for at least 1 s.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What happens if the validation loss is not improved over 5 epochs?,answer: The learning rate is halved.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What type of loss function is used and on what signals is it defined?,answer: The scale-invariant loss function is used, defined on the estimated time-domain signal and its magnitude.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " How is the TF-GridNet model modified for frame-online processing?,answer: The modifications include using layer normalization after the first Conv2D layer, a causal full-band self-attention module, and a modified sub-band temporal module with unidirectional LSTM.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " How is DNN2 fine-tuned after training to convergence?,answer: DNN2 is fine-tuned by adding an additional loss term that computes multi-resolution loss between the NAL-R fitted target anechoic speech and DNN estimates.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What parameters does the resulting TF-GridNet model have?,answer: The resulting TF-GridNet model has around 8 million parameters.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What are the evaluation metrics reported on the CEC2 development set?,answer: The evaluation metrics reported are SI-SDR, STOI, PESQ, and HASPI.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}, {"question": " What compensation strategies are used to address listener adaptation and compression in the proposed system?,answer: NAL-R fitting and dynamic range compression in the STFT domain are used.", "ref_chunk": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}], "doc_text": "2.336 0.842 0.942 0.943 + DNN2 + NAL-R \ufb01ne-tune + DNN2 + NAL-R \ufb01ne-tune + External Data CH1 Left from the multi-channel mixture signal using complex spec- tral mapping. We use the Adam optimizer with 0.001 learning rate and batch size 1. We clip gradients with L2 norm more than 1. The MISO DNNs are trained on random 4 s segments of the input signal, which are constrained in such a way that the target and the interferer- only beginning part should appear both at least for 1 s. We use the whole development set for validation. The learning rate is halved if the validation loss is not improved over 5 epochs. We follow the scale-invariant loss function suggested in [2], which is de\ufb01ned on the estimated time-domain signal and its magnitude. Differently from [2], we use a multi-resolution STFT \ufb01lterbanks with 512-, 1024-, 2048-, 256- and 128-sample windows to compute multiple magnitude losses. We observed that the magnitude loss leads to clearly better HASPI. Since this loss is scale-invariant, to prevent clipping in inference we re-scale the MISO DNNs\u2019 estimates in a frame-online fashion using the output of the MCWF. Fig. 2: Proposed frame-online speaker-conditioned MISO-TF-GridNet. depicted in Fig. 2. Since it uses complex spectral mapping, we feed all channels by simply stacking the real and imaginary (RI) compo- nents of all input channels [2] to TF-GridNet, converting it to a MISO model named as MISO-TF-GridNet. We use feature-wise linear mod- ulation (FiLM) [8] over the speaker embedding extracted by DNNspk at the beginning of each TF-GridNet block to condition the separa- tion. Note that TF-GridNet proposed in [1] is non-causal. Here we make the following modi\ufb01cations to achieve frame-online process- ing. Firstly, we use layer normalization (LN) after the \ufb01rst Conv2D layer. Secondly, we use a causal full-band self-attention module by masking the attention matrix. Thirdly, we use a modi\ufb01ed sub-band temporal module, where we employ an unidirectional LSTM (instead of bidirectional as in [1]) and, in the unfolding operation, we stack only the previous frames\u2019 features with the current. This way, the sub-band temporal module can output one frame for each frame in the input if the stride is set to 1. To further improve HASPI, after training DNN2 to convergence, we \ufb01ne-tune DNN2 by adding an additional loss term where the same multi-resolution loss is computed between NAL-R \ufb01tted target ane- choic speech and NAL-R \ufb01tted DNN estimates. Since listener au- diograms are not available in the training set, we randomly sample audiograms from the development set listeners to perform this \ufb01tting- aware training. We use the same hyper-parameters suggested in [1], except that E is set to 2 here. The resulting TF-GridNet has around 8 M param- eters and the full iNeube-X model (with DNN1 and DNN2) roughly doubles that amount. 2. RESULTS ON DEVELOPMENT SET We report our results obtained on the CEC2 development set in Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that SI-SDR, STOI and PESQ are computed before listener adaptation and compression while HASPI is computed after. Together with the proposed iNeuBe-X, we also report the performance for two non- causal methods: iNeube (only DNN1, based on TCNDenseNet) [2] and a DNN-based masked beamforming approach from the ESPNet- SE++ toolkit [12], which employs a two-layer bidirectional LSTM (BLSTM) and weighted prediction error (WPE) followed by min- imum variance distortionless response (MVDR) beamforming. As another comparison, we report the scores of the unprocessed mix- tures and the oracle target speech. The latter can be considered as a reasonable upper bound for our proposed system, since we employ a similar compensation strategy. Note that the CEC2 baseline system does not perform enhancement, and only performs listener adapta- tion and dynamic range compression. For our proposed systems, we report performance for various con\ufb01gurations and training strategies in the third panel: iNeube-X with only DNN1, iNeube-X with DNN2 trained using the additional loss after listener adaptation (+ DNN2 + NAL-R \ufb01ne-tune), and with external data (+ DNN2 + NAL-R \ufb01ne-tune + External Data). The two latter systems are the ones we submitted to CEC2. We found that an additional iteration of beam- forming followed by post-\ufb01ltering led to slightly better HASPI, at a cost of increased computational overhead. Thus we only submitted signals obtained after the \ufb01rst DNN2 re\ufb01nement step. 1.4. STFT-domain NAL-R and Dynamic Range Compression We found that enhancement alone is not suf\ufb01cient for obtaining an high HASPI. Without any compensation to listener\u2019s hearing loss, the HASPI for the oracle target speech is less than 0.7 on the develop- ment set. To perform \ufb01tting in a frame-online fashion, we perform the NAL-R \ufb01tting [9] and dynamic range compression in the STFT domain. We followed the dynamic range compression algorithm im- plemented in [10], and set the knee to -10 dB, threshold -40 dB, ratio 1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As we use in iNeuBe-X 16 ms STFT windows, the length of the NAL- R equalization \ufb01lter was set to 80 taps. When fed the oracle target anechoic speech, our STFT-domain NAL-R \ufb01tting plus compression algorithms produces an HASPI score of 0.987, which is slightly lower than the 0.998 score obtained by the CEC2 baseline \ufb01tting algorithm. This is likely due to the shorter NAL-R \ufb01lter used in our algorithms, but our oracle score is still acceptable. 1.5. System Training Con\ufb01gurations and External Data Usage We trained models on two datasets, the original CEC2 data, which we scaled up to 16k scenes using the provided scene generation script, and this 16k scenes plus additional 6k scenes generated using the same con\ufb01guration except for external clean speech taken from Lib- riVox in order to increase speaker variations. We emphasize that our system enjoys an algorithmic latency of 4 ms, and we con\ufb01rm that the proposed system satis\ufb01es the 5 ms con- straint on algorithmic latency using this public code1. Regarding training, we \ufb01rst train DNN1 with DNNspk to conver- gence, and then freeze these"}