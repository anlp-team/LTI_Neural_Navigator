{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Improving_Language_Models_with_Advantage-based_Offline_Policy_Gradients_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the abbreviation A-LOL stand for in the text?", "answer": " Advantage-Leftover Lunch", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " Where can the code for A-LOL be found?", "answer": " https://github.com/abaheti95/LoL-RL", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " How is language generation tasks framed as an RL game in the text?", "answer": " With the single-action assumption", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " According to the text, what does the single-action assumption allow incorporating during offline finetuning?", "answer": " Any pretrained attribute-specific classifiers or human-designed scoring functions as a reward", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " What is the main learning objective of A-LOL derived using in the text?", "answer": " Offline policy gradient", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " What is the relationship between A-LOL and log-likelihood loss discussed in the text?", "answer": " A-LOL is discussed in relation to log-likelihood loss", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " What is the shorthand for importance weight in the text?", "answer": " r(\u03b8, ref)", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " How is the advantage defined in the Advantage LOL RL objective in the text?", "answer": " The policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " How is the value estimate module of \u03c0ref computed in the text?", "answer": " By initializing a small network of multi-head attention and a single-layer MLP on top of frozen parameters of \u03c0ref", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}, {"question": " How is the value estimate of \u03c0ref trained in the text?", "answer": " It is trained on the rewards achieved by \u03c0ref on the validation set with mean squared error loss", "ref_chunk": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}], "doc_text": "that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL. 2 ADVANTAGE-LEFTOVER LUNCH RL Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and a another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2 1We do not compare with other online RL (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023; Peng et al., 2023; Zhu et al., 2023) that are very similar to PPO and preference-based method (Zhao et al., 2023) similar to DPO. We also avoid comparing with offline RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023) and model architecture changes (Kim et al., 2022; Snell et al., 2023). 2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A. 2 Preprint under review. Figure 1: Illustration of Advantage-Leftover Lunch RL in practice. We first supervised finetune the reference policy (\u03c0ref) on the training data as a precursor to A-LOL training. Then, an external reward model is employed to train the value estimate layer (V\u03c0ref ) on frozen \u03c0ref. Subsequently, using the reference policy values on Dtr, we can find instances with positive advantage. A-LOL then multiplies the positive advantage and importance weight with negative log likelihood to train target LM (\u03c0\u03b8). Evaluation on Dtest shows LM trained with A-LOL achieves higher average reward and better distribution compared to the reference policy. 2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES We consider language generation as a sequence-to-sequence task containing training Dtr and valida- tion Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that consider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023; Ramamurthy et al., 2023), we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward to the sum of all the functions. 2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL To derive our main learning equation, we start with the Offline Policy Gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref (x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences X . We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective: d\u03c0ref(x) (cid:88) (cid:88) J(\u03b8) = max R(x, y, \u22c6)\u03c0\u03b8(y|x) \u03b8 x\u2208X y\u2208Y where Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ (cid:88) (cid:88) \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] y\u2208Y y\u2208Y We then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows, \u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref[R(x, y, \u22c6) (cid:125) (cid:124) (cid:123)(cid:122) reward \u03c0\u03b8(y|x) \u03c0ref(y|x) (cid:123)(cid:122) (cid:125) (cid:124) importance weight ] \u2207\u03b8 ln \u03c0\u03b8(y|x) (cid:125) (cid:123)(cid:122) (cid:124) NLL Here, the importance weight3 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are a subset of the d\u03c0ref, i.e. all possible texts in the language generation task X . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we update the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization): \u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] 3http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/ 3 (1) (2) (3) (4) Preprint under review. where r(\u03b8, ref) = \u03c0\u03b8(y|x) \u03c0ref(y|x) is the shorthand for importance weight. For boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is difficult, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective. \u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln \u03c0\u03b8(y|x)] To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate"}