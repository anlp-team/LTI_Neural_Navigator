{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_3D_Semantic_Segmentation_in_the_Wild:_Learning_Generalized_Models_for_Adverse-Condition_Point_Clouds_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is PointDR?", "answer": " PointDR is a point cloud randomization technique that consists of two complementary designs including geometry style randomization and embedding aggregation.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " What is the goal of geometry style randomization in PointDR?", "answer": " The goal of geometry style randomization is to enrich the geometry styles and expand the distribution of training point cloud data.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " How are weak and strong spatial augmentations applied in PointDR?", "answer": " Weak and strong spatial augmentations are applied by obtaining two copies of the input point cloud scan x, including a weak-view xw = AW(x) and a strong-view xs = AS(x).", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " What is the purpose of embedding aggregation in PointDR?", "answer": " The purpose of embedding aggregation is to aggregate encoded embeddings of randomized point clouds for learning domain-invariant representations.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " What is the role of contrastive learning in PointDR?", "answer": " Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes, ultimately leading to a robust and generalizable segmentation model.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " How is the contrastive loss defined in PointDR?", "answer": " The contrastive loss is defined as Lct = 1/N \u03a3 i=1 (- log(exp(fs_i * B+ / \u03c4) / \u03a3 j=1(exp(fs_i * Bj / \u03c4))), where \u03c4 is a temperature hyper-parameter.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " What is stored in the memory bank B in PointDR?", "answer": " In PointDR, the memory bank B stores feature prototypes of each semantic class for more robust and stable contrastive learning.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " How does the momentum-updated memory bank contribute to contrastive learning in PointDR?", "answer": " The momentum-updated memory bank provides feature prototypes of each semantic class, which helps in pulling point feature embeddings of the same classes closer in contrastive learning.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " What technique is adopted in PointDR to encourage the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representations?", "answer": " Embedding aggregation is adopted in PointDR to encourage the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representations.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}, {"question": " How does PointDR aim to create different point cloud views with various spatial perturbations?", "answer": " PointDR achieves this by employing geometry style randomization to create different point cloud views with various spatial perturbations.", "ref_chunk": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}], "doc_text": "that consists of a feature extractor E and a clas- sifier G. Note under the setup of domain generalization, target data will not be accessed in training as they could be hard and even impossible to acquire at the training stage. 4.2. Point Cloud Domain Randomization Inspired by domain randomization studies in 2D com- puter vision research [44, 45], we explore how to employ domain randomization for learning domain generalizable models for point Specifically, we design PointDR, a point \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 truck car road person trunk motorcycle building motorcyclist other-vehicle vegetation terrain fence traffic-sign parking (a)Snow(b)Rain(c)Dense fog(d)Light fog sidewalk pole bicycle other-ground unlabeled invalid bicyclist Figure 3. Examples of LiDAR point cloud scans captured under different adverse weather including snow, rain, dense fog, and light fog (the first row) and corresponding dense annotations in SemanticSTF (the second row). \ud835\udc53\ud835\udc53\ud835\udc60\ud835\udc60 Gradient stop Input \ud835\udc65\ud835\udc65 Pointembedding Feature extractor \ud835\udc38\ud835\udc38 \ud835\udc9c\ud835\udc9c\ud835\udc46\ud835\udc46 Memory bank \u212c Projector \ud835\udcab\ud835\udcab Classifier \ud835\udc3a\ud835\udc3a \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 \ud835\udc3f\ud835\udc3f\ud835\udc50\ud835\udc50\ud835\udc50\ud835\udc50 Momentum update Geometry style randomizationEmbedding aggregation \ud835\udc9c\ud835\udc9c\ud835\udc4a\ud835\udc4a Figure 4. The framework of our point cloud randomization method (PointDR): Geometry style randomization creates different point cloud views with various spatial perturbations while embedding aggregation encourages the feature extractor to aggregate random- ized point embeddings to learn perturbation-invariant representa- tions, ultimately leading to a generalizable segmentation model. invariant representations. We adopt contrastive learn- ing [17] as illustrated in Fig. 4. Given the randomized point clouds xw and xs, we first feed them into the feature extrac- tor E and a projector P (a two-layer MLP) which outputs normalized point feature embeddings f w and f s, respec- w C \u2208 RD\u00d7C (D: feature dimen- tively (f = P(E(x))). f sion; C: number of semantic classes) is then derived by class-wise averaging the feature embeddings f w in a batch, which is stored in a memory bank B \u2208 RD\u00d7C that has no backpropagation and is momentum updated by iterations w (i.e., B \u2190 m \u00d7 B + (1 \u2212 m) \u00d7 f C with a momentum coeffi- cient m). Finally, we employ each point feature embedding f s i of the strong-view f s as query and feature embeddings in B as keys for contrastive learning, where the key sharing the same semantic class as the query is positive key B+ and the rest are negative keys. The contrastive loss is defined as cloud randomization technique that consists of two com- plementary designs including geometry style randomization and embedding aggregation as illustrated in Fig. 4. Geometry style randomization aims to enrich the geome- try styles and expand the distribution of training point cloud data. Given a point-cloud scan x as input, we apply weak and strong spatial augmentation to obtain two copies of x including a weak-view xw = AW (x) and a strong-view xs = AS(x). For the augmentation schemes of AW , we follow existing supervised learning methods [43] and adopt the simple random rotation and random scaling. While for the augmentation schemes of AS, we further adopt random dropout, random flipping, random noise perturbation, and random jittering on top of AW to obtain a more diverse and complex copy of the input point cloud scan x. Embedding aggregation aims to aggregate encoded em- beddings of randomized point clouds for learning domain- Lct = 1 N N (cid:88) i=1 \u2212 log exp (f s j=1 exp (f s i B+/\u03c4 ) (cid:80)C i Bj/\u03c4 ) where \u03c4 is a temperature hyper-parameter [49]. Note there is no back-propagation for the \u201cignore\u201d class in optimizing the contrastive loss. Contrastive learning pulls point feature embeddings of the same classes closer while pushing away point feature embeddings of different classes. Therefore, optimizing the proposed contrastive loss will aggregate randomized point cloud features and learn perturbation-invariant representa- tions, ultimately leading to a robust and generalizable seg- mentation model. The momentum-updated memory bank provides feature prototypes of each semantic class for more robust and stable contrastive learning. Combining the supervised cross-entropy loss Lce for weakly-augmented point clouds in Eq. 1, the overall train- 5 (1) Methods r a c e l c . i b e l c . t m k c u r t . v - h t o . s r e p t s l c . i b t s l c . t m d a o r . i k r a p . w e d i s . g - h t o . d l i u b e c n e f . t e g e v k n u r t . a r r e t e l o p . f a r t g o f - D g o f - L n i a R w o n S mIoU Oracle 89.4 42.1 0.0 59.9 61.2 69.6 39.0 0.0 82.2 21.5 58.2 45.6 86.1 63.6 80.2 52.0 77.6 50.1 61.7 51.9 54.6 57.9 53.7 54.7 SemanticKITTI\u2192SemanticSTF 55.9 0.0 0.2 1.9 10.9 10.3 6.0 0.0 61.2 10.9 32.0 0.0 67.9 41.6 49.8 27.9 40.8 29.6 17.5 29.5 26.0 28.4 21.4 24.4 Baseline 62.1 0.0 15.5 3.0 11.5 5.4 2.0 0.0 58.4 12.8 26.7 1.1 72.1 43.6 52.9 34.2 43.5 28.4 15.5 29.3 25.6 29.4 24.8 25.7 Dropout [39] 74.4 0.0 0.0 23.3 0.6 19.7 0.0 0.0 60.3 10.8 33.9 0.7 72.0 45.2 58.7 17.5 42.4 22.1 9.7 26.3 27.8 30.0 24.5 25.9 Perturbation 57.8 1.8 3.8 16.7 3.7 26.5 0.0 2.0 65.7 2.9 32.5 0.3 71.0 48.7 53.8 20.5 45.4 25.9 15.8 29.7 25.0 28.6 25.6 26.0 PolarMix [50] 63.6 0.0 2.6 0.1 11.4 28.1 0.0 0.0 67.0 14.1 37.9 0.3 67.3 41.2 57.1 27.4 47.9 28.2 16.2 30.4 28.1 32.8 25.2 26.9 MMD [26] 65.9 0.0 0.0 17.7 0.4 8.4 0.0 0.0 59.6 12.0 35.0 1.6 74.0 47.5 60.7 15.8 48.9 26.1 27.5 28.9 27.6 30.1 24.6 26.4 PCL [56] PointDR (Ours) 67.3 0.0 4.5 19.6 9.0 18.8 2.7 0.0 62.6 12.9 38.1 0.6 73.3 43.8 56.4 32.2 45.7 28.7 27.4 31.3 29.7"}