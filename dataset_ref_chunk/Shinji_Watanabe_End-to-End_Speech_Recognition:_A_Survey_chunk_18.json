{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_18.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the Hybrid autoregressive transducer (HAT) mentioned in the text?,        answer: To simplify the framework by utilizing the prediction network as the internal LM.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " What is the benefit of using Large-scale Pretrained LMs in NLP tasks?,        answer: They provide representative models based on Transformer LMs, such as BERT and GPT-2, that can improve E2E ASR systems.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " How do E2E models differ from classical ASR systems in terms of language modeling?,        answer: E2E models avoid the separation between acoustic model and language model, defining a joint model instead.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " What is the advantage of combining a separate LM with E2E models, despite having an internal LM?,        answer: Combining a separate LM allows for further exploitation of text-only training data.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " How does language model perplexity relate to word error rate?,        answer: Language model perplexity is observed to correlate well with word error rate.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " What method led to a significant gain in ASR performance in the middle of 2019 as mentioned in the text?,        answer: Data augmentation method represented by SpecAugment.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " What is the significance of self-supervised learning and semi-supervised learning in improving ASR performance?,        answer: These techniques, observed in the Librispeech benchmark in 2021, led to significant gains through utilizing self-supervised and semi-supervised learning.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " How was the first streaming E2E model, deployed to production for the Pixel 4 smartphone, designed?,        answer: It used a streaming RNN-T first-pass system and re-scored first-pass hypotheses with an AED system in the second pass.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " What advancements were made for the Pixel 5 smartphone in 2020 to reduce user-perceived latency?,        answer: Advancements included end-to-end endpointing and FastEmit to encourage faster microphone closing and the model to emit tokens earlier.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}, {"question": " How was the model for the Pixel 6 smartphone in 2021 further improved?,        answer: Improvements included using conformer layers for the encoder, a small embedding prediction network for the decoder, and taking advantage of the tensor processing unit (TPU) on the device.    ", "ref_chunk": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}], "doc_text": "internal LM using the transcript of the training data. Hybrid autoregressive trans- ducer (HAT) [47] extends RNN-T so that the model becomes the internal LM when the encoder output is eliminated, i.e., set to zero. This approach simplifies the framework by utilizing the prediction network as the internal LM, which avoids training an additional LM and using it in the inference time. In the work of [304], an approach similar to HAT has been proposed where the internal LM is formulated on top of standard RNN-T and attention-based encoder-decoder models, respectively. In [128], several techniques to estimate internal LMs have been proposed for AED models, where an estimated bias vector is fed to the LM instead of a zero vector. The bias vector can be estimated by averaging encoder states or context vectors, or by a small LSTM predicting the context vector based on the decoder label context, only. These techniques to estimate the internal LM were also evaluated for RNN-T in [305]. C. Use of Large-scale Pretrained LMs In recent years, LMs trained with large-scale text data are available for different NLP tasks. BERT [306] and GPT-2 [307] are representative models based on Transformer LMs. Such LMs have also been applied to E2E ASR systems in different ways, e.g., N-best rescoring [308] and dialog context embedding [309]. Relationship to Classical ASR The architecture of classical ASR systems provides a sepa- ration between the acoustic model and the language model. In contrast to this, E2E models avoid this separation and define a joint model. While this allows for training with a single objective, it limits training of the (implicit) prior to the transcriptions of the audio training data. To exploit further text-only training data, usually a separate LM is combined with E2E models, nonetheless. However, due to the implicit prior of E2E models, i.e. the internal language model, combination with separate language models is not straightforward and language model estimation requires corresponding internal and compensation approaches, e.g. [303], [47], [304], [128], [310]. At least from the recognition accuracy perspective, it remains unclear, if the clear separation of acoustic modeling and language modeling in the classical ASR architecture is a disadvantage because of separate training objectives, or rather an advantage, since text-only training data may be used easily. Also, language model perplexity, is observed to correlate well with word error rate [311], [312], [313], [314]. Furthermore, discriminative approaches to language modeling [315] may be viewed as a step towards joint modeling. the language model training objective, i.e. VIII. OVERALL PERFORMANCE TRENDS OF E2E APPROACHES IN COMMON BENCHMARKS This section summarizes various techniques with the com- mon ASR benchmarks based on switchboard (SWBD) [316] in Figure 9 and Librispeech [317] in Figure 10 to see the trajectory of the techniques developed in end-to-end ASR. We choose these two databases because they are widely used in speech and machine learning communities and cover sponta- neous (SWBD) and read speech (Librispeech) speaking styles. Figures 9 and 10 show that the performance improvement relative to the initial works [147], [79] based on the E2E models is significant, and the error rates of all tasks become less than half of the original error rates!16 Although the overall trends show that the ASR performance has steadily improved over time, there are several remarkable gains. One significant gain observed in both benchmarks in the middle of 2019 comes from the data augmentation method represented by SpecAugment [205], [206], as discussed in Section V-G. The subsequent gains mostly come from the exploration of the new neural network architectures, including transformer [102], [318], conformer [45], [103], and contextnet [97] on top of SpecAugment, as discussed in Section IV-C. Such an exploration is also performed in language modeling to improve the ASR performance [296], [102]. The final gain observed in the Librispeech benchmark in 2021 is based on self-supervised learning [25], [319] and semi-supervised learning [320], [321]. These techniques utilize a considerable 16 For latest https://github.com/syhw/wer are we benchmarks https://github.com/thu-spmi/ASR-Benchmarks/blob/main/README.md. readers who want know the to update can also check of these and This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 swb HMM AED AED chm AEDWER (%)010203040501/1/20171/1/20181/1/20191/1/20201/1/2021 AED AED AED AED AED AED AED AED IX. DEPLOYMENT OF E2E MODELS Many of the ideas discussed in this paper have been explored by various industry research labs [326], [327], [328], inter alia. In this section, we [329], [330], [331], [265], review the development of on-device production-level systems at Google as a typical case study for deployment. The first streaming E2E model, deployed to production, was launched in 2019 for the Pixel 4 smartphone [22], [332]. This model used a streaming RNN-T first-pass system, while re-scoring first-pass hypotheses with an AED system in the second pass. In addition, FST-based contextual biasing [92] was employed in the model, which was critical to obtain accurate results for diverse queries. This model ran on CPU and was much faster than real time. Fig. 9. E2E ASR performance improvement in the switchboard task. AED test_clean ContextNet Transducer transducer AED AED AED transformerWER (%)0510151/1/20197/1/20191/1/20207/1/20201/1/20217/1/2021 HMM CTC test_other AED AED AED AED AED AED transducer In 2020, for the Pixel 5 smartphone [333], the system was improved further to reduce user-perceived latency (i.e., the time between when the user speaks, and when words appear on the device). This included advancements such as end-to-end endpointing [113] to encourage faster microphone closing; as well as FastEmit [91] to encourage the model to emit tokens earlier. Finally, in 2021 the model was further improved for the Pixel 6 smartphone [334], to take advantage of the tensor processing unit (TPU) [85] on the device. Improvements include the use of conformer layers for the encoder [45]; a small embedding prediction network for the decoder [104]; a"}