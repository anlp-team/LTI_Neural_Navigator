{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Enhancing_Speech-To-Speech_Translation_with_Multiple_TTS_Targets_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main issue that direct speech-to-speech translation (S2ST) models usually face?", "answer": " Direct S2ST models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " How do previous works typically address the data scarcity issue in direct S2ST models?", "answer": " Previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT).", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What did the analysis of changing synthesized target speech for direct S2ST models find?", "answer": " Simply combining the target speech from different TTS systems can potentially improve the S2ST performances.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What framework is proposed in the text to optimize the S2ST system with multiple targets from different TTS systems?", "answer": " A multi-task framework is proposed that jointly optimizes the S2ST system with multiple targets from different TTS systems.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What is the improvement in BLEU score achieved by the proposed framework on the Fisher Spanish-English dataset?", "answer": " The proposed framework achieves a 2.8 BLEU score improvement over the baselines on the Fisher Spanish-English dataset.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What is the main contribution of the proposed framework discussed in the text?", "answer": " The proposed framework combines knowledge from different TTS data, showing reasonable improvements according to the experiments.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What are the three components of conventional cascaded S2ST models mentioned in the text?", "answer": " The three components are automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS).", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What is the main focus of speech-to-speech translation (S2ST)?", "answer": " S2ST focuses on translating speech from a source language into the speech of a target language.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " Why do previous works in the field of S2ST incorporate TTS systems?", "answer": " Previous works incorporate TTS systems to form the dataset for S2ST to enable training for S2ST models, since obtaining parallel S2ST corpora is difficult.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}, {"question": " What type of units are used in the direct S2ST model proposed by Lee et al.?", "answer": " The direct S2ST model proposed by Lee et al. uses discrete speech units as the prediction target of the system.", "ref_chunk": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}], "doc_text": "3 2 0 2 r p A 0 1 ] D S . s c [ 1 v 8 1 6 4 0 . 4 0 3 2 : v i X r a ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS Jiatong Shi1, Yun Tang2, Ann Lee2, Hirofumi Inaguma2, Changhan Wang2, Juan Pino2, Shinji Watanabe1 1 Carnegie Mellon University 2 Meta AI {jiatongs,swatanbe}@cs.cmu.edu, {yuntang,annl}@meta.com ABSTRACT It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually uti- lize text-to-speech (TTS) systems to generate samples in the tar- get language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the syn- thesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for di- rect S2ST models. We \ufb01nd that simply combining the target speech from different TTS systems can potentially improve the S2ST per- formances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset. edge across multiple TTS systems, we further propose a framework that jointly optimized the S2ST systems with multiple targets from different TTS systems. Results show that our proposed method could signi\ufb01cantly improve the S2ST performances over baseline models. To be speci\ufb01c, our proposed framework shows a 2.8 BLEU score improvement over the best baseline system with a single TTS target on the Fisher Spanish-English dataset [18]. The contribution of this work can be summarized as follows: We \ufb01rst investigate the effect of different TTS systems for target synthesized speech for S2ST. We propose a multi-task framework that combines knowledge from different TTS data, which shows reasonable improve- ments according to our experiments. 2. METHODOLOGY Index Terms\u2014 speech-to-speech translation, augmentation, discrete units 1. INTRODUCTION text-to-speech In this section, we \ufb01rst review the background of this research, in- cluding the S2ST system with discrete units and various TTS sys- tems used in this work. Then, we introduce our proposed frame- work for combining knowledge from target speech from different TTS systems. Speech-to-speech translation (S2ST) focuses on translating speech from a source language into the speech of a target language [1]. Con- ventional cascaded S2ST models decompose the task into three com- ponents, including automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) [2, 3]. Alternatively, some previous works adopt end-to-end speech-to-text translation (S2TT) instead of ASR and MT. However, it would introduce high computational costs and inference latency for further application. To mitigate the issue, recent literature focuses on building direct S2ST models without three standalone modules [4\u201312]. The training of direct S2ST models needs inevitably large amounts of parallel S2ST corpora, which are far more dif\ufb01cult to obtain than conventional cascaded methods [6]. To mitigate the issue and enable the training for S2ST models, previous works in- corporated TTS systems to form the dataset for S2ST [4\u201311]. Nearly all the published datasets on S2ST are extended from speech-to-text corpora where the target speech for S2ST is synthesized by TTS systems [13\u201315]. When synthesizing the target speech for S2ST, researchers in previous works usually select a speci\ufb01c TTS sys- tem. For instances, in [14], they utilized a variant of Non-attentive Tacotron (NAT) [16], while in [15], they adopted Fastspeech2 [17]. To the best of our knowledge, there is no investigation into how different synthesized target speech would affect the S2ST modeling. To \ufb01ll the research gap aforementioned, this paper focuses on the effect of different synthesized speech from various TTS systems. We \ufb01nd simply using training data from multiple TTS systems can im- prove the performance of S2ST. To further utilize the shared knowl- 2.1. Background S2ST with discrete units: Speech self-supervised learning (SSL) models have shown outstanding performances on various tasks [19, 20]. Notably, they are also applicable to synthesis tasks [21\u201323]. To apply SSL representations, a common strategy is to discretize them into speech units through clustering approaches [21, 24]. Pre- vious works have shown that the discrete units can disentangle lin- guistic content from other acoustic properties (e.g., speaker iden- tity or prosody information), resulting in easier learning of linguistic information directly from speech [21]. Due to this reason, Lee et al. proposed a direct S2ST model, which uses discrete speech units as the prediction target of the system [7]. Their experiments also demonstrated their superiority over the translatotron-based methods [4, 5] and comparable performances to the cascaded S2ST systems [2, 3, 25]. The discrete units in their system are generated from the K-Means clustering over the representation from a pre-trained Hu- BERT model [26]. TTS systems: As mentioned in Sec. 1, previous studies usually em- ploy TTS systems to generate the target speech for S2ST. The trans- latotron series of works mainly adopt auto-regressive (AR) TTS sys- tems (i.e., NAT) [4, 5, 14], while there are also other studies that ap- ply non-auto-regressive (NAR) TTS such as Fastspeech2 [15]. All these models are text2Mel models, where they convert the text to Mel spectrogram, so they need additional vocoders to get the wave- form of speech. The choices of vocoders also vary, including non- parametric Grif\ufb01n-Lim and neural vocoders. Encoder UnitHiFi-GAN [Y]328791\u2026[N]3108743\u2026 HuBERT TargetLangTTS1 TargetLangTTS2Discreteunits1Discreteunits2 Decoder1 LengthAdaptor Decoder2 HuBERT Fig. 1. The framework of our proposed S2ST model using multi- ple TTS targets. The blue blocks represent the S2ST modeling; the green blocks are modules used to generate target discrete units; the gray blocks are the targets of the S2ST model, while the \ufb01rst token is for predicting which TTS target is used for inference; the yellow block is for model inference. Details are explained in Sec. 2.2. In this work, we select three TTS models: Tacotron2 (TT2) [27], Fastspeech2 (FS2) [17], and VITS [28]. Tacotron2 is"}