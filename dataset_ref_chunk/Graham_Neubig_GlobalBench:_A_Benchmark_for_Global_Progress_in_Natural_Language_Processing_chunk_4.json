{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_GlobalBench:_A_Benchmark_for_Global_Progress_in_Natural_Language_Processing_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How many languages and tasks are currently supported by GlobalBench?", "answer": " GlobalBench currently supports dataset submissions of 6671 languages and 17 tasks.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " How many system outputs are covered by GlobalBench at the time of writing?", "answer": " GlobalBench covers 1,128 system outputs at the time of writing.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " What are the six NLP tasks covered by GlobalBench?", "answer": " The six NLP tasks covered by GlobalBench are named entity recognition (NER), text pair classification, text classification, extractive QA, machine translation, and KG link tail prediction.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " How much of the first languages of people in the world do the existing systems included in GlobalBench cover?", "answer": " The existing systems included in GlobalBench already cover 4.72% to 59.34% of the first languages of people in the world, depending on the task.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " Which task has the highest estimated overall demographic and linguistic global average?", "answer": " Named entity recognition (NER) has the highest estimated overall demographic and linguistic global average.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " What is the difference between the demographic utility score and the linguistic utility score for NER?", "answer": " The demographic utility score for NER is 0.4489, while the linguistic utility score for NER is only 0.0067.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " How does the Gini coefficient vary across tasks in Table 2?", "answer": " Tasks like text classification and KG prediction, which only have datasets for English, have Gini values almost equal to 1.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " How is the performance variation across languages per task represented?", "answer": " For each task, the highest performance amongst all systems for a specific language is used to represent the performance for that (task, language) pair.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " How many system submissions are there for NER and KG Link Tail Prediction?", "answer": " There are 450 system submissions for NER and 28 system submissions for KG Link Tail Prediction.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}, {"question": " What do the chromatic analysis figures in Figure 4 show for NER and KG Link Tail Prediction?", "answer": " The chromatic analysis figures show the system analysis across language populations for NER with high system performances for many languages, while KG Link Tail Prediction has monolingual coverage and low performance.", "ref_chunk": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}], "doc_text": "the current state of systems that have been submitted, to demonstrate how GlobalBench can guide and incentivize future 3https://explainaboard.inspiredco.ai/. 4https://github.com/ExpressAI/DataLab. participation and improvement in language tech- nology. 5.1 How inclusive is GlobalBench? The inclusive design of GlobalBench elucidated in Section 2 means that it can support any NLP task and dataset integrated within the ExplainaBoard software. ExplainaBoard, and subsequently Glob- alBench, currently supports dataset submissions of 6671 languages and 17 tasks. GlobalBench now covers 966 datasets in 190 languages. We have 1,128 system outputs at the time of writing, span- ning 6 NLP tasks: named entity recognition (NER), text pair classification, text classification, extrac- tive QA, machine translation, and KG link tail pre- diction; over a total of 62 languages. With the existing systems included in GlobalBench, we al- ready cover 4.72% to 59.34% of the first languages of people in the world depending on the task, as detailed in Table 2. We focus on analyzing system submission for these six tasks below. 5.2 What is our Current Progress as Measured by GlobalBench? Next, we discuss the current state of NLP progress through the lens of GlobalBench. To do so, we display the demographic- and linguistic-weighted global average scores in Table 2. Variance in estimated utility across tasks: In Table 2, we observe that NER has the highest es- timated overall demographic and linguistic global average. Additionally, NER and MT have the high- est language coverage (60 languages). This is be- cause these tasks have been subject to extensive and impressive multilingual dataset development and evaluation efforts by authors of FLORES (Goyal et al., 2022), MasakhaNER (Adelani et al., 2021), and NusaCrowd (Cahyawijaya et al., 2022), which are all included in GlobalBench. In contrast, the estimated demographic- and linguistic- weighted utility for tasks like KG link prediction or multiple- choice QA are low. These are tasks where intensive data creation efforts have traditionally focused on English. The multilingual datasets that do exist are less widely used and/or not yet included in Glob- alBench. However, GlobalBench can help iden- tify these coverage failures and improve accuracy (\u00a75.3). Linguistic vs. demographic utility: In addition, we observe that overall linguistic utility scores across all tasks is very low, and is substantially Category Task Number of Datasets Languages System Outputs Metric Text Classification Text Classification 127 12 199 Accuracy Sequence Labeling Named Entity Recognition Word Segmentation Chunking 78 1 2 60 1 1 450 - - F1 F1 F1 Cloze Generative Multiple Choice 10 21 1 2 - CorrectCount Accuracy Text Pair Classification Text Pair Classification 57 30 96 Accuracy Span Text Classification Span Text Classification 4 1 Accuracy Text Editing Grammatical Error Correction 10 1 SeqCorrectCount Question Answering Extractive Multiple Choice Open Domain 80 72 4 18 2 2 185 - - F1 Acc. ExactMatch Conditional Generation Machine Translation Summarization Code Generation 242 251 4 60 55 5 170 - - Bleu Bleu Bleu KG Prediction KG Prediction 3 1 28 Hits Language Modeling Language Modeling Perplexity Table 1: Overall Statistics of supported tasks, datasets and system outputs. We currently have 1128 system outputs across 6 tasks and 62 languages. Refer to \u00a75 for a detailed analysis. lower than the corresponding demographic utility scores. For instance, the demographic utility score for NER is 0.4489, while the linguistic utility score for the same is only 0.0067. This makes clear that the systems submitted to GlobalBench are currently doing a better job of covering widely-spoken lan- guages, but are doing less well at covering all of the languages in the world. Task Demo. Avg. \u2191 Ling. Avg. \u2191 Gini \u2193 % Pop. NER Extractive QA Text Pair Classif. MT Text Classif. KG Prediction 0.4489 0.3460 0.3465 0.0485 0.0477 0.0221 0.0067 0.0020 0.0019 0.0002 0.0001 0.0001 0.9920 0.9975 0.9976 0.9987 0.9997 0.9998 59.34% 44.46% 39.02% 10.58% 4.72% 4.72% Table 2: Demographic and linguistic global averages, equity values (Gini), and percentage of world population covered by current submissions of system results to GlobalBench. Tasks shown in this table have at least one system submission. Equity of systems across languages: Since we calculate the Gini coefficient accounting for all 6671 languages, all values are extremely high (near- ing 1). Text classification and KG Prediction only have datasets for English, hence the Gini values almost equal 1. We also note that, despite NER and MT having the same language coverage, MT has a higher Gini value, indicating that amongst the languages supported, NER has a more uniform distribution of performance as compared to MT. Variation across languages per task: We also maintain a ranking of system performance for each language as described in \u00a72.2. For each task, we take the highest performance amongst all systems, to represent the performance for a (task, language) pair. Next, we rank all system performances across languages for each task. For example, suppose we have 4 systems with performances P1, P2, P3, and P4 for task T1, where P1 and P2 are in language L1, and P3 and P4 are in language L2. If P1 > P2, then P1 is used to represent the performance of T1 in L1; similarly, if P3 > P4, then P3 is used to represent the performance of T1 in L2. We then rank the system performances of L1 and L2 under task T1, i.e., we compare P1 and P3 to see which language sees higher system performance. Figure 4a shows a chromatic visualization of system analysis across language populations for NER. GlobalBench sup- ports 78 datasets and 60 languages for this task, and sees 450 system submissions of this task. There- fore, we can see high system performances for many languages. However, KG Link Tail Predic- tion does not have many submitted systems (28), with monolingual coverage and low performance, as shown in Figure 4b. For a more comprehensive set of chromatic analysis figures for each task with (a) NER (b) KG Link Tail Prediction Figure 4: Variations across languages for a task: Rank- ing of system performance for each language in NER (above) and KG Prediction (below)."}