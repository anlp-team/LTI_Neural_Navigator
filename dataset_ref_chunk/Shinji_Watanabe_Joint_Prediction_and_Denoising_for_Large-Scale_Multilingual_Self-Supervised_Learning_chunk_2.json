{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Joint_Prediction_and_Denoising_for_Large-Scale_Multilingual_Self-Supervised_Learning_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What does the ESPNet toolkit open-source?,answer: all code and trained models", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " What is the pre-training objective used in the HuBERT WavLabLM model?,answer: masked-prediction based pre-training (MPPT)", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " How are MPPT models trained?,answer: by predicting discretized representations of masked regions of the input speech", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " How do Wav2vec 2.0 and w2v-BERT differ in obtaining discrete representations?,answer: They learn the discrete units as part of the SSL training, allowing the representations to be trained end-to-end", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " Why do models like Wav2vec 2.0 and w2v-BERT require an additional loss?,answer: To keep the discrete units diverse and prevent codebook collapse", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " What approach does HuBERT take to SSL?,answer: An offline clustering step using k-means prior to pre-training", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " How does WavLM address noisy and multi-speaker settings during training?,answer: By augmenting the input speech via dynamic mixing with noise or overlapping speech", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " What is the main purpose of the Openli110 training corpus?,answer: To conduct large-scale multilingual pre-training", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " What architecture does WavLabLM use for pre-training?,answer: HuBERT architecture, modified from wav2vec 2.0, consisting of a CNN feature extractor, Transformer encoder, and codebook output layer", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}, {"question": " What is a major novelty of the WavLabLM model?,answer: Enhancing model robustness to noise in multilingual settings by applying joint denoising task", "ref_chunk": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}], "doc_text": "We open-source all code and trained models in the ESPNet toolkit [32]. 2. BACKGROUND 2.1. HuBERT WavLabLM uses masked-prediction based pre-training (MPPT), which has become a popular pre-training objec- tive among SOTA SSL models [3, 11, 12, 14, 27]. MPPT models are trained by predicting discretized representations of masked regions of the input speech. In doing so, the model is forced to leverage the unmasked audio as context for its predictions. MPTT models largely differ in how the discrete represen- tations are obtained. Wav2vec 2.0 [11] and w2v-BERT [14] learn the discrete units as part of the SSL training, allowing the representations to also be trained end-to-end. While powerful, these models are prone to codebook collapse during training [30], and require an additional loss to keep the discrete units diverse. We instead adopt the offline clustering approach of HuBERT [12], which has been successfully been adapted to other SSL models such as WavLM [3]. HuBERT takes an iterative approach to SSL through an offline clustering step: prior to pre-training, the input data is clustered using k-means. HuBERT is thus trained to predict the frame-level cluster as- signments of masked regions of the input speech. Table 1. List of corpora in our pre-training data. For Vox- Populi, we only use recordings from the annotated subset (but discard the transcriptions for unsupervised SSL). Dataset Languages Hours Speech Type MLS Common Voice Googlei18n VoxPopuli BABEL FLEURS 8 92 34 16 17 102 22,185 13,600 1,328 1,024 1000 950 Read Read Mixed Spontaneous Spontaneous Read Total 136 39,087 2.2. WavLM While many SSL models are trained on clean audiobook read- speech [10\u201312], real world scenarios often feature noisy and multi-speaker settings. To address this, WavLM [3] jointly trains the model to perform both masked speech prediction and denoising. This is done by augmenting the input speech via dynamic mixing with either noise or overlapping speech. For every utterance u in an input batch B, a random value v is sampled from the continuous distribution U(0, 1). If v is less than the augmentation probability pn, another random value w is sampled from U(0, 1). If w is less than utterance mixing probability pu, another utterance un in the batch B is sampled for mixing. Otherwise, a random Deep Noise Suppression (DNS) noise un is used [33]. A mixing energy ratio e is then sampled for the mixed au- dio. If another utterance is used, e is sampled from U(\u22125, 20). Otherwise, e is sampled from U(\u22125, 5). The starting times- tamp of u for the mixing t is sampled in two stages. First, we sample a minimum length lmin from the discrete uniform distribution U(0, len(u) ). This way, t can be sampled from U(lmin, len(un)). The same process is applied again to ob- tain tn, the starting timestamp for mixing in un. Next, the (cid:80) u\u2217u energy of the utterances is calculated as Eu = len(u) and for u and un respectively. Then, the mixing En = 2 (cid:80) un\u2217un len(un) (cid:113) Eu e\u2217En scale s is calculated as s = . Finally, the portion where the augmentation is applied u[t : t + l] is summed with the scaled noise portion s \u2217 un[tn : tn + l]. By applying this augmentation during training, WavLM is made more robust to noisy and multi-speaker settings, and achieves SOTA results on the SUPERB Benchmark [16]. 3. PRE-TRAINING WAVLABLM 3.1. Pre-training Data To conduct pre-training, we build Openli110, a large-scale multilingual training corpus that covers 109 languages. It con- sists of Common Voice[31], VoxPopuli [34], MLS [35], and Table 2. The top 15 languages in our SSL pre-training set by geographic region and hours. Together, they account for 92.5% of the data. Language Region Hours English Kinyarwanda Esperanto German Catalan Belarussian French Spanish Kabyle Ganda Sundanese Italian Javanese Bengali Sinhala Bashkir West Europe Sub-Saharan Africa Constructed West Europe West Europe East Europe West Europe West Europe North Africa Sub-Saharan Africa Southeast Asia West Europe Southeast Asia South Asia South Asia Central Asia 23663 1984 1360 1256 1184 965 934 523 518 362 330 314 294 239 218 216 Total 34362 Googlei18n 2. To extend coverage to 136 languages, we also include BABEL [36] and FLEURS [37]. A full break down of our pre-training data by corpus is available in Table 1. We filter out audio clips longer than 15 seconds due to GPU memory constraints, leaving us with 40k hours of pre-training data. We want to emphasize the geographical diversity of this data: the top 15 languages are spread across 7 different regions across the globe (Table 2). In contrast, the entire top 15 languages used for the SOTA XLS-R [24] are West/East European. We use the FLEURS development split for a balanced validation, which has similar amounts of data per language. 3.2. Pre-Training Architecture WavLabLM uses the HuBERT [12] architecture, which modi- fied wav2vec 2.0 [11] for codebook prediction. It consists of a CNN feature extractor, a Transformer [38] encoder, and a codebook output layer. WavLabLM also uses the HuBERT pre-training objective detailed in Section 2.1; it predicts the cluster assignments of each masked audio frame. One major novelty of our work is to enhance model robustness to noise in multilingual settings by applying the WavLM joint denoising task discussed in Section 2.2. We do not adopt the architec- tural changes introduced by WavLM, such as gated relative positional bias and scaled softmax. This is because it prevents the application of the second novelty of our method, continual learning from existing HuBERT models, as described below. The simplicity of the HuBERT approach to SSL allows 2Resources 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, and 86 from openslr.org it to be both scalable and versatile. Since the clusters are obtained offline, we can reduce resource consumption by us- ing existing SSL models for feature extraction. To show that powerful multilingual SSL is feasible without large-scale"}