{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Improving_Language_Models_with_Advantage-based_Offline_Policy_Gradients_chunk_11.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the COMET model as mentioned in the text?", "answer": " The COMET model is focused on predicting the cause/effect of various social situations.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " How did A-LOL improve the performance of the COMETDIS TIL model?", "answer": " A-LOL variants helped the COMETDIS TIL model obtain the highest COMET critic score by improving approximately 8% on top of its reference policy.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What issue did LMs trained on the knowledge-grounded dialog task face according to the text?", "answer": " LMs trained on the knowledge-grounded dialog task failed to maintain faithfulness to the given knowledge and often hallucinated incorrect information or opinions.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What is FaithDial and how did it address the issue identified in the knowledge-grounded dialog task?", "answer": " FaithDial is a smaller and more faithful training set constructed by rewriting hallucinated responses. It was created to address the issue of maintaining faithfulness to the given knowledge in dialog models.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What was the reward composition used for evaluation in the Knowledge-Grounded Dialog task on the FaithDial test set?", "answer": " The reward in the Knowledge-Grounded Dialog task included a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What training set did the models trained with A-LOL and its variants consistently outperform according to the text?", "answer": " Models trained with A-LOL and its variants consistently outperformed models trained on the baselines and were resilient to the bad quality WoW training data.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What was one variant of A-LOL that went into degeneration due to over-optimization of KL penalty in the experiment?", "answer": " The A-LOL KL variant went into degeneration due to over-optimization of KL penalty.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What was one observation made about the model generations compared to humans in the text?", "answer": " All model generations were much less diverse compared to humans, indicating that there is still progress to be made in this aspect.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " What was the purpose of constructing the ATOMIC10x dataset as mentioned in the text?", "answer": " The ATOMIC10x dataset was constructed to improve the performance of the COMET model by training on a smaller high-quality subset.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}, {"question": " How did the aggressive filtering of data according to a critic classifier impact the performance of the COMET model?", "answer": " The aggressive filtering of data improved the performance of the COMET model trained on the smaller high-quality subset, but it may lose valuable training signal in return.", "ref_chunk": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}], "doc_text": "bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 12. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs. 16A-LOL without clipping quickly started receiving nan values in loss. 18 Preprint under review. Table 5: Commonsense Transformer quality improvement evaluated with average COMET critic classifier probability as reward on the ATOMIC20 20 test set. We also report the generation length and corpus diversity of all methods along with the human-written test set performance in the last row. We do not report the baselines that didn\u2019t improve over the reference policy. Models trained with A-LOL variants show the most improvement compared to the baselines. Model/Algo. COMET Critic Length Distinct-1/2/3 (COMETDIS TIL ) \u03c0ref + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq 84.6 88.5 88.8 89.4 92.2 92.8 93.0 3.3 4.6 5.0 4.6 4.5 4.4 4.4 .041/.145/.267 .040/.167/.331 .038/.159/.314 .041/.170/.336 .040/.170/.342 .040/.169/.335 .040/.171/.346 human-written 93.5 3.4 .103/.423/.706 C ADDITIONAL EXPERIMENTS AND RESULTS C.1 COMMONSENSE REASONING TASK Commonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDIS TIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDIS TIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC20 20 test split. Due to the large training set, we only finetune the COMETDIS TIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset. Results Table 5 shows that COMETDIS TIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made. C.2 KNOWLEDGE-GROUNDED DIALOG TASK LMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. 19 Preprint under review. Table 6: Evaluation for Knowledge-Grounded Dailog task on FaithDial test set. The reward comprises a sum of three classifier probabilities (FaithCritic, CoLA fluency, dialog engagement) and a length penalized TF-IDF diversity score. Along with the length and corpus-level distinct-n-gram diversity metrics, we also report Coverage and Density (lower is better) that quantify the lexical overlap between knowledge and responses. For comparison, the scores of human responses in the FaithDial test set are shown in the last row. Models trained with A-LOL and its variants consistently outperform the baselines and are also resilient to the bad quality WoW training data. Model/Algo. Reward FaithCritic Fluency Engagement Diversity Coverage \u2193 Density \u2193 Length Distinct-1/2/3 Models trained on WoW training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.52 2.55 2.61 2.68 2.72 2.80 2.83 2.88 2.81 .64 .67 .71 .79 .80 .87 .90 .91 .87 .91 .91 .92 .91 .91 .92 .92 .93 .92 .72 .73 .73 .73 .74 .75 .75 .76 .75 .25 .25 .26 .26 .27 .26 .27 .28 .27 .51 .53 .51 .62 .50 .53 .56 .46 .52 5.43 5.88 5.03 7.30 4.51 5.31 5.39 3.39 4.76 16.2 16.3 15.2 16.0 13.7 14.5 13.5 12.0 14.1 .167/.500/.697 .170/.500/.694 .174/.516/.712 .170/.483/.650 .185/.530/.727 .180/.515/.700 .186/.516/.695 .187/.516/.705 .183/.518/.705 Models trained on FaithDial training set \u03c0ref + NLL + wBC + R GOLD + R-LOL + A-LOL (ref. free) + A-LOL + A-LOL seq + A-LOL KL (DialoGPT) 2.89 2.89 2.90 2.90 2.91 2.93 2.94 2.94 2.92 .99 .99 .98 .99 .98 .98 .98 .97 .98 .90 .91 .91 .91 .91 .92 .93 .93 .92 .75 .75 .75 .74 .76 .76 .77 .78 .77 .25 .25 .25 .26 .26 .26 .26 .26 .26 .34 .32 .34 .40 .36 .33 .33 .32 .33 2.31 2.01 2.36"}