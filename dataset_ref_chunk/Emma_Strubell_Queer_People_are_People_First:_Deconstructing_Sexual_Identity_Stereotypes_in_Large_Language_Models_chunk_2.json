{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Emma_Strubell_Queer_People_are_People_First:_Deconstructing_Sexual_Identity_Stereotypes_in_Large_Language_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the paper mentioned in the text?", "answer": " The main focus is on computational fairness analysis and mitigating bias in text through style transfer.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " How do Sun et al. (2021) create gender-neutral text?", "answer": " They use a rule-based method to convert gendered pronouns to singular they pronouns and swap gendered words with gender-neutral versions.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " What method do Tokpo and Calders (2022) use to tackle bias in text generation?", "answer": " They formulate the task as a neural style transfer problem and use an adversarial approach to retain the style of the written text.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " How do authors like Sheng et al. (2019) analyze bias in the outputs of language models?", "answer": " They focus on bias association of input templates with the output of language models and perform a text-to-text generation task.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " What is the focus of the study mentioned in the text regarding bias detection?", "answer": " The focus is on providing the large language model (LLM) with various contexts in addition to bias association trigger words and performing a text-to-text generation task.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " How is bias in language model outputs quantitatively measured in the text?", "answer": " Bias is quantitatively measured by looking at the most frequently occurring words across outputs generated by different trigger words and using the pointwise mutual information concept to identify words that occur more in the outputs of queer trigger words compared to straight counterparts.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " What approach is discussed in debiasing LLM output by Gupta et al. (2022)?", "answer": " They engineer prompts to reduce bias in distilled language models by augmenting the dataset with corresponding counterfactual sentences.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " How do Ma et al. (2020) propose to reduce implicit bias in text?", "answer": " They formulate the debiasing method as a style transfer problem and focus on increasing the overall regard score of the sentence.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " What technique is used by Yang (2022) to remove sarcastic words in text generation?", "answer": " SHAP (SHapley Additive ex- Planations) is used to delete the words that lead to an input text being marked as sarcastic in a text-to-text style transfer problem.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}, {"question": " What is the core concept discussed in Gira et al. (2022) for reducing bias in pre-trained language models?", "answer": " They implement a fine-tuning technique on a dataset augmented with additional data to reduce bias in pre-trained language models.", "ref_chunk": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}], "doc_text": "computational fair- ness analysis. In addition, we try to mitigate the bias by formulating a text-to-text style transfer problem. We discuss related work for each compo- nent of our work separately in subsections below. 2.1 Generating gender-neutral text Sun et al. (2021) developed a technique to create a gender-inclusive English language text re-writer. The authors devised a rule-based method to convert gendered pronouns with the singular they pronoun. They also swapped gendered words like fireman, mother, brother, etc. with their gender-neutral ver- sions like firefighter, parent, sibling, etc. To ensure the rewritten sentence is semantically and gram- matically correct, authors used dependency parser and a language model for corrections. To gender neutralize the biographies, we use the method de- scribed in this paper. Vanmassenhove et al. (2021) introduce the algorithm NeuTral Rewriter which, like the previous paper, uses both rule-based and automatic neural method to convert gendered text to gender-neutral text. Earlier works have employed a somewhat dif- ferent methodology. Tokpo and Calders (2022) formulate the task as a neural style transfer prob- lem. Following an adversarial approach to generate text, they try to retain the style of written text. This method is susceptible to changing the context of the sentence which is not desirable in our case. 2.2 LLM prompting for bias detection Previously, there are some works in which the au- thors use different prompts to study the biases in the outputs of language models. Hassan et al. (2021) statistically analyze the results that words gener- ated by large language models put differently-abled people at a disadvantage. In addition, the authors did some analysis based on gender and race. Their methodology to analyze the text produced by lan- guage models included the use of template sen- tence fragments. The authors use template-based prompts (with a focus on bias association) to do next word prediction. Sheng et al. (2019) focuses mainly on the bias association of the input template with the output of a language model. Their tem- plates contain mentions of different demographic groups and perform a text-to-text generation task. In our work, as compared to the papers men- tioned above, we will focus on providing the LLM with several different contexts in addition to the bias association trigger words. Moreover, like Sheng et al. (2019), we will be performing a text- to-text generation task. There are several other papers in which the au- thors have released datasets of prompts to detect the biases in the outputs of a language model (Nan- gia et al., 2020; Nadeem et al., 2021; Gehman et al., 2020). Nangia et al. (2020) detect stereotypical bias in masked language models. The prompts used by Nadeem et al. (2021) and Gehman et al. (2020) can be used for autoregressive language models. As mentioned above, the focus of our study is to detect and measure the bias in autoregressive LLM outputs for different sexual identity trigger words with different contextual information. 2.3 Fairness analysis of LLM output In our work, the focus is on qualitatively and quan- titatively measuring the bias in language model\u2019s outputs. Hassan et al. (2021) use a hierarchical Dirichlet process on BERT-predicted output (Jelodar et al., 2017). It can be used to look at abstract topics in the generated text by an LLM. In our work, we will look at the most frequently occurring words across the outputs generated by different sexual identity trigger words. Our work will use the concept of pointwise mutual information (Church and Hanks, 1990) to find those words which occur more in the outputs of queer trigger words in contrast to the out- puts of their corresponding straight counterparts. Further, Hassan et al. (2021) quantitatively ana- lyze the outputs of language model by performing sentiment analysis. However, Sheng et al. (2019) introduce the regard score or regard metric which measures the social perception of a person from a specific demographic group. In other words, it is a measure of how a person is perceived by the society. As in this paper the authors have shown that regard score is a better measure than sentiment analysis to look at the bias in outputs of language models, we will be using this to quantify the representational bias. 2.4 Debiasing LLM Output Gupta et al. (2022) discuss a method in which they engineer prompts to reduce bias in distilled lan- guage models. The core concept that they want to mitigate the bias of a teacher model to pass onto 3 the distilled model. They augment the dataset by finding the corresponding counterfactual sentences for the given data and modify the probabilities of teacher model based on counterfactuals. In Gira et al. (2022), the authors aimed to reduce bias in pre-trained language models by implementing a fine-tuning technique on a dataset that had been augmented with additional data. Such methods fo- cus on reducing the bias by training the models in specific ways. However, in our work, we will focus on post-hoc debiasing technique for language models with fixed weights. The debiasing method introduced by Ma et al. (2020) has been formulated as a style transfer prob- lem to reduce the implicit bias in text. The Pow- erTransformer technique is based on the concept of connotation frames (Sap et al., 2017). In our work, we will also formulate the LLM output debi- asing task as text-to-text style transfer task as we would like to keep the contextual meaning intact but would like to increase the overall regard score of the sentence. Other works (Li et al., 2018; Hu et al., 2018) have devised ways for controllable text generation using neural style transfer methods. Yang (2022) use SHAP (SHapley Additive ex- Planations) (Lundberg and Lee, 2017) to delete the words that lead to an input text being marked as sarcastic. They formulate the problem of remov- ing sarcasm as a text-to-text style transfer problem. They find alternative words for the sarcastic words detected using SHAP with a language model. A similar approach will be used in our work to find the"}