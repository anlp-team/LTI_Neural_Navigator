{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daphne_Ippolito_Extracting_Training_Data_from_Diffusion_Models_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What approach is taken to count for near-duplication in the training dataset?", "answer": " Instead of searching for bit-for-bit identical duplicates, the approach is to account for near-duplication by using CLIP embeddings.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " How is the computation of near-duplicates made more efficient?", "answer": " Computation is made more efficient by first embedding each image into a 512 dimensional vector using CLIP, and then performing comparisons in this lower-dimensional space.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " What strategy is employed for the extraction attack in this study?", "answer": " The study designs a new membership inference attack strategy based on the intuition that for diffusion models, samples are likely memorized if they have high cosine similarity.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " How are the generations in the clique determined to be memorized images?", "answer": " If the largest clique in the graph, representing generations, is at least size 10 and near-identical, it is predicted to be a memorized image.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " What is the purpose of the modi\ufb01ed Euclidean distance measure used in the study?", "answer": " The modi\ufb01ed Euclidean distance measure is used to compute similarity among images in the clique by measuring the distance between image tiles.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " How are images generated in the first step of the extraction methodology?", "answer": " Images are generated by querying the Gen function in a black-box manner using selected prompts as inputs.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " What is the importance of the second step in the extraction methodology?", "answer": " The second step involves flagging and identifying generations that appear to be memorized training images.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " How many candidate images are generated for each text prompt in the study?", "answer": " 500 candidate images are generated for each text prompt to increase the likelihood of finding memorization.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " What is the total number of generated images used to evaluate the effectiveness of the attack?", "answer": " The study uses a total of 175 million generated images to evaluate the attack.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}, {"question": " How many memorized images are identified with 0 false positives in the study?", "answer": " The study identifies 50 memorized images with 0 false positives out of 175 million generated images.", "ref_chunk": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}], "doc_text": "because these are orders of magnitude more likely to be memorized than non-duplicated examples [47, 41]. If we search for images that are bit-for-bit identically duplicated in the training dataset, we would signi\ufb01cantly undercount the true rate of duplication. Instead, we ac- count for near-duplication. Ideally, we would search for any training examples that are nearly duplicated with a Original: Generated: Figure 3: Examples of the images that we extract from Stable Diffusion v1.4 using random sampling and our mem- bership inference procedure. The top row shows the original images and the bottom row shows our extracted images. pixel-level (cid:96)2 distance below some threshold. But this is computationally intractable, as it would require an all- pairs comparison of 160 million images in Stable Dif- fusion\u2019s training set, each of which is a 512 \u00d7 512 \u00d7 3 dimensional vector. Instead, we \ufb01rst embed each image to a 512 dimensional vector using CLIP [54], and then perform the all-pairs comparison between images in this lower-dimensional space (increasing ef\ufb01ciency by over 1500\u00d7). We count two examples as near-duplicates if their CLIP embeddings have a high cosine similarity. For each of these near-duplicated images, we use the corre- sponding captions as the input to our extraction attack. 4.2.1 Extraction Methodology Our extraction approach adapts the methodology from prior work [11] to images and consists of two steps: threat model in this section, we do not have access to the loss and cannot exploit techniques from state-of-the- art membership inference attacks [11]. We instead de- sign a new membership inference attack strategy based on the intuition that for diffusion models, with high prob- ability Gen(p; r1) (cid:54)= Gen(p; r2) for two different random initial seeds r1, r2. On the other hand, if Gen(p; r1) \u2248d Gen(p; r2) under some distance measure d, it is likely that these generated samples are memorized examples. The 500 images that we generate for each prompt have different (but unknown) random seeds. We can therefore construct a graph over the 500 generations by connect- ing an edge between generation i and j if xi \u2248d x j. If the largest clique in this graph is at least size 10 (i.e., \u2265 10 of the 500 generations are near-identical), we pre- dict that this clique is a memorized image. Empirically, clique-\ufb01nding is more effective than searching for pairs of images x1 \u2248d x2 as it has fewer false positives. 1. Generate many examples using the diffusion model in the standard sampling manner and with the known prompts from the prior section. 2. Perform membership inference to separate the model\u2019s novel generations from those generations which are memorized training examples. To compute the distance measure d among the images in the clique, we use a modi\ufb01ed Euclidean (cid:96)2 distance. In particular, we found that many generations were often spuriously similar according to (cid:96)2 distance (e.g., they all had gray background). We therefore instead divide each image into 16 non-overlapping 128 \u00d7 128 tiles and mea- sure the maximum of the (cid:96)2 distance between any pair of image tiles between the two images. Generating many images. The \ufb01rst step is trivial but computationally expensive: we query the Gen function in a black-box manner using the selected prompts as in- put. To reduce the computational overhead of our experi- ments, we use the timestep-resampled generation imple- mentation that is available in the Stable Diffusion code- base [58]. This process generates images in a more ag- gressive fashion by removing larger amounts of noise at each time step and results in slightly lower visual \ufb01delity at a signi\ufb01cant (\u223c 10\u00d7) performance increase. We gener- ate 500 candidate images for each text prompt to increase the likelihood that we \ufb01nd memorization. Performing membership inference. The second step requires \ufb02agging generations that appear to be memo- rized training images. Since we assume a black-box 4.2.2 Extraction Results In order to evaluate the effectiveness of our attack, we select the 350,000 most-duplicated examples from the training dataset and generate 500 candidate images for each of these prompts (totaling 175 million generated im- ages). We \ufb01rst sort all of these generated images by or- dering them by the mean distance between images in the clique to identify generations that we predict are likely to be memorized training data. We then take each of these generated images and annotate each as either \u201cextracted\u201d or \u201cnot extracted\u201d by comparing it to the training images under De\ufb01nition 1. We \ufb01nd 94 images are ((cid:96)2, 0.15)- extracted. To ensure that these images not only match 5 0 100MemorizedExamplesExtracted 60 ManualInspection 0.6 1.0AttackPrecision 0.7 0.9 40 0.8 80 (\u20182,0.15)-Extraction 20 Figure 4: Our attack reliably separates novel genera- tions from memorized training examples, under two def- initions of memorization\u2014either ((cid:96)2, 0.15)-extraction or manual human inspection of generated images. some arbitrary de\ufb01nition, we also manually annotate the top-1000 generated images as either memorized or not memorized by visual analysis, and \ufb01nd that a further 13 (for a total of 109 images) are near-copies of training examples even if they do not \ufb01t our 2-norm de\ufb01nition. Figure 3 shows a subset of the extracted images that are reproduced with near pixel-perfect accuracy; all images have an (cid:96)2 difference under 0.05. (As a point of refer- ence, re-encoding a PNG as a JPEG with quality level 50 results in an (cid:96)2 difference of 0.02 on average.) Given our ordered set of annotated images, we can also compute a curve evaluating the number of extracted images to the attack\u2019s false positive rate. Our attack is exceptionally precise: out of 175 million generated images, we can identify 50 memorized images with 0 false positives, and all our memorized images can be ex- tracted with a precision above 50%. Figure 4 contains the precision-recall curve for both memorization de\ufb01nitions. Measuring (k, (cid:96), \u03b4 )-eidetic memorization. In De\ufb01ni- tion 2 we introduced an adaptation of Eidetic memo- rization [11] tailored to the domain of generative im- age models. As mentioned earlier, we compute similar- ity between pairs of images with"}