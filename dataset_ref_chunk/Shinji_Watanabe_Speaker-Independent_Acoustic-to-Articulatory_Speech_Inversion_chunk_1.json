{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Speaker-Independent_Acoustic-to-Articulatory_Speech_Inversion_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the research described in the text?", "answer": " The main focus is on building an acoustic-to-articulatory inversion (AAI) model to capture the mechanics of speech production.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What dataset is used to train the AAI models in the research?", "answer": " The MOCHA-TIMIT and HPRC datasets are used to train the AAI models.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " How much improvement did the research show in correlation on the electromagnetic articulography (EMA) dataset?", "answer": " The research showed a 12.5% improvement in correlation on the EMA dataset.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What does the 12-dimensional EMA features contain?", "answer": " The 12-dimensional EMA features contain the midsagittal x and y coordinates of jaw, lip, and tongue positions.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What is the purpose of acoustic-to-articulatory inversion (AAI)?", "answer": " AAI aims to automatically estimate articulatory features directly from speech signals.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What method is used to improve the state-of-the-art in AAI according to the research?", "answer": " The research uses autoregression, adversarial training, and self-supervision to improve the state-of-the-art in AAI.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " How is the interpretability of the speech representations showcased in the research?", "answer": " The interpretability is showcased by directly comparing the behavior of estimated representations with speech production behavior.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What is the significance of building speaker-independent speech processing models with articulatory data?", "answer": " It allows for generalizing the models to unseen speakers and improves the interpretability of speech representations.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " What is one of the challenges mentioned in working with articulatory data?", "answer": " One of the challenges is that most articulatory datasets contain limited hours of high-quality speech data.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}, {"question": " How is the efficacy of the inversion approach evaluated on speakers without articulatory labels?", "answer": " The efficacy is evaluated using resynthesis-based AAI evaluation metric that does not rely on articulatory labels.", "ref_chunk": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}], "doc_text": "3 2 0 2 l u J 4 2 ] S A . s s e e [ 2 v 4 7 7 6 0 . 2 0 3 2 : v i X r a SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION Peter Wu1, Li-Wei Chen2, Cheol Jun Cho1, Shinji Watanabe2, Louis Goldstein3, Alan W Black2, Gopala K. Anumanchipalli1, 1University of California, Berkeley, 2Carnegie Mellon University, 3University of Southern California ABSTRACT To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promis- ing inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic- to-articulatory inversion (AAI) model that leverages self- supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulogra- phy (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these represen- tations through directly comparing the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset. Fig. 1. EMA features (points) and tract variables (segments). Diagram extends [23]. Details in Sections 2.1 and 3.1. Index Terms\u2014 articulatory inversion, articulatory speech processing 1. INTRODUCTION Interpretable representations of speech waveforms are valu- able for tasks like diagnosing voice disorders [1] and building generalizable, controllable speech synthesizers [2, 3, 4]. Ar- ticulatory speech processing is a promising direction for mak- ing speech representations interpretable, since methods in this direction aim to directly model information about vocal tract physiology [5, 6, 7]. Specifically, articulatory representations directly describe the movement of articulators, e.g., the jaw, lips, and tongue. Currently, building speaker-independent speech process- ing models with articulatory data remains challenging, since such data is limited. Most articulatory datasets [8, 9, 10, 11] contain only a few hours of high-quality speech, much less than popular non-articulatory ones [12]. This is mainly since the equipment and processes used to acquire articulatory la- bels are expensive [13]. A promising, cost-effective alter- native to manually collecting articulatory data is acoustic-to- articulatory inversion (AAI), which aims to automatically es- timate articulatory features directly from speech signals [14]. In recent years, deep learning algorithms have become popular state-of-the-art methods for AAI [15, 16, 17, 18, 19, 20, 21, 22]. Since these methods are data-driven and depend on the limited amount of articulatory data, they are unable to sufficiently generalize to unseen speakers to our knowledge. In this work, we aim to help bridge this gap through improv- ing the AAI deep learning framework. Specifically, we exam- ine different architecture configurations, features and training objectives to showcase a suitable framework that improves the state-of-the-art by 12.5% using autoregression, adversar- ial training, and self supervision. We also evaluate predicted features through the articulatory phonology lens, comparing phoneme-level articulatory trajectories with expected human speech production behavior. Analyzing trajectories this way allows us to perform evaluation on any speaker, not just those who have recorded articulatory labels. Our analysis of esti- mated articulator movements also highlights the interpretabil- ity of our speech representations. Finally, we also evaluate inversion on speakers without articulatory labels using resyn- thesis and show that our inversion approach is also effective here. Code and additional related information are available at https://github.com/articulatory/articulatory. 2. DATASETS 2.1. MOCHA-TIMIT EMA Dataset We train our AAI models on an 8-speaker dataset using MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2 hours of 16 kHz speech. Data is collected with the speaker Fig. 2. Estimating the palate location with a convex hull on tongue data for HPRC speaker F04. Details in Section 3.1. instrumented in an electromagnetic articulography (EMA) machine providing 200 Hz samples of EMA features. These 12-dimensional features contain the midsagittal x and y coor- dinates of jaw, lip, and tongue positions. Specifically, the 6 x-y positions correspond to the lower incisor, upper lip, lower lip, tongue tip, tongue body, and tongue dorsum, as visualized in Figure 1. To study how well our AAI approach general- izes to unseen speakers, we arbitrarily choose MSAK as the unseen speaker, evaluating on all MSAK data and training on the other 7 speakers. Within the 7-speaker training set, we randomly set off 10% of the utterances for the validation set. 2.2. HPRC EMA Dataset We also follow previous works [13, 24, 25] to evaluate on the Haskins Production Rate Comparison (HPRC) [11] database. HPRC is an 8-speaker dataset containing 7.9 hours of 44.1 kHz speech and 100 Hz EMA. We downsampled the audio to 16kHz before further processing. As done in previous work [13], we only consider information along the midsagit- tal plane and do not use the provided mouth left and jaw left data, matching our MOCHA-TIMIT EMA feature set described in Section 2.1. For our AAI train-val-test split, we hold out 1 male and 1 female speaker for our test set and train on the remaining 6 speakers, as done in [15]. While [15] also used these two speakers in their validation set, we put all of the data from both speakers in the test set in order to fully experiment within the unseen speaker setting. We note that our formulation in- creases the difficulty of the task compared to [15] since hy- perparameter tuning would not lead to overfitting on the test speakers. However, our AAI method still obtains state-of-the- art Pearson correlation scores with this dataset, as detailed in Section 5. For our train-val split, we use the same 90%-10% approach as the MOCHA-TIMIT split in Section 2.1 above. 2.3. Combined EMA Dataset To train more generalizable models using a larger articula- tory dataset, we combine MOCHA-TIMIT [8, 9, 10] and HPRC [11], forming a 16-speaker EMA dataset. Here, we create a train-val-test split with 1000 val and 1000 test utter- ances, randomly assigning each utterance to one of the three sets. We note that all speakers appear in all three sets. Our resynthesis"}