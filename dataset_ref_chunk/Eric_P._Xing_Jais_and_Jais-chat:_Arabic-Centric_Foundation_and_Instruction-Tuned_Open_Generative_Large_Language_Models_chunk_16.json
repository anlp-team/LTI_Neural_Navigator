{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the title of the paper by Joseph Le Roux and Michalis Vazirgiannis?", "answer": " AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " Where was the Seventh Arabic Natural Language Processing Workshop held?", "answer": " Abu Dhabi, United Arab Emirates", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " Who are the authors of the paper on Scaling laws for neural language models?", "answer": " Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " In which year was the Europarl paper published?", "answer": " 2005", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " What is the benchmark dataset introduced by Tom Kwiatkowski et al.?", "answer": " Natural Questions", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " What is the Enron corpus used for?", "answer": " Email classification research", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " Who are the authors of the paper on tokenization impacts in multilingual language modeling?", "answer": " Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " What is the main focus of the paper by Wuwei Lan et al.?", "answer": " Arabic information extraction", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " What is the title of the paper by Stephanie Lin, Jacob Hilton, and Owain Evans?", "answer": " TruthfulQA: Measuring how models mimic human falsehoods", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}, {"question": " What is the aim of the Bactrian-X model by Haonan Li et al.?", "answer": " A multilingual replicable instruction-following model with low-rank adaptation", "ref_chunk": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}], "doc_text": "Joseph Le Roux, and Michalis Vazirgiannis. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the Seventh Arabic Natural Language Processing Workshop, WANLP, pages 31\u2013 42, Abu Dhabi, United Arab Emirates, 2022. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [Koe05] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Machine Translation summit, volume 5, pages 79\u201386, 2005. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. 30 [KY04] Bryan Klimt and Yiming Yang. The Enron corpus: A new dataset for email classification research. In Proceedings of the European Conference on Machine Learning, ECML, pages 217\u2013226, Pisa, Italy, 2004. [LBM23] Tomasz Limisiewicz, Ji\u02c7r\u00b4\u0131 Balhar, and David Mare\u02c7cek. Tokenization impacts multilingual lan- guage modeling: Assessing vocabulary allocation and overlap across languages. In Findings of the Association for Computational Linguistics, ACL, pages 5661\u20135681, Toronto, Canada, 2023. [LCXR20] Wuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing, EMNLP, pages 4727\u20134734, Online, 2020. [LH18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, ICLR, Vancouver, VC, Canada, 2018. [LHE22] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu- man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL, pages 3214\u20133252, Dublin, Ireland, 2022. [LKW+23] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-X: A multilingual replicable instruction-following model with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [LLG+19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. [LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. ChatGPT as a factual inconsistency evalu- ator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023. [LXL+17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale In Proceedings of the 2017 Conference ReAding comprehension dataset from examinations. on Empirical Methods in Natural Language Processing, EMNLP, pages 785\u2013794, Copenhagen, Denmark, 2017. [LZK+23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timo- thy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv: 2306.09212, 2023. [MBMSK18] Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. SemEval-2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, SemEval, pages 1\u201317, New Orleans, Louisiana, 2018. [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor con- duct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 2381\u20132391, Brussels, Belgium, 2018. [MDM+20] Hamdy Mubarak, Kareem Darwish, Walid Magdy, Tamer Elsayed, and Hend Al-Khalifa. Overview of OSACT4 Arabic offensive language detection shared task. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 48\u201352, Marseille, France, 2020. [MGU+22] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. Quantifying privacy risks of masked language models using membership inference at- tacks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro- cessing, EMNLP, pages 8332\u20138347, Abu Dhabi, United Arab Emirates, 2022. 31 [MTG+23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self- Refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. [MWS+23] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetun- ing. arXiv preprint arXiv:2211.01786, 2023. [MWZ+19] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin- son, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220\u2013229, 2019. [NAME+22] El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Alcoba Inciarte, and Md Tawkat Islam Khondaker. JASMINE: Arabic GPT models for few-shot learning. arXiv preprint arXiv:2212.10755, 2022. [NEAM22] El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. AraT5: In Proceedings of the 60th Annual Text-to-text transformers for Arabic language generation. Meeting of the Association for Computational Linguistics, ACL, pages 628\u2013647, Dublin, Ireland, 2022. [NVBB20] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge In Proceedings of the 2020 dataset for measuring social biases in masked language models. Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1953\u20131967, Online, 2020. [OEB+19] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Proceedings and Michael Auli. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), NAACL, pages 48\u201353, Minneapolis, MN, USA, 2019. fairseq: A fast, extensible toolkit for sequence modeling. [Ope23] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser"}