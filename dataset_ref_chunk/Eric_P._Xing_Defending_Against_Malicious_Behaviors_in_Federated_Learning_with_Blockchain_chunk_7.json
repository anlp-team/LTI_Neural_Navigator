{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Defending_Against_Malicious_Behaviors_in_Federated_Learning_with_Blockchain_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the batch size used in the experiment?", "answer": " The batch size used in the experiment is 128.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What metric is used as both the local validation score and evaluation metric?", "answer": " The binary accuracy is used as both the local validation score and evaluation metric.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What library is used to implement all baselines?", "answer": " All baselines are implemented in PyTorch 1.12.1.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " How is the reward-and-slash design deployed in a private blockchain?", "answer": " The reward-and-slash design is deployed in a private blockchain using Ethereum smart contracts.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What is the value of \u03f5 that is set based on empirical experience?", "answer": " The value of \u03f5 is set to 0.05.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What is the first baseline considered, and what approach does it represent?", "answer": " The first baseline is the Oracle approach, a centralized baseline without malicious attacks, which provides the upper-bound performance of the experiment.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " How many baselines are considered in the study?", "answer": " Four baselines are considered in the study.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " In FedAVG w/ block, how are the clients utilized for training?", "answer": " In FedAVG w/ block, 90% of the clients are used as voters.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What issue can be caused by a too small \u03f5 value?", "answer": " A too small \u03f5 value can cause large oscillation, which slows the convergence.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}, {"question": " What happens to malicious voters in the system according to the results?", "answer": " Malicious voters will be eliminated from the system shortly, with their average tokens declining to 0 within approximately 40 epochs.", "ref_chunk": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}], "doc_text": "10\u22123 and batch size 128. No data aug- mentation is applied. We use the binary accuracy as both the local validation score and evaluation metric. We consider a simple data poisoning attack [14], where malicious clients are trained to confuse the model. All baselines are implemented in PyTorch 1.12.1 [43] on one NVIDIA Tesla T4 GPU. We leverage Ethereum smart contracts to deploy our reward-and- slash design in a private blockchain and simulate the training process using the Python library Web3.py5. We set \u03f5 = 0.05 5https://web3py.readthedocs.io/en/v5/ based on empirical experience.6 3) Baselines: We consider 4 baselines. The first one is an Oracle approach, a centralized baseline without malicious attacks. The Oracle should provide the upper-bound perfor- mance of the experiment. The second one is FedAVG without malicious attacks (denoted as FedAVG w/o mal), which is equivalent to FedAVG under \u03b7 = 0 and should provide the upper-bound performance for a decentralized environment. The third one is FedAVG under malicious attacks (denoted as FedAVG w/ mal), where \u03b7 of clients are malicious. The fourth one is the proposed method, FedAVG with blockchain under malicious attacks (denoted as FedAVG w/ block). For FL baselines, 10% of clients are randomly selected to perform local training at each epoch. For FedAVG w/ block, we simply use the remaining 90% of the clients as voters. 6We notice that too small \u03f5 can cause large oscillation, which slows the convergence, and too large \u03f5 can facilitate the convergence at the expense of decreased detection performance, i.e. the system fails to remove the majority of malicious clients. 7 8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 TABLE I PERFORMANCE COMPARISON UNDER DIFFERENT VALUES OF THE RATIO OF MALICIOUS CLIENTS (\u03b7). THE REPORTED NUMBERS OF THE PERFORMANCE ARE MEAN AND STANDARD DEVIATION UNDER 5 RANDOM SEEDS. Model FedAVG w/ mal FedAVG w/ block (Ours) FedAVG w/o mal Oracle \u03b7 = 0.1 0.963 \u00b1 0.017 0.965 \u00b1 0.008 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.2 0.946 \u00b1 0.034 0.969 \u00b1 0.003 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.3 0.801 \u00b1 0.222 0.952 \u00b1 0.020 0.975 \u00b1 0.004 0.971 \u00b1 0.007 \u03b7 = 0.4 0.709 \u00b1 0.266 0.955 \u00b1 0.021 0.975 \u00b1 0.004 0.971 \u00b1 0.007 50 Malicious, p = 8 0 150 40 Average Tokens 60 200 Honest, p = 8 80 100 Epoch Honest, p = 8 40 Malicious, p = 8 200 100 150 Epoch 80 50 60 0 Average Tokens 150 Epoch Malicious, p = 8 100 80 100 Average Tokens 0 200 50 60 Honest, p = 8 20 40 Honest, p = 8 50 150 0 20 Average Tokens Epoch 200 80 60 100 100 Malicious, p = 8 40 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 7. Token distribution results for clients when setting the parameter for slashing proposers as \u03b3p = 8. The expected average token of malicious proposers fluctuates down during the training process. 40 p = 8 0 100 150 80 p = 4 200 Epoch 60 Malicious Client Tokens p = 32 p = 2 20 50 p = 16 50 p = 4 p = 32 Malicious Client Tokens 40 100 100 200 60 p = 8 0 p = 16 20 80 150 Epoch p = 2 100 p = 16 0 p = 4 50 40 Malicious Client Tokens 100 80 p = 32 p = 2 Epoch 20 60 0 200 150 p = 8 200 p = 2 50 p = 4 Malicious Client Tokens 40 60 120 20 Epoch p = 8 p = 32 80 0 100 p = 16 0 150 100 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 8. Token distribution results for malicious clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of malicious proposers exhibits a higher rate of decrease when a large value of \u03b3p is selected. 150 100 p = 4 Epoch 0 p = 8 40 Honest Client Tokens 100 p = 32 80 60 200 p = 2 p = 16 50 p = 16 50 60 120 200 100 80 0 p = 2 40 Honest Client Tokens 100 150 p = 8 Epoch p = 4 p = 32 160 100 150 p = 8 80 0 p = 16 p = 32 Epoch 140 40 200 120 p = 4 50 p = 2 Honest Client Tokens 60 100 p = 32 40 200 50 p = 2 160 150 p = 8 180 0 p = 16 100 Honest Client Tokens Epoch p = 4 60 200 100 120 80 140 (a) \u03b7 = 0.1. (b) \u03b7 = 0.2. (c) \u03b7 = 0.3. (d) \u03b7 = 0.4. Fig. 9. Token distribution results for honest clients when choosing \u03b3p = 2, 4, 8, 16, and 32. The expected average token of honest proposers displays a higher rate of growth when a large value of \u03b3p is selected. B. Results malicious voters will be eliminated from the system shortly (i.e. their average tokens decline to 0 within \u2248 40 epochs). 1) Empirical Analysis on Malicious Voters: To empirically validate the theoretical result in Sec. IV-E, we first simulate a hypothetical scenario where there are only honest proposers. As there are more honest proposers than malicious proposers at each round on average, the effect of malicious weights can be seen as slowing the convergence and decreasing the global performance, which will be validated in Sec. V-A3. Note, due to A4, we further simplify the scenario to focus on the behavior of malicious voters. As shown in Fig. 4, given the set of the hyperparameter for slashing voters \u03b3r = {2, 4, 8, 16, 32}, the 2) Comparison with Baselines: Following Theorem 1 and Sec. V-B1, we now are certain that there will be no de facto"}