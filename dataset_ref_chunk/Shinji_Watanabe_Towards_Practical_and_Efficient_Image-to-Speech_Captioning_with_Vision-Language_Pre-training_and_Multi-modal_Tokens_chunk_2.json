{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_Towards_Practical_and_Efficient_Image-to-Speech_Captioning_with_Vision-Language_Pre-training_and_Multi-modal_Tokens_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main objective of the learning problem in the Im2Sp model?", "answer": " To translate the input image into speech that correctly describes the image content. ", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " How does using discovered discrete acoustic units instead of directly predicting continuous speech features improve the performance of Im2Sp?", "answer": " It allows the model to focus more on the linguistic modeling of speech while suppressing other factors in the speech.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " What is the main difference in utilizing speech units discovered from the HuBERT model compared to previous methods?", "answer": " It eliminates the need for complex processes involving predicting the Mel-spectrogram and converting the raw waveform.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " What is the role of the image encoder \u03a6 in the Im2Sp model?", "answer": " It extracts visual features from the input image.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " How does the speech decoder \u03a8 in the Im2Sp model generate speech units?", "answer": " It generates speech units describing the visual features in an autoregressive manner until End of Sequence (EOS) is predicted.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " What is attached to the visual features before feeding them into the speech decoder in the Im2Sp model?", "answer": " An embedding of the Beginning of Sequence (BOS) token.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " How is the objective function of the proposed Im2Sp model represented?", "answer": " argmax \u03b8 S \u03a3k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the previous prediction, and \u03b8 is the model parameters.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " What is the motivation behind incorporating vision-language pre-training into the Im2Sp model?", "answer": " To bring knowledge of image comprehension and language generation from a pre-trained vision-language model into the Im2Sp model.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " How does the utilization of image units instead of raw images contribute to efficient image-to-speech captioning?", "answer": " It reduces data storage and computational memory costs for multi-modal processing systems.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}, {"question": " Why is training large-scale multi-modal speech processing systems more challenging than NLP systems?", "answer": " Because they require much more data storage and computational memory costs due to utilizing visual and audio modalities.", "ref_chunk": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}], "doc_text": "state-of-the-art neural MOS evaluation [26, 27], we show the proposed Im2Sp model can generate natural speech with having the correct description for input images. 2. METHOD Fig. 1 shows the proposed Im2Sp framework. Let x \u2208 RH\u00d7W \u00d7C be an input image and y \u2208 RT be the ground-truth speech caption with a sample rate of 16kHz. Here, H, W , and C represent the image size of height, width, and channel, respectively, and T repre- sents the length of the waveform. The main objective of our learning problem is to translate the input image x into speech y that correctly describes the image content. To improve the performance of Im2Sp, we propose leveraging the knowledge of a pre-trained model trained on large-scale image-text data. Moreover, we improve the efficiency of the Im2Sp model by introducing multi-modal tokens. The details of the proposed method are described in the following subsections. 2.1. Speech Unit Extraction Previous Im2Sp methods [5, 28, 29] showed that by utilizing discov- ered discrete acoustic units instead of directly predicting continuous speech features (e.g., Mel-spectrogram), we can improve the perfor- mance of Im2Sp. This is because by extracting the discrete acoustic units from the speech, we can focus more on the linguistic modeling of speech while suppressing the other factors in the speech [6, 10]. Different from the previous works [28, 29] that utilize discrete acoustic units derived from Mel-spectrogram such as the code- book of VQ-VAE, we utilize speech units, discovered from the recent self-supervised speech model, HuBERT [14]. Hence, we eliminate the need for complex processes involving predicting the Mel-spectrogram from discrete acoustic units and converting the raw waveform from the predicted Mel-spectrogram, as required by previous methods. Instead, we can directly convert the waveform from the speech units by utilizing a speech unit-based vocoder [30, 31], with even more natural speech sound. Specifically, we extract speech features using a pre-trained HuBERT [14] and perform K- means clustering to obtain the discretized units, following [6]. Then, we remove sequential repetitions of the units and finally obtain our speech units u \u2208 {1, . . . , Nu}S which will be used for the proposed Im2Sp. Here, Nu and S represent the token size and length of speech units, respectively. As HuBERT downsamples the raw audio y by a factor of 320, our speech units u have a much lower frame rate than the raw audio (i.e., S < T /320). 2.2. Image-to-Speech with Vision-Language Pre-training Fig. 1b shows the overall architecture of the proposed Im2Sp model. It is mainly composed of an image encoder \u03a6 and a speech de- coder \u03a8. The image encoder is designed with Vision Transformer (ViT) [32] which is showing promising results in diverse vision tasks [33]. When an input image x is given, the image encoder \u03a6 extracts the visual features fv by downsampling the spatial size as follows, fv = \u03a6(x) \u2208 R(H/P \u2217W/P +1)\u00d7D, where P represents the patch size of ViT, D represents the embedding dimension, and the additional 1 dimension (i.e., +1) comes from the attached CLS token. By treating TextEmbedding Quantizer ImageEncoder \u03a6Input Image ImageEncoder \u03a6Input Image Speech Classifier : BOS: EOS : BOS: EOS Decoder \u03a8(Transformer) Decoder \u03a8(Transformer) a zebra grazing in some green grassOutput Text(a) Vision-Language Pre-training(b) Training Image-to-Speech Captioning a zebra grazing in some green grassText AudioEmbedding Tokenizer Output Speech Unit 25314832371645836 Classifier Fig. 1. Illustration of the proposed image-to-speech captioning framework. (a) By employing the vision-language pre-training strat- egy, (b) we can bring the learned knowledge of image comprehen- sion and language generation into our image-to-speech captioning. the flattened spatial region of fv as a sequence, it is employed as a visual condition for the speech decoder \u03a8. Therefore, the speech de- coder \u03a8 can generate the speech units u describing the conditioned visual features fv. After the visual features fv, an embedding of BOS (Beginning of Sequence) token is attached and the speech de- coder predicts the speech units u in an autoregressive manner until EOS (End of Sequence) is predicted. The objective function of the proposed Im2Sp can be represented as follows, argmax \u03b8 S (cid:88) k=1 log p(uk|u<k, x; \u03b8), where uk represents the current prediction, u<k represents the pre- vious prediction, and \u03b8 is the model parameters including image en- coder, text decoder, and embedding layers for speech units. Motivated by the recent progress in vision-language pre-training (Fig. 1a) [20, 21], we try to bring the image understanding knowl- edge and language generation knowledge of the large-scale pre- trained vision-language model into our Im2Sp model. Hence, we can alleviate the limitation in the Im2Sp task, where there is rela- tively limited availability of paired image and human spoken speech compared to the abundance of image-text paired data. Specifically, both the image encoder and the speech decoder are initialized from a pre-trained vision-language model, GiT [21]. GiT is pre-trained with text generation from images, thus the model knows how the image can be comprehended and can be described in language. Please note that the weight of the speech decoder is initialized with the text decoder of GiT. As the speech units mainly hold linguistic information [9, 15], we can transfer the language modeling ability of the pre-trained text decoder of the vision-language model into our spoken language generation [9]. The knowledge transferring from the vision-language model into the Im2Sp model is shown in Fig. 1. 2.3. Efficient Image-to-Speech Captioning with Image Units Multi-modal processing systems, especially utilizing visual and au- dio modalities, require much more data storage and computational (1) Image Units Tokenizer(ViT-VQGAN) 1524563239395353306879303693365486496301 \u2026 ImageEncoder \u03a6Image Fig. 2. The extraction of image units by using vector quantization. Extracted image units are utilized for inputs instead of raw images. memory costs than text-only systems. This is why training large- scale multi-modal speech processing systems is significantly more challenging than NLP systems, with most of the development tak- ing place in the industry. These days, [3, 34] showed that we can represent the image with compressed discrete representations while"}