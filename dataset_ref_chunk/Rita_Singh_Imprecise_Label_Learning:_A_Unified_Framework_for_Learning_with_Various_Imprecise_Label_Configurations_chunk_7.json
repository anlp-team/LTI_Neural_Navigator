{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_7.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What datasets were used for evaluation in the study?,        answer: CIFAR-10 and CIFAR-100    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " How were the labeled datasets sampled in the study?,        answer: The labeled dataset was first sampled and treated other samples as unlabeled.    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What is the purpose of generating partial labels in the study?,        answer: To generate partial labels and randomly corrupt the true label of the partial labels.    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What is the range of the number of labels (l) for CIFAR-10 in the study?,        answer: l \u2208 {1000, 5000, 50000}    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What does the \u03b7 parameter represent in the study?,        answer: The noise ratio for noisy labels    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " Which methods were compared with the proposed method in Table 4?,        answer: PiCO+, IRNet, and DALI    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What was the main outcome when comparing the proposed method with previous methods in the study?,        answer: The proposed method achieved the best performance    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What factor demonstrated the effectiveness of the proposed method in dealing with mixed imprecise labels?,        answer: Outperforming PiCO+ and DALI with mixup    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What type of algorithm was employed by the proposed method for maximum likelihood estimation?,        answer: Expectation-maximization (EM) algorithm    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}, {"question": " What is the potential broader impact of the ILL framework according to the study?,        answer: To transform domains where obtaining precise labels poses a challenge    ", "ref_chunk": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}], "doc_text": "76.52 77.53\u00b10.24 70.22 70.10 71.98 74.31 76.55 76.96\u00b10.02 66.14 68.77 71.04 71.79 76.09 76.43\u00b10.27 PiCO+ (Wang et al., 2022b) IRNet (Lian et al., 2022b) DALI (Xu et al., 2023) PiCO+ Mixup (Xu et al., 2023) DALI Mixup (Xu et al., 2023) Ours 0.3 92.32 92.81 93.44 94.02 95.52 96.2\u00b10.02 92.22 92.18 93.25 94.03 95.41 95.87\u00b10.14 89.95 91.35 92.42 92.94 94.67 95.22\u00b10.06 0.05 69.40 70.73 72.28 73.06 76.87 77.07\u00b10.16 66.67 69.33 71.35 71.37 75.23 76.34\u00b10.08 62.24 68.09 70.05 67.56 74.49 75.13\u00b10.63 Table 5: Robust test accuracy results of our method on more mixture of imprecise label configura- tions. l, q and \u03b7 are the number of labels, partial, and noise ratio. l q \u03b7=0.0 CIFAR10 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 l q \u03b7=0.0 CIFAR100 \u03b7=0.1 \u03b7=0.2 \u03b7=0.3 5,000 0.1 0.3 0.5 95.29\u00b10.18 95.13\u00b10.16 95.04\u00b10.10 93.90\u00b10.11 92.95\u00b10.37 92.18\u00b10.52 92.02\u00b10.22 90.14\u00b10.61 88.39\u00b10.62 89.02\u00b10.63 87.31\u00b10.27 83.09\u00b10.56 10,000 0.01 0.05 0.10 69.90\u00b10.23 69.85\u00b10.20 68.92\u00b10.45 68.74\u00b10.15 68.08\u00b10.28 67.15\u00b10.63 66.87\u00b10.34 66.78\u00b10.43 64.44\u00b11.29 65.34\u00b10.02 64.83\u00b10.17 60.26\u00b11.96 1,000 0.1 0.3 0.5 94.48\u00b10.09 94.35\u00b10.05 93.92\u00b10.29 91.68\u00b10.17 89.94\u00b11.90 86.34\u00b12.37 87.17\u00b10.51 82.06\u00b11.52 70.86\u00b12.78 81.04\u00b11.13 69.20\u00b12.16 38.19\u00b16.55 5,000 0.01 0.05 0.10 65.66\u00b10.27 65.06\u00b10.04 63.32\u00b10.55 63.13\u00b10.27 62.28\u00b10.47 58.73\u00b11.33 60.93\u00b10.17 58.92\u00b10.34 53.27\u00b11.57 58.36\u00b10.56 53.24\u00b11.69 46.19\u00b11.04 4.4 MIXED IMPRECISE LABEL LEARNING Setup. We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set l \u2208 {1000, 5000, 50000} for CIFAR-10, and l \u2208 {5000, 10000, 50000} for CIFAR-100. For partial labels, we set q \u2208 {0.1, 0.3, 0.5} for CIFAR-10, and q \u2208 {0.01, 0.05, 0.1} for CIFAR-100. For noisy labels, we set \u03b7 \u2208 {0, 0.1, 0.2, 0.3} for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ (Wang et al., 2022b), IRNet (Lian et al., 2022b), and DALI (Xu et al., 2023).6 Results. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more complicated data annotations. 5 CONCLUSION We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels Y , imprecise label information I, and data X. It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential broader impact of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three 6Although there are also prior efforts on partial semi-supervised learning (Wang et al., 2019b; Wang & Zhang, 2020), they do not scale on simple dataset even on CIFAR-10. Thus, they are ignored for comparison. 9 Preprint imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning (Ilse et al., 2018) and multi-label crowd-sourcing learning (Ibrahim et al., 2023). However, it is also crucial to acknowledge the limitations of the ILL framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We anticipate that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field. 10 Preprint REFERENCES Yelp dataset: http://www.yelp.com/dataset_challenge. URL http://www.yelp.com/ dataset_challenge. Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In ECAI, volume 2, pp. 11, 2002. Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. Uncertainty in Artificial Intelligence, pp. 236\u2013246. PMLR, 2021. Eric Arazo, Diego Ortego, Paul Albert, Noel E. O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019. Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138. IEEE, 2020. Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021. Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2682\u20132686, 2016. doi: 10.1109/ICASSP.2016.7472164. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2019a. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raf- fel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019b. David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain"}