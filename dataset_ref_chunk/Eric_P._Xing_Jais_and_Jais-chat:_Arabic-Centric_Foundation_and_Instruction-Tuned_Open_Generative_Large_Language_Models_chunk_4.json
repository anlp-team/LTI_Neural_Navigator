{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is EuroParl and what was it initially introduced for?", "answer": " EuroParl is a multilingual parallel corpus initially introduced for machine translation.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What does the NIH Grant Abstracts dataset include?", "answer": " The NIH Grant Abstracts dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What is the GitHub dataset and why was it included?", "answer": " The GitHub dataset consists of a large collection of open-source code repositories and was included to improve the model\u2019s downstream performance on code-related tasks.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What are some standard preprocessing steps mentioned in the text?", "answer": " Some standard preprocessing steps mentioned in the text include filtering, normalizing, and cleaning.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What is the domain distribution in the mixed pre-training dataset?", "answer": " The mixed pre-training dataset consists of Arabic (29%), English (59%), and programming code (12%).", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What is the purpose of the detokenization step in the preprocessing pipeline?", "answer": " The purpose of the detokenization step is to achieve consistency by leaving non-tokenized input unchanged.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " How is the Arabic text normalized in the preprocessing pipeline?", "answer": " The Arabic text is normalized using the Camel toolset for Arabic, which includes removing non-printable Unicode characters and rare diacritic marks.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What is the approximate balance suggested by the Chinchilla scaling law between model size and data?", "answer": " The Chinchilla scaling law suggests an optimal balance of approximately twenty tokens per parameter.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What informed many of the choices of heuristics used in the final Arabic preprocessing pipeline?", "answer": " Experimentation with smaller LLMs informed many of the choices of heuristics used in the final Arabic preprocessing pipeline.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}, {"question": " What is a commonly reported phenomenon in LLM research regarding model performance?", "answer": " A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones.", "ref_chunk": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}], "doc_text": "Total 278 Table 2: Composition and breakdown of our English and programming code datasets. EuroParl is a multilingual parallel corpus initially introduced for machine translation [Koe05], but has also been utilized in several other fields of NLP [GW06, VH08, CDS17]. The version used in this work consists of the proceedings of the European Parliament in 21 European languages from 1996 until 2012. PhilPapers: A collection of open-access philosophy publications from the Center for Digital Philosophy, University of Western Ontario.16 YouTube Subtitles: This dataset consists of text from human-generated closed captions on YouTube17. It provides not only multilingual data, but also a variety of content including educational material, popular culture, and natural dialogue. NIH Grant Abstracts: This dataset includes abstracts of awarded applications from the EXPORTER service, covering fiscal years 1985-present. It was included because it features high-quality scientific writing.18 Enron Emails: This dataset [KY04] is widely used for analyzing email usage patterns. It was included to aid in understanding the modality of email communications, which is typically not found in other datasets. GitHub: This dataset19 consists of a large collection of open-source code repositories [BMR+20]. It was included to improve the model\u2019s downstream performance on code-related tasks, given GPT-3\u2019s ability to generate plausible code completions without any explicitly gathered code datasets. Table 3 summarizes the composition of our dataset: a total of 395B tokens, including Arabic, English, and programming code. 2.1 Preprocessing Pipeline Preprocessing, which includes filtering, normalizing, and cleaning, has been shown to be a vital step in training high-quality LLMs. We apply several standard preprocessing steps, combined with modules targeted at getting high-quality Arabic content, in a data processing pipeline to generate our Arabic dataset of 72B tokens. 16https://philpapers.org/ 17https://github.com/sdtblck/youtube_subtitle_dataset 18https://exporter.nih.gov/ 19https://github.com/EleutherAI/github-downloader 7 Domain Original + Translation + Upsampling Percentage Arabic English Programming code 55B 232B 46B 72B 232B 46B 116B 232B 46B 29% 59% 12% Total 395B 100% Table 3: Distribution of the three primary domains in our mixed pre-training dataset: we first augment the Arabic data by adding 18B translated tokens, and then upsample the resulting Arabic dataset 1.6 times. (The numbers 72B and 395B are correct, and the summation discrepancies are due to rounding.) An outline of our preprocessing pipeline for Arabic is provided in Figure 2. As explained above, the raw data is primarily sourced from publicly available databases, such as Abu El Khair or BAAI, as well as through in-house web scraping and machine translation of high-quality English sources. Given that some of these sources have already been preprocessed or tokenized for NLP applications, it is essential to standardize our input. We thus subject all sources to an initial detokenization step (which leaves non-tokenized input unchanged) to achieve consistency. A document, at this step, is one article/web page, depending on the source. We then apply a large number of filtering rules in order to eliminate documents that are noisy or low-quality. This includes removing extremely short or very long documents, or those that do not include a sufficiently high proportion of Arabic characters or sentences, which could be indicators of a document in a different language where Arabic characters appear only incidentally. We also remove documents that contain words more than 100 characters long, which can indicate the presence of extremely long URLs and/or an otherwise noisy document. Once a document has passed the filtering step, it is subject to cleaning and normalization. We remove non- printable Unicode characters and rare diacritic marks, and normalize the text using the Camel toolset for Arabic [OZK+20]. We remove embedded JavaScript and HTML (which are common sources of noise in web-scraped datasets), and highly-frequent words and phrases (which are typically boilerplate text, such as a news channel name). We normalize Arabic punctuation marks, and use a lightweight n-gram LM to further identify and remove noisy n-grams. Finally, we apply a fuzzy deduplication step using standard locality-sensitive hashing techniques. After this deduplication step, the size of the English dataset was about 20% of the original. Translation Removing data with noise Deduplication Publicly availabledataset (Open-Source) Replace ? with \u061f Frequently occurringword removal All words chars < 100 Arabic textNormalization < 20% Cleaneddata Diacritics removal Retrieving original text fromtokenized dataset Main sources of dataNLP/ Engg. / HPCNLP/ Engg. / HPCNLP/ Engg.NLPNLPNLP / Engg. Detokenisation Web Scraping > 20 CleaningNLP Cleansing using standardand language speci\ufb01c rules Number of words > 70% Citations removal Punctuationsand Numbers Special Symbol Arabic sentences Consecutive duplicatesentence removal Noisy n-gram removal Javascript and HTMLremoval Filtering < 10% Raw Data Incomprehensibleunicode \ufb01xation Removing highly similar orduplicate text in corpus > 50% Arabic chars Figure 2: Our Arabic preprocessing pipeline. 8 Vocabulary Vocab Size English Arabic Code GPT-2 BERT Arabic BLOOM Jais 50,257 32,000 250,000 84,992 1.095 1.632 1.083 1.010 4.171 1.125 1.195 1.050 1.294 1.313 1.000 1.006 Table 4: Fertility scores of Jais tokenizer measured against tokenizers of other systems on English, Arabic, and code validation datasets. Things were more challenging for Arabic. Unlike English, where several large-scale and open-access datasets already exist, and established preprocessing pipelines are available, for Arabic, this pipeline had to be custom-built. Experimentation with smaller LLMs informed many of the choices of heuristics we used in our final preprocessing pipeline. Given the limited amount of available Arabic data, we took care not to filter Arabic content as aggressively as for English. 2.2 Mixing Arabic and English Data A commonly reported phenomenon in LLM research is that larger LLMs generally perform better than smaller ones; this trend is clearly visible on public LLM leaderboards20 and is also evident in the recent LLaMA2 release [TMS+23].21 In general, the quality of a model is limited by two main factors: (i) data availability, and (ii) computational cost. While the latter can be overcome with improved hardware, the former is a fundamental obstacle. The Chinchilla scaling law [HBM+22] tells us that the optimal balance between model size and data is approximately twenty tokens per parameter. This is why for English, the largest open-source LLMs until recently had about 30B parameters, as publicly available datasets such as Red"}