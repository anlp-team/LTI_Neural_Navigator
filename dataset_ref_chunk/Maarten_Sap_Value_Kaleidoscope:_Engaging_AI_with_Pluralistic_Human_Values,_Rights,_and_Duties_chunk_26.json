{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Maarten_Sap_Value_Kaleidoscope:_Engaging_AI_with_Pluralistic_Human_Values,_Rights,_and_Duties_chunk_26.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What were the two main reasons for creating the VALUEPRISM dataset?", "answer": " 1) To understand pluralistic human values, rights, and duties in large language models, and 2) To support open, value pluralistic modeling", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What is the recommendation for the use of the dataset in real-world applications?", "answer": " The dataset should only be used for research purposes and should not be used for real-world decision-making, advice, or commercial applications", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " Who funded the creation of the dataset?", "answer": " Funding came from the DARPA ITM program and the Allen Institute for AI (AI2)", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What is the type of instances found in the dataset?", "answer": " Situations in plain-text English spans containing candidate values, rights, duties, valence relations, and explanations", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What are the recommended data splits for evaluation measures?", "answer": " Training, validation, and testing splits are recommended. Accuracy for valence and relevance, and perplexity for generation and explanation", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What models were initially trained on the dataset?", "answer": " T5-based models were trained and assessed for valence, relevance, generation, and explanation", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " How was the data collected for the dataset?", "answer": " Situations were provided by volunteer users of the Delphi user demo, and candidate values, rights, duties were generated by GPT-4", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What were the results of the human studies conducted on the dataset?", "answer": " Crowdworkers found the data to be high-quality 91% of the time and had difficulty surfacing missed values, rights, or duties", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " What is the licensing status of the dataset?", "answer": " Intended for research use only under the []\u2212T aylor license", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}, {"question": " Are relationships between instances made explicit in the data?", "answer": " No, there are no relationships between instances beyond the task-specific connections", "ref_chunk": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}], "doc_text": "the value, right, or duty might connect to the action. Your job is to determine if this explanation is Right Figure 15: Relevance, explanation, and valence annotation MTurk template. N Data Sheet Here we include a Datasheet for Datasets (Gebru et al. 2018) to document the dataset. N.1 Motivation for Dataset Creation Why was the dataset created? VALUEPRISM was created 1) to understand what plural- istic human values, rights, and duties are already present in large language models, and 2) to serve as a resource to to support open, value pluralistic modeling (e.g., KALEIDO). It contains human-written situations about which to rea- son and machine-generated candidate values, rights, duties, along with their valences and post-hoc explanations relating them to the situations. What (other) tasks could the dataset be used for? The situations could also be used as a rich, diverse dataset of mostly everyday situations for further decision-making work. Are there obvious tasks for which it should not be used? The dataset should only be used for research purposes, and should not be used for real-world decision-making, ad- vice, or commercial applications. Has the dataset been used for any tasks already? The dataset has only been used so far to train KALEIDO. If so, where are the results so others can compare? Results in body of this paper. Who funded the creation of the dataset? Funding for this dataset came from the DARPA ITM pro- gram and the Allen Institute for AI (AI2). If there is an associated grant, provide the grant number. FA8650-23-C-7316 N.2 Dataset Composition What are the instances? Are there multiple types of in- stances? Situations are plain-text English spans. Each one contains several candidate values, rights, and duties, along with a va- lence relation (supports, opposes, either) and a free-text ex- planation. Statistics are found in Table 6. For seq2seq training, we take this data to make 4 subtask splits: generation of a relevant value, right, or duty from a situation, valence of a value, right, or duty in relation to a situation, an explanation of how a value, right, or duty may connect to a situation, and a set of positive and negative pairs for determining whether a value, right, or duty is relevant for a given action. For relevance, we use \u201cwas generated\u201d as a proxy for relevant, and negatively sample values, rights, and duties that were generated for other situations. Statistics can be found in 7. Are relationships between instances made explicit in the data? There are no relationships between instances beyond the fact that each situation has several seq2seq tasks, which can be trivially reconstructed. How many instances of each type are there? Statistics in Table 7. What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)? Features/at- tributes? Situations are raw free-text, but the rest of the dataset is structured. All values, rights, and duties are free-text con- nected to a situation, along with a corresponding type (ei- ther \u201cValue\u201d, \u201cRight\u201d, or \u201cDuty\u201d); valences are connected to a situation and specific value, right, or duty, and are of types \u201cSupports\u201d, \u201cOpposes\u201d, or \u201cEither\u201d supports or opposes; rel- evances are connected to a situation and specific value, right, or duty, and are of type \u201cYes\u201d or \u201cNo\u201d; and explanations are free-text associated with a situation and particular value, right, or duty. Is there a label/target associated with instances? If the instances are related to people, are subpopulations iden- tified (e.g., by age, gender, etc.) and what is their distri- bution? There are labels associated with instances for valence and relevance. The instances are not related to people. Is everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external re- sources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data? Everything is included and does not rely on external re- sources. We intend the data for research use only under the []\u2212T aylor license Are there recommended data splits or evaluation mea- testing; accura- sures? (e.g., cy/AUC) training, development, Yes, there are recommended training, validation, and test- ing splits. We recommend and report accuracy for valence and relevance, and perplexity for generation and explana- tion. What experiments were initially run on this dataset? Have a summary of those results and, if available, pro- vide the link to a paper with more information here. T5-based models were trained on splits of this data and were tested on both the synthetic data (Section D) and were assessed by humans (Sections 5.1 and 3). While the inter- ested reader should defer to the paper for more results, hu- mans found that the distilled models matched the test out- put quality for valence and explanation, surpassed the test quality for generating sets of values, rights, and duties, and output relevances that correlated with human judgments. We also run two human studies on the dataset (Sections 4.1 and E). Crowdworkers agree the data is high-quality 91% of the time, and have trouble surfacing values, rights, or duties that are missed, providing suggestions less than 1% of the time. Additionally, in an attempt to understand if the dataset aligns best with any demographic groups, we re- cruit 613 crowdworkers to mark personal agreement with the data, and do not find significant takeaways for which groups are represented best in the data. N.3 Data Collection Process How was the data collected? (e.g., hardware appara- tus/sensor, manual human curation, software program, software interface/API; how were these constructs/mea- sures/methods validated?) The situations were provided by volunteer users of the Delphi user demo, and the candidate values, rights, duties and their corresponding relations were generated by a large language model, GPT-4. Who was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?) Data was collected and by the authors of the paper. The dataset was not collected through crowdworkers, but"}