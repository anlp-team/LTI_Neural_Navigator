{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Graham_Neubig_CodeBERTScore:_Evaluating_Code_Generation_with_Pretrained_Models_of_Code_chunk_4.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the correlation metrics mentioned in Table 1?", "answer": " BLEU, CodeBLEU, ROUGE-1, ROUGE-2, ROUGE-L, METEOR, chrF, CrystalBLEU", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " Which metric achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correctness across all four languages?", "answer": " METEOR", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " In which languages does CodeBERTScore achieve a comparable correlation with METEOR?", "answer": " Java and JavaScript", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " What does Table 2 show in terms of correlation metrics?", "answer": " The correlation with human preference", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " Which metric achieves the highest correlation with human preference according to the text?", "answer": " CodeBERTScore", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " How does CodeBERTScore perform compared to chrF and ROUGE-L in evaluating code generation models?", "answer": " CodeBERTScore outperforms chrF and ROUGE-L by a significant margin", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " Can CodeBERTScore be used in a new language without a language-specific model?", "answer": " Generally, CodeBERT-base achieves close performance to a language-specific model", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " What does Figure 4 depict in terms of model comparison?", "answer": " The Kendall-Tau and Spearman correlation on different datasets with language-specific pretrained model and with the base CodeBERT", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " What do the results in Figure 5 suggest about transformer layers?", "answer": " Higher layers encode the semantic information more accurately, but the final layers may be more task-specific", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}, {"question": " How does using context affect the correlation of CodeBERTScore, according to the text?", "answer": " Using context increases the correlation from 0.50 to 0.52", "ref_chunk": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}], "doc_text": "correlation between different metrics 4https://huggingface.co/datasets/nuprl/MultiPL-E 5https://octoverse.github.com/2022/ top-programming-languages Metric \u03c4 Java rs \u03c4 C++ rs Python rs \u03c4 JavaScript rs \u03c4 BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU .481 .496 .516 .525 .508 .558 .532 .471 .361 .324 .318 .315 .344 .383 .319 .273 .112 .175 .262 .270 .258 .301 .319 .046 .301 .201 .260 .273 .288 .321 .321 .095 .393 .366 .368 .365 .338 .418 .394 .391 .352 .326 .334 .322 .350 .402 .379 .309 .248 .261 .279 .261 .271 .324 .302 .118 .343 .299 .280 .292 .293 .415 .374 .059 CodeBERTScore .553 .369 .327 .393 .422 .415 .319 .402 Table 1: Kendall-Tau (\u03c4 ) and Spearman (rs) correlations of each metric with the functional correctness on Hu- manEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3. Metric BLEU CodeBLEU ROUGE-1 ROUGE-2 ROUGE-L METEOR chrF CrystalBLEU \u03c4 .374 .350 .397 .429 .420 .366 .470 .411 rp .604 .539 .604 .629 .619 .581 .635 .598 rs .543 .495 .570 .588 .574 .540 .623 .576 achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correct- ness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surpris- ingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, Code- BERTScore is more correlated with functional correctness than all baselines. CodeBertScore .517 .674 .662 5 Analysis Table 2: The Kendall-Tau (\u03c4 ), Pearson (rp) and Spear- man (rs) correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard de- viations are provided in Table 4. We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying Code- BERTScore to new datasets and scenarios. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant mar- gin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These re- sults show that generated code that is preferred by CodeBERTScore\u2014 also tends to be preferred by human programmers. Correlation with functional correctness Ta- ble 1 shows the correlation between different met- rics and functional correctness: CodeBERTScore Can we use CodeBERTScore in a new lan- guage without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don\u2019t have a language-specific model? We com- pare the language-specific models to CodeBERT- base in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experi- ments and correlation metrics, using the language- specific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERT- base can still provide close performance even without language-specific pretraining. 0.7 0.65 0.65 \u03c4 -Lang-specific \u03c4 -Base model rs-Lang-specific rs-Base model 0.6 0.52 0.52 0.56 0.56 0.5 0.46 0.45 0.4 0.3 0.34 0.35 0.33 0.32 0.38 0.39 0.37 0.35 0.37 0.38 0.34 0.3 0.2 0.1 0 CoNaLa HumanEval-Python HumanEval-Java HumanEval-C++ HumanEval-JavaScript Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model). 0.45 tion, while higher layers encode deeper semantic meaning in natural language. 0.4 0.35 0.3 Java C++ JavaScript Python 0.25 0 2 4 6 Layer 8 10 12 Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers. Which transformer layer should we use? We further investigate the impact of using hidden states from different layers of the model \u2014 the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The re- sults are shown in Figure 5: generally, the deeper the layer \u2013 the higher the average correlation be- tween CodeBERTScore and functional correct- ness, across all programming languages. However in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher lay- ers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow informa- Does encoding natural language context help? One major difference between CodeBERTScore and BERTScore is that CodeBERTScore lever- ages the context for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that us- ing context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natu- ral language instructions, we believe that Code- BERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or gen- erating code given the preceding code context. CodeBERTScore allows soft matching of tokens The heatmaps in Figure 6 show the sim- ilarity scores between tokens in CodeBERTScore. both shutil.rmtree and For os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens. example, In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface- form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore as- signs non-zero scores to each token with meaning- ful alignments, such as matching [sq,rt] with [_0,5], since a square root"}