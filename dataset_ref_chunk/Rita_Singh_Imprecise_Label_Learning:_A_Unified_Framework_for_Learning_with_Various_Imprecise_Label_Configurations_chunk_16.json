{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Rita_Singh_Imprecise_Label_Learning:_A_Unified_Framework_for_Learning_with_Various_Imprecise_Label_Configurations_chunk_16.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What setup is followed for experiments of SSL in the text?", "answer": " For experiments of SSL, the setup followed is training and evaluation protocols of USB on image and text classification.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " How is the labeled dataset constructed for semi-supervised learning?", "answer": " For semi-supervised learning, the labeled dataset is constructed by uniformly selecting l/C samples from each class and treating the remaining samples as the unlabeled dataset.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " What backbone is adopted for text classification tasks?", "answer": " BERT is adopted as the backbone for text classification tasks.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " Which datasets are used for image classification tasks?", "answer": " ImageNet-1K, CIFAR-100, EuroSAT, STL-10, TissueMNIST, and Semi-Aves are used for image classification tasks.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " What hyper-parameters are shown in Table 9 for vision experiments of USB?", "answer": " The hyper-parameters shown in Table 9 for vision experiments of USB include Image Size, Model, Labeled Batch size, Unlabeled Batch size, Learning Rate, Weight Decay, Layer Decay Rate, LR Scheduler, Training epochs, Classes, Model EMA Momentum, Prediction EMA Momentum, Weak Augmentation, and Strong Augmentation.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " What is the main focus of the ILL framework based on the results?", "answer": " The ILL framework demonstrates comparable performance as previous methods based on the results.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " What is discussed in Table 12 regarding error rate comparison?", "answer": " Table 12 discusses error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " What noise learning experiments are conducted following SOP?", "answer": " Experiments of noisy label learning following SOP are conducted for noisy label learning.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " How is symmetric noise introduced to CIFAR-10 and CIFAR-100?", "answer": " Symmetric noise is introduced to CIFAR-10 and CIFAR-100 by uniformly flipping labels for a probability \u03b7 into other classes.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}, {"question": " In asymmetric noise scenarios, how are the labels flipped?", "answer": " In asymmetric noise scenarios, the labels are randomly flipped for particular pairs.", "ref_chunk": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}], "doc_text": "e and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4. D.3 SEMI-SUPERVISED LEARNING D.3.1 SETUP For experiments of SSL, we follow the training and evaluation protocols of USB (Wang et al., 2022d) on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select l/C samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K (Deng et al., 2009) Vision Transformers (Dosovitskiy et al., 2020) are used, including CIFAR-100 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), STL-10 (Coates et al., 2011), TissueMNIST (Yang et al., 2021a;b), Semi-Aves (Su & Maji, 2021). For text classification tasks, we adopt BERT (Devlin et al., 2018) as backbone, including IMDB (Maas et al., 2011), Amazon Review (McAuley & Leskovec, 2013), Yelp Review (yel), AG News (Zhang et al., 2015) , Yahoo Answer (Chang et al., 2008). The hyper-parameters strictly follow USB, and are shown in Table 9 and Table 10. Table 9: Hyper-parameters of semi-supervised learning used in vision experiments of USB. Hyper-parameter CIFAR-100 STL-10 Euro-SAT TissueMNIST Semi-Aves Image Size Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 96 ViT-S-P4-32 ViT-B-P16-96 ViT-S-P4-32 32 32 32 ViT-T-P4-32 5e-4 1e-4 16 16 5e-5 5e-5 0.5 0.95 5e-4 1.0 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 20 0.95 100 10 0.0 0.999 Random Crop, Random Horizontal Flip RandAugment (Cubuk et al., 2020) 10 10 224 ViT-S-P16-224 1e-3 0.65 200 Table 10: Hyper-parameters of semi-supervised learning NLP experiments in USB. AG News Yahoo! Answer Hyper-parameter IMDB Amazom-5 Yelp-5 Max Length Model Labeled Batch size Unlabeled Batch size Learning Rate Weight Decay Layer Decay Rate LR Scheduler Training epochs Classes Model EMA Momentum Prediction EMA Momentum Weak Augmentation Strong Augmentation 5e-5 0.65 4 512 Bert-Base 4 4 1e-4 5e-5 1e-5 1e-4 0.65 0.75 \u03b7 = \u03b70 cos( 7\u03c0k 16K ) 10 0.75 10 2 5 0.0 0.999 None Back-Translation (Xie et al., 2020a) 5e-5 0.75 5 D.3.2 RESULTS In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 11 and Table 12. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods. 26 (18) Preprint Table 11: Error rate comparison of different number of labels on CIFAR-100, STL-10, EuroSAT, TissueMNIST, and SemiAves for semi-supervised learning. We use USB (Wang et al., 2022d) image classification task results. The best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets CIFAR-100 STL-10 EuroSat TissueMNIST SemiAves # Labels 200 400 40 100 20 40 80 400 3959 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) ReMixMatch (Berthelot et al., 2019a) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 33.99\u00b10.95 35.47\u00b10.40 31.49\u00b11.33 38.22\u00b10.71 22.21\u00b12.21 22.32\u00b11.73 29.60\u00b10.90 26.76\u00b11.12 30.61\u00b10.98 35.08\u00b10.69 23.78\u00b11.08 21.40\u00b10.30 22.67\u00b11.32 25.32\u00b10.29 26.03\u00b10.30 21.34\u00b10.50 26.72\u00b10.72 16.86\u00b10.57 16.66\u00b10.62 19.56\u00b10.52 18.24\u00b10.36 19.38\u00b10.10 25.35\u00b10.50 17.06\u00b10.78 15.65\u00b10.26 16.84\u00b10.66 19.14\u00b11.33 18.67\u00b11.69 18.45\u00b11.47 58.77\u00b11.98 13.08\u00b13.34 13.64\u00b12.49 16.15\u00b11.89 14.40\u00b13.11 16.22\u00b15.95 15.12\u00b11.88 11.77\u00b13.20 12.73\u00b13.22 13.55\u00b13.16 10.77\u00b10.60 24.19\u00b110.15 10.69\u00b10.51 36.74\u00b11.24 7.21\u00b10.39 7.62\u00b11.90 8.11\u00b10.68 8.17\u00b10.78 7.85\u00b10.74 9.56\u00b11.35 7.55\u00b11.86 8.52\u00b10.53 7.84\u00b11.72 25.46\u00b11.36 26.83\u00b11.46 26.16\u00b10.96 24.85\u00b14.85 5.05\u00b11.05 7.02\u00b10.79 13.44\u00b13.53 5.17\u00b10.57 11.19\u00b10.90 5.75\u00b10.43 7.66\u00b10.60 6.50\u00b10.78 5.75\u00b10.62 15.70\u00b12.12 15.85\u00b11.66 10.09\u00b10.94 17.28\u00b12.67 5.07\u00b10.56 4.75\u00b11.10 5.91\u00b12.02 5.58\u00b10.81 6.96\u00b10.87 4.81\u00b11.05 5.27\u00b10.89 5.78\u00b10.51 5.90\u00b11.42 56.92\u00b14.54 62.06\u00b13.43 57.49\u00b15.47 55.53\u00b11.51 58.77\u00b14.43 58.35\u00b14.87 55.37\u00b14.50 58.36\u00b13.80 56.98\u00b12.93 59.04\u00b14.90 60.88\u00b14.31 58.24\u00b13.08 57.98\u00b13.66 50.86\u00b11.79 55.12\u00b12.53 51.30\u00b11.73 49.64\u00b12.28 49.82\u00b11.18 52.40\u00b12.08 51.24\u00b11.56 51.89\u00b13.21 51.97\u00b11.55 52.92\u00b11.04 52.93\u00b11.56 52.19\u00b11.35 51.73\u00b12.84 40.35\u00b10.30 38.55\u00b10.21 38.82\u00b10.04 37.25\u00b10.08 30.20\u00b10.03 31.75\u00b10.13 31.90\u00b10.06 32.48\u00b10.15 32.38\u00b10.16 38.65\u00b10.18 33.85\u00b10.08 32.85\u00b10.31 31.80\u00b10.22 Ours 22.06\u00b11.06 17.40\u00b11.04 11.09\u00b10.71 8.10\u00b11.02 5.86\u00b11.06 5.74\u00b11.13 57.99\u00b12.16 50.95\u00b12.03 33.08\u00b10.26 Table 12: Error rate comparison of different number of labels on IMDB, AG News, Amazon Review, Yahoo Answers, and Yelp Review for semi-supervised learning. We use USB (Wang et al., 2022d) text classification task results. Best results are indicated in bold. Our results are averaged over 3 independent runs. Datasets IMDB AG News Amazon Review Yahoo Answers Yelp Review # Labels 20 100 40 200 250 1000 500 2000 250 1000 Pseudo-Label (Lee et al., 2013) Mean-Teacher (Tarvainen & Valpola, 2017) VAT (Miyato et al., 2018) MixMatch (Berthelot et al., 2019b) AdaMatch (Berthelot et al., 2021) FixMatch (Sohn et al., 2020) FlexMatch (Zhang et al., 2021b) Dash (Xu et al., 2021) CoMatch (Li et al., 2021a) SimMatch (Zheng et al., 2022) FreeMatch (Wang et al., 2023) SoftMatch (Chen et al., 2023) 45.45\u00b14.43 20.06\u00b12.51 25.93\u00b12.58 26.12\u00b16.13 8.09\u00b10.99 7.72\u00b10.33 7.82\u00b10.77 8.34\u00b10.86 7.44\u00b10.30 7.93\u00b10.55 8.94\u00b10.21 7.76\u00b10.58 19.67\u00b11.01 13.97\u00b11.49 11.61\u00b11.79 15.47\u00b10.65 7.11\u00b10.20 7.33\u00b10.13 7.41\u00b10.38 7.55\u00b10.35 7.72\u00b11.14 7.08\u00b10.33 7.95\u00b10.45 7.97\u00b10.72 19.49\u00b13.07 15.17\u00b11.21 14.70\u00b11.19 13.50\u00b11.51 11.73\u00b10.17 30.17\u00b11.87 16.38\u00b13.94 17.67\u00b13.19 11.95\u00b10.76 14.26\u00b11.51 12.98\u00b10.58 11.90\u00b10.27 14.69\u00b11.88 13.93\u00b10.65 11.71\u00b10.84 11.75\u00b10.60 11.22\u00b10.95 11.71\u00b11.95 12.08\u00b10.73 13.76\u00b11.67 10.75\u00b10.35 12.45\u00b11.37 11.73\u00b10.63 11.72\u00b11.58 53.45\u00b11.9 52.14\u00b10.52 49.83\u00b10.46 59.54\u00b10.67 46.72\u00b10.72 47.61\u00b10.83 45.73\u00b11.60 47.10\u00b10.74 48.76\u00b10.90 45.91\u00b10.95 46.41\u00b10.60 45.29\u00b10.95 47.00\u00b10.79 47.66\u00b10.84 46.54\u00b10.31 61.69\u00b13.32 42.27\u00b10.25 43.05\u00b10.54 42.25\u00b10.33 43.09\u00b10.60 43.36\u00b10.21 42.21\u00b10.30 42.64\u00b10.06 42.21\u00b10.20 37.70\u00b10.65 37.09\u00b10.18 34.87\u00b10.41 35.75\u00b10.71 32.75\u00b10.35 33.03\u00b10.49 35.61\u00b11.08 35.26\u00b10.33 33.48\u00b10.51 33.06\u00b10.20 32.77\u00b10.26 33.07\u00b10.31 32.72\u00b10.31 33.43\u00b10.28 31.50\u00b10.35 33.62\u00b10.14 30.44\u00b10.31 30.51\u00b10.53 31.13\u00b10.18 31.19\u00b10.29 30.25\u00b10.35 30.16\u00b10.21 30.32\u00b10.18 30.44\u00b10.62 54.51\u00b10.82 50.60\u00b10.62 52.97\u00b11.41 53.98\u00b10.59 45.40\u00b10.96 46.52\u00b10.94 43.35\u00b10.69 45.24\u00b12.02 45.40\u00b11.12 46.12\u00b10.48 47.95\u00b11.45 44.09\u00b10.50 47.33\u00b10.20 47.21\u00b10.31 45.30\u00b10.32 51.70\u00b10.68 40.16\u00b10.49 40.65\u00b10.46 40.51\u00b10.34 40.14\u00b10.79 40.27\u00b10.51 40.26\u00b10.62 40.37\u00b11.00 39.76\u00b10.13 Ours 7.32\u00b10.12 7.64\u00b10.67 14.77\u00b11.59 12.21\u00b10.82 43.96\u00b10.32 42.32\u00b10.02 33.80\u00b10.25 30.86\u00b10.17 44.82\u00b10.17 39.67\u00b10.71 D.4 NOISY LABEL LEARNING D.4.1 SETUP We conduct experiments of noisy label learning following SOP (Liu et al., 2022). We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \u03b7 into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs"}