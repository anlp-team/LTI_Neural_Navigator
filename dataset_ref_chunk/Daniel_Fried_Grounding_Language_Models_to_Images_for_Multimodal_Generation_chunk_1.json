{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Daniel_Fried_Grounding_Language_Models_to_Images_for_Multimodal_Generation_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of the proposed method in the text?,        answer: The purpose of the proposed method is to ground pre-trained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data and generate text interleaved with retrieved images.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What abilities of language models learned from large scale text-only pretraining are leveraged in the proposed method?,        answer: The proposed method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " How does the proposed method enable cross-modality interactions?,        answer: The proposed method keeps the language model frozen and fine-tunes input and output linear layers to enable cross-modality interactions.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What types of birds can eat the seeds attracted by the feeder mentioned in the text?,        answer: The types of birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird, and the goldfinch.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What bird is depicted in the feeder image in the text?,        answer: The bird depicted in the feeder image is a male song sparrow (Passer domesticus) eating seeds.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What are some of the new vision-and-language capabilities that the proposed approach equips text-only models with?,        answer: The proposed approach equips text-only models with new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What is the main contribution of the proposed method according to the text?,        answer: The main contribution of the proposed method is Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " How long did it take to train the model mentioned in the text on a single GPU?,        answer: The model mentioned in the text is trained in less than 24 hours on a single GPU.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " What are some of the demonstrated capabilities of the proposed approach mentioned in the text?,        answer: Some demonstrated capabilities of the proposed approach include contextual image retrieval, strong zero-shot performance on visual dialogue, and improved sensitivity to discourse context for image retrieval.    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}, {"question": " Where can interested individuals access the code and pretrained models related to the proposed method?,        answer: Interested individuals can access the code and pretrained models related to the proposed method at https://github.com/kohjingyu/fromage    ", "ref_chunk": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}], "doc_text": "3 2 0 2 n u J 3 1 ] L C . s c [ 4 v 3 2 8 3 1 . 1 0 3 2 : v i X r a Grounding Language Models to Images for Multimodal Inputs and Outputs Jing Yu Koh 1 Ruslan Salakhutdinov 1 Daniel Fried 1 Abstract We propose an efficient method to ground pre- trained text-only language models to the visual domain, enabling them to process arbitrarily in- terleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and fine- tune input and output linear layers to enable cross- modality interactions. This allows our model to process arbitrarily interleaved image-and-text in- puts, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contex- tual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained lan- guage models in visually grounded settings. This feeder can attract many types of birds. The birds that can eat the seeds include the song sparrow, the bluebird, the titmouse, the black-headed hummingbird and the gold\ufb01nch. What is this bird? This is a male song sparrow (Passer domesticus) in a feeder. The bird is eating seeds. Here are pictures of some of the birds that can eat the seeds that this feeder attracts. What other birds can this feeder attract? Show me pictures of some of these birds. 1. Introduction Trained at massive scale on large text corpora, large lan- guage models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answer- ing complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models ex- hibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. the capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). In this paper, we show that we are able to efficiently leverage 1Carnegie Mellon University. Correspondence to: Jing Yu Koh <jingyuk@cs.cmu.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). We propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multi- modal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multi- task objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval 1 Grounding Language Models to Images for Multimodal Inputs and Outputs (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximum- likelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embed- dings for its paired image. Most of the model is kept frozen, and we only update the weights of the linear layers and the [RET] token embedding during training. Hence, our proposed method is very computationally and memory effi- cient.1 Once trained, our model exhibits several capabilities. It retains the original abilities of the text-only LLM to gen- erate text, but also attains new multimodal dialogue and reasoning abilities. Our proposed method is model agnos- tic, and can be applied to ground larger or stronger LLMs released in the future. Our main contributions include: Proposing Frozen Retrieval Over Multimodal Data for Autoregressive Generation (FROMAGe), a model efficiently trained by visually grounding LLMs with image captioning and contrastive learning. FROMAGe learns strong few-shot multimodal abilities from image- caption pairs alone, while other models require web- scale interleaved image-text data (Alayrac et al., 2022; Aghajanyan et al., 2022). Demonstrating that autoregressive LLMs can perform text-to-image retrieval with greater sensitivity to in- put text. Our approach is more accurate on long and complex free-form text compared to existing models. Showing that the existing capabilities of pretrained text-only LLMs, such as in-context learning, input sen- sitivity, and dialogue generation, can be leveraged for visually grounded tasks. We demonstrate: (1) con- textual image retrieval given sequences of interleaved images and text, (2) strong zero-shot performance on visual dialogue, and (3) improved sensitivity to dis- course context for image retrieval. Our findings pave the way towards models capable of con- ditioning on and generating long, coherent, multimodal sequences, and provide further insights into the abilities of pretrained text-only LLMs on visually grounded tasks. Our code and pretrained models are made publicly available2 to encourage future work and exploration. 1Our model is trained in less than 24 hours on a single GPU. 2https://github.com/kohjingyu/fromage 2. Related Work Large language models. Large language models have recently received significant attention in the machine learn- ing and natural language processing communities, in part due to their intriguing abilities to perform in-context learn- ing (Brown et al., 2020; Chan et al., 2022) and long-form generation (Dai et al., 2019; Tan et al., 2021; Yang et al., 2022). Most state-of-the-art models are variants of the"}