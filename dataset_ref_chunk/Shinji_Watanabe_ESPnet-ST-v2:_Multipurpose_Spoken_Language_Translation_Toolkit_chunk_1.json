{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_ESPnet-ST-v2:_Multipurpose_Spoken_Language_Translation_Toolkit_chunk_1.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the objective of the ESPnet-ST-v2 project?", "answer": " The objective of the project is to contribute to the diversity of the open-source spoken language translation ecosystem.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What are the three main spoken language translation tasks that ESPnet-ST-v2 focuses on?", "answer": " ESPnet-ST-v2 focuses on offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST) translation.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What is the difference between speech-to-text (ST) and speech-to-speech (S2ST) translation tasks?", "answer": " Speech-to-text (ST) translates speech input into textual output, while speech-to-speech (S2ST) translates speech input into target speech.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " How does simultaneous speech-to-text (SST) differ from speech-to-text (ST) translation?", "answer": " Simultaneous speech-to-text (SST) translation requires systems to produce textual outputs incrementally while ingesting speech input.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What kind of models does ESPnet-ST-v2 offer?", "answer": " ESPnet-ST-v2 offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What are some common approaches to speech-to-text (ST) translation?", "answer": " Common approaches to ST include coupling statistical automatic speech recognition (ASR) and text-to-text translation (MT) as well as end-to-end differentiable (E2E) approaches.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What distinguishes ESPnet-ST-v2 from other open source spoken language translation toolkits?", "answer": " ESPnet-ST-v2 offers a wide variety of approaches for offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST) translation tasks.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " How does ESPnet-ST-v2 benchmark its performance?", "answer": " ESPnet-ST-v2 benchmarks its performance against top IWSLT shared task systems and other prior works to evaluate the performance of its models.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " What are some features of the modular design of ESPnet-ST-v2?", "answer": " The modular design allows for easy interchangeability of major neural network modules such as frontends, encoders, decoders, search, and loss functions.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}, {"question": " How does the software architecture of ESPnet-ST-v2 in Figure 1 improve over the architecture of ESPnet-ST-v1?", "answer": " The modular design of ESPnet-ST-v2 improves over ESPnet-ST-v1 by making it easier to extend and modify the toolkit, and allowing modules to be readily used for adjacent tasks.", "ref_chunk": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}], "doc_text": "3 2 0 2 l u J 6 ] D S . s c [ 3 v 6 9 5 4 0 . 4 0 3 2 : v i X r a ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Brian Yan*1 Yifan Peng1 Dan Berrebbi1 Moto Hira2 Jiatong Shi*1 Siddharth Dalmia1 Tomoki Hayashi4 Soumi Maiti1 Yun Tang2 Hirofumi Inaguma2 Peter Pol\u00e1k3 Xiaohui Zhang2 Patrick Fernandes1 Zhaoheng Ni2 Juan Pino2 Shinji Watanabe1,5 1Carnegie Mellon University 2Meta AI 3Charles University 4Nagoya University 5Johns Hopkins University {byan, jiatongs}@cs.cmu.edu Abstract ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broad- ening interests of the spoken language trans- lation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simul- taneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) \u2013 each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language trans- lation toolkits. This toolkit offers state-of- the-art architectures such as transducers, hy- brid CTC/attention, multi-decoders with search- able intermediates, time-synchronous block- wise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking be- hind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.1 1 Introduction The objective of this project is to contribute to the diversity of the open-source spoken language translation ecosystem. Toward this, we launched this ESPnet-ST-v2 update in collaboration with re- searchers working on Fairseq (Ott et al., 2019) and TorchAudio (Yang et al., 2021b). This project fo- cuses on: offline speech-to-text (ST), simultaneous speech-to-text (SST), and offline speech-to-speech (S2ST). These three spoken language translation tasks have drawn significant interest, as evidenced by rising IWSLT2 shared task participation. The ST task can be considered a base form of spoken language translation. Early approaches to ST stemmed from coupling statistical automatic speech recognition (ASR) (Huang et al., 2014) and text-to-text translation (MT) (Al-Onaizan et al., 1999), and this type of cascaded approach is still 1Please see our documentation for ST/SST and S2ST to get started. Example models and tutorials are provided. 2International Workshop on Spoken Language Translation common in the neural network era (Bentivogli et al., 2021; Zhang et al., 2022). End-to-end differen- tiable (E2E) approaches have recently emerged as an alternative offering greater simplicity and su- perior performance in some cases (Inaguma et al., 2021b); however, E2E approaches still benefit from techniques originating from ASR and MT (Gaido et al., 2021; Inaguma et al., 2021a). SST modifies ST by imposing an additional streaming requirement, where systems are expected to produce textual outputs while incrementally in- gesting speech input. Both the aforementioned cas- caded and end-to-end approaches to ST have been adapted for SST (Ma et al., 2020b; Iranzo-S\u00e1nchez et al., 2021; Chen et al., 2021), although the more direct nature of the latter may be advantageous for latency-sensitive applications. On the other hand, S2ST extends ST by producing target speech rather than target text. Again, cascaded approaches of ST followed by text-to-speech (TTS) came first (Waibel et al., 1991; Black et al., 2002) and E2E approaches followed (Jia et al., 2019; Lee et al., 2022a; Jia et al., 2022a; Inaguma et al., 2022), with the latter offering smaller footprints and greater potential to retain source speech characteristics. Given the recent swell in E2E ST, SST, and S2ST research, we have revamped ESPnet-ST (Inaguma et al., 2020) which previously only supported E2E ST. In particular, this work: Implements ST, SST, and S2ST using common Pytorch-based modules, including encoders, de- coders, loss functions, search algorithms, and self-supervised representations. Builds a variety of example E2E models: attentional encoder-decoders, CTC/attention, multi-decoders with searchable intermediates, and transducers for ST. Blockwise attentional encoder-decoders, time-synchronous blockwise CTC/attention and blockwise transducers for SST. Spectral models (i.e. Translatotron) and discrete unit based models for S2ST. Benchmarks the ST, SST, and S2ST performance of ESPnet-ST-v2 against top IWSLT shared task systems and other prior works. With this major update, ESPnet-ST-v2 keeps pace with the interests of the community and offers a va- riety of unique features, making it a valuable com- plement to Fairseq (Wang et al., 2020), NeurST (Zhao et al., 2021), and other spoken language translation toolkits. 2 Related Works ESPnet-ST-v2 follows a long line of open-source speech processing toolkits which can support spo- ken language translation (Zenkel et al., 2018; Shen et al., 2019; Kuchaiev et al., 2019; Hayashi et al., 2020; Wang et al., 2020; Zhao et al., 2021). In Table 1 we compare ESPnet-ST-v2 to Fairseq (Wang et al., 2020) and NeurST (Zhao et al., 2021), two toolkits which also cover multiple types of spo- ken language translation. Fairseq and NeurST offer cascaded and E2E approaches to ST and SST (some of which are not offered by ESPnet-ST-v2). Mean- while, ESPnet-ST-v2 focuses on E2E approaches and offers multiple unique core architectures not covered by the other toolkits. For S2ST, Fairseq and ESPnet-ST-v2 both offer a range of approaches. All told, ESPnet-ST-v2 offers the greatest variety across ST, SST, and S2ST \u2013 however, we view these toolkits as complementary. The following section elaborates on the unique features of ESPnet-ST-v2. 3 ESPnet-ST-v2 In this section, we first describe the overall design and then introduce a few key features. 3.1 Modular Design Figure 1 illustrates the software architecture of ESPnet-ST-v2. This modular design is an improve- ment over the ESPnet-ST-v1 where monolithic model and task definitions made it more difficult to extend and modify the toolkit. We also designed ESPnet-ST-v2 such that modules developed for ad- jacent tasks (e.g. ASR, TTS, MT) can also be readily used for spoken language translation. In ESPnet-ST-v2 major neural network mod- ules, such as frontends, encoders, decoders, search, and loss functions, inherit from common abstract classes making them easy to interchange. These modules, which are detailed further in the next FEATURES E S P N E T-S T-V 2 E S P N E T-S T-V 1 F AIR S E Q-S2T N E U R S T Offline ST End-to-End Architecture(s) Attentional Enc-Dec CTC/Attention Transducer Hierarchical Encoders Multi-Decoder Cascaded"}