{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Shinji_Watanabe_End-to-End_Speech_Recognition:_A_Survey_chunk_17.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is a ConvLM and how does it differ from FNN-LM?", "answer": " ConvLM is a model with stacking blocks and residual connections that are more parameter efficient and able to utilize longer histories compared to FNN-LM.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " How are Transformer LMs different from RNN-LMs in considering history?", "answer": " Transformer LMs can take all tokens in the history into account through self-attention mechanism without summarizing them, allowing for better performance than RNN-LMs.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " What is one advantage of using Transformer-XL?", "answer": " Transformer-XL reuses already computed activations to incorporate information on farther previous tokens, allowing for utilization of longer contextual information.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " What is the purpose of LM fusion in E2E ASR?", "answer": " The purpose of LM fusion is to improve the recognition accuracy of E2E ASR by leveraging benefits of external LMs.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " What is the difference between shallow fusion and deep fusion approaches?", "answer": " Shallow fusion combines E2E and LM scores at inference time, while deep fusion fine-tunes all network parameters jointly to improve recognition accuracy by better collaboration between models.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " How does cold fusion differ from deep fusion in LM combination?", "answer": " Cold fusion combines a pretrained LM with E2E model while freezing LM parameters, reducing language bias and making E2E model more robust to domain mismatches.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " What is the purpose of internal LM estimation in reducing language bias?", "answer": " Internal LM estimation helps reduce language bias in training data by estimating language priors and subtracting LM score from ASR score, making the model more domain independent.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " How does the density ratio method contribute to the training process?", "answer": " The density ratio method trains by approximating the acoustic probability density, making the E2E model score more domain independent and achieving higher recognition accuracy with external LM.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " What is the advantage of using Fusion Approaches in E2E ASR?", "answer": " Fusion Approaches help reduce the mismatch between models in different data sets, leading to improved recognition accuracy by combining the benefits of external LMs.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}, {"question": " How does gating blocks in ConvLM help in improving ASR performance?", "answer": " Gating blocks in ConvLM are stacked multiple times with residual connections, allowing for efficient parameter usage and utilization of longer histories compared to other models.", "ref_chunk": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}], "doc_text": "and gating blocks are typically stacked multiple times with residual connections. In [293], a ConvLM with 14 blocks has been applied for E2E ASR. Similar to FNN-LM, ConvLM allow us to use only a fixed history size, but they are more parameter efficient and easier to utilize longer histories than the FNN-LM by stacking the layers. Thus, they achieve competitive performance to that of RNN-LMs [292], even with the finite history consisting of short tokens such as characters [294]. Moreover, they are highly parallelizable and thus suitable for training the model with a large training data set. 5) Transformer LM: Transformer architecture [44] has been applied to LMs [295] and used for ASR [102], [296], where the LMs are designed as a Transformer decoder without any inputs from other modules such as encoders. The hidden vector is computed as: i) + h\u2032 hi = FFN(h\u2032 i h\u2032 i = MHA(ei, e1:i, e1:i) + ei where FFN(\u00b7) and MHA(\u00b7, \u00b7, \u00b7) denote a feed forward network and a multi-head attention module, respectively. The multi- head attention and feed-forward blocks are typically stacked multiple times, e.g., 6 times [102], to obtain the final hidden vector. The advantage of Transformer LMs is that they can take all tokens in the history into account through the self- attention mechanism without summarizing them into a fixed- size memory like RNN-LMs. Thus, the long history can be fully considered with attention to predict token, achieving better performance than RNN-LMs. However, the computational complexity increases quadratically as the length of the sequence. Therefore, the history length is typically limited to a fixed size or within every single sentence. To overcome this limitation, Transformer-XL [297] reuses already computed activations, which includes information on farther previous tokens, and the model is trained with a truncated back-propagation through time (BPTT) algorithm [298]. Com- pressive Transformer [299] extends this approach to utilize even longer contextual information by incorporating a com- pression step to keep older, but important, information in a fixed-size memory network. the next B. Fusion Approaches There are several ways to incorporate an external LM into E2E ASR, called LM fusion. Their purpose is to improve the recognition accuracy of E2E ASR by leveraging the benefits of the external LM described in the first part of this section. However, there can be a mismatch in the prediction between the E2E model and the LM when trained on different data sets, and therefore the LM may not collaborate well with the E2E model. Researchers have investigated various LM fusion approaches to reduce the mismatch between models in different situations. 1) Shallow Fusion: Shallow fusion is the most popular approach to combine the pretrained E2E model and LM in the inference time. As we described in Sec. VI-F, shallow fusion simply combines the E2E and LM scores by a log- linear combination as (9) where \u03b3 is a scaling factor for the LM [255][256][257]. The advantage of this approach is that it is easy and effective when there are no major mismatches between the source and target domains. 2) Deep Fusion: Deep fusion [300] is an approach to combine an LM with an E2E model using a joint network. Given a pretrained E2E model and an LM, all the network parameters are fine-tuned jointly so that the models collaborate better to improve the recognition accuracy, where the joint network is used to combine the E2E and LM states through a gating mechanism that controls the contribution of the LM according to the current state. 3) Cold fusion: Cold fusion [301] is another approach to combine a pretrained LM like deep fusion, but the E2E model is learned while freezing the LM parameters. Since the E2E model is aware of the LM throughout training, it learns to use the LM to reduce language specific information and capture This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 17 This article has been accepted for publication in IEEE/ACM Transactions on Audio, Speech and Language Processing. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2023.3328283 18 only the relevant information to map the source to the target sequence. This mechanism reduces the role of LM in the E2E model and alleviates the language bias of the training data. Accordingly, the E2E model becomes more robust to domain mismatches between the training data and the target domain. Unlike deep fusion, cold fusion makes it possible to combine the E2E model with a pretrained LM for the target domain, improving the recognition accuracy. Component fusion [302] extends cold fusion to use a pretrained LM with transcriptions of the training data for the E2E model, more focusing on reducing the bias of the training data. 4) Internal LM Estimation: There is another approach to reduce language bias in training data through shallow fusion. The language bias is a problem when a big domain mismatch exists between the source domain (training data) and the target domain (test data) because the E2E model scores are strongly dependent on the language priors in the source domain. To remove such a bias from the score, we can explicitly estimate the LM that represents the language priors, called Internal LM, and subtract the LM score from the ASR score of Eq. (9): Score(C|X) = log P\u03c6(C|X) \u2212 \u03b3\u03c6 log P\u03c6(C) + \u03b3\u03c4 log P\u03c4 (C) where subscripts \u03c6 and \u03c4 indicate the source and target domains, respectively. \u03b3\u03c6 and \u03b3\u03c4 are their scaling factors. Sub- tracting the internal LM score corresponds to approximating acoustic probability density P\u03c6(X|C) because P\u03c6(X|C) \u221d P\u03c6(C|X)/P\u03c6(C) is satisfied for fixed X, where the ASR score can be seen as a classical hybrid ASR system. Ac- cordingly, the subtracted E2E model score plays a role of acoustic model and makes it more domain independent in terms of language, achieving a higher recognition accuracy in combination with the external LM P\u03c4 (C). The density ratio method [303] trains an"}