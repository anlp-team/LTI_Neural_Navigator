{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Jamie_Callan_KALE:_Using_a_K-Sparse_Projector_for_Lexical_Expansion_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What are the two methods mentioned for composing texts?", "answer": " The original representations only (BM25) or original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query).", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What were the hyperparameters for the BM25 method?", "answer": " The hyperparameters were kept at default values, which are \ud835\udc581 = 0.9 and \ud835\udc4f = 0.4.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " How are methods with BM25 scoring expanded with KALE terms?", "answer": " They are expanded with TF=1 for KALE terms.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What impact do KALE terms have for DeepImpact?", "answer": " KALE terms have an impact of 10 for DeepImpact.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What is the significance of indexing and storing the expanded corpus on disk?", "answer": " It allows the expanded corpus to be easily accessed and searched at query time.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " How was KALE implemented?", "answer": " KALE was implemented with the Pytorch library.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " How many epochs were the models trained for?", "answer": " The models were trained for 20 epochs.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What toolkit was used for indexing and BM25 search?", "answer": " The Pyserini toolkit was used.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What issues are addressed in the experimental evaluation section?", "answer": " The effect of different vocabulary sizes, the effect of the number of artificial terms added to the queries/documents, the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary, the distribution of the generated term posting lists, and the out-of-domain performance of the proposed method.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}, {"question": " What was the rationale behind varying the size of the KALE vocabulary?", "answer": " To assess if the vocabulary size affects the effectiveness and efficiency of the model, and to determine the optimal size for better performance.", "ref_chunk": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}], "doc_text": "texts can be composed either by the original representations only (what is referred to as BM25), or by the original terms reweighted/expanded by another method (e.g., DeepCT, or DocT5Query). The BM25 hy- perparameters were kept at default (\ud835\udc581 = 0.9, \ud835\udc4f = 0.4). Methods with BM25 scoring are expanded with TF=1 for KALE terms. For im- pact indexes (i.e., DeepImpact and EfficientSPLADE), the artificial vocabulary is also indexed with a constant impact. For DeepImpact, KALE terms have an impact of 10. Regarding EfficientSPLADE, the generated vocabulary is indexed with an impact of 30. Figure 3 illustrates this process, for TF=1, and a vocabulary size of 1,024. Once the corpus is expanded, it can be indexed and stored in disk. At query time, KALE expands the query, and searches the index. KALE was implemented with the Pytorch library. The teacher LM was kept fixed as the dual-encoder checkpoint from Sentence Transformers [31] named msmarco-distilbert-cos-v5. Models were trained during 20 epochs, with a maximum learning rate of 1e-4, and a linear learning rate scheduler. An epoch was considered as a whole pass through the entire set of 500k training queries. Indexing and BM25 search were done with the Pyserini toolkit [19], with 12 threads and a batch size of 64. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. 5 EXPERIMENTAL EVALUATION This section presents and discusses experimental results, guided by several research issues to be assessed, namely: (1) the effect of different vocabulary sizes; (2) the effect of the number of artifi- cial terms added to the queries/documents; (3) the usefulness of complementing existing learned sparse retrievers with the KALE vocabulary; (4) the distribution of the generated term posting lists; and (5) the out-of-domain performance of the proposed method. 5.1 Evaluating the Vocabulary Size KALE argues that the generated vocabulary is able to capture se- mantic concepts that existing lexical terms might not be able to clearly capture. Smaller vocabularies should imply more abstract concepts, which are perhaps not expressive enough to accurately represent a document, at least on their own. As the vocabulary size increases, KALE should have more expressive power to accurately extract the important concepts from the corpus, leading to better effectiveness. Ideally, higher vocabulary sizes should also result in a smaller average posting list size, and consequent faster search. This subsection presents experiments varying the size of the KALE vocabulary, with a fixed number of 16 artificial terms for the query, and 64 artificial terms for the passage (i.e., relatively small values in both cases). Two settings were tested, namely one where only the artificial vocabulary was indexed and searched (i.e., KALE only), and another where the lexical English vocabulary was expanded with the KALE terms, and search was performed with BM25 (i.e., BM25+KALE). The average query latency for MSMARCO queries, in milliseconds, was also measured. Table 1 presents the results from the aforementioned experi- ments, divided into four table blocks. The first block features the lexical baseline BM25, together with the Teacher LM. This shows not only the effectiveness drop when distilling teacher knowledge into KALE, but also the efficiency gain. The second block changes the vocabulary size in the KALE only scenario. Regarding accuracy, every vocabulary size either equalled or outperformed BM25 with the original English terms. Both ac- curacy and efficiency improved as the vocabulary size increased. For sufficiently large sizes, KALE was faster than BM25. Besides outperforming BM25, the KALE vocabulary performed closely to DocT5Query, despite using a smaller DistilBERT model instead of the heavier T5 backbone. Significant improvements on accuracy halted after 8,192 KALE terms, likely given that the number of query and passage terms from KALE was being kept fixed. Larger vocabularies may require more terms to accurately describe the contents of a query/document since, in that setting, KALE is trained to encode its input over a more fine-grained set of concepts. The third block provides a different view, where the artificial terms were combined with the existing English vocabulary. In this setting, every experiment significantly improved over the BM25 baseline, showing the proposed approach to be strong at comple- menting existing vocabularies. Very small vocabulary sizes (i.e. 512 and 768) were suboptimal, and presumably the concepts were ex- cessively abstract. The effectiveness peak was reached with 1,024 terms, and larger vocabulary sizes performed worse. The reason may again be that larger vocabularies require more terms to accu- rately describe a query/document, and these hyperparameters were kept fixed in these experiments. ICTIR \u201923, July 23, 2023, Taipei, Taiwan. Lu\u00eds Borges, Bruno Martins, & Jamie Callan Table 1: Experimental results when varying the size of the KALE vocabulary. The number of terms in the KALE query representation (\ud835\udc58\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc5f \ud835\udc66) was set to 16, while \ud835\udc58\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52 was set to 64. BM25+KALE denotes BM25 search with KALE terms, and the original lexical terms. QL denotes the average MSMARCO query latency, and was measured in milliseconds. The teacher model used for distillation (msmarco-distilbert-cos-v5) is also included in the first block of results, in order to assess the extent to which accuracy was decreased when generating the sparse representations. The symbol \u2020 denotes statistically significant improvement over BM25, for a paired t-test with a p-value of 0.05. MSMARCO Dev TREC DL 19 TREC DL 20 Method BM25 Teacher KALE only KALE only KALE only KALE only KALE only KALE only KALE only KALE only BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+KALE BM25+Teacher |V| - - 512 768 1024 8192 32768 65536 98304 131072 512 768 1024 8192 32768 65536 98304 131072 768 MRR@10 0.184 0.338\u2020 0.202\u2020 0.223\u2020 0.231\u2020 0.252\u2020 0.254\u2020 0.251\u2020 0.254\u2020 0.251\u2020 0.294\u2020 0.308\u2020 0.309\u2020 0.306\u2020 0.301\u2020 0.296\u2020 0.296\u2020 0.297\u2020 0.293\u2020 Recall@10 0.379 0.586\u2020 0.400\u2020 0.430\u2020 0.442\u2020 0.455\u2020 0.456\u2020 0.445\u2020 0.453\u2020 0.450\u2020 0.565\u2020 0.569\u2020 0.567\u2020 0.549\u2020 0.539\u2020 0.532\u2020 0.535\u2020 0.533\u2020 0.546\u2020 NDCG@10 0.506 0.680\u2020 0.521 0.535 0.520 0.498 0.550 0.535 0.547 0.559 0.653\u2020 0.657\u2020 0.630\u2020 0.582 0.615\u2020 0.592 0.628\u2020 0.613 0.651\u2020 Recall@10 0.129 0.143 0.103 0.100 0.100 0.095 0.105 0.100 0.097 0.106 0.151 0.149 0.133 0.130 0.126 0.122 0.132 0.112 0.153 NDCG@10 0.480 0.645\u2020 0.498 0.507"}