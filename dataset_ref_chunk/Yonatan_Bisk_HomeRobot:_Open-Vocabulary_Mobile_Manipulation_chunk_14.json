{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Yonatan_Bisk_HomeRobot:_Open-Vocabulary_Mobile_Manipulation_chunk_14.txt", "num_qa_pairs": 10, "qa_list": [{"question": " How is the map initialized at the beginning of an episode?,        answer: The map is initialized with all zeros.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " Where does the agent start at the beginning of an episode?,        answer: The agent starts at the center of the map facing east.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What is the goal of the heuristic frontier-based exploration policy?,        answer: To select the point closest to the robot within the boundary between the explored and unexplored region of the map.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What method is used to plan a path based on the long-term goal?,        answer: The Fast Marching Method.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What does the robot do if it cannot get within a certain distance of the goal objects?,        answer: It plans to a point on the frontier instead.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " How does the navigation system handle small objects that are hard to locate?,        answer: It leverages the typically larger start_receptacle goals for finding objects.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What are the changes made to the original planning policy in relation to start_receptacle goals?,        answer: 1. Plan to reach the object if object and start_receptacle co-occur. 2. Plan to reach the start_receptacle if the object is not found.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What is a challenge faced in tuning the navigation system?,        answer: The planner is a discrete planner that cannot take orientation into account and relies on a 5cm discretization of the world.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What type of planners are suggested as potential solutions for navigation challenges?,        answer: Sampling-based motion planners like RRT-Connect or optimization-based planners specifically designed for open-vocabulary navigation planning.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}, {"question": " What information does the navigation policy rely on?,        answer: Accurate pose information from Hector SLAM.    ", "ref_chunk": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}], "doc_text": "physical world. Map channels K = C + 4 where C is the number of semantic object categories, and the remaining 4 channels represent the obstacles, the explored area, and the agent\u2019s current and past locations. An entry in the map is one if the cell contains an object of a particular semantic category, an obstacle, or is explored, and zero otherwise. The map is initialized with all zeros at the beginning of an episode and the agent starts at the center of the map facing east. Frontier Exploration Policy. We explore the environment with a heuristic frontier-based exploration policy [106]. This heuristic selects as the goal the point closest to the robot in geodesic distance within the boundary between the explored and unexplored region of the map. Navigation Planner. Given a long-term goal output by the frontier exploration policy, we use the Fast Marching Method [107] as in [105] to plan a path and the first low-level action along this path deterministically. Although the semantic exploration policy acts at a coarse time scale, the planner acts at a fine time scale: every step we update the map and replan the path to the long-term goal. The robot attempts to plan to goals if they have been seen; if it cannot get within a certain distance of the goal objects, then it will instead plan to a point on the frontier. Navigating to objects on start_receptacle. Since small objects (e.g. action_figure, apple) can be hard to locate from a distance, we leverage the typically larger start_receptacle goals for finding objects. We make the following changes to the original planning policy [108]: 1. If object and start_receptacle co-occur in at least one cell of the semantic map, plan to reach the object 2. If the object is not found but start_receptacle appears in the semantic map after exclud- ing the regions within 1m of the agent\u2019s past locations, plan to reach the start_receptacle 27 Figure 17: Real-world examples (also see Fig 2). Our system is able to find held-out objects in an unseen environment and navigate to receptacles in order to place them, all with no information about the world at all, other than the relevant classes. However, we see this performance is highly dependent on perception performance for now; many real-world examples also fail due to near-miss collisions. 3. Otherwise, plan to reach the closest frontier In step 2, we exclude the regions that the agent has been close to, to prevent it from re-visiting already visited instances of start_receptacle. E.6 Navigation Limitations We implemented a navigation system that was previously used in extensive real-world experiments [2], but needed to tune it extensively for it to get close enough to objects to grasp and manipulate them. The original version by Gervet et al. [2] was focused on finding very large objects from a limited set of only six classes. Ours supports many more, but as a result, tuning it to both be able to grasp objects and avoid collisions in all cases is difficult. This is partly because the planner is a discrete planner based on the Fast Marching Method [107], which cannot take orientation into account and relies on a 5cm discretization of the world. ampling- based motion planners like RRT-Connect [104], or like that used in the Task and Motion Planning literature [100, 8], may offer better solutions. Alternately, we could explore optimization-based planners specifically designed for open-vocabulary navigation planning, as has recently been pro- posed [12]. Our navigation policy relies on accurate pose information from Hector SLAM [103], and unfortunately does not handle dynamic obstacles. It also models the robot\u2019s location as a cylinder; the Stretch\u2019s center of rotation is slightly offset from the center of this cylinder, which is not currently accounted for. Again, sampling-based planners might be better here. F Reinforcement Learning Baseline We train four different RL policies: FindObject, FindReceptacle, GazeAtObject, and PlaceObject. 28 F.1 Action Space F.1.1 Navigation Skills FindObject and FindReceptacle are, collectively, navigation skills. For these two skills, we use the discrete action space, as mentioned in Sec. D.6. In our experiments, we found the discrete action space was better at exploration and easier to train. F.1.2 Manipulation Skills For our manipulation skills, we using a continuous action space to give the skills fine grained control. In the real world, low-level controllers have limits on the distance the robot can move in any particular step. Thus, in simulation, we limit our base action space by only allowing forward motions between 10-25 cm, or turning by 5-30 degrees in a single step. The head tilt, pan and gripper\u2019s yaw, roll and pitch can be changed by at most 0.02-0.1 radians in a single step. The arm\u2019s extension and lift can be changed by at most 2-10cm in a single step. We learn by teleporting the base and arm to the target locations. F.2 Observation Space Policies have access to depth from the robot head camera, and semantic segmentation, as well as the robot\u2019s pose relative to the starting pose (from SLAM in the real world), camera pose, and the robot\u2019s joint states, including the gripper. RGB image is available to the agent but not used during training. F.3 Training Setup All skills are trained using a slack reward of -0.005 per step, incentivizing completion of task using minimum number of steps. For faster training, we learn our policies using images with a reduced resolution of 160x120 (compared to Stretch\u2019s original resolution of 640x480). F.3.1 Navigation Skills We train FindObject and FindReceptacle policies for the agent to reach a candidate object or a candidate target receptacle respectively. The training procedure is the same for both skills. We pass in the CLIP [14] embedding corresponding with the goal object, as well as segmentation masks corresponding with the detected target objects. The agent is spawned arbitrarily, but at least 3 meters from the target, and must move until within 0.1 meters of a goal \u201cviewpoint,\u201d where the object is visible. Input observations: Robot head camera"}