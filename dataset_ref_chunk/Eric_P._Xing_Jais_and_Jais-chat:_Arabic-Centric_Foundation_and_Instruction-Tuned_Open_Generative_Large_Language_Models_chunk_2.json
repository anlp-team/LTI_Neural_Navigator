{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_Jais_and_Jais-chat:_Arabic-Centric_Foundation_and_Instruction-Tuned_Open_Generative_Large_Language_Models_chunk_2.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the main focus of the research and development efforts on large language models (LLMs) that have been discussed in the text?", "answer": " The main focus has been on English.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " Why is Arabic considered an underrepresented language in the LLM space according to the text?", "answer": " Arabic is considered underrepresented because most LLMs are primarily trained and instruction-tuned for English and are not able to extend their capabilities to languages other than English.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What is the name of the powerful Arabic-centric decoder-only LLM with 13B parameters mentioned in the text?", "answer": " Jais", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " How many Arabic tokens were pretraining data for Jais based on the text?", "answer": " 72 billion Arabic tokens were pretraining data for Jais.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " Why did the authors of the text train bilingual models instead of relying solely on Arabic pretraining data?", "answer": " To address the limited availability of high-quality Arabic data, the authors trained bilingual models by augmenting the Arabic pretraining data with abundant English pretraining data.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What percentage of the pretraining data consisted of Arabic data for the model Jais?", "answer": " Arabic data constituted 33% of the pretraining data for the model Jais.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What improvements did the authors adopt from the literature to enhance the performance of the model?", "answer": " The authors adopted ALiBi positional encodings, SwiGLU activation function, maximal update parametrization, and a custom-built tokenizer to enhance the performance of the model.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What are some of the safety measures implemented in the deployed system for the instruction-tuned model Jais-chat as mentioned in the text?", "answer": " Safety measures implemented include safety prompts, keyword-based filtering, and external classifiers.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What areas were evaluated to compare Jais and Jais-chat with other models in the text?", "answer": " They were evaluated across Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}, {"question": " What is the purpose of releasing the models Jais2 and Jais-chat3 to the public according to the text?", "answer": " By making the models publicly available, the authors hope to enable further research and development, stimulating innovation and practical applications for the Arabic and global communities.", "ref_chunk": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}], "doc_text": ". . . 26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 10 Acknowledgments A Detailed Zero-Shot Evaluation Results B Jais-chat Response Examples C Model Cards 2 3 4 7 9 24 25 26 38 40 49 1 Introduction Large language models (LLMs) have revolutionized the field of natural language processing (NLP), demon- strating remarkable capabilities in generating high-quality texts and resulting in widespread adoption across a diverse array of practical NLP applications and domains. Yet, the main focus of research and development ef- forts so far has been on English. While recent LLMs such as Falcon [AAA+23], PALM [CND+22] and LLaMA [TLI+23, TMS+23], among others, are able to process data in multiple languages, they were nevertheless pri- marily trained and instruction-tuned for English. As a result, they are not able to extend their understanding and generation capabilities to languages other than English. In this work, we aim to bridge this gap. We focus on Arabic, one of the world\u2019s most spoken languages with over 400M speakers, which has been noticeably un- derrepresented in the LLM space so far. In particular, we develop Jais, a powerful Arabic-centric decoder-only LLM with 13B parameters, based on the GPT-3 generative pretraining architecture [BMR+20]. The primary challenge in developing an Arabic LLM is the limited availability of high-quality Arabic data. As compared to English, where corpora of size up to two trillion tokens are readily available [TMS+23], Arabic corpora are significantly smaller in size. As part of this work, we have collected the largest Arabic corpora to date, consisting of 72 billion tokens. However, this dataset is still not sufficiently large for the purposes of training an Arabic LLM capable of demonstrating emergent capabilities [Ope23]. To address this, we train bilingual models, by augmenting the limited Arabic pretraining data with abundant English pretraining data. We pretrain Jais on 395 billion tokens, including 72 billion Arabic tokens (which we repeat 1.6 times, to obtain an effective total of 116 billion Arabic tokens), 232 billion English tokens, and the remainder being code in various programming languages. As part of our effort, we have designed and developed a specialized Arabic text processing pipeline that includes thorough data filtering and cleaning to produce high-quality Arabic data. Unlike previous massively multilingual LLMs such as BLOOM [SFA+23] or mT0 [MWS+23], which con- tain more than 50 languages, we do not include languages aside from Arabic and English in any significant percentage. Neither do we relegate Arabic to a minority in the pretraining dataset. Instead, Arabic data con- stitutes 33% of our pretraining. Our choice of mixing two languages attains the best of both worlds; the LLM is highly fluent in Arabic, with linguistic capability as well as cultural awareness and sensitivity. At the same time, it is on par with recent English LLMs in terms of reasoning capacity and world knowledge, capabilities we observe to have transferred from English to Arabic and vice-versa. Building upon the standard transformer architecture [VUWS22] in the form of its GPT-3 variant, we adopt a number of improvements from the literature including (i) ALiBi [PSL22] positional encodings, which enable the model to extrapolate to longer contexts at inference, (ii) SwiGLU activation function [Sha20] to improve the performance, (iii) maximal update parametrization to perform hyperparameter optimization based on exper- iments with smaller models [YHB+21], and (iv) a custom-built tokenizer that weighs both languages equally. We further develop an instruction-tuned version of our model, Jais-chat, which uses over 3.6 million Arabic and 6 million English instruction-response pairs. Considering the inherent safety concerns of LLMs, we further fine-tune it with safety-oriented instructions. In our deployed system which provides an interactive interface to the instruction-tuned model 1, we add extra guardrails in the form of safety prompts, keyword-based filtering, and external classifiers. An example conversation with Jais-chat on this interface is shown in Figure 1. We evaluate Jais and Jais-chat across a wide array of Arabic and English NLP benchmarks, addressing reasoning, knowledge, misinformation, and bias. The results show that Jais is superior in Arabic compared to other models of similar size, while also being competitive in English, despite being trained on significantly less English data. We are releasing the following models: Jais2: base pretrained 13B foundation model; Jais-chat3: instruction-tuned 13B version of Jais, optimized for dialog interaction. By making our models publicly available, we hope to enable further research and development in this area, stimulating innovation and practical applications that can better serve the Arabic and the global communities. Despite our significant efforts to ensure safety, we recognize that the models are not foolproof and may not cover all cases. Therefore, we strongly urge all adopters to exercise caution and to conduct additional safety testing before deploying our models. For this purpose, we outline responsible release notes in Section 9. 1https://arabic-gpt.ai 2https://huggingface.co/inception-mbzuai/jais-13b 3https://huggingface.co/inception-mbzuai/jais-13b-chat 3 Figure 1: English\u2013Arabic multiturn dialogue using Jais-chat. 2 Pretraining Data We pretrain the LLM on hundreds of billions of words of diverse text from a variety of sources in order to develop a strong foundation in the target language(s) while at the same time establishing a broad factual knowl- edge base in the model. In settings such as clinical domains, research has shown that larger-scale LLMs exhibit improved emergent capabilities [SAT+22]. Note that LLMs such as LLaMA [TLI+23] and Falcon [AAA+23] are predominantly trained on a single language: English. While these models exhibit impressive linguistic and reasoning capabilities, their abilities do not extend so well to other languages such as Arabic, as we will demonstrate experimentally below. 4 Language Dataset Token count Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Abu El-Khair [AEK16] Aranews [GEQ12] C4 [RSR+20] ArabicNews 2020 Maktabah8 UN [ZJDP16] Arabic Wikipedia7 En2Ar Wikipedia Baai1 (ArabicWeb22-A)5 Baai2 (ArabicWeb16) [SKF+16] Baai3 (OSCAR)6 Baai4 (ArabicWeb22-B)5 Baai5 (CC100) [CKG+20] Baai7 (Arabic Tweets)5 Misc10 260,407,899 203,982,492 25,010,967,834 1,870,309,674 1,785,221,183 492,787,444 173,873,517"}