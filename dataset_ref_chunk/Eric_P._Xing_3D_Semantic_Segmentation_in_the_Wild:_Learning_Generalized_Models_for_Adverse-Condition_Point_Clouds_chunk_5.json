{"file_path": "/Users/sz904/Desktop/11711/LTI_Neural_Navigator/data/2024-02-26/chunk_paper_txt/Eric_P._Xing_3D_Semantic_Segmentation_in_the_Wild:_Learning_Generalized_Models_for_Adverse-Condition_Point_Clouds_chunk_5.txt", "num_qa_pairs": 10, "qa_list": [{"question": " What is the purpose of SemanticSTF in the experiments mentioned in the text?", "answer": " SemanticSTF is used for benchmarking different learning setups and network architectures on point cloud segmentation.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What is the goal of domain generalization in the context of point cloud segmentation?", "answer": " The goal of domain generalization is to train a model using only annotated data from a single domain and test its ability to perform well on unseen domains.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " Why is the SemanticKITTI \u2192 SemanticSTF benchmark considered challenging?", "answer": " The SemanticKITTI \u2192 SemanticSTF benchmark is challenging because it involves a shift from normal-to-adverse weather conditions and a shift from synthetic-to-real data.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What backbone model is used in the experiments for domain generalization?", "answer": " MinkowskiNet with TorchSparse library is used as the backbone model.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What are the evaluating classes used in both domain generalization benchmarks?", "answer": " All 19 evaluating classes of SemanticKITTI are used in both benchmarks.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What are the two baseline methods selected for evaluating PointDR in the experiments?", "answer": " The two baseline methods are data augmentation (Dropout, Noise perturbation, PolarMix) and 2D domain generalization methods (MMD, PCL).", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " How does the proposed PointDR compare to the baseline methods in terms of generalization?", "answer": " The proposed PointDR achieves the best generalization consistently across both benchmarks compared to the baseline methods.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What are the principal weather conditions under which the data augmentation methods work effectively?", "answer": " The data augmentation methods work effectively for data captured in rainy and snowy weather conditions.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " Which generalization method is effective for point clouds under dense fog and rain?", "answer": " The 2D generalization method MMD is effective for point clouds under dense fog and rain.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}, {"question": " What does the performance of the methods evaluated in the domain generalization tasks depend on?", "answer": " The performance variations are largely attributed to the different properties of point clouds captured under different weather conditions.", "ref_chunk": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}], "doc_text": "31.9 26.2 28.6 SynLiDAR\u2192SemanticSTF 27.1 3.0 0.6 15.8 0.1 25.2 1.8 5.6 23.9 0.3 14.6 0.6 36.3 19.9 37.9 17.9 41.8 9.5 2.3 16.9 17.2 17.2 11.9 15.0 Baseline 28.0 3.0 1.4 9.6 0.0 17.1 0.8 0.7 34.2 6.8 19.1 0.1 35.5 19.1 42.3 17.6 36.0 14.0 2.8 15.3 16.6 20.4 14.0 15.2 Dropout [39] 27.1 2.3 2.3 16.0 0.1 23.7 1.2 4.0 27.0 3.6 16.2 0.8 29.2 16.7 35.3 22.7 38.3 17.9 5.1 16.3 16.7 19.3 13.4 15.2 Perturbation 39.2 1.1 1.2 8.3 1.5 17.8 0.8 0.7 23.3 1.3 17.5 0.4 45.2 24.8 46.2 20.1 38.7 7.6 1.9 16.1 15.5 19.2 15.6 15.7 PolarMix [50] 25.5 2.3 2.1 13.2 0.7 22.1 1.4 7.5 30.8 0.4 17.6 0.2 30.9 19.7 37.6 19.3 43.5 9.9 2.6 17.3 16.3 20.0 12.7 15.1 MMD [26] 30.9 0.8 1.4 10.0 0.4 23.3 4.0 7.9 28.5 1.3 17.7 1.2 39.4 18.5 40.0 16.0 38.6 12.1 2.3 17.8 16.7 19.3 14.1 15.5 PCL [56] PointDR (Ours) 37.8 2.5 2.4 23.6 0.1 26.3 2.2 3.3 27.9 7.7 17.5 0.5 47.6 25.3 45.7 21.0 37.5 17.9 5.5 19.5 19.9 21.1 16.9 18.5 Table 2. Experiments on domain generalization with SemanticKITTI [2] or SynLiDAR [51] as source and SemanticSTF as target. ing objective of PointDR can be formulated by: LPointDR = Lce + \u03bbctLct 5. Evaluation of Semantic Segmentation SemanticSTF can be adopted for benchmarking different learning setups and network architectures on point cloud segmentation. We perform experiments over two typical learning setups including domain generalization and unsu- pervised domain adaptation. In addition, we evaluate sev- eral state-of-the-art point-cloud segmentation networks to examine their generalization capabilities. 5.1. Domain Generalization We first study domain generalizable point cloud segmen- tation. For DG, we can only access an annotated source domain during training and the trained model is expected to generalize well to unseen target domains. Leveraging SemanticSTF, we build two DG benchmarks and examine how PointDR helps learn a universal 3DSS model that can work under different weather conditions. The first benchmark is SemanticKITTI [2] \u2192 Seman- ticSTF where SemanticKITTI is a large-scale real-world 3DSS dataset collected under normal weather conditions. This benchmark serves as a solid testing ground for eval- uating domain generalization performance from normal to adverse weather conditions. The second benchmark is Syn- LiDAR [51] \u2192 SemanticSTF where SynLiDAR is a large- scale synthetic 3DSS dataset. The motivation of this bench- mark is that learning a universal 3DSS model from synthetic point clouds that can work well across adverse weather is of high research and application value considering the (2) challenges in point cloud collection and annotation. Note this benchmark is more challenging as the domain discrep- ancy comes from both normal-to-adverse weather distribu- tion shift and synthetic-to-real distribution shift. Setup. We use all 19 evaluating classes of SemanticKITTI in both domain generalization benchmarks. The category of invalid in SemanticSTF is mapped to the ignored since SemanticKITTI and SynLiDAR do not cover this cate- gory. We adopt MinkowskiNet [7] (with TorchSparse li- brary [43]) as the backbone model, which is a sparse convo- lutional network that provides state-of-the-art performance with decent efficiency. We adopt the evaluation metrics of Intersection over the Union (IoU) for each segmentation class and the mean IoU (mIoU) over all classes. All ex- periments are run over a single NVIDIA 2080Ti (11GB). More implementation details are provided in the appendix. Baseline Methods. Since domain generalizable 3DSS is far under-explored, there is little existing baseline that can be directly adopted for benchmarking. We thus select two closely related approaches as baseline to evaluate the pro- posed PointDR. The first approach is data augmentation and we select three related augmentation methods includ- ing Dropout [39] that randomly drops out points to simulate LiDAR points missing in adverse weather, Noise perturba- tion that adds random points in the 3D space to simulate noise points as introduced by particles like falling snow, and PolarMix [50] that mixes point clouds of different sources for augmentation. The second approach is to adapt 2D do- main generalization methods for 3DSS. We select two 2D domain generalization methods including the widely stud- ied MMD [26] and the recently proposed PCL [56]. Results. Table 2 shows experimental results over the valida- 6 Method Lce Lct B mIoU Baseline PointDR-CT PointDR \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 24.4 27.4 28.6 Table 3. Ablation study of PointDR over domain generalized seg- mentation task SemanticKITTI\u2192SemanticSTF. tion set of SemanticSTF. For both benchmarks, the Baseline is a source-only model that is trained by using the training data of SemanticKITTI or SynLiDAR. We can see that the Baseline achieves very low mIoU while evaluated over the validation set of SemanticSTF, indicating the large domain discrepancy between point clouds of normal and adverse weather conditions. In addition, all three data augmentation methods improve the model generalization consistently but the performance gains are limited especially for the chal- lenging benchmark SynLiDAR\u2192 SemanticSTF. The two 2D generalization methods both help SemanticKITTI \u2192 SemanticSTF clearly but show very limited improvement over SynLiDAR \u2192 SemanticSTF. The proposed PointDR achieves the best generalization consistently across both benchmarks, demonstrating its superior capability to learn perturbation-invariant point cloud representations and ef- fectiveness while handling all-weather 3DSS tasks. We also evaluate the compared domain generalization methods over each individual adverse weather condition as shown in Table 2. It can be observed that the three data augmentation methods work for data captured in rainy and snowy weather only. The 2D generalization method MMD shows clear effectiveness for point clouds under dense fog and rain while PCL works for point clouds under rainy and snowy weather instead. We conjecture that the performance variations are largely attributed to the different properties of point clouds captured under different weather conditions. For example, more points are missing in rain while object points often deform due to the covered snow (more illus- trations are provided in the appendix). Such data variations lead to different domain discrepancies across weather which further leads to different performances of the compared methods. As PointDR learns perturbation-tolerant represen-"}